<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Regression</title>

<script src="site_libs/header-attrs-2.15/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TYR0QMREEW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TYR0QMREEW');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
#rmd-source-code {
  display: none;
}
</style>


<link rel="stylesheet" href="includes/custom.css" type="text/css" />
<link rel="stylesheet" href="font/css/roboto.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Analysis Guides for the Developmental Psychopathology Lab</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="dataManagement.html">Data Management</a>
    </li>
    <li>
      <a href="developmentalScaling.html">Developmental Scaling</a>
    </li>
    <li>
      <a href="eda.html">Exploratory Data Analysis</a>
    </li>
    <li>
      <a href="factorAnalysis.html">Factor Analysis</a>
    </li>
    <li>
      <a href="figures.html">Figures</a>
    </li>
    <li>
      <a href="git.html">Git, GitLab, and GitHub</a>
    </li>
    <li>
      <a href="hlm.html">Hierarchical Linear Modeling</a>
    </li>
    <li>
      <a href="irt.html">Item Response Theory</a>
    </li>
    <li>
      <a href="markdown.html">Markdown</a>
    </li>
    <li>
      <a href="sem.html#mediation">Mediation</a>
    </li>
    <li>
      <a href="regression.html#moderation">Moderation/Interaction</a>
    </li>
    <li>
      <a href="pca.html">Principal Component Analysis</a>
    </li>
    <li>
      <a href="R.html">R</a>
    </li>
    <li>
      <a href="regression.html">Regression</a>
    </li>
    <li>
      <a href="SPSS.html">SPSS</a>
    </li>
    <li>
      <a href="sem.html">Structural Equation Modeling</a>
    </li>
  </ul>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://devpsylab.github.io/LabWiki">Lab Wiki</a>
</li>
<li>
  <a href="https://developmental-psychopathology.lab.uiowa.edu">Lab Website</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Regression</h1>

</div>


<div id="preamble" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Preamble</h1>
<div id="install-libraries" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Install Libraries</h2>
<pre class="r fold-hide"><code>#install.packages(&quot;remotes&quot;)
#remotes::install_git(&quot;https://research-git.uiowa.edu/PetersenLab/petersenlab.git&quot;)</code></pre>
</div>
<div id="load-libraries" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Load Libraries</h2>
<pre class="r fold-hide"><code>library(&quot;petersenlab&quot;)
library(&quot;MASS&quot;)
library(&quot;tidyverse&quot;)
library(&quot;psych&quot;)
library(&quot;rms&quot;)
library(&quot;robustbase&quot;)
library(&quot;brms&quot;)
library(&quot;cvTools&quot;)
library(&quot;car&quot;)
library(&quot;mgcv&quot;)
library(&quot;AER&quot;)
library(&quot;foreign&quot;)
library(&quot;olsrr&quot;)
library(&quot;quantreg&quot;)
library(&quot;mblm&quot;)
library(&quot;effects&quot;)
library(&quot;correlation&quot;)
library(&quot;interactions&quot;)
library(&quot;lavaan&quot;)
library(&quot;regtools&quot;)
library(&quot;mice&quot;)</code></pre>
</div>
</div>
<div id="import-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Import Data</h1>
<pre class="r fold-hide"><code>mydata &lt;- read.csv(&quot;https://osf.io/8syp5/download&quot;)</code></pre>
</div>
<div id="data-preparation" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Data Preparation</h1>
<pre class="r fold-hide"><code>mydata$countVariable &lt;- as.integer(mydata$bpi_antisocialT2Sum)
mydata$orderedVariable &lt;- factor(mydata$countVariable, ordered = TRUE)

mydata$female &lt;- NA
mydata$female[which(mydata$sex == &quot;male&quot;)] &lt;- 0
mydata$female[which(mydata$sex == &quot;female&quot;)] &lt;- 1</code></pre>
</div>
<div id="linear-regression" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Linear Regression</h1>
<div id="linear-regression-model" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear regression model</h2>
<pre class="r fold-show"><code>multipleRegressionModel &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>confint(multipleRegressionModel)</code></pre>
<pre><code>                            2.5 %    97.5 %
(Intercept)             1.0809881 1.3156128
bpi_antisocialT1Sum     0.4290884 0.5019688
bpi_anxiousDepressedSum 0.1035825 0.2179258</code></pre>
<div id="remove-missing-data" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Remove missing data</h3>
<pre class="r fold-show"><code>multipleRegressionModelNoMissing &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata,
                                       na.action = na.omit)

multipleRegressionModelNoMissing &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit)</code></pre>
</div>
</div>
<div id="linear-regression-model-on-correlationcovariance-matrix-for-pairwise-deletion" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Linear regression model on correlation/covariance matrix (for pairwise deletion)</h2>
<pre class="r fold-show"><code>multipleRegressionModelPairwise &lt;- setCor(
  y = &quot;bpi_antisocialT2Sum&quot;,
  x = c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;),
  data = cov(mydata[,c(&quot;bpi_antisocialT2Sum&quot;,&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;),
  n.obs = nrow(mydata))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r fold-show"><code>summary(multipleRegressionModelPairwise)</code></pre>
<pre><code>
Multiple Regression from raw data 
setCor(y = &quot;bpi_antisocialT2Sum&quot;, x = c(&quot;bpi_antisocialT1Sum&quot;, 
    &quot;bpi_anxiousDepressedSum&quot;), data = cov(mydata[, c(&quot;bpi_antisocialT2Sum&quot;, 
    &quot;bpi_antisocialT1Sum&quot;, &quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;), 
    n.obs = nrow(mydata))

Multiple Regression from matrix input 

Beta weights 
                        bpi_antisocialT2Sum
(Intercept)                           0.000
bpi_antisocialT1Sum                  -0.031
bpi_anxiousDepressedSum              -1.004

Multiple R 
bpi_antisocialT2Sum 
                  1 

Multiple R2 
bpi_antisocialT2Sum 
                  1 

Cohen&#39;s set correlation R2 
[1] 1

Squared Canonical Correlations
NULL</code></pre>
<pre class="r fold-show"><code>multipleRegressionModelPairwise[c(&quot;coefficients&quot;,&quot;se&quot;,&quot;Probability&quot;,&quot;R2&quot;,&quot;shrunkenR2&quot;)]</code></pre>
<pre><code>$coefficients
                        bpi_antisocialT2Sum
(Intercept)                       0.0000000
bpi_antisocialT1Sum              -0.0311043
bpi_anxiousDepressedSum          -1.0040177

$se
                        bpi_antisocialT2Sum
(Intercept)                             NaN
bpi_antisocialT1Sum                     NaN
bpi_anxiousDepressedSum                 NaN

$Probability
                        bpi_antisocialT2Sum
(Intercept)                             NaN
bpi_antisocialT1Sum                     NaN
bpi_anxiousDepressedSum                 NaN

$R2
bpi_antisocialT2Sum 
                  1 

$shrunkenR2
bpi_antisocialT2Sum 
                Inf </code></pre>
</div>
<div id="linear-regression-model-with-robust-covariance-matrix-rms" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Linear regression model with robust covariance matrix (rms)</h2>
<pre class="r fold-show"><code>rmsMultipleRegressionModel &lt;- robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                         data = mydata,
                                         x = TRUE,
                                         y = TRUE))

rmsMultipleRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Linear Regression Model
 
 ols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
     data = mydata, x = TRUE, y = TRUE)
 
 
                 Model Likelihood    Discrimination    
                       Ratio Test           Indexes    
 Obs    2874    LR chi2    873.09    R2       0.262    
 sigma1.9786    d.f.            2    R2 adj   0.261    
 d.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    
 
 Residuals
 
     Min      1Q  Median      3Q     Max 
 -8.3755 -1.2337 -0.2212  0.9911 12.8017 
 
 
                         Coef   S.E.   t     Pr(&gt;|t|)
 Intercept               1.1983 0.0622 19.26 &lt;0.0001 
 bpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 
 bpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 
 </code></pre>
<pre class="r fold-show"><code>confint(rmsMultipleRegressionModel)</code></pre>
<pre><code>                             2.5 %    97.5 %
Intercept               1.07631713 1.3202837
bpi_antisocialT1Sum     0.42056957 0.5104877
bpi_anxiousDepressedSum 0.09606644 0.2254418</code></pre>
</div>
<div id="robust-linear-regression-mm-type-iteratively-reweighted-least-squares-regression" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Robust linear regression (MM-type iteratively reweighted least squares regression)</h2>
<pre class="r fold-show"><code>robustLinearRegression &lt;- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude)

summary(robustLinearRegression)</code></pre>
<pre><code>
Call:
lmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)
 \--&gt; method = &quot;MM&quot;
Residuals:
     Min       1Q   Median       3Q      Max 
-8.43518 -1.06680 -0.06707  1.14090 13.05599 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              0.94401    0.05406  17.464  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.49135    0.02237  21.966  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.15766    0.03102   5.083 3.96e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Robust residual standard error: 1.628 
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.326, Adjusted R-squared:  0.3255 
Convergence in 14 IRWLS iterations

Robustness weights: 
 12 observations c(52,347,354,517,709,766,768,979,1618,2402,2403,2404)
     are outliers with |weight| = 0 ( &lt; 3.5e-05); 
 283 weights are ~= 1. The remaining 2579 ones are summarized as
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
0.0001311 0.8599000 0.9470000 0.8814000 0.9816000 0.9990000 
Algorithmic parameters: 
       tuning.chi                bb        tuning.psi        refine.tol 
        1.548e+00         5.000e-01         4.685e+00         1.000e-07 
          rel.tol         scale.tol         solve.tol       eps.outlier 
        1.000e-07         1.000e-10         1.000e-07         3.479e-05 
            eps.x warn.limit.reject warn.limit.meanrw 
        2.365e-11         5.000e-01         5.000e-01 
     nResample         max.it       best.r.s       k.fast.s          k.max 
           500             50              2              1            200 
   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n 
           200              0           1000              0           2000 
                  psi           subsampling                   cov 
           &quot;bisquare&quot;         &quot;nonsingular&quot;         &quot;.vcov.avar1&quot; 
compute.outlier.stats 
                 &quot;SM&quot; 
seed : int(0) </code></pre>
<pre class="r fold-show"><code>confint(robustLinearRegression)</code></pre>
<pre><code>                             2.5 %    97.5 %
(Intercept)             0.83801566 1.0499981
bpi_antisocialT1Sum     0.44749135 0.5352118
bpi_anxiousDepressedSum 0.09683728 0.2184779</code></pre>
</div>
<div id="least-trimmed-squares-regression-for-removing-outliers" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Least trimmed squares regression (for removing outliers)</h2>
<pre class="r fold-show"><code>ltsRegression &lt;- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                        data = mydata,
                        na.action = na.exclude)

summary(ltsRegression)</code></pre>
<pre><code>
Call:
ltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals (from reweighted LS):
    Min      1Q  Median      3Q     Max 
-3.9387 -0.9170  0.0000  0.9957  3.9305 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
Intercept                0.74396    0.04808  15.474  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.54326    0.01527  35.579  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.15249    0.02350   6.489 1.02e-10 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.539 on 2713 degrees of freedom
Multiple R-Squared: 0.4154, Adjusted R-squared: 0.4149 
F-statistic: 963.8 on 2 and 2713 DF,  p-value: &lt; 2.2e-16 </code></pre>
</div>
<div id="bayesian-linear-regression" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Bayesian linear regression</h2>
<pre class="r fold-show"><code>bayesianRegularizedRegression &lt;- brm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     chains = 4,
                                     iter = 2000,
                                     seed = 52242)</code></pre>
<pre class="r fold-show"><code>summary(bayesianRegularizedRegression)</code></pre>
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   1.20      0.06     1.09     1.31 1.00     4628
bpi_antisocialT1Sum         0.47      0.02     0.43     0.50 1.00     3493
bpi_anxiousDepressedSum     0.16      0.03     0.10     0.22 1.00     3476
                        Tail_ESS
Intercept                   3534
bpi_antisocialT1Sum         2895
bpi_anxiousDepressedSum     3086

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     1.98      0.03     1.93     2.03 1.00     4163     2897

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<div id="generalized-linear-regression" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Generalized Linear Regression</h1>
<div id="generalized-regression-model" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Generalized regression model</h2>
<p>In this example, we predict a count variable that has a poisson distribution. We could change the distribution.</p>
<pre class="r fold-show"><code>generalizedRegressionModel &lt;- glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = &quot;poisson&quot;,
                                  na.action = na.exclude)

summary(generalizedRegressionModel)</code></pre>
<pre><code>
Call:
glm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    family = &quot;poisson&quot;, data = mydata, na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.2767  -0.9731  -0.1403   0.5939   6.0151  

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             0.459999   0.019650  23.410  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.141577   0.004891  28.948  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.047567   0.008036   5.919 3.23e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 5849.4  on 2873  degrees of freedom
Residual deviance: 4562.6  on 2871  degrees of freedom
  (8656 observations deleted due to missingness)
AIC: 11482

Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r fold-show"><code>confint(generalizedRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                             2.5 %     97.5 %
(Intercept)             0.42136094 0.49838751
bpi_antisocialT1Sum     0.13196544 0.15113708
bpi_anxiousDepressedSum 0.03177425 0.06327445</code></pre>
</div>
<div id="generalized-regression-model-rms" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Generalized regression model (rms)</h2>
<p>In this example, we predict a count variable that has a poisson distribution. We could change the distribution.</p>
<pre class="r fold-show"><code>rmsGeneralizedRegressionModel &lt;- Glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE,
                                     family = &quot;poisson&quot;)

rmsGeneralizedRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
          countVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

General Linear Model
 
 Glm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
     family = &quot;poisson&quot;, data = mydata, x = TRUE, y = TRUE)
 
 
                        Model Likelihood    
                              Ratio Test    
       Obs    2874    LR chi2    1286.72    
 Residual d.f.2871    d.f.             2    
       g 0.3873236    Pr(&gt; chi2) &lt;0.0001    
 
                         Coef   S.E.   Wald Z Pr(&gt;|Z|)
 Intercept               0.4600 0.0196 23.41  &lt;0.0001 
 bpi_antisocialT1Sum     0.1416 0.0049 28.95  &lt;0.0001 
 bpi_anxiousDepressedSum 0.0476 0.0080  5.92  &lt;0.0001 
 </code></pre>
<pre class="r fold-show"><code>confint(rmsGeneralizedRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in summary.rms(fitted): adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum</code></pre>
</div>
<div id="bayesian-generalized-linear-model" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Bayesian generalized linear model</h2>
<p>In this example, we predict a count variable that has a poisson distribution. We could change the distribution. For example, we could use Gamma regression, <code>family = Gamma</code>, when the response variable is continuous and positive, and the coefficient of variation–rather than the variance–is constant.</p>
<pre class="r fold-show"><code>bayesianGeneralizedLinearRegression &lt;- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                           data = mydata,
                                           family = poisson,
                                           chains = 4,
                                           seed = 52242,
                                           iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianGeneralizedLinearRegression)</code></pre>
<pre><code> Family: poisson 
  Links: mu = log 
Formula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.46      0.02     0.42     0.50 1.00     3053
bpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2791
bpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2532
                        Tail_ESS
Intercept                   2974
bpi_antisocialT1Sum         2579
bpi_anxiousDepressedSum     2575

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="robust-generalized-regression" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Robust generalized regression</h2>
<pre class="r fold-show"><code>robustGeneralizedRegression &lt;- glmrob(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      family = &quot;poisson&quot;,
                                      na.action = na.exclude)

summary(robustGeneralizedRegression)</code></pre>
<pre><code>
Call:  glmrob(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,      family = &quot;poisson&quot;, data = mydata, na.action = na.exclude) 


Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             0.365934   0.020794  17.598  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.156275   0.005046  30.969  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.050526   0.008332   6.064 1.32e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
Robustness weights w.r * w.x: 
 2273 weights are ~= 1. The remaining 601 ones are summarized as
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1286  0.5550  0.7574  0.7183  0.8874  0.9982 

Number of observations: 2874 
Fitted by method &#39;Mqle&#39;  (in 5 iterations)

(Dispersion parameter for poisson family taken to be 1)

No deviance values available 
Algorithmic parameters: 
   acc    tcc 
0.0001 1.3450 
maxit 
   50 
test.acc 
  &quot;coef&quot; </code></pre>
<pre class="r fold-show"><code>confint(robustGeneralizedRegression)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in glm.control(acc = 1e-04, test.acc = &quot;coef&quot;, maxit = 50, tcc = 1.345): unused arguments (acc = 1e-04, test.acc = &quot;coef&quot;, tcc = 1.345)</code></pre>
</div>
<div id="ordinal-regression-model-rms" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Ordinal regression model (rms)</h2>
<pre class="r fold-show"><code>ordinalRegressionModel &lt;- robcov(orm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE))

ordinalRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
        orderedVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Logistic (Proportional Odds) Ordinal Regression Model
 
 orm(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
     data = mydata, x = TRUE, y = TRUE)
 
 
 Frequencies of Responses
 
   0   1   2   3   4   5   6   7   8   9  10  11  12  14 
 455 597 527 444 313 205 133  80  52  33  16   9   6   4 
 
 
                        Model Likelihood               Discrimination    Rank Discrim.    
                              Ratio Test                      Indexes          Indexes    
 Obs          2874    LR chi2     881.26    R2                  0.268    rho     0.506    
 Distinct Y     14    d.f.             2    R2(2,2874)          0.264                     
 Median Y        3    Pr(&gt; chi2) &lt;0.0001    R2(2,2803.4)        0.269                     
 max |deriv| 6e-06    Score chi2  916.79    |Pr(Y&gt;=median)-0.5| 0.188                     
                      Pr(&gt; chi2) &lt;0.0001                                                  
 
                         Coef   S.E.   Wald Z Pr(&gt;|Z|)
 bpi_antisocialT1Sum     0.4404 0.0208 21.16  &lt;0.0001 
 bpi_anxiousDepressedSum 0.1427 0.0266  5.36  &lt;0.0001 
 </code></pre>
<pre class="r fold-show"><code>confint(ordinalRegressionModel)</code></pre>
<pre><code>                              2.5 %     97.5 %
y&gt;=1                             NA         NA
y&gt;=2                    -0.79571122 -0.5575596
y&gt;=3                             NA         NA
y&gt;=4                             NA         NA
y&gt;=5                             NA         NA
y&gt;=6                             NA         NA
y&gt;=7                             NA         NA
y&gt;=8                             NA         NA
y&gt;=9                             NA         NA
y&gt;=10                            NA         NA
y&gt;=11                            NA         NA
y&gt;=12                            NA         NA
y&gt;=14                            NA         NA
bpi_antisocialT1Sum      0.39961501  0.4811892
bpi_anxiousDepressedSum  0.09053245  0.1948370</code></pre>
</div>
<div id="bayesian-ordinal-regression-model" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Bayesian ordinal regression model</h2>
<pre class="r fold-show"><code>bayesianOrdinalRegression &lt;- brm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                 data = mydata,
                                 family = cumulative(),
                                 chains = 4,
                                 seed = 52242,
                                 iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianOrdinalRegression)</code></pre>
<pre><code> Family: cumulative 
  Links: mu = logit; disc = identity 
Formula: orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept[1]               -0.60      0.06    -0.73    -0.48 1.00     4626
Intercept[2]                0.67      0.06     0.56     0.78 1.00     5687
Intercept[3]                1.57      0.06     1.45     1.70 1.00     4939
Intercept[4]                2.40      0.07     2.26     2.54 1.00     4633
Intercept[5]                3.15      0.08     2.99     3.31 1.00     4346
Intercept[6]                3.84      0.09     3.66     4.03 1.00     4524
Intercept[7]                4.52      0.11     4.31     4.74 1.00     4638
Intercept[8]                5.15      0.13     4.90     5.41 1.00     4695
Intercept[9]                5.79      0.15     5.50     6.10 1.00     4710
Intercept[10]               6.51      0.19     6.16     6.90 1.00     5155
Intercept[11]               7.17      0.24     6.70     7.66 1.00     5239
Intercept[12]               7.87      0.33     7.25     8.56 1.00     5835
Intercept[13]               8.91      0.53     8.02    10.03 1.00     6030
bpi_antisocialT1Sum         0.44      0.02     0.40     0.48 1.00     3936
bpi_anxiousDepressedSum     0.14      0.03     0.09     0.19 1.00     4692
                        Tail_ESS
Intercept[1]                3275
Intercept[2]                3236
Intercept[3]                3579
Intercept[4]                3633
Intercept[5]                3310
Intercept[6]                3649
Intercept[7]                3355
Intercept[8]                3295
Intercept[9]                3341
Intercept[10]               3020
Intercept[11]               3106
Intercept[12]               3455
Intercept[13]               3399
bpi_antisocialT1Sum         3142
bpi_anxiousDepressedSum     3548

Family Specific Parameters: 
     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
disc     1.00      0.00     1.00     1.00   NA       NA       NA

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="bayesian-count-regression-model" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Bayesian count regression model</h2>
<pre class="r fold-show"><code>bayesianCountRegression &lt;- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                               data = mydata,
                               family = &quot;poisson&quot;,
                               chains = 4,
                               seed = 52242,
                               iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianCountRegression)</code></pre>
<pre><code> Family: poisson 
  Links: mu = log 
Formula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.46      0.02     0.42     0.50 1.00     3053
bpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2791
bpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2532
                        Tail_ESS
Intercept                   2974
bpi_antisocialT1Sum         2579
bpi_anxiousDepressedSum     2575

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="logistic-regression-model-rms" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Logistic regression model (rms)</h2>
<pre class="r fold-show"><code>logisticRegressionModel &lt;- robcov(lrm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      x = TRUE,
                                      y = TRUE))

logisticRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
                 female     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                      2                    7613                    7616 

Logistic Regression Model
 
 lrm(formula = female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
     data = mydata, x = TRUE, y = TRUE)
 
 
                        Model Likelihood       Discrimination    Rank Discrim.    
                              Ratio Test              Indexes          Indexes    
 Obs          3914    LR chi2      66.63       R2       0.023    C       0.571    
  0           1965    d.f.             2      R2(2,3914)0.016    Dxy     0.142    
  1           1949    Pr(&gt; chi2) &lt;0.0001    R2(2,2935.5)0.022    gamma   0.147    
 max |deriv| 1e-12                             Brier    0.246    tau-a   0.071    
 
                         Coef    S.E.   Wald Z Pr(&gt;|Z|)
 Intercept                0.3002 0.0529  5.67  &lt;0.0001 
 bpi_antisocialT1Sum     -0.1244 0.0166 -7.48  &lt;0.0001 
 bpi_anxiousDepressedSum  0.0382 0.0253  1.51  0.1314  
 </code></pre>
<pre class="r fold-show"><code>confint(logisticRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in summary.rms(fitted): adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum</code></pre>
</div>
<div id="bayesian-logistic-regression-model" class="section level2" number="5.9">
<h2><span class="header-section-number">5.9</span> Bayesian logistic regression model</h2>
<pre class="r fold-show"><code>bayesianLogisticRegression &lt;- brm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = bernoulli,
                                  chains = 4,
                                  seed = 52242,
                                  iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianLogisticRegression)</code></pre>
<pre><code> Family: bernoulli 
  Links: mu = logit 
Formula: female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 3914) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.30      0.05     0.20     0.41 1.00     3975
bpi_antisocialT1Sum        -0.13      0.02    -0.16    -0.09 1.00     2967
bpi_anxiousDepressedSum     0.04      0.03    -0.01     0.09 1.00     3115
                        Tail_ESS
Intercept                   3063
bpi_antisocialT1Sum         2794
bpi_anxiousDepressedSum     2947

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<div id="hierarchical-linear-regression" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Hierarchical Linear Regression</h1>
<pre class="r fold-show"><code>model1 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum,
             data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit,
             na.action = na.exclude)

model2 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
             data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit,
             na.action = na.exclude)

summary(model2)$adj.r.squared</code></pre>
<pre><code>[1] 0.2614694</code></pre>
<pre class="r fold-show"><code>anova(model1, model2)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Sum of Sq"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2872","2":"11358.86","3":"NA","4":"NA","5":"NA","6":"NA","_rn_":"1"},{"1":"2871","2":"11239.86","3":"1","4":"119.0016","5":"30.39663","6":"3.834142e-08","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>summary(model2)$adj.r.squared - summary(model1)$adj.r.squared</code></pre>
<pre><code>[1] 0.007559301</code></pre>
</div>
<div id="moderation" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Moderated Multiple Regression</h1>
<p><a href="https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html" class="uri">https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html</a></p>
<p>Make sure to mean-center or orthogonalize predictors before computing the interaction term.</p>
<div id="model" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Model</h2>
<pre class="r fold-show"><code>states &lt;- as.data.frame(state.x77)
interactionModel &lt;- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)</code></pre>
</div>
<div id="plots" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Plots</h2>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy, modx = Murder)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy, modx = Murder, plot.points = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy, modx = Murder, interval = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-28-3.png" width="672" /></p>
<pre class="r fold-show"><code>johnson_neyman(interactionModel, pred = Illiteracy, modx = Murder, alpha = .05)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder is OUTSIDE the interval [-0.75, 11.74], the slope of Illiteracy
is p &lt; .05.

Note: The range of observed values of Murder is [1.40, 15.10]</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-28-4.png" width="672" /></p>
</div>
<div id="simple-slopes-analysis" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Simple Slopes Analysis</h2>
<pre class="r fold-show"><code>sim_slopes(interactionModel, pred = Illiteracy, modx = Murder, johnson_neyman = FALSE)</code></pre>
<pre><code>SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy when Murder =  3.68646 (- 1 SD): 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  395.34   274.84     1.44   0.16

Slope of Illiteracy when Murder =  7.37800 (Mean): 

   Est.     S.E.   t val.      p
------- -------- -------- ------
  37.12   192.56     0.19   0.85

Slope of Illiteracy when Murder = 11.06954 (+ 1 SD): 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -321.10   183.49    -1.75   0.09</code></pre>
<pre class="r fold-show"><code>sim_slopes(interactionModel,
           pred = Illiteracy,
           modx = Murder,
           modx.values = c(0, 5, 10),
           johnson_neyman = FALSE)</code></pre>
<pre><code>SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy when Murder =  0.00: 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  753.07   385.90     1.95   0.06

Slope of Illiteracy when Murder =  5.00: 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  267.88   240.49     1.11   0.27

Slope of Illiteracy when Murder = 10.00: 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -217.32   176.21    -1.23   0.22</code></pre>
</div>
<div id="johnson-neyman-intervals" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Johnson-Neyman intervals</h2>
<p>Indicates all the values of the moderator for which the slope of the predictor is statistically significant.</p>
<pre class="r fold-show"><code>sim_slopes(interactionModel, pred = Illiteracy, modx = Murder, johnson_neyman = TRUE)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder is OUTSIDE the interval [-0.75, 11.74], the slope of Illiteracy
is p &lt; .05.

Note: The range of observed values of Murder is [1.40, 15.10]

SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy when Murder =  3.68646 (- 1 SD): 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  395.34   274.84     1.44   0.16

Slope of Illiteracy when Murder =  7.37800 (Mean): 

   Est.     S.E.   t val.      p
------- -------- -------- ------
  37.12   192.56     0.19   0.85

Slope of Illiteracy when Murder = 11.06954 (+ 1 SD): 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -321.10   183.49    -1.75   0.09</code></pre>
<pre class="r fold-show"><code>probe_interaction(interactionModel,
                  pred = Illiteracy,
                  modx = Murder,
                  cond.int = TRUE,
                  interval = TRUE,
                  jnplot = TRUE)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder is OUTSIDE the interval [-0.75, 11.74], the slope of Illiteracy
is p &lt; .05.

Note: The range of observed values of Murder is [1.40, 15.10]</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre><code>SIMPLE SLOPES ANALYSIS 

When Murder =  3.68646 (- 1 SD): 

                                 Est.     S.E.   t val.      p
--------------------------- --------- -------- -------- ------
Slope of Illiteracy            395.34   274.84     1.44   0.16
Conditional intercept         4523.23   134.99    33.51   0.00

When Murder =  7.37800 (Mean): 

                                 Est.     S.E.   t val.      p
--------------------------- --------- -------- -------- ------
Slope of Illiteracy             37.12   192.56     0.19   0.85
Conditional intercept         4586.22    85.52    53.63   0.00

When Murder = 11.06954 (+ 1 SD): 

                                 Est.     S.E.   t val.      p
--------------------------- --------- -------- -------- ------
Slope of Illiteracy           -321.10   183.49    -1.75   0.09
Conditional intercept         4649.22   118.97    39.08   0.00</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-30-2.png" width="672" /></p>
</div>
</div>
<div id="missingness" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Approaches to Handling Missingness</h1>
<div id="listwiseDeletion" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Listwise deletion</h2>
<p>Listwise deletion deletes every row in the data file that has a missing value for one of the model variables.</p>
<pre class="r fold-show"><code>listwiseDeletionModel &lt;- lm(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  na.action = na.exclude)

summary(listwiseDeletionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>confint(listwiseDeletionModel)</code></pre>
<pre><code>                            2.5 %    97.5 %
(Intercept)             1.0809881 1.3156128
bpi_antisocialT1Sum     0.4290884 0.5019688
bpi_anxiousDepressedSum 0.1035825 0.2179258</code></pre>
</div>
<div id="pairwiseDeletion" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Pairwise deletion</h2>
<p>Also see here:</p>
<ul>
<li><a href="https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002" class="uri">https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002</a></li>
<li><a href="https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure" class="uri">https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure</a></li>
<li><a href="https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re" class="uri">https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re</a></li>
<li><a href="https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values" class="uri">https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values</a></li>
<li><a href="https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps" class="uri">https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps</a></li>
</ul>
<p>Adapted from here: <a href="https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise" class="uri">https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise</a></p>
<pre class="r fold-show"><code>modelData &lt;- mydata[,c(&quot;bpi_antisocialT2Sum&quot;,&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)]
varMeans &lt;- colMeans(modelData, na.rm = TRUE)
varCovariances &lt;- cov(modelData, use = &quot;pairwise&quot;)

pairwiseRegression_syntax &lt;- &#39;
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
&#39;

pairwiseRegression_fit &lt;- lavaan(
  pairwiseRegression_syntax,
  sample.mean = varMeans,
  sample.cov = varCovariances,
  sample.nobs = sum(complete.cases(modelData))
)

summary(
  pairwiseRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)</code></pre>
<pre><code>lavaan 0.6-12 ended normally after 12 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                         4

  Number of observations                          2874

Model Test User Model:
                                                      
  Test statistic                                 0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Regressions:
                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  bpi_antisocialT2Sum ~                                                      
    bpi_antsclT1Sm         0.441    0.018   24.611    0.000    0.441    0.456
    bp_nxsDprssdSm         0.137    0.028    4.854    0.000    0.137    0.090

Intercepts:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    1.175    0.058   20.125    0.000    1.175    0.523

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    3.755    0.099   37.908    0.000    3.755    0.743

R-Square:
                   Estimate
    bpi_antsclT2Sm    0.257</code></pre>
</div>
<div id="fiml" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Full-information maximum likelihood (FIML)</h2>
<pre class="r fold-show"><code>fimlRegression_syntax &lt;- &#39;
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
&#39;

fimlRegression_fit &lt;- lavaan(
  fimlRegression_syntax,
  data = mydata,
  missing = &quot;ML&quot;,
)</code></pre>
<pre><code>Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 7616 cases were deleted due to missing values in 
          exogenous variable(s), while fixed.x = TRUE.</code></pre>
<pre class="r fold-show"><code>summary(
  fimlRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)</code></pre>
<pre><code>lavaan 0.6-12 ended normally after 12 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                         4

                                                  Used       Total
  Number of observations                          3914       11530
  Number of missing patterns                         2            

Model Test User Model:
                                                      
  Test statistic                                 0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Regressions:
                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  bpi_antisocialT2Sum ~                                                      
    bpi_antsclT1Sm         0.466    0.019   25.062    0.000    0.466    0.466
    bp_nxsDprssdSm         0.161    0.029    5.516    0.000    0.161    0.102

Intercepts:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    1.198    0.060   20.039    0.000    1.198    0.516

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    3.911    0.103   37.908    0.000    3.911    0.725

R-Square:
                   Estimate
    bpi_antsclT2Sm    0.275</code></pre>
</div>
<div id="imputation" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Multiple imputation</h2>
<pre class="r fold-show"><code>modelData_imputed &lt;- mice(
  modelData,
  m = 5,
  method = &quot;pmm&quot;) # predictive mean matching; can choose among many methods</code></pre>
<pre><code>
 iter imp variable
  1   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum</code></pre>
<pre class="r fold-show"><code>imputedData_fit &lt;- with(
  modelData_imputed,
  lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum))

imputedData_pooledEstimates &lt;- pool(imputedData_fit)
summary(imputedData_pooledEstimates)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["fct"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["df"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"1.2124076","3":"0.20780441","4":"5.834369","5":"4.071746","6":"0.004069114"},{"1":"bpi_antisocialT1Sum","2":"0.4250376","3":"0.04994047","4":"8.510885","5":"4.238663","6":"0.000806101"},{"1":"bpi_anxiousDepressedSum","2":"0.2226497","3":"0.06985335","4":"3.187387","5":"4.368442","6":"0.029414365"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="model-building-steps" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Model Building Steps</h1>
<ol style="list-style-type: decimal">
<li>Examine extent and type of missing data, consider <a href="#missingness">how to handle missing values</a> (<a href="#imputation">multiple imputation</a>, <a href="#fiml">FIML</a>, <a href="#pairwiseDeletion">pairwise deletion</a>, <a href="#listwiseDeletion">listwise deletion</a>)
<ul>
<li>Little’s MCAR test from <code>mcar_test()</code> function of the <code>njtierney/naniar</code> package</li>
<li>Bayesian handling of missing data: <a href="https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html" class="uri">https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html</a></li>
</ul></li>
<li>Examine descriptive statistics, consider variable transformations</li>
<li>Examine scatterplot matrix to examine whether associations between predictor and outcomes are linear or nonlinear</li>
<li>Model building: theory and empiricism (stepwise regression, likelihood ratio tests, k-fold cross validation to check for over-fitting compared to simpler model)</li>
<li>Test assumptions
<ul>
<li>Examine whether predictors have linear association with outcome (Residual Plots, Marginal Model Plots, Added-Variable Plots—check for non-horizontal lines)</li>
<li>Examine whether residuals have constant variance across levels of outcome and predictors Residual Plots, Spread-Level Plots—check for fan-shaped plot or other increasing/decreasing structure)</li>
<li>Examine whether predictors show multicollinearity (VIF)</li>
<li>Examine whether residuals are normally distributed (QQ plot and density plot)</li>
<li>Examine influence of influential observations (high outlyingness and leverage) on parameter stimates by iteratively removing influential observations and refitting (Added-Variable Plots)</li>
</ul></li>
<li>Handle violated assumptions, select final set of predictors/outcomes and transformation of each</li>
<li>Use k-fold cross validation to identify the best estimation procedure (OLS, MM, or LTS) on the final model</li>
<li>Use identified estimation procedure to fit final model and determine the best parameter point estimates</li>
<li>Calculate bootstrapped estimates to determine the confidence intervals around the parameter estimates of the final model</li>
</ol>
</div>
<div id="bootstrapped-estimates" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Bootstrapped Estimates</h1>
<p>To determine the confidence intervals of parameter estimates</p>
<div id="linear-regression-1" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Linear Regression</h2>
<pre class="r fold-show"><code>multipleRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(multipleRegressionModelBootstrapped)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["R"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["original"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["bootBias"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bootSE"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bootMed"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1000","2":"1.1983004","3":"0.0001399656","4":"0.06219558","5":"1.1985816","_rn_":"(Intercept)"},{"1":"1000","2":"0.4655286","3":"-0.0002643626","4":"0.02361857","5":"0.4659304","_rn_":"bpi_antisocialT1Sum"},{"1":"1000","2":"0.1607541","3":"0.0007453450","4":"0.03301517","5":"0.1626931","_rn_":"bpi_anxiousDepressedSum"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>confint(multipleRegressionModelBootstrapped, level = .95, type = &quot;bca&quot;)</code></pre>
<pre><code>Warning in confint.boot(multipleRegressionModelBootstrapped, level = 0.95, : BCa
method fails for this problem. Using &#39;perc&#39; instead</code></pre>
<pre><code>Bootstrap percent confidence intervals

                             2.5 %    97.5 %
(Intercept)             1.07810767 1.3230860
bpi_antisocialT1Sum     0.41987371 0.5127292
bpi_anxiousDepressedSum 0.09642172 0.2243630</code></pre>
<pre class="r fold-show"><code>hist(multipleRegressionModelBootstrapped)</code></pre>
<pre><code>Warning in confint.boot(x, type = ci, level = level): BCa method fails for this
problem. Using &#39;perc&#39; instead</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
<div id="generalized-regression" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Generalized Regression</h2>
<pre class="r fold-show"><code>generalizedRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(generalizedRegressionModelBootstrapped)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["R"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["original"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["bootBias"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bootSE"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bootMed"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1000","2":"1.1983004","3":"0.0019858510","4":"0.06254580","5":"1.2008616","_rn_":"(Intercept)"},{"1":"1000","2":"0.4655286","3":"-0.0002735168","4":"0.02237860","5":"0.4657328","_rn_":"bpi_antisocialT1Sum"},{"1":"1000","2":"0.1607541","3":"-0.0005392890","4":"0.03344411","5":"0.1602973","_rn_":"bpi_anxiousDepressedSum"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>confint(generalizedRegressionModelBootstrapped, level = .95, type = &quot;bca&quot;)</code></pre>
<pre><code>Warning in confint.boot(generalizedRegressionModelBootstrapped, level = 0.95, :
BCa method fails for this problem. Using &#39;perc&#39; instead</code></pre>
<pre><code>Bootstrap percent confidence intervals

                            2.5 %    97.5 %
(Intercept)             1.0796086 1.3271666
bpi_antisocialT1Sum     0.4202552 0.5093450
bpi_anxiousDepressedSum 0.0946665 0.2269459</code></pre>
<pre class="r fold-show"><code>hist(generalizedRegressionModelBootstrapped)</code></pre>
<pre><code>Warning in confint.boot(x, type = ci, level = level): BCa method fails for this
problem. Using &#39;perc&#39; instead</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
</div>
<div id="cross-validation" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Cross Validation</h1>
<p>To examine degree of prediction error and over-fitting to determine best model <a href="https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best" class="uri">https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best</a></p>
<div id="k-fold-cross-validation" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> K-fold cross validation</h2>
<pre class="r fold-show"><code>kFolds &lt;- 10
replications &lt;- 20

folds &lt;- cvFolds(nrow(mydata), K = kFolds, R = replications)

fitLm &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = &quot;na.exclude&quot;)

fitLmrob &lt;- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                  data = mydata,
                  na.action = &quot;na.exclude&quot;)

fitLts &lt;- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                 data = mydata,
                 na.action = &quot;na.exclude&quot;)

cvFitLm &lt;- cvLm(fitLm, K = kFolds, R = replications)

cvFitLmrob &lt;- cvLmrob(fitLmrob, K = kFolds, R = replications)
cvFitLts &lt;- cvLts(fitLts, K = kFolds, R = replications)

cvFits &lt;- cvSelect(OLS = cvFitLm, MM = cvFitLmrob, LTS = cvFitLts)
cvFits</code></pre>
<pre><code>
10-fold CV results:
  Fit       CV
1 OLS 1.980595
2  MM 1.026337
3 LTS 1.006934

Best model:
   CV 
&quot;LTS&quot; </code></pre>
<pre class="r fold-show"><code>bwplot(cvFits, xlab = &quot;Root Mean Square Error&quot;, xlim= c(0, max(cvFits$cv$CV) + 0.2))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
</div>
<div id="examining-model-fits" class="section level1" number="12">
<h1><span class="header-section-number">12</span> Examining Model Fits</h1>
<div id="effect-plots" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Effect Plots</h2>
<pre class="r fold-show"><code>allEffects(multipleRegressionModel)</code></pre>
<pre><code> model: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum

 bpi_antisocialT1Sum effect
bpi_antisocialT1Sum
       0      3.2      6.5      9.8       13 
1.394591 2.884283 4.420527 5.956772 7.446463 

 bpi_anxiousDepressedSum effect
bpi_anxiousDepressedSum
       0        2        4        6        8 
2.502636 2.824145 3.145653 3.467161 3.788669 </code></pre>
<pre class="r fold-show"><code>plot(allEffects(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
<div id="confidence-ellipses" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Confidence Ellipses</h2>
<pre class="r fold-show"><code>confidenceEllipse(multipleRegressionModel, levels = c(0.5, .95))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
<div id="data-ellipse" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Data Ellipse</h2>
<pre class="r fold-show"><code>mydata_nomissing &lt;- na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)])
dataEllipse(mydata_nomissing$bpi_antisocialT1Sum, mydata_nomissing$bpi_antisocialT2Sum, levels = c(0.5, .95))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
</div>
<div id="diagnostics" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Diagnostics</h1>
<div id="assumptions" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Assumptions</h2>
<div id="linearAssociation" class="section level3" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> 1. Linear relation between predictors and outcome</h3>
<div id="ways-to-test" class="section level4" number="13.1.1.1">
<h4><span class="header-section-number">13.1.1.1</span> Ways to Test</h4>
<div id="before-model-fitting" class="section level5" number="13.1.1.1.1">
<h5><span class="header-section-number">13.1.1.1.1</span> Before Model Fitting</h5>
<ul>
<li>scatterplot matrix</li>
<li>distance correlation</li>
</ul>
<div id="scatterplot-matrix" class="section level6" number="13.1.1.1.1.1">
<h6><span class="header-section-number">13.1.1.1.1.1</span> Scatterplot Matrix</h6>
<pre class="r fold-show"><code>scatterplotMatrix(~ bpi_antisocialT2Sum + bpi_antisocialT1Sum + bpi_anxiousDepressedSum, data = mydata)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
</div>
<div id="distance-correlation" class="section level6" number="13.1.1.1.1.2">
<h6><span class="header-section-number">13.1.1.1.1.2</span> Distance correlation</h6>
<p>The distance correlation is an index of the degree of the linear and non-linear association between two variables.</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;distance&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.231721","4":"0.95","5":"0.1968321","6":"0.2660239","7":"484.0006","8":"4128499","9":"0","10":"Distance (Bias Corrected) correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="after-model-fitting" class="section level5" number="13.1.1.1.2">
<h5><span class="header-section-number">13.1.1.1.2</span> After Model Fitting</h5>
<p>Check for nonlinearities (non-horizontal line) in plots of:</p>
<ul>
<li>Residuals versus fitted values (<a href="#residualPlots">Residual Plots</a>)—best</li>
<li>Residuals versus predictors (<a href="#residualPlots">Residual Plots</a>)</li>
<li>Outcome versus fitted values (<a href="#marginalModelPlots">Marginal Model Plots</a>)</li>
<li>Outcome versus predictors, ignoring other predictors (<a href="#marginalModelPlots">Marginal Model Plots</a>)</li>
<li>Outcome versus predictors, controlling for other predictors (<a href="#addedVariablePlots">Added-Variable Plots</a>)</li>
</ul>
</div>
</div>
<div id="ways-to-handle" class="section level4" number="13.1.1.2">
<h4><span class="header-section-number">13.1.1.2</span> Ways to Handle</h4>
<ul>
<li>Transform outcome/predictor variables (Box-Cox transformations)</li>
<li>Semi-parametric regression models: Generalized additive models (GAM)</li>
<li>Non-parametric regression models: Nearest-Neighbor Kernel Regression</li>
</ul>
<div id="semi-parametric-or-non-parametric-regression-models" class="section level5" number="13.1.1.2.1">
<h5><span class="header-section-number">13.1.1.2.1</span> Semi-parametric or non-parametric regression models</h5>
<p><a href="http://www.lisa.stat.vt.edu/?q=node/7517" class="uri">http://www.lisa.stat.vt.edu/?q=node/7517</a></p>
<p>Note: using semi-parametric or non-parametric models increases fit in context of nonlinearity at the expense of added complexity. Make sure to avoid fitting an overly complex model (e.g., use k-fold cross validation). Often, the simpler (generalized) linear model is preferable to semi-paremetric or non-parametric approaches</p>
<div id="semi-parametric-generalized-additive-models" class="section level6" number="13.1.1.2.1.1">
<h6><span class="header-section-number">13.1.1.2.1.1</span> Semi-parametric: Generalized Additive Models</h6>
<p><a href="http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models" class="uri">http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models</a></p>
<pre class="r fold-show"><code>generalizedAdditiveModel &lt;- gam(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                family = gaussian(),
                                data = mydata, na.action = na.exclude)

summary(generalizedAdditiveModel)</code></pre>
<pre><code>
Family: gaussian 
Link function: identity 

Formula:
bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum

Parametric coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1


R-sq.(adj) =  0.261   Deviance explained = 26.2%
GCV = 3.9191  Scale est. = 3.915     n = 2874</code></pre>
<pre class="r fold-show"><code>confint(generalizedAdditiveModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Warning in X[, nonA, drop = FALSE] %*% B0[nonA] + O: longer object length is not
a multiple of shorter object length</code></pre>
<pre><code>Error in glm.control(nthreads = 1, irls.reg = 0, epsilon = 1e-07, maxit = 200, : unused arguments (nthreads = 1, irls.reg = 0, mgcv.tol = 1e-07, mgcv.half = 15, rank.tol = 1.49011611938477e-08, nlm = list(7, 1e-06, 2, 1e-04, 200, FALSE), optim = list(1e+07), newton = list(1e-06, 5, 2, 30, FALSE), outerPIsteps = 0, idLinksBases = TRUE, scalePenalty = TRUE, efs.lspmax = 15, efs.tol = 0.1, keepData = FALSE, scale.est = &quot;fletcher&quot;, edge.correct = FALSE)</code></pre>
</div>
<div id="non-parametric-nearest-neighbor-kernel-regression" class="section level6" number="13.1.1.2.1.2">
<h6><span class="header-section-number">13.1.1.2.1.2</span> Non-parametric: Nearest-Neighbor Kernel Regression</h6>
</div>
</div>
</div>
</div>
<div id="exogeneity" class="section level3" number="13.1.2">
<h3><span class="header-section-number">13.1.2</span> 2. Exogeneity</h3>
<p>Exogeneity means that the association between predictors and outcome is fully causal and unrelated to other variables.</p>
<div id="ways-to-test-1" class="section level4" number="13.1.2.1">
<h4><span class="header-section-number">13.1.2.1</span> Ways to Test</h4>
<ul>
<li>Durbin-Wu-Hausman test of endogeneity</li>
</ul>
<div id="durbin-wu-hausman-test-of-endogeneity" class="section level5" number="13.1.2.1.1">
<h5><span class="header-section-number">13.1.2.1.1</span> Durbin-Wu-Hausman test of endogeneity</h5>
<p>The instrumental variables (2SLS) estimator is implemented in the <code>R</code> package <code>AER</code> as command:</p>
<pre class="r fold-show"><code>ivreg(y ~ x1 + x2 + w1 + w2 | z1 + z2 + z3 + w1 + w2)</code></pre>
<p>where <code>x1</code> and <code>x2</code> are endogenous regressors, <code>w1</code> and <code>w2</code> exogeneous regressors, and <code>z1</code> to <code>z3</code> are excluded instruments.</p>
<p>Durbin-Wu-Hausman test:</p>
<pre class="r fold-show"><code>hsng2 &lt;- read.dta(&quot;http://www.stata-press.com/data/r11/hsng2.dta&quot;)

fiv &lt;- ivreg(rent ~ hsngval + pcturban | pcturban + faminc + reg2 + reg3 + reg4, data = hsng2) #Housing values are likely endogeneous and therefore instrumented by median family income (faminc) and 3 regional dummies (reg2, reg4, reg4)

summary(fiv, diagnostics = TRUE)</code></pre>
<pre><code>
Call:
ivreg(formula = rent ~ hsngval + pcturban | pcturban + faminc + 
    reg2 + reg3 + reg4, data = hsng2)

Residuals:
     Min       1Q   Median       3Q      Max 
-84.1948 -11.6023  -0.5239   8.6583  73.6130 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.207e+02  1.571e+01   7.685 7.55e-10 ***
hsngval     2.240e-03  3.388e-04   6.612 3.17e-08 ***
pcturban    8.152e-02  3.082e-01   0.265    0.793    

Diagnostic tests:
                 df1 df2 statistic  p-value    
Weak instruments   4  44     13.30  3.5e-07 ***
Wu-Hausman         1  46     15.91 0.000236 ***
Sargan             3  NA     11.29 0.010268 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 22.86 on 47 degrees of freedom
Multiple R-Squared: 0.5989, Adjusted R-squared: 0.5818 
Wald test: 42.66 on 2 and 47 DF,  p-value: 2.731e-11 </code></pre>
<p>The Eicker-Huber-White covariance estimator which is robust to heteroscedastic error terms is reported after estimation with <code>vcov = sandwich</code> in <code>coeftest()</code></p>
<p>First stage results are reported by explicitly estimating them. For example:</p>
<pre class="r fold-show"><code>first &lt;- lm(hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, data = hsng2)

summary(first)</code></pre>
<pre><code>
Call:
lm(formula = hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, 
    data = hsng2)

Residuals:
   Min     1Q Median     3Q    Max 
-10504  -5223  -1162   2939  46756 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.867e+04  1.200e+04  -1.557 0.126736    
pcturban     1.822e+02  1.150e+02   1.584 0.120289    
faminc       2.731e+00  6.819e-01   4.006 0.000235 ***
reg2        -5.095e+03  4.122e+03  -1.236 0.223007    
reg3        -1.778e+03  4.073e+03  -0.437 0.664552    
reg4         1.341e+04  4.048e+03   3.314 0.001849 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 9253 on 44 degrees of freedom
Multiple R-squared:  0.6908,    Adjusted R-squared:  0.6557 
F-statistic: 19.66 on 5 and 44 DF,  p-value: 3.032e-10</code></pre>
<p>In case of a single endogenous variable (K = 1), the F-statistic to assess weak instruments is reported after estimating the first stage with, for example:</p>
<pre class="r fold-show"><code>waldtest(first, . ~ . - faminc - reg2 - reg3 - reg4)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"44","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"48","2":"-4","3":"13.29778","4":"3.495112e-07","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>or in case of heteroscedatistic errors:</p>
<pre class="r fold-show"><code>waldtest(first, . ~ . - faminc - reg2 - reg3- reg4, vcov = sandwich)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"44","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"48","2":"-4","3":"12.97475","4":"4.643466e-07","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="ways-to-handle-1" class="section level4" number="13.1.2.2">
<h4><span class="header-section-number">13.1.2.2</span> Ways to Handle</h4>
<ul>
<li>Conduct an experiment/RCT with random assignment</li>
<li>Instrumental variables</li>
</ul>
</div>
</div>
<div id="homoscedasticity-of-residuals" class="section level3" number="13.1.3">
<h3><span class="header-section-number">13.1.3</span> 3. Homoscedasticity of residuals</h3>
<p>Homoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome/predictors (i.e., the residuals show constant variance as a function of outcome/predictors).</p>
<div id="ways-to-test-2" class="section level4" number="13.1.3.1">
<h4><span class="header-section-number">13.1.3.1</span> Ways to Test</h4>
<ul>
<li>Plot residuals vs. outcome and predictor variables (<a href="#residualPlots">Residual Plots</a>)</li>
<li>Plot residuals vs. fitted values (<a href="#residualPlots">Residual Plots</a>)</li>
<li>Time Series data: Plot residuals vs. time</li>
<li>Spread-level plot</li>
<li>Breusch-Pagan test: <code>bptest()</code> function from <code>lmtest</code> package</li>
<li>Goldfeld-Quandt Test</li>
</ul>
<div id="residuals-vs.-outcome-and-predictor-variables" class="section level5" number="13.1.3.1.1">
<h5><span class="header-section-number">13.1.3.1.1</span> Residuals vs. outcome and predictor variables</h5>
<p>Plot residuals, or perhaps their absolute values, versus the outcome and predictor variables (<a href="#residualPlots">Residual Plots</a>). Examine whether residual variance is constant at all levels of other variables or whether it increases/decreases as a function of another variable (or shows some others structure, e.g., small variance at low and high levels of a predictor and high variance in the middle). Note that this is <em>different than whether the residuals show non-linearities</em>—i.e., a non-horizontal line, which would indicate a <a href="#linearAssociation">nonlinear association between variables</a> (see Assumption #1, above). Rather, here we are examining whether there is change in the variance as a function of another variable (e.g., a fan-shaped <a href="#residualPlots">Residual Plot</a>)</p>
</div>
<div id="spread-level-plot" class="section level5" number="13.1.3.1.2">
<h5><span class="header-section-number">13.1.3.1.2</span> Spread-level plot</h5>
<p>Examining whether level (e.g., mean) depends on spread (e.g., variance)—plot of log of the absolute Studentized residuals against the log of the fitted values</p>
<pre class="r fold-show"><code>spreadLevelPlot(multipleRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<pre><code>
Suggested power transformation:  0.5304728 </code></pre>
</div>
<div id="breusch-pagan-test" class="section level5" number="13.1.3.1.3">
<h5><span class="header-section-number">13.1.3.1.3</span> Breusch-Pagan test</h5>
<p><a href="http://www.inside-r.org/packages/cran/lmtest/docs/bptest" class="uri">http://www.inside-r.org/packages/cran/lmtest/docs/bptest</a></p>
<pre class="r fold-show"><code>bptest(multipleRegressionModel)</code></pre>
<pre><code>
    studentized Breusch-Pagan test

data:  multipleRegressionModel
BP = 94.638, df = 2, p-value &lt; 2.2e-16</code></pre>
</div>
<div id="goldfeld-quandt-test" class="section level5" number="13.1.3.1.4">
<h5><span class="header-section-number">13.1.3.1.4</span> Goldfeld-Quandt Test</h5>
<pre class="r fold-show"><code>gqtest(multipleRegressionModel)</code></pre>
<pre><code>
    Goldfeld-Quandt test

data:  multipleRegressionModel
GQ = 1.0755, df1 = 1434, df2 = 1434, p-value = 0.08417
alternative hypothesis: variance increases from segment 1 to 2</code></pre>
</div>
<div id="test-of-dependence-of-spread-on-level" class="section level5" number="13.1.3.1.5">
<h5><span class="header-section-number">13.1.3.1.5</span> Test of dependence of spread on level</h5>
<pre class="r fold-show"><code>ncvTest(multipleRegressionModel)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 232.1196, Df = 1, p = &lt; 2.22e-16</code></pre>
</div>
<div id="test-of-dependence-of-spread-on-predictors" class="section level5" number="13.1.3.1.6">
<h5><span class="header-section-number">13.1.3.1.6</span> Test of dependence of spread on predictors</h5>
<pre class="r fold-show"><code>ncvTest(multipleRegressionModel, ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
Chisquare = 233.2878, Df = 2, p = &lt; 2.22e-16</code></pre>
</div>
</div>
<div id="ways-to-handle-2" class="section level4" number="13.1.3.2">
<h4><span class="header-section-number">13.1.3.2</span> Ways to Handle</h4>
<ul>
<li>If residual variance increases as a function of the fitted values, consider a poisson regression model (especially for count data), for which the variance does increase with the mean</li>
<li>If residual variance differs as a function of model predictors, consider adding interactions/product terms (the effect of one variable depends on the level of another variable)</li>
<li>Try transforming the outcome variable to be normally distributed (log-transform if the errors seem consistent in percentage rather than absolute terms)</li>
<li>If the error variance is proportional to a variable <span class="math inline">\(z\)</span>, then fit the model using Weighted Least Squares (WLS), with the weights given be <span class="math inline">\(1/z\)</span></li>
<li>Weighted least squares (WLS) using the “weights” argument of the <code>lm()</code> function; to get weights, see: <a href="https://stats.stackexchange.com/a/100410/20338" class="uri">https://stats.stackexchange.com/a/100410/20338</a></li>
<li>Huber-White standard errors (a.k.a. “Sandwich” estimates) from a heteroscedasticity-corrected covariance matrix
<ul>
<li><code>coeftest()</code> function from the <code>sandwich</code> package along with hccm sandwich estimates from the <code>car</code> package</li>
<li><code>robcov()</code> function from the <code>rms</code> package</li>
</ul></li>
<li>Time series data: ARCH (auto-regressive conditional heteroscedasticity) models</li>
<li>Time series data: seasonal patterns can be addressed by applying log transformation to outcome variable</li>
</ul>
<div id="huber-white-standard-errors" class="section level5" number="13.1.3.2.1">
<h5><span class="header-section-number">13.1.3.2.1</span> Huber-White standard errors</h5>
<p>Standard errors (SEs) on the diagonal increase</p>
<pre class="r fold-show"><code>vcov(multipleRegressionModel)</code></pre>
<pre><code>                          (Intercept) bpi_antisocialT1Sum
(Intercept)              0.0035795220       -0.0006533375
bpi_antisocialT1Sum     -0.0006533375        0.0003453814
bpi_anxiousDepressedSum -0.0003167540       -0.0002574524
                        bpi_anxiousDepressedSum
(Intercept)                       -0.0003167540
bpi_antisocialT1Sum               -0.0002574524
bpi_anxiousDepressedSum            0.0008501565</code></pre>
<pre class="r fold-show"><code>hccm(multipleRegressionModel)</code></pre>
<pre><code>Error in V %*% t(X) %*% apply(X, 2, &quot;*&quot;, (e^2)/factor): non-conformable arguments</code></pre>
<pre class="r fold-show"><code>summary(multipleRegressionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>coeftest(multipleRegressionModel, vcov = sandwich)</code></pre>
<pre><code>
t test of coefficients:

                        Estimate Std. Error t value  Pr(&gt;|t|)    
(Intercept)             1.198300   0.062211 19.2618 &lt; 2.2e-16 ***
bpi_antisocialT1Sum     0.465529   0.022929 20.3030 &lt; 2.2e-16 ***
bpi_anxiousDepressedSum 0.160754   0.032991  4.8727  1.16e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r fold-show"><code>coeftest(multipleRegressionModel, vcov = hccm)</code></pre>
<pre><code>Error in V %*% t(X) %*% apply(X, 2, &quot;*&quot;, (e^2)/factor): non-conformable arguments</code></pre>
<pre class="r fold-show"><code>robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
           data = mydata,
           x = TRUE,
           y = TRUE))</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Linear Regression Model
 
 ols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
     data = mydata, x = TRUE, y = TRUE)
 
 
                 Model Likelihood    Discrimination    
                       Ratio Test           Indexes    
 Obs    2874    LR chi2    873.09    R2       0.262    
 sigma1.9786    d.f.            2    R2 adj   0.261    
 d.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    
 
 Residuals
 
     Min      1Q  Median      3Q     Max 
 -8.3755 -1.2337 -0.2212  0.9911 12.8017 
 
 
                         Coef   S.E.   t     Pr(&gt;|t|)
 Intercept               1.1983 0.0622 19.26 &lt;0.0001 
 bpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 
 bpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 
 </code></pre>
<pre class="r fold-show"><code>robcov(ols(t_ext ~ m_ext + age,
           data = mydata,
           x = TRUE,
           y = TRUE),
       cluster = mydata$tcid) #account for nested data within subject</code></pre>
<pre><code>Error in eval(predvars, data, callenv): object &#39;t_ext&#39; not found</code></pre>
</div>
</div>
</div>
<div id="errors-are-independent" class="section level3" number="13.1.4">
<h3><span class="header-section-number">13.1.4</span> 4. Errors are independent</h3>
<p>Independent errors means that the errors are uncorrelated with each other.</p>
<div id="ways-to-test-3" class="section level4" number="13.1.4.1">
<h4><span class="header-section-number">13.1.4.1</span> Ways to Test</h4>
<ul>
<li>Plot residuals vs. predictors (<a href="#residualPlots">Residual Plots</a>)</li>
<li>Time Series data: Residual time series plot (residuals vs. row number)</li>
<li>Time Series data: Table or plot of residual autocorrelations</li>
<li>Time Series data: Durbin-Watson statistic for test of significant residual autocorrelation at lag 1</li>
</ul>
</div>
<div id="ways-to-handle-3" class="section level4" number="13.1.4.2">
<h4><span class="header-section-number">13.1.4.2</span> Ways to Handle</h4>
<ul>
<li>Generalized least squares (GLS) models are capable of handling correlated errors: <a href="https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html" class="uri">https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html</a></li>
<li>Regression with cluster variable
<ul>
<li><code>robcov()</code> from <code>rms</code> package</li>
</ul></li>
<li><a href="hlm.html">Hierarchical linear modeling</a>
<ul>
<li><a href="hlm.html#linear">Linear mixed effects models</a></li>
<li><a href="hlm.html#generalized">Generalized linear mixed effects models</a></li>
<li><a href="hlm.html#nonlinear">Nonlinear mixed effects models</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="no-multicollinearity" class="section level3" number="13.1.5">
<h3><span class="header-section-number">13.1.5</span> 5. No multicollinearity</h3>
<p>Multicollinearity occurs when the predictors are correlated with each other.</p>
<div id="ways-to-test-4" class="section level4" number="13.1.5.1">
<h4><span class="header-section-number">13.1.5.1</span> Ways to Test</h4>
<ul>
<li>Variance Inflation Factor (VIF)</li>
<li>Generalized Variance Inflation Factor (GVIF)—when models have related regressors (multiple polynomial terms or contrasts from same predictor)</li>
<li>Correlation</li>
<li>Tolerance</li>
<li>Condition Index</li>
</ul>
<div id="variance-inflation-factor-vif" class="section level5" number="13.1.5.1.1">
<h5><span class="header-section-number">13.1.5.1.1</span> Variance Inflation Factor (VIF)</h5>
<p><span class="math display">\[
\text{VIF} = 1/\text{Tolerance}
\]</span></p>
<p>If the variance inflation factor of a predictor variable were 5.27 (<span class="math inline">\(\sqrt{5.27} = 2.3\)</span>), this means that the standard error for the coefficient of that predictor variable is 2.3 times as large (i.e., confidence interval is 2.3 times wider) as it would be if that predictor variable were uncorrelated with the other predictor variables.</p>
<p>VIF = 1: Not correlated<br />
1 &lt; VIF &lt; 5: Moderately correlated<br />
VIF &gt; 5 to 10: Highly correlated (multicollinearity present)</p>
<pre class="r fold-show"><code>vif(multipleRegressionModel)</code></pre>
<pre><code>    bpi_antisocialT1Sum bpi_anxiousDepressedSum 
               1.291545                1.291545 </code></pre>
</div>
<div id="generalized-variance-inflation-factor-gvif" class="section level5" number="13.1.5.1.2">
<h5><span class="header-section-number">13.1.5.1.2</span> Generalized Variance Inflation Factor (GVIF)</h5>
<p>Useful when models have related regressors (multiple polynomial terms or contrasts from same predictor)</p>
</div>
<div id="correlation" class="section level5" number="13.1.5.1.3">
<h5><span class="header-section-number">13.1.5.1.3</span> Correlation</h5>
<p>correlation among all independent variables the correlation coefficients should be smaller than .08</p>
<pre class="r fold-show"><code>cor(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;)</code></pre>
<pre><code>                        bpi_antisocialT1Sum bpi_anxiousDepressedSum
bpi_antisocialT1Sum                1.000000                0.497279
bpi_anxiousDepressedSum            0.497279                1.000000</code></pre>
</div>
<div id="tolerance" class="section level5" number="13.1.5.1.4">
<h5><span class="header-section-number">13.1.5.1.4</span> Tolerance</h5>
<p>The tolerance is an index of the influence of one independent variable on all other independent variables.</p>
<p><span class="math display">\[
\text{tolerance} = 1/\text{VIF}
\]</span></p>
<p>T &lt; 0.2: there might be multicollinearity in the data<br />
T &lt; 0.01: there certainly is multicollinarity in the data</p>
</div>
<div id="condition-index" class="section level5" number="13.1.5.1.5">
<h5><span class="header-section-number">13.1.5.1.5</span> Condition Index</h5>
<p>The condition index is calculated using a factor analysis on the independent variables. Values of 10-30 indicate a mediocre multicollinearity in the regression variables. Values &gt; 30 indicate strong multicollinearity.</p>
<p>For how to interpret, see here: <a href="https://stats.stackexchange.com/a/87610/20338" class="uri">https://stats.stackexchange.com/a/87610/20338</a></p>
<pre class="r fold-show"><code>ols_eigen_cindex(multipleRegressionModel)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Eigenvalue"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Condition Index"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["intercept"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bpi_antisocialT1Sum"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bpi_anxiousDepressedSum"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.4393975","2":"1.000000","3":"0.05150939","4":"0.04435009","5":"0.05806627"},{"1":"0.3575629","2":"2.611951","3":"0.40330814","4":"0.01600701","5":"0.75801976"},{"1":"0.2030395","2":"3.466179","3":"0.54518247","4":"0.93964290","5":"0.18391397"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="ways-to-handle-4" class="section level4" number="13.1.5.2">
<h4><span class="header-section-number">13.1.5.2</span> Ways to Handle</h4>
<ul>
<li>Remove highly correlated (i.e., redundant) predictors</li>
<li>Average the correlated predictors</li>
<li><a href="pca.html">Principal Component Analysis</a> for data reduction</li>
<li>Standardize predictors</li>
<li>Center the data (deduct the mean)</li>
<li>Singular-value decomposition of the model matrix or the mean-centered model matrix</li>
<li>Conduct a <a href="#factorAnalysis.html">factor analysis</a> and rotate the factors to ensure independence of the factors</li>
</ul>
</div>
</div>
<div id="errors-are-normally-distributed" class="section level3" number="13.1.6">
<h3><span class="header-section-number">13.1.6</span> 6. Errors are normally distributed</h3>
<div id="ways-to-test-5" class="section level4" number="13.1.6.1">
<h4><span class="header-section-number">13.1.6.1</span> Ways to Test</h4>
<ul>
<li>Probability Plots
<ul>
<li><a href="#qqPlot">Normal Quantile (QQ) Plots</a> (based on non-cumulative distribution of residuals)</li>
<li><a href="#ppPlot">Normal Probability (PP) Plots</a> (based on cumulative distribution of residuals)</li>
</ul></li>
<li><a href="#densityPlotResiduals">Density Plot of Residuals</a></li>
<li>Statistical Tests
<ul>
<li>Kolmogorov-Smirnov test</li>
<li>Shapiro-Wilk test</li>
<li>Jarque-Bera test</li>
<li>Anderson-Darling test (best test)</li>
</ul></li>
<li>Examine influence of outliers</li>
</ul>
</div>
<div id="ways-to-handle-5" class="section level4" number="13.1.6.2">
<h4><span class="header-section-number">13.1.6.2</span> Ways to Handle</h4>
<ul>
<li>Apply a transformation to the predictor or outcome variable</li>
<li>Exclude outliers</li>
<li>Robust regression
<ul>
<li>Best when no outliers: MM-type regression estimator
<ul>
<li><code>lmrob()</code>/<code>glmrob()</code> function of <code>robustbase</code> package</li>
<li>Iteratively reweighted least squares (IRLS): <code>rlm(, method = "MM")</code> function of <code>MASS</code> package: <a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm" class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a></li>
</ul></li>
<li>Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers
<ul>
<li><code>ltsReg()</code> function of <code>robustbase</code> package (best)</li>
</ul></li>
<li>Best when single predictor: Theil-Sen estimator
<ul>
<li><code>mblm(, repeated = FALSE)</code> function of <code>mblm</code> package</li>
</ul></li>
<li>Robust correlation
<ul>
<li>Spearman’s rho: <code>cor(, method = "spearman")</code></li>
<li>Percentage bend correlation</li>
<li>Minimum vollume ellipsoid</li>
<li>Minimum covariance determinant:</li>
<li>Winsorized correlation</li>
<li>Biweight midcorrelation</li>
</ul></li>
<li>Not great options:
<ul>
<li>Quantile (L1) regression: <code>rq()</code> function of <code>quantreg</code> package</li>
</ul></li>
</ul></li>
</ul>
<div id="transformations-of-outcome-variable" class="section level5" number="13.1.6.2.1">
<h5><span class="header-section-number">13.1.6.2.1</span> Transformations of Outcome Variable</h5>
<div id="box-cox-transformation" class="section level6" number="13.1.6.2.1.1">
<h6><span class="header-section-number">13.1.6.2.1.1</span> Box-Cox Transformation</h6>
<p>Useful if the outcome is strictly positive (or add a constant to outcome to make it strictly positive)</p>
<p>lambda = -1: inverse transformation<br />
lambda = -0.5: 1/sqrt(Y)<br />
lambda = 0: log transformation<br />
lambda = 0.5: square root<br />
lambda = 0.333: cube root<br />
lambda = 1: no transformation<br />
lambda = 2: squared<br />
</p>
<div id="raw-distribution" class="section level7" number="13.1.6.2.1.1.1">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.1</span> Raw distribution</p>
<pre class="r fold-show"><code>plot(density(na.omit(mydata$bpi_antisocialT2Sum)))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
</div>
<div id="add-constant-to-outcome-to-make-it-strictly-positive" class="section level7" number="13.1.6.2.1.1.2">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.2</span> Add constant to outcome to make it strictly positive</p>
<pre class="r fold-show"><code>strictlyPositiveDV &lt;- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude)</code></pre>
</div>
<div id="identify-best-power-transformation-lambda" class="section level7" number="13.1.6.2.1.1.3">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.3</span> Identify best power transformation (lambda)</p>
<p>Consider rounding the power to a common value (square root = .5; cube root = .333; squared = 2)</p>
<pre class="r fold-show"><code>boxCox(strictlyPositiveDV)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r fold-show"><code>powerTransform(strictlyPositiveDV) </code></pre>
<pre><code>Estimated transformation parameter 
      Y1 
0.234032 </code></pre>
<pre class="r fold-show"><code>transformedDV &lt;- powerTransform(strictlyPositiveDV)

summary(transformedDV)</code></pre>
<pre><code>bcPower Transformation to Normality 
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1     0.234        0.23       0.1825       0.2856

Likelihood ratio test that transformation parameter is equal to 0
 (log transformation)
                           LRT df       pval
LR test, lambda = (0) 78.04052  1 &lt; 2.22e-16

Likelihood ratio test that no transformation is needed
                           LRT df       pval
LR test, lambda = (1) 853.8675  1 &lt; 2.22e-16</code></pre>
</div>
<div id="transform-the-dv" class="section level7" number="13.1.6.2.1.1.4">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.4</span> Transform the DV</p>
<pre class="r fold-show"><code>mydata$bpi_antisocialT2SumTransformed &lt;- bcPower(mydata$bpi_antisocialT2Sum + 1, coef(transformedDV))

plot(density(na.omit(mydata$bpi_antisocialT2SumTransformed)))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
</div>
<div id="compare-residuals-from-model-with-and-without-transformation" class="section level7" number="13.1.6.2.1.1.5">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.5</span> Compare residuals from model with and without transformation</p>
<div id="model-without-transformation" class="section level8" number="13.1.6.2.1.1.5.1">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.5.1</span> Model without transformation</p>
<pre class="r fold-show"><code>summary(modelWithoutTransformation &lt;- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude))</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              2.19830    0.05983  36.743  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>plot(density(na.omit(studres(modelWithoutTransformation))))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
</div>
<div id="model-with-transformation" class="section level8" number="13.1.6.2.1.1.5.2">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.5.2</span> Model with transformation</p>
<pre class="r fold-show"><code>summary(modelWithTransformation &lt;- lm(bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      na.action = na.exclude))</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.3429 -0.4868  0.0096  0.4922  2.9799 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)             0.800373   0.022024  36.342  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.165476   0.006841  24.189  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.055910   0.010733   5.209 2.03e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.7283 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.2477,    Adjusted R-squared:  0.2472 
F-statistic: 472.7 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>plot(density(na.omit(studres(modelWithTransformation))))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
</div>
</div>
<div id="constructed-variable-test-plot" class="section level7" number="13.1.6.2.1.1.6">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.6</span> Constructed Variable Test &amp; Plot</p>
<p>A significant <em>p</em>-value indicates a strong need to transform variable:</p>
<pre class="r fold-show"><code>multipleRegressionModel_constructedVariable &lt;- update(multipleRegressionModel, . ~ . + boxCoxVariable(bpi_antisocialT1Sum + 1))
summary(multipleRegressionModel_constructedVariable)$coef[&quot;boxCoxVariable(bpi_antisocialT1Sum + 1)&quot;, , drop = FALSE]</code></pre>
<pre><code>                                           Estimate Std. Error   t value
boxCoxVariable(bpi_antisocialT1Sum + 1) -0.06064732 0.04683415 -1.294938
                                         Pr(&gt;|t|)
boxCoxVariable(bpi_antisocialT1Sum + 1) 0.1954457</code></pre>
<p>Plot allows us to see whether the need for transformation is spread through data or whether it is just dependent on a small fraction of observations:</p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_constructedVariable, &quot;boxCoxVariable(bpi_antisocialT1Sum + 1)&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
</div>
<div id="inverse-response-plot" class="section level7" number="13.1.6.2.1.1.7">
<p class="heading"><span class="header-section-number">13.1.6.2.1.1.7</span> Inverse Response Plot</p>
<p>The black line is the best-fitting power transformation:</p>
<pre class="r fold-show"><code>inverseResponsePlot(strictlyPositiveDV, lambda = c(-1,-0.5,0,1/3,.5,1,2))</code></pre>
<img src="regression_files/figure-html/unnamed-chunk-66-1.png" width="672" />
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["lambda"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.8481126","2":"2940.938"},{"1":"-1.0000000","2":"3329.131"},{"1":"-0.5000000","2":"3189.376"},{"1":"0.0000000","2":"3052.679"},{"1":"0.3333333","2":"2984.003"},{"1":"0.5000000","2":"2960.827"},{"1":"1.0000000","2":"2944.657"},{"1":"2.0000000","2":"3116.590"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="yeo-johnson-transformations" class="section level6" number="13.1.6.2.1.2">
<h6><span class="header-section-number">13.1.6.2.1.2</span> Yeo-Johnson Transformations</h6>
<p>Useful if the outcome is not strictly positive.</p>
<pre class="r fold-show"><code>yjPower(DV, lambda)</code></pre>
</div>
</div>
<div id="transformations-of-predictor-variable" class="section level5" number="13.1.6.2.2">
<h5><span class="header-section-number">13.1.6.2.2</span> Transformations of Predictor Variable</h5>
<div id="component-plus-residual-plots-partial-residual-plots" class="section level6" number="13.1.6.2.2.1">
<h6><span class="header-section-number">13.1.6.2.2.1</span> Component-Plus-Residual Plots (Partial Residual Plots)</h6>
<p>Linear model:</p>
<pre class="r fold-show"><code>crPlots(multipleRegressionModelNoMissing, order = 1)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>Quadratic model:</p>
<pre class="r fold-show"><code>crPlots(multipleRegressionModelNoMissing, order = 2)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<pre class="r fold-show"><code>multipleRegressionModel_quadratic &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + bpi_anxiousDepressedSum,
                                        data = mydata,
                                        na.action = na.exclude)

summary(multipleRegressionModel_quadratic)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6238 -1.2610 -0.2484  0.9923 12.9008 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               1.099237   0.076509  14.367  &lt; 2e-16 ***
bpi_antisocialT1Sum       0.547680   0.043723  12.526  &lt; 2e-16 ***
I(bpi_antisocialT1Sum^2) -0.010221   0.004925  -2.075    0.038 *  
bpi_anxiousDepressedSum   0.161734   0.029144   5.549 3.13e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.977 on 2870 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.2631,    Adjusted R-squared:  0.2623 
F-statistic: 341.5 on 3 and 2870 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>anova(multipleRegressionModel_quadratic, multipleRegressionModel)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Sum of Sq"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2870","2":"11223.01","3":"NA","4":"NA","5":"NA","6":"NA","_rn_":"1"},{"1":"2871","2":"11239.86","3":"-1","4":"-16.84449","5":"4.307551","6":"0.03803241","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>crPlots(multipleRegressionModel_quadratic, order = 1)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
</div>
<div id="ceres-plot-combining-conditional-expectations-and-residuals" class="section level6" number="13.1.6.2.2.2">
<h6><span class="header-section-number">13.1.6.2.2.2</span> CERES Plot (Combining conditional Expectations and RESiduals)</h6>
<p>Useful when nonlinear associations among the predictors are very strong (component-plus-residual plots may appear nonlinear even though the true partial regression is linear, a phenonomen called leakage)</p>
<pre class="r fold-show"><code>ceresPlots(multipleRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<pre class="r fold-show"><code>ceresPlots(multipleRegressionModel_quadratic)</code></pre>
<pre><code>Warning in ceresPlots.default(multipleRegressionModel_quadratic): Factors
skipped in drawing CERES plots.</code></pre>
<pre><code>Warning in min(x): no non-missing arguments to min; returning Inf</code></pre>
<pre><code>Warning in max(x): no non-missing arguments to max; returning -Inf</code></pre>
<pre><code>Error in plot.window(...): need finite &#39;ylim&#39; values</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-71-2.png" width="672" /></p>
</div>
<div id="box-tidwell-method-for-choosing-predictor-transformations" class="section level6" number="13.1.6.2.2.3">
<h6><span class="header-section-number">13.1.6.2.2.3</span> Box-Tidwell Method for Choosing Predictor Transformations</h6>
<p>predictors must be strictly positive (or add a constant to make it strictly positive)</p>
<pre class="r fold-show"><code>boxTidwell(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1),
           other.x = NULL, #list variables not to be transformed in other.x
           data = mydata,
           na.action = na.exclude)</code></pre>
<pre><code>                               MLE of lambda Score Statistic (z) Pr(&gt;|z|)
I(bpi_antisocialT1Sum + 1)           0.90820             -1.0591   0.2896
I(bpi_anxiousDepressedSum + 1)       0.36689             -1.0650   0.2869

iterations =  4 </code></pre>
</div>
<div id="constructed-variables-plot" class="section level6" number="13.1.6.2.2.4">
<h6><span class="header-section-number">13.1.6.2.2.4</span> Constructed-Variables Plot</h6>
<pre class="r fold-show"><code>multipleRegressionModel_cv &lt;- lm(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1) + I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) + I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)),
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel_cv)$coef[&quot;I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))&quot;, , drop = FALSE]</code></pre>
<pre><code>                                                    Estimate Std. Error
I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -0.1691003 0.06887107
                                                    t value   Pr(&gt;|t|)
I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -2.455317 0.01418393</code></pre>
<pre class="r fold-show"><code>summary(multipleRegressionModel_cv)$coef[&quot;I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))&quot;, , drop = FALSE]</code></pre>
<pre><code>                                                            Estimate Std. Error
I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.00828867  0.1384918
                                                             t value  Pr(&gt;|t|)
I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.05984953 0.9522831</code></pre>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_cv, &quot;I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_cv, &quot;I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-73-2.png" width="672" /></p>
</div>
</div>
<div id="robust-models" class="section level5" number="13.1.6.2.3">
<h5><span class="header-section-number">13.1.6.2.3</span> Robust models</h5>
<p>Resources</p>
<ul>
<li>For comparison of methods, see Book: “Modern Methods for Robust Regression”</li>
<li><a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm" class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a></li>
<li><a href="http://stats.stackexchange.com/a/46234/20338" class="uri">http://stats.stackexchange.com/a/46234/20338</a></li>
<li><a href="http://cran.r-project.org/web/views/Robust.html" class="uri">http://cran.r-project.org/web/views/Robust.html</a></li>
</ul>
<div id="robust-correlation" class="section level6" number="13.1.6.2.3.1">
<h6><span class="header-section-number">13.1.6.2.3.1</span> Robust correlation</h6>
<div id="spearmans-rho" class="section level7" number="13.1.6.2.3.1.1">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.1</span> Spearman’s rho</p>
<p>Spearman’s rho is a non-parametric correlation.</p>
<pre class="r fold-show"><code>cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum) #Pearson r, correlation that is sensitive to outliers</code></pre>
<pre><code>
    Pearson&#39;s product-moment correlation

data:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum
t = 31.274, df = 2873, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.4761758 0.5307381
sample estimates:
      cor 
0.5039595 </code></pre>
<pre class="r fold-show"><code>cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum, method = &quot;spearman&quot;) #Spearman&#39;s rho, a rank correlation that is less sensitive to outliers</code></pre>
<pre><code>Warning in cor.test.default(mydata$bpi_antisocialT1Sum,
mydata$bpi_antisocialT2Sum, : Cannot compute exact p-value with ties</code></pre>
<pre><code>
    Spearman&#39;s rank correlation rho

data:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum
S = 1997347186, p-value &lt; 2.2e-16
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.4956973 </code></pre>
</div>
<div id="minimum-vollume-ellipsoid" class="section level7" number="13.1.6.2.3.1.2">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.2</span> Minimum vollume ellipsoid</p>
<pre class="r fold-show"><code>cov.mve(na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)]), cor = TRUE)</code></pre>
<pre><code>$center
bpi_antisocialT1Sum bpi_antisocialT2Sum 
           2.056773            1.952882 

$cov
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum           2.1044887           0.8162201
bpi_antisocialT2Sum           0.8162201           2.1211668

$msg
[1] &quot;146 singular samples of size 3 out of 1500&quot;

$crit
[1] 3.79572

$best
   [1]    1    3    8    9   10   12   13   14   15   16   17   18   19   21
  [15]   22   23   24   39   40   43   44   46   48   50   55   57   60   63
  [29]   65   67   69   70   76   79   80   81   82   86   87   88   95  102
  [43]  104  105  106  107  108  119  120  121  122  123  124  129  130  131
  [57]  132  133  134  138  139  140  141  142  143  145  148  149  150  153
  [71]  154  155  160  166  168  169  170  176  177  178  179  180  181  182
  [85]  183  184  190  191  192  194  196  197  198  200  201  202  205  206
  [99]  207  209  212  214  227  228  231  233  234  235  238  239  240  241
 [113]  242  243  244  245  246  247  249  251  254  255  256  262  265  269
 [127]  272  275  281  282  284  289  290  291  293  295  296  297  298  299
 [141]  300  301  306  307  308  310  313  315  329  330  331  332  333  334
 [155]  335  339  341  342  343  345  346  350  355  356  357  359  364  367
 [169]  370  371  376  379  380  381  382  386  387  388  389  390  391  397
 [183]  399  401  403  404  407  408  409  410  411  413  416  421  423  425
 [197]  426  430  431  432  435  437  438  440  441  442  443  445  448  449
 [211]  450  451  453  456  458  459  461  462  463  464  466  467  468  469
 [225]  471  473  474  476  477  478  480  482  483  484  486  493  494  495
 [239]  496  497  498  499  500  501  502  503  504  505  506  507  509  510
 [253]  513  518  519  520  521  522  523  524  525  527  528  529  531  532
 [267]  533  534  535  536  537  538  539  541  543  546  550  553  554  557
 [281]  558  562  564  566  568  572  575  578  579  583  585  599  600  601
 [295]  605  608  609  610  615  616  618  619  620  621  622  623  625  627
 [309]  628  631  633  634  635  636  637  639  641  642  643  646  647  648
 [323]  649  650  657  662  667  670  672  673  674  675  676  679  680  681
 [337]  682  683  685  686  687  690  691  693  696  701  704  705  706  707
 [351]  713  714  715  717  718  719  720  722  723  728  730  732  733  734
 [365]  736  737  740  741  742  743  744  745  746  747  751  752  758  762
 [379]  763  764  765  769  774  776  779  780  782  783  786  789  791  798
 [393]  799  801  802  803  805  806  808  809  810  811  812  814  817  818
 [407]  819  822  824  826  827  828  832  834  838  839  840  842  843  847
 [421]  848  849  852  855  857  858  859  860  861  862  865  866  867  868
 [435]  869  871  872  873  874  877  880  881  882  888  889  892  893  895
 [449]  898  899  900  901  902  903  906  907  909  912  913  914  918  920
 [463]  921  923  925  926  927  928  931  932  933  934  935  936  937  939
 [477]  940  941  943  944  945  946  949  950  951  954  955  956  957  959
 [491]  962  964  965  967  970  973  974  975  976  977  980  983  985  987
 [505]  988  990  992  995  998 1000 1001 1002 1003 1005 1006 1007 1008 1013
 [519] 1014 1015 1016 1017 1019 1021 1022 1023 1024 1025 1026 1027 1029 1030
 [533] 1032 1034 1036 1037 1041 1043 1044 1046 1047 1049 1050 1051 1053 1054
 [547] 1055 1057 1058 1059 1061 1064 1066 1067 1070 1072 1075 1079 1083 1086
 [561] 1087 1088 1091 1094 1095 1097 1101 1105 1109 1111 1112 1113 1114 1115
 [575] 1119 1121 1123 1124 1128 1129 1135 1136 1139 1144 1145 1146 1147 1148
 [589] 1149 1150 1152 1160 1162 1163 1164 1165 1171 1176 1177 1178 1179 1180
 [603] 1181 1182 1184 1185 1186 1187 1188 1189 1190 1191 1197 1198 1200 1201
 [617] 1203 1205 1206 1207 1208 1209 1214 1215 1216 1217 1218 1219 1220 1221
 [631] 1223 1224 1229 1230 1231 1232 1233 1234 1235 1242 1244 1245 1246 1247
 [645] 1249 1250 1251 1254 1255 1261 1262 1263 1265 1266 1267 1269 1271 1272
 [659] 1273 1274 1275 1276 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289
 [673] 1290 1291 1292 1293 1294 1296 1299 1301 1304 1306 1309 1310 1311 1312
 [687] 1314 1315 1316 1318 1319 1322 1323 1325 1326 1333 1334 1336 1337 1338
 [701] 1339 1341 1343 1344 1347 1348 1349 1350 1352 1353 1354 1355 1356 1357
 [715] 1358 1359 1360 1361 1362 1363 1365 1366 1367 1368 1369 1370 1371 1372
 [729] 1373 1374 1376 1377 1380 1381 1382 1383 1387 1388 1389 1390 1391 1395
 [743] 1396 1397 1398 1399 1400 1401 1402 1405 1408 1412 1413 1414 1417 1419
 [757] 1421 1422 1423 1424 1426 1428 1430 1432 1433 1434 1438 1439 1440 1444
 [771] 1447 1448 1449 1456 1458 1464 1467 1468 1469 1470 1471 1483 1485 1486
 [785] 1489 1495 1499 1500 1502 1504 1506 1508 1513 1514 1515 1516 1517 1518
 [799] 1519 1521 1522 1524 1525 1527 1531 1532 1533 1534 1535 1536 1537 1540
 [813] 1541 1542 1545 1546 1547 1549 1551 1553 1554 1560 1561 1563 1565 1571
 [827] 1572 1575 1576 1577 1583 1584 1585 1586 1588 1589 1590 1591 1593 1594
 [841] 1595 1596 1597 1598 1600 1602 1604 1606 1611 1612 1613 1614 1617 1621
 [855] 1623 1624 1625 1626 1627 1634 1640 1641 1644 1652 1656 1657 1661 1664
 [869] 1665 1672 1674 1675 1686 1690 1691 1692 1694 1699 1701 1705 1706 1709
 [883] 1710 1711 1712 1714 1716 1718 1719 1721 1722 1723 1724 1731 1732 1738
 [897] 1739 1742 1750 1753 1754 1757 1759 1760 1761 1763 1764 1765 1772 1774
 [911] 1775 1776 1777 1778 1781 1791 1795 1797 1798 1801 1802 1803 1804 1806
 [925] 1807 1809 1810 1812 1813 1815 1818 1821 1824 1827 1834 1835 1836 1837
 [939] 1842 1846 1847 1848 1849 1850 1851 1852 1855 1858 1859 1860 1861 1863
 [953] 1864 1866 1867 1868 1870 1881 1884 1885 1888 1890 1892 1902 1904 1910
 [967] 1921 1922 1923 1924 1925 1926 1927 1932 1933 1935 1936 1938 1940 1941
 [981] 1943 1947 1948 1950 1951 1954 1955 1956 1957 1960 1961 1962 1963 1966
 [995] 1969 1972 1973 1974 1979 1981 1984 1986 1988 1990 1992 1993 1994 1999
[1009] 2002 2005 2006 2011 2017 2019 2021 2022 2027 2029 2032 2034 2036 2038
[1023] 2042 2045 2051 2053 2054 2059 2061 2062 2063 2068 2069 2070 2071 2073
[1037] 2074 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089
[1051] 2096 2097 2098 2099 2100 2101 2102 2104 2105 2108 2110 2112 2113 2114
[1065] 2116 2118 2119 2123 2124 2125 2126 2127 2129 2130 2137 2138 2139 2143
[1079] 2151 2157 2158 2160 2161 2163 2166 2167 2168 2169 2171 2174 2175 2176
[1093] 2177 2178 2179 2180 2181 2183 2185 2189 2190 2191 2200 2201 2202 2203
[1107] 2205 2206 2208 2210 2212 2215 2219 2221 2222 2223 2225 2226 2227 2228
[1121] 2231 2232 2234 2236 2237 2238 2239 2241 2242 2245 2248 2249 2253 2255
[1135] 2258 2259 2260 2261 2262 2264 2269 2271 2272 2273 2274 2275 2280 2283
[1149] 2285 2290 2291 2298 2299 2301 2302 2305 2306 2307 2311 2314 2317 2319
[1163] 2322 2323 2324 2325 2326 2329 2330 2331 2332 2334 2336 2337 2338 2341
[1177] 2342 2343 2344 2348 2349 2350 2351 2352 2354 2356 2359 2364 2368 2369
[1191] 2371 2373 2377 2379 2380 2384 2385 2386 2388 2398 2400 2402 2410 2411
[1205] 2414 2415 2416 2418 2424 2425 2428 2429 2437 2442 2443 2444 2445 2446
[1219] 2447 2448 2449 2450 2451 2452 2453 2454 2462 2463 2464 2465 2466 2467
[1233] 2469 2472 2473 2475 2477 2481 2483 2485 2489 2490 2491 2492 2495 2496
[1247] 2497 2498 2499 2500 2503 2504 2506 2507 2508 2509 2510 2511 2512 2513
[1261] 2522 2523 2525 2526 2527 2529 2530 2531 2532 2533 2535 2536 2539 2541
[1275] 2543 2545 2546 2547 2554 2561 2562 2563 2564 2565 2566 2570 2572 2573
[1289] 2575 2577 2578 2581 2583 2585 2588 2590 2591 2597 2598 2599 2603 2604
[1303] 2605 2606 2607 2608 2613 2614 2615 2617 2618 2619 2620 2621 2627 2634
[1317] 2635 2636 2638 2639 2640 2641 2642 2643 2644 2645 2647 2653 2655 2657
[1331] 2658 2659 2663 2664 2667 2668 2670 2672 2673 2674 2676 2677 2678 2679
[1345] 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698 2700 2701
[1359] 2702 2703 2704 2707 2709 2713 2714 2717 2718 2721 2722 2723 2727 2728
[1373] 2729 2731 2733 2736 2738 2739 2740 2741 2742 2746 2748 2753 2757 2759
[1387] 2760 2763 2764 2765 2766 2771 2776 2777 2778 2781 2783 2786 2788 2789
[1401] 2790 2792 2797 2799 2800 2803 2804 2808 2809 2810 2813 2814 2817 2819
[1415] 2821 2826 2831 2832 2834 2838 2839 2844 2846 2847 2848 2849 2850 2851
[1429] 2852 2855 2856 2857 2862 2863 2868 2870 2871 2872 2873

$cor
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum           1.0000000           0.3863195
bpi_antisocialT2Sum           0.3863195           1.0000000

$n.obs
[1] 2875</code></pre>
</div>
<div id="minimum-covariance-determinant" class="section level7" number="13.1.6.2.3.1.3">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.3</span> Minimum covariance determinant</p>
<pre class="r fold-show"><code>cov.mcd(na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)]), cor = TRUE)</code></pre>
<pre><code>$center
bpi_antisocialT1Sum bpi_antisocialT2Sum 
           2.181181            2.094954 

$cov
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum            2.379036            1.096437
bpi_antisocialT2Sum            1.096437            2.482409

$msg
[1] &quot;143 singular samples of size 3 out of 1500&quot;

$crit
[1] -0.3817849

$best
   [1]    1    3    8    9   10   12   13   14   15   16   17   18   19   21
  [15]   22   23   24   39   40   42   43   44   46   48   50   55   56   57
  [29]   60   61   63   65   67   69   70   76   79   80   81   82   86   87
  [43]   88   93   95   97   98   99  102  104  105  106  107  108  116  117
  [57]  119  120  121  122  123  124  129  130  131  132  133  134  138  139
  [71]  140  141  142  143  145  148  149  150  153  154  155  160  161  166
  [85]  168  169  170  176  177  178  179  180  181  182  183  184  187  188
  [99]  190  191  192  194  195  196  197  198  200  201  202  205  206  207
 [113]  209  212  213  214  227  228  231  232  233  234  235  237  238  239
 [127]  240  241  242  243  244  245  246  247  249  251  253  254  255  256
 [141]  261  262  265  268  269  272  275  281  282  284  289  290  291  293
 [155]  294  295  296  297  298  299  300  301  304  307  308  310  313  315
 [169]  329  330  331  332  333  334  335  339  341  342  343  345  346  350
 [183]  355  356  357  359  364  367  370  371  376  379  380  381  382  386
 [197]  387  388  389  390  391  397  399  401  403  404  407  408  409  410
 [211]  411  413  416  421  422  423  425  426  430  431  432  435  437  438
 [225]  440  441  442  443  445  448  449  450  451  453  455  456  458  459
 [239]  461  462  463  464  466  467  468  469  471  473  474  476  477  478
 [253]  480  482  483  484  486  493  494  495  496  497  498  499  500  501
 [267]  502  503  504  505  506  507  509  510  513  518  519  520  521  522
 [281]  523  524  525  527  528  529  531  532  533  534  535  536  537  538
 [295]  539  541  543  545  546  550  553  554  557  558  562  564  566  568
 [309]  572  575  578  579  582  583  585  599  600  601  605  608  609  610
 [323]  615  616  617  618  619  620  621  622  623  625  626  627  628  631
 [337]  633  634  635  636  637  639  641  642  643  646  647  648  649  650
 [351]  657  660  662  667  670  672  673  674  675  676  679  680  681  682
 [365]  683  685  686  687  690  691  693  696  701  704  705  706  707  710
 [379]  713  714  715  717  718  719  720  721  722  723  728  730  732  733
 [393]  734  736  737  740  741  742  743  744  745  746  747  751  752  758
 [407]  762  763  764  765  769  772  774  776  779  780  782  783  786  789
 [421]  791  798  799  801  802  803  805  806  808  809  810  811  812  813
 [435]  814  817  818  819  822  824  826  827  828  832  834  836  837  838
 [449]  839  840  842  843  847  848  849  852  855  857  858  859  860  861
 [463]  862  865  866  867  868  869  871  872  873  874  877  880  881  882
 [477]  888  889  892  893  895  898  899  900  901  902  903  905  906  907
 [491]  909  912  913  914  918  920  921  923  925  926  927  928  931  932
 [505]  933  934  935  936  937  939  940  941  943  944  945  946  949  950
 [519]  951  954  955  956  957  959  960  962  964  965  967  970  972  973
 [533]  974  975  976  977  980  983  985  987  988  990  992  995  998 1000
 [547] 1001 1002 1003 1004 1005 1006 1007 1008 1013 1014 1015 1016 1017 1019
 [561] 1021 1022 1023 1024 1025 1026 1027 1029 1030 1032 1034 1036 1037 1041
 [575] 1043 1044 1046 1047 1049 1050 1051 1053 1054 1055 1057 1058 1059 1061
 [589] 1064 1066 1067 1070 1072 1075 1076 1079 1083 1086 1087 1088 1091 1094
 [603] 1095 1097 1101 1105 1109 1111 1112 1113 1114 1115 1119 1121 1123 1124
 [617] 1125 1128 1129 1132 1135 1136 1138 1139 1144 1145 1146 1147 1148 1149
 [631] 1150 1152 1154 1158 1160 1162 1163 1164 1165 1171 1176 1177 1178 1179
 [645] 1180 1181 1182 1184 1185 1186 1187 1188 1189 1190 1191 1197 1198 1200
 [659] 1201 1203 1205 1206 1207 1208 1209 1214 1215 1216 1217 1218 1219 1220
 [673] 1221 1223 1224 1229 1230 1231 1232 1233 1234 1235 1242 1243 1244 1245
 [687] 1246 1247 1248 1249 1250 1251 1254 1255 1259 1261 1262 1263 1265 1266
 [701] 1267 1269 1271 1272 1273 1274 1275 1276 1280 1281 1282 1283 1284 1285
 [715] 1286 1287 1288 1289 1290 1291 1292 1293 1294 1296 1299 1301 1302 1303
 [729] 1304 1306 1309 1310 1311 1312 1314 1315 1316 1318 1319 1320 1322 1323
 [743] 1325 1326 1333 1334 1336 1337 1338 1339 1341 1343 1344 1347 1348 1349
 [757] 1350 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1365
 [771] 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1380 1381
 [785] 1382 1383 1387 1388 1389 1390 1391 1395 1396 1397 1398 1399 1400 1401
 [799] 1402 1405 1408 1412 1413 1414 1417 1419 1421 1422 1423 1424 1426 1428
 [813] 1430 1432 1433 1434 1438 1439 1440 1444 1445 1447 1448 1449 1456 1458
 [827] 1459 1464 1467 1468 1469 1470 1471 1483 1485 1486 1489 1495 1499 1500
 [841] 1502 1504 1505 1506 1508 1513 1514 1515 1516 1517 1518 1519 1521 1522
 [855] 1524 1525 1527 1531 1532 1533 1534 1535 1536 1537 1539 1540 1541 1542
 [869] 1543 1544 1545 1546 1547 1548 1549 1551 1553 1554 1560 1561 1563 1565
 [883] 1569 1571 1572 1575 1576 1577 1583 1584 1585 1586 1588 1589 1590 1591
 [897] 1592 1593 1594 1595 1596 1597 1598 1600 1602 1604 1605 1606 1611 1612
 [911] 1613 1614 1617 1621 1623 1624 1625 1626 1627 1634 1640 1641 1644 1652
 [925] 1656 1657 1661 1664 1665 1672 1674 1675 1686 1687 1689 1690 1691 1692
 [939] 1694 1699 1701 1705 1706 1708 1709 1710 1711 1712 1714 1716 1718 1719
 [953] 1721 1722 1723 1724 1731 1732 1738 1739 1742 1750 1753 1754 1757 1759
 [967] 1760 1761 1763 1764 1765 1766 1772 1774 1775 1776 1777 1778 1781 1791
 [981] 1795 1797 1798 1801 1802 1803 1804 1806 1807 1809 1810 1812 1813 1815
 [995] 1817 1818 1821 1824 1827 1834 1835 1836 1837 1840 1842 1846 1847 1848
[1009] 1849 1850 1851 1852 1855 1858 1859 1860 1861 1863 1864 1866 1867 1868
[1023] 1870 1881 1884 1885 1888 1890 1892 1902 1904 1910 1915 1921 1922 1923
[1037] 1924 1925 1926 1927 1932 1933 1935 1936 1938 1940 1941 1943 1947 1948
[1051] 1950 1951 1954 1955 1956 1957 1960 1961 1962 1963 1966 1967 1969 1970
[1065] 1971 1972 1973 1974 1979 1981 1984 1986 1988 1990 1992 1993 1994 1995
[1079] 1999 2002 2005 2006 2009 2011 2012 2017 2019 2021 2022 2027 2029 2032
[1093] 2034 2036 2038 2042 2043 2045 2049 2051 2053 2054 2055 2059 2061 2062
[1107] 2063 2068 2069 2070 2071 2073 2074 2077 2078 2079 2080 2081 2082 2083
[1121] 2084 2085 2086 2087 2088 2089 2094 2096 2097 2098 2099 2100 2101 2102
[1135] 2104 2105 2108 2110 2112 2113 2114 2116 2117 2118 2119 2123 2124 2125
[1149] 2126 2127 2129 2130 2131 2137 2138 2139 2143 2145 2151 2157 2158 2160
[1163] 2161 2163 2166 2167 2168 2169 2171 2174 2175 2176 2177 2178 2179 2180
[1177] 2181 2183 2185 2188 2189 2190 2191 2200 2201 2202 2203 2205 2206 2207
[1191] 2208 2210 2212 2215 2219 2221 2222 2223 2225 2226 2227 2228 2231 2232
[1205] 2234 2236 2237 2238 2239 2241 2242 2245 2248 2249 2253 2255 2257 2258
[1219] 2259 2260 2261 2262 2264 2269 2271 2272 2273 2274 2275 2280 2283 2285
[1233] 2290 2291 2297 2298 2299 2301 2302 2305 2306 2307 2311 2314 2316 2317
[1247] 2319 2320 2322 2323 2324 2325 2326 2328 2329 2330 2331 2332 2334 2336
[1261] 2337 2338 2340 2341 2342 2343 2344 2345 2348 2349 2350 2351 2352 2354
[1275] 2356 2359 2362 2364 2367 2368 2369 2371 2373 2377 2378 2379 2380 2384
[1289] 2385 2386 2388 2398 2400 2402 2410 2411 2414 2415 2416 2418 2420 2424
[1303] 2425 2428 2429 2437 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451
[1317] 2452 2453 2454 2462 2463 2464 2465 2466 2467 2469 2472 2473 2475 2477
[1331] 2481 2483 2485 2489 2490 2491 2492 2495 2496 2497 2498 2499 2500 2503
[1345] 2504 2506 2507 2508 2509 2510 2511 2512 2513 2520 2522 2523 2525 2526
[1359] 2527 2528 2529 2530 2531 2532 2533 2535 2536 2539 2541 2543 2545 2546
[1373] 2547 2554 2561 2562 2563 2564 2565 2566 2570 2571 2572 2573 2575 2577
[1387] 2578 2581 2582 2583 2584 2585 2588 2590 2591 2597 2598 2599 2603 2604
[1401] 2605 2606 2607 2608 2613 2614 2615 2617 2618 2619 2620 2621 2627 2634
[1415] 2635 2636 2638 2639 2640 2641 2642 2643 2644 2645 2647 2653 2655 2656
[1429] 2657 2658 2659 2663 2664 2667 2668 2669 2670 2672 2673 2674 2676 2677
[1443] 2678 2679 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698
[1457] 2700 2701 2702 2703 2704 2707 2709 2711 2713 2714 2717 2718 2721 2722
[1471] 2723 2724 2727 2728 2729 2731 2733 2736 2738 2739 2740 2741 2742 2746
[1485] 2748 2753 2757 2759 2760 2763 2764 2765 2766 2771 2773 2776 2777 2778
[1499] 2781 2783 2784 2786 2788 2789 2790 2792 2797 2799 2800 2803 2804 2806
[1513] 2807 2808 2809 2810 2813 2814 2817 2819 2821 2825 2826 2831 2832 2834
[1527] 2838 2839 2844 2846 2847 2848 2849 2850 2851 2852 2855 2856 2857 2862
[1541] 2863 2868 2869 2870 2871 2872 2873

$cor
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum           1.0000000           0.4511764
bpi_antisocialT2Sum           0.4511764           1.0000000

$n.obs
[1] 2875</code></pre>
</div>
<div id="winsorized-correlation" class="section level7" number="13.1.6.2.3.1.4">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.4</span> Winsorized correlation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], winsorize = 0.2, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["int"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4651836","4":"0.95","5":"0.4360424","6":"0.4933503","7":"28.16721","8":"2873","9":"2.369015e-154","10":"Winsorized Pearson correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="percentage-bend-correlation" class="section level7" number="13.1.6.2.3.1.5">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.5</span> Percentage bend correlation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;percentage&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4837302","4":"0.95","5":"0.4552238","6":"0.511246","7":"29.62478","8":"2873","9":"1.536055e-168","10":"Percentage Bend correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="biweight-midcorrelation" class="section level7" number="13.1.6.2.3.1.6">
<p class="heading"><span class="header-section-number">13.1.6.2.3.1.6</span> Biweight midcorrelation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;biweight&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4792883","4":"0.95","5":"0.465104","6":"0.4932265","7":"58.63389","8":"2873","9":"0","10":"Biweight correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="robust-regression-with-a-single-predictor" class="section level6" number="13.1.6.2.3.2">
<h6><span class="header-section-number">13.1.6.2.3.2</span> Robust regression with a single predictor</h6>
<div id="theil-sen-estimator" class="section level7" number="13.1.6.2.3.2.1">
<p class="heading"><span class="header-section-number">13.1.6.2.3.2.1</span> Theil-Sen estimator</p>
<p>The Theil-Sen single median estimator is robust to outliers; have to remove missing values first</p>
<pre class="r fold-show"><code>mydata_subset &lt;- na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)])[1:400,]
mblm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, repeated = FALSE)</code></pre>
<pre><code>
Call:
mblm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, 
    repeated = FALSE)

Coefficients:
        (Intercept)  bpi_antisocialT1Sum  
                1.0                  0.6  </code></pre>
</div>
</div>
<div id="robust-multiple-regression" class="section level6" number="13.1.6.2.3.3">
<h6><span class="header-section-number">13.1.6.2.3.3</span> Robust multiple regression</h6>
<p>Best when no outliers: MM-type regression estimator</p>
<div id="robust-linear-regression" class="section level7" number="13.1.6.2.3.3.1">
<p class="heading"><span class="header-section-number">13.1.6.2.3.3.1</span> Robust linear regression</p>
<p>MM-type regression estimator (best):</p>
<pre class="r fold-show"><code>lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)</code></pre>
<pre><code>
Call:
lmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,     data = mydata, na.action = na.exclude)
 \--&gt; method = &quot;MM&quot;
Coefficients:
            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                 0.9440                   0.4914                   0.1577  </code></pre>
<p>Iteratively reweighted least squares (IRLS): <a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm" class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a></p>
<pre class="r fold-show"><code>rlm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    na.action = na.exclude)</code></pre>
<pre><code>Call:
rlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)
Converged in 6 iterations

Coefficients:
            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              0.9838033               0.4873118               0.1620020 

Degrees of freedom: 2874 total; 2871 residual
  (8656 observations deleted due to missingness)
Scale estimate: 1.62 </code></pre>
</div>
<div id="robust-generalized-regression-1" class="section level7" number="13.1.6.2.3.3.2">
<p class="heading"><span class="header-section-number">13.1.6.2.3.3.2</span> Robust generalized regression</p>
<pre class="r fold-show"><code>glmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    family = &quot;poisson&quot;,
    na.action = &quot;na.exclude&quot;)</code></pre>
<pre><code>
Call:  glmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +      bpi_anxiousDepressedSum, family = &quot;poisson&quot;, data = mydata,      na.action = &quot;na.exclude&quot;) 

Coefficients:
            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                0.37281                  0.15589                  0.05026  

Number of observations: 2874 
Fitted by method  &#39;Mqle&#39; </code></pre>
<p>Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers</p>
<pre class="r fold-show"><code>ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
       data = mydata,
       na.action = na.exclude)</code></pre>
<pre><code>
Call:
ltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +     bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Coefficients:
              Intercept      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                 0.7622                   0.5501                   0.1048  

Scale estimate 1.758 </code></pre>
<div id="not-great-options" class="section level8" number="13.1.6.2.3.3.2.1">
<p class="heading"><span class="header-section-number">13.1.6.2.3.3.2.1</span> Not great options:</p>
<p>Quantile (L1) regression: <code>rq()</code> function of quantreg package</p>
<pre class="r fold-show"><code>rq(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)</code></pre>
<pre><code>Call:
rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Coefficients:
            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   0.88                    0.52                    0.12 

Degrees of freedom: 11530 total; 11527 residual</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="examining-model-assumptions" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Examining Model Assumptions</h2>
<div id="distribution-of-residuals" class="section level3" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Distribution of Residuals</h3>
<div id="qqPlot" class="section level4" number="13.2.1.1">
<h4><span class="header-section-number">13.2.1.1</span> QQ plots</h4>
<p><a href="http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html" class="uri">http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html</a></p>
<pre class="r fold-show"><code>qqPlot(multipleRegressionModel, main = &quot;QQ Plot&quot;, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<pre><code>[1] 8955 8956</code></pre>
<pre class="r fold-show"><code>qqnorm(resid(multipleRegressionModel))
qqnorm(resid(rmsMultipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-2.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(robustLinearRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-3.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(ltsRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-4.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(generalizedRegressionModel))
qqnorm(resid(rmsGeneralizedRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-5.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(robustGeneralizedRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-86-6.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(multilevelRegressionModel))</code></pre>
<pre><code>Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;resid&#39;: object &#39;multilevelRegressionModel&#39; not found</code></pre>
</div>
<div id="ppPlot" class="section level4" number="13.2.1.2">
<h4><span class="header-section-number">13.2.1.2</span> PP plots</h4>
<pre class="r fold-show"><code>ppPlot(multipleRegressionModel)
ppPlot(rmsMultipleRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(robustLinearRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-87-2.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(ltsRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-87-3.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(generalizedRegressionModel)
ppPlot(rmsGeneralizedRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-87-4.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(robustGeneralizedRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-87-5.png" width="672" /></p>
</div>
<div id="densityPlotResiduals" class="section level4" number="13.2.1.3">
<h4><span class="header-section-number">13.2.1.3</span> Density Plot of Residuals</h4>
<pre class="r fold-show"><code>studentizedResiduals &lt;- na.omit(rstudent(multipleRegressionModel))
plot(density(studentizedResiduals), col=&quot;red&quot;)
xfit &lt;- seq(min(studentizedResiduals, na.rm=TRUE), max(studentizedResiduals, na.rm=TRUE), length=40)
lines(xfit, dnorm(xfit), col=&quot;gray&quot;) #compare to normal distribution</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
</div>
</div>
<div id="residualPlots" class="section level3" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Residual Plots</h3>
<p>Residual plots are plots of the residuals versus observed/fitted values.</p>
<p>Includes plots of a) model residuals versus observed values on predictors and b) model residuals versus model fitted values.</p>
<p>Note: have to remove <code>na.action = na.exclude</code></p>
<p>Tests include:</p>
<ul>
<li>lack-of-fit test for every numeric predictor, t-test for the regressor, added to the model, indicating no lack-of-fit for this type</li>
<li>Tukey’s test for nonadditivity: adding the squares of the fitted values to the model and refitting (evaluates adequacy of model fit)</li>
</ul>
<pre class="r fold-show"><code>residualPlots(multipleRegressionModelNoMissing, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<pre><code>                        Test stat Pr(&gt;|Test stat|)  
bpi_antisocialT1Sum       -2.0755          0.03803 *
bpi_anxiousDepressedSum   -1.2769          0.20175  
Tukey test                -2.3351          0.01954 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="marginalModelPlots" class="section level3" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Marginal Model Plots</h3>
<p>Marginal model plots are plots of the outcome versus predictors/fitted values.</p>
<p>Includes plots of a) observed outcome values versus values on predictors (ignoring the other predictors) and b) observed outcome values versus model fitted values.</p>
<pre class="r fold-show"><code>marginalModelPlots(multipleRegressionModel, sd = TRUE, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
</div>
<div id="addedVariablePlots" class="section level3" number="13.2.4">
<h3><span class="header-section-number">13.2.4</span> Added-Variable Plots</h3>
<p>Added-variable plots are plots of the partial association between the outcome and each predictor, controlling for all other predictors.</p>
<p>Useful for identifying jointly influential observations and studying the impact of observations on the regression coefficients.</p>
<p>y-axis: residuals from model with all predictors excluding the predictor of interest</p>
<p>x-axis: residuals from model regressing predictor of interest on all other predictors</p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<div id="refit-model-removing-jointly-influential-observations" class="section level4" number="13.2.4.1">
<h4><span class="header-section-number">13.2.4.1</span> Refit model removing jointly influential observations</h4>
<pre class="r fold-show"><code>multipleRegressionModel_removeJointlyInfluentialObs &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                                          data = mydata,
                                                          na.action = na.exclude,
                                                          subset = -c(6318,6023,4022,4040))

avPlots(multipleRegressionModel_removeJointlyInfluentialObs, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<pre class="r fold-show"><code>compareCoefs(multipleRegressionModel, multipleRegressionModel_removeJointlyInfluentialObs)</code></pre>
<pre><code>Calls:
1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)
2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(6318, 6023, 4022, 4040),
   na.action = na.exclude)

                        Model 1 Model 2
(Intercept)              1.1983  1.1949
SE                       0.0598  0.0599
                                       
bpi_antisocialT1Sum      0.4655  0.4649
SE                       0.0186  0.0188
                                       
bpi_anxiousDepressedSum  0.1608  0.1665
SE                       0.0292  0.0295
                                       </code></pre>
</div>
</div>
<div id="outlier-test" class="section level3" number="13.2.5">
<h3><span class="header-section-number">13.2.5</span> Outlier test</h3>
<p>Locates the largest Studentized residuals in absolute value and computes the Bonferroni-corrected p-values based on a t-test for linear models.</p>
<p>Test of outlyingness, i.e., how likely one would have a residual of a given magnitude in a normal distribution with the same sample size</p>
<p>Note: it does not test how extreme an observation is relative to its distribution (i.e., leverage)</p>
<pre class="r fold-show"><code>outlierTest(multipleRegressionModel)</code></pre>
<pre><code>     rstudent unadjusted p-value Bonferroni p
8955 6.519574         8.2999e-11   2.3854e-07
8956 6.519574         8.2999e-11   2.3854e-07
8957 6.182195         7.2185e-10   2.0746e-06
2560 4.914941         9.3800e-07   2.6958e-03
1385 4.573975         4.9884e-06   1.4337e-02</code></pre>
</div>
<div id="observations-with-high-leverage" class="section level3" number="13.2.6">
<h3><span class="header-section-number">13.2.6</span> Observations with high leverage</h3>
<p>Identifies observations with high leverage (i.e., high hat values)</p>
<p>hat values are an index of leverage (observations that are far from the center of the regressor space and have greater influence on OLS regression coefficients)</p>
<pre class="r fold-show"><code>hist(hatvalues(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(hatvalues(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-2.png" width="672" /></p>
<pre class="r fold-show"><code>influenceIndexPlot(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-3.png" width="672" /></p>
<pre class="r fold-show"><code>influencePlot(multipleRegressionModel, id = TRUE) # circle size is proportional to Cook&#39;s Distance</code></pre>
<img src="regression_files/figure-html/unnamed-chunk-94-4.png" width="672" />
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["StudRes"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Hat"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["CookD"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"-3.940128","2":"0.006990052","3":"0.03624388","_rn_":"5286"},{"1":"-4.264903","2":"0.009024093","3":"0.05488391","_rn_":"10898"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>leveragePlots(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-5.png" width="672" /></p>
</div>
<div id="observations-with-high-influence-on-ols-regression-coefficients" class="section level3" number="13.2.7">
<h3><span class="header-section-number">13.2.7</span> Observations with high influence (on OLS regression coefficients)</h3>
<p><a href="https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html" class="uri">https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html</a></p>
<pre class="r fold-show"><code>head(influence.measures(multipleRegressionModel)$infmat)</code></pre>
<pre><code>        dfb.1_     dfb.b_T1     dfb.b_DS       dffit     cov.r       cook.d
1  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
2  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
3 -0.006546394  0.008357248 -0.006308529 -0.01004886 1.0024362 0.0000336708
4  0.042862749 -0.025185789 -0.007782877  0.04286275 0.9998621 0.0006121902
5  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
6 -0.018327367  0.010769006  0.003327823 -0.01832737 1.0015775 0.0001119888
           hat
1 0.0000000000
2 0.0000000000
3 0.0014593072
4 0.0009143185
5 0.0000000000
6 0.0009143185</code></pre>
</div>
<div id="dfbeta" class="section level3" number="13.2.8">
<h3><span class="header-section-number">13.2.8</span> DFBETA</h3>
<pre class="r fold-show"><code>head(dfbeta(multipleRegressionModel))</code></pre>
<pre><code>    (Intercept) bpi_antisocialT1Sum bpi_anxiousDepressedSum
1  0.0000000000        0.0000000000            0.000000e+00
2  0.0000000000        0.0000000000            0.000000e+00
3 -0.0003917284        0.0001553400           -1.839704e-04
4  0.0025639901       -0.0004679817           -2.268890e-04
5  0.0000000000        0.0000000000            0.000000e+00
6 -0.0010966309        0.0002001580            9.704151e-05</code></pre>
<pre class="r fold-show"><code>hist(dfbeta(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<pre class="r fold-show"><code>dfbetasPlots(multipleRegressionModel, id = TRUE)</code></pre>
<pre><code>Error in dfbetasPlots.lm(multipleRegressionModel, id = TRUE): argument 2 matches multiple formal arguments</code></pre>
</div>
<div id="dffits" class="section level3" number="13.2.9">
<h3><span class="header-section-number">13.2.9</span> DFFITS</h3>
<pre class="r fold-show"><code>head(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<pre><code>     8472      4468      1917      8955      8956      1986 
0.2664075 0.2428065 0.2237760 0.1972271 0.1972271 0.1923034 </code></pre>
<pre class="r fold-show"><code>hist(dffits(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(dffits(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-97-2.png" width="672" /></p>
<pre class="r fold-show"><code>plot(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-97-3.png" width="672" /></p>
</div>
<div id="cooksDistance" class="section level3" number="13.2.10">
<h3><span class="header-section-number">13.2.10</span> Cook’s Distance</h3>
<p>Observations that are both outlying (have a high residual from the regression line) and have high leverage (are far from the center of the regressor space) have high influence on the OLS regression coefficients. An observation will have less influence if it lies on the regression line (not an outlier, i.e., has a low residual) or if it has low leverage (i.e., has a value near the center of a predictor’s distribution).</p>
<pre class="r fold-show"><code>head(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<pre><code>      2765       1371       2767       8472       6170       6023 
0.05488391 0.03624388 0.03624388 0.02358305 0.02323282 0.02315496 </code></pre>
<pre class="r fold-show"><code>hist(cooks.distance(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(cooks.distance(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-98-2.png" width="672" /></p>
<pre class="r fold-show"><code>plot(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-98-3.png" width="672" /></p>
<div id="refit-model-removing-values-with-high-cooks-distance" class="section level4" number="13.2.10.1">
<h4><span class="header-section-number">13.2.10.1</span> Refit model removing values with high cook’s distance</h4>
<pre class="r fold-show"><code>multipleRegressionModel_2 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371))

multipleRegressionModel_3 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767))

multipleRegressionModel_4 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472))

multipleRegressionModel_5 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170))

multipleRegressionModel_6 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023))

multipleRegressionModel_7 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023,2766))</code></pre>
<div id="examine-how-much-regression-coefficients-change-when-excluding-influential-observations" class="section level5" number="13.2.10.1.1">
<h5><span class="header-section-number">13.2.10.1.1</span> Examine how much regression coefficients change when excluding influential observations</h5>
<pre class="r fold-show"><code>compareCoefs(multipleRegressionModel, multipleRegressionModel_2, multipleRegressionModel_3, multipleRegressionModel_4, multipleRegressionModel_5, multipleRegressionModel_6, multipleRegressionModel_7)</code></pre>
<pre><code>Calls:
1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)
2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371), na.action =
   na.exclude)
3: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767), 
  na.action = na.exclude)
4: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472),
   na.action = na.exclude)
5: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170), na.action = na.exclude)
6: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170, 6023), na.action = na.exclude)
7: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170, 6023, 2766), na.action = na.exclude)

                        Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7
(Intercept)              1.1983  1.1704  1.1577  1.1676  1.1620  1.1537  1.1439
SE                       0.0598  0.0597  0.0596  0.0596  0.0596  0.0595  0.0595
                                                                               
bpi_antisocialT1Sum      0.4655  0.4739  0.4779  0.4748  0.4744  0.4793  0.4836
SE                       0.0186  0.0185  0.0185  0.0185  0.0185  0.0185  0.0185
                                                                               
bpi_anxiousDepressedSum  0.1608  0.1691  0.1726  0.1699  0.1770  0.1742  0.1743
SE                       0.0292  0.0290  0.0290  0.0289  0.0290  0.0290  0.0289
                                                                               </code></pre>
</div>
</div>
<div id="examine-how-much-regression-coefficients-change-when-using-least-trimmed-squares-lts-that-is-resistant-to-outliers" class="section level4" number="13.2.10.2">
<h4><span class="header-section-number">13.2.10.2</span> Examine how much regression coefficients change when using least trimmed squares (LTS) that is resistant to outliers</h4>
<pre class="r fold-show"><code>coef(lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
        data = mydata,
        na.action=&quot;na.exclude&quot;))</code></pre>
<pre><code>            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              1.1983004               0.4655286               0.1607541 </code></pre>
<pre class="r fold-show"><code>coef(ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = &quot;na.exclude&quot;))</code></pre>
<pre><code>              Intercept     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              0.7439567               0.5432644               0.1524929 </code></pre>
</div>
</div>
<div id="resources" class="section level3" number="13.2.11">
<h3><span class="header-section-number">13.2.11</span> Resources</h3>
<p>Book: An R Companion to Applied Regression</p>
<p><a href="http://people.duke.edu/~rnau/testing.htm" class="uri">http://people.duke.edu/~rnau/testing.htm</a></p>
<p><a href="http://www.statmethods.net/stats/rdiagnostics.html" class="uri">http://www.statmethods.net/stats/rdiagnostics.html</a></p>
<p><a href="http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html" class="uri">http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html</a></p>
<p><a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions" class="uri">https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions</a></p>
</div>
</div>
</div>
<div id="session-info" class="section level1" number="14">
<h1><span class="header-section-number">14</span> Session Info</h1>
<pre class="r fold-hide"><code>sessionInfo()</code></pre>
<pre><code>R version 4.2.1 (2022-06-23)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04.4 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
 [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       
 [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   
 [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          
[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] mice_3.14.0            regtools_1.7.0         gtools_3.9.3          
 [4] FNN_1.1.3.1            lavaan_0.6-12          interactions_1.1.5    
 [7] correlation_0.8.2      effects_4.2-2          mblm_0.12.1           
[10] quantreg_5.94          olsrr_0.5.3            foreign_0.8-82        
[13] AER_1.2-10             sandwich_3.0-2         lmtest_0.9-40         
[16] zoo_1.8-10             mgcv_1.8-40            nlme_3.1-157          
[19] car_3.1-0              carData_3.0-5          cvTools_0.3.2         
[22] brms_2.17.0            Rcpp_1.0.9             robustbase_0.95-0     
[25] rms_6.3-0              SparseM_1.81           Hmisc_4.7-1           
[28] Formula_1.2-4          survival_3.3-1         lattice_0.20-45       
[31] psych_2.2.5            forcats_0.5.1          stringr_1.4.0         
[34] dplyr_1.0.9            purrr_0.3.4            readr_2.1.2           
[37] tidyr_1.2.0            tibble_3.1.8           ggplot2_3.3.6         
[40] tidyverse_1.3.2        MASS_7.3-57            petersenlab_0.1.2-9010

loaded via a namespace (and not attached):
  [1] utf8_1.2.2             lme4_1.1-30            tidyselect_1.1.2      
  [4] htmlwidgets_1.5.4      grid_4.2.1             jtools_2.2.0          
  [7] munsell_0.5.0          codetools_0.2-18       interp_1.1-3          
 [10] DT_0.24                miniUI_0.1.1.1         withr_2.5.0           
 [13] Brobdingnag_1.2-7      colorspace_2.0-3       highr_0.9             
 [16] knitr_1.39             rstudioapi_0.13        stats4_4.2.1          
 [19] bayesplot_1.9.0        labeling_0.4.2         rstan_2.26.13         
 [22] lgr_0.4.3              mnormt_2.1.0           datawizard_0.5.0      
 [25] farver_2.1.1           bridgesampling_1.1-2   coda_0.19-4           
 [28] vctrs_0.4.1            generics_0.1.3         TH.data_1.1-1         
 [31] float_0.3-0            xfun_0.32              R6_2.5.1              
 [34] markdown_1.1           cachem_1.0.6           assertthat_0.2.1      
 [37] promises_1.2.0.1       scales_1.2.0           multcomp_1.4-20       
 [40] nnet_7.3-17            googlesheets4_1.0.1    gtable_0.3.0          
 [43] processx_3.7.0         goftest_1.2-3          rlang_1.0.4           
 [46] MatrixModels_0.5-0     text2vec_0.6.1         splines_4.2.1         
 [49] gargle_1.2.0           broom_1.0.0            checkmate_2.1.0       
 [52] inline_0.3.19          yaml_2.3.5             reshape2_1.4.4        
 [55] abind_1.4-5            modelr_0.1.8           threejs_0.3.3         
 [58] crosstalk_1.2.0        backports_1.4.1        httpuv_1.6.5          
 [61] tensorA_0.36.2         tools_4.2.1            ellipsis_0.3.2        
 [64] jquerylib_0.1.4        posterior_1.3.0        RColorBrewer_1.1-3    
 [67] ggridges_0.5.3         plyr_1.8.7             base64enc_0.1-3       
 [70] ps_1.7.1               prettyunits_1.1.1      rpart_4.1.16          
 [73] deldir_1.0-6           haven_2.5.0            cluster_2.1.3         
 [76] survey_4.1-1           fs_1.5.2               magrittr_2.0.3        
 [79] data.table_1.14.2      colourpicker_1.1.1     reprex_2.0.2          
 [82] mlapi_0.1.1            googledrive_2.0.0      mvtnorm_1.1-3         
 [85] matrixStats_0.62.0     hms_1.1.1              shinyjs_2.1.0         
 [88] mime_0.12              evaluate_0.16          xtable_1.8-4          
 [91] RhpcBLASctl_0.21-247.1 shinystan_2.6.0        jpeg_0.1-9            
 [94] readxl_1.4.1           shape_1.4.6            gridExtra_2.3         
 [97] rstantools_2.2.0       compiler_4.2.1         V8_4.2.1              
[100] crayon_1.5.1           rje_1.11.0             minqa_1.2.4           
[103] StanHeaders_2.26.13    htmltools_0.5.3        later_1.3.0           
[106] tzdb_0.3.0             RcppParallel_5.1.5     lubridate_1.8.0       
[109] DBI_1.1.3              dbplyr_2.2.1           boot_1.3-28           
[112] Matrix_1.4-1           cli_3.3.0              mitools_2.4           
[115] insight_0.18.2         parallel_4.2.1         igraph_1.3.4          
[118] pkgconfig_2.0.3        rsparse_0.5.0          foreach_1.5.2         
[121] xml2_1.3.3             dygraphs_1.1.1.6       pbivnorm_0.6.0        
[124] bslib_0.4.0            estimability_1.4.1     rvest_1.0.2           
[127] distributional_0.3.0   callr_3.7.1            digest_0.6.29         
[130] rmarkdown_2.15         cellranger_1.1.0       htmlTable_2.4.1       
[133] nortest_1.0-4          curl_4.3.2             shiny_1.7.2           
[136] nloptr_2.0.3           lifecycle_1.0.1        jsonlite_1.8.0        
[139] viridisLite_0.4.0      fansi_1.0.3            pillar_1.8.0          
[142] loo_2.5.1              fastmap_1.1.0          httr_1.4.4            
[145] DEoptimR_1.0-11        pkgbuild_1.3.1         glue_1.6.2            
[148] xts_0.12.1             bayestestR_0.12.1      mix_1.0-11            
[151] iterators_1.0.14       png_0.1-7              shinythemes_1.2.0     
[154] glmnet_4.1-4           pander_0.6.5           stringi_1.7.8         
[157] sass_0.4.2             polspline_1.1.20       latticeExtra_0.6-30   </code></pre>
</div>

<div id="rmd-source-code">---
title: "Regression"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      error = TRUE,
                      comment = "",
                      class.source = "fold-show")
```

# Preamble

## Install Libraries

```{r, class.source = "fold-hide"}
#install.packages("remotes")
#remotes::install_git("https://research-git.uiowa.edu/PetersenLab/petersenlab.git")
```

## Load Libraries

```{r, message = FALSE, warning = FALSE, class.source = "fold-hide"}
library("petersenlab")
library("MASS")
library("tidyverse")
library("psych")
library("rms")
library("robustbase")
library("brms")
library("cvTools")
library("car")
library("mgcv")
library("AER")
library("foreign")
library("olsrr")
library("quantreg")
library("mblm")
library("effects")
library("correlation")
library("interactions")
library("lavaan")
library("regtools")
library("mice")
```

# Import Data

```{r, class.source = "fold-hide"}
mydata <- read.csv("https://osf.io/8syp5/download")
```

# Data Preparation

```{r, class.source = "fold-hide"}
mydata$countVariable <- as.integer(mydata$bpi_antisocialT2Sum)
mydata$orderedVariable <- factor(mydata$countVariable, ordered = TRUE)

mydata$female <- NA
mydata$female[which(mydata$sex == "male")] <- 0
mydata$female[which(mydata$sex == "female")] <- 1
```

# Linear Regression

## Linear regression model

```{r}
multipleRegressionModel <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel)
confint(multipleRegressionModel)
```

### Remove missing data

```{r}
multipleRegressionModelNoMissing <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata,
                                       na.action = na.omit)

multipleRegressionModelNoMissing <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit)
```

## Linear regression model on correlation/covariance matrix (for pairwise deletion)

```{r, warning = FALSE}
multipleRegressionModelPairwise <- setCor(
  y = "bpi_antisocialT2Sum",
  x = c("bpi_antisocialT1Sum","bpi_anxiousDepressedSum"),
  data = cov(mydata[,c("bpi_antisocialT2Sum","bpi_antisocialT1Sum","bpi_anxiousDepressedSum")], use = "pairwise.complete.obs"),
  n.obs = nrow(mydata))

summary(multipleRegressionModelPairwise)
multipleRegressionModelPairwise[c("coefficients","se","Probability","R2","shrunkenR2")]
```

## Linear regression model with robust covariance matrix (rms)

```{r}
rmsMultipleRegressionModel <- robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                         data = mydata,
                                         x = TRUE,
                                         y = TRUE))

rmsMultipleRegressionModel
confint(rmsMultipleRegressionModel)
```

## Robust linear regression (MM-type iteratively reweighted least squares regression)

```{r}
robustLinearRegression <- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude)

summary(robustLinearRegression)
confint(robustLinearRegression)
```

## Least trimmed squares regression (for removing outliers)

```{r}
ltsRegression <- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                        data = mydata,
                        na.action = na.exclude)

summary(ltsRegression)
```

## Bayesian linear regression

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianRegularizedRegression <- brm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     chains = 4,
                                     iter = 2000,
                                     seed = 52242)
```

```{r}
summary(bayesianRegularizedRegression)
```

# Generalized Linear Regression

## Generalized regression model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
generalizedRegressionModel <- glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = "poisson",
                                  na.action = na.exclude)

summary(generalizedRegressionModel)
confint(generalizedRegressionModel)
```

## Generalized regression model (rms)

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
rmsGeneralizedRegressionModel <- Glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE,
                                     family = "poisson")

rmsGeneralizedRegressionModel
confint(rmsGeneralizedRegressionModel)
```

## Bayesian generalized linear model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.
For example, we could use Gamma regression, `family = Gamma`, when the response variable is continuous and positive, and the coefficient of variation--rather than the variance--is constant.

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianGeneralizedLinearRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                           data = mydata,
                                           family = poisson,
                                           chains = 4,
                                           seed = 52242,
                                           iter = 2000)
```

```{r}
summary(bayesianGeneralizedLinearRegression)
```

## Robust generalized regression

```{r}
robustGeneralizedRegression <- glmrob(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      family = "poisson",
                                      na.action = na.exclude)

summary(robustGeneralizedRegression)
confint(robustGeneralizedRegression)
```

## Ordinal regression model (rms)

```{r}
ordinalRegressionModel <- robcov(orm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE))

ordinalRegressionModel
confint(ordinalRegressionModel)
```

## Bayesian ordinal regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianOrdinalRegression <- brm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                 data = mydata,
                                 family = cumulative(),
                                 chains = 4,
                                 seed = 52242,
                                 iter = 2000)
```

```{r}
summary(bayesianOrdinalRegression)
```

## Bayesian count regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianCountRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                               data = mydata,
                               family = "poisson",
                               chains = 4,
                               seed = 52242,
                               iter = 2000)
```

```{r}
summary(bayesianCountRegression)
```

## Logistic regression model (rms)

```{r}
logisticRegressionModel <- robcov(lrm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      x = TRUE,
                                      y = TRUE))

logisticRegressionModel
confint(logisticRegressionModel)
```

## Bayesian logistic regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianLogisticRegression <- brm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = bernoulli,
                                  chains = 4,
                                  seed = 52242,
                                  iter = 2000)
```

```{r}
summary(bayesianLogisticRegression)
```

# Hierarchical Linear Regression

```{r}
model1 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum,
             data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit,
             na.action = na.exclude)

model2 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
             data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit,
             na.action = na.exclude)

summary(model2)$adj.r.squared

anova(model1, model2)
summary(model2)$adj.r.squared - summary(model1)$adj.r.squared
```

# Moderated Multiple Regression {#moderation}

https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html

Make sure to mean-center or orthogonalize predictors before computing the interaction term.

## Model

```{r}
states <- as.data.frame(state.x77)
interactionModel <- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)
```

## Plots

```{r}
interact_plot(interactionModel, pred = Illiteracy, modx = Murder)
interact_plot(interactionModel, pred = Illiteracy, modx = Murder, plot.points = TRUE)
interact_plot(interactionModel, pred = Illiteracy, modx = Murder, interval = TRUE)

johnson_neyman(interactionModel, pred = Illiteracy, modx = Murder, alpha = .05)
```

## Simple Slopes Analysis

```{r}
sim_slopes(interactionModel, pred = Illiteracy, modx = Murder, johnson_neyman = FALSE)
sim_slopes(interactionModel,
           pred = Illiteracy,
           modx = Murder,
           modx.values = c(0, 5, 10),
           johnson_neyman = FALSE)
```

## Johnson-Neyman intervals

Indicates all the values of the moderator for which the slope of the predictor is statistically significant.

```{r}
sim_slopes(interactionModel, pred = Illiteracy, modx = Murder, johnson_neyman = TRUE)

probe_interaction(interactionModel,
                  pred = Illiteracy,
                  modx = Murder,
                  cond.int = TRUE,
                  interval = TRUE,
                  jnplot = TRUE)
```

# Approaches to Handling Missingness {#missingness}

## Listwise deletion {#listwiseDeletion}

Listwise deletion deletes every row in the data file that has a missing value for one of the model variables.

```{r}
listwiseDeletionModel <- lm(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  na.action = na.exclude)

summary(listwiseDeletionModel)
confint(listwiseDeletionModel)
```

## Pairwise deletion {#pairwiseDeletion}

Also see here:

- https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002
- https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure
- https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re
- https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values
- https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps

Adapted from here: https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise

```{r}
modelData <- mydata[,c("bpi_antisocialT2Sum","bpi_antisocialT1Sum","bpi_anxiousDepressedSum")]
varMeans <- colMeans(modelData, na.rm = TRUE)
varCovariances <- cov(modelData, use = "pairwise")

pairwiseRegression_syntax <- '
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
'

pairwiseRegression_fit <- lavaan(
  pairwiseRegression_syntax,
  sample.mean = varMeans,
  sample.cov = varCovariances,
  sample.nobs = sum(complete.cases(modelData))
)

summary(
  pairwiseRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)
```

## Full-information maximum likelihood (FIML) {#fiml}

```{r}
fimlRegression_syntax <- '
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
'

fimlRegression_fit <- lavaan(
  fimlRegression_syntax,
  data = mydata,
  missing = "ML",
)

summary(
  fimlRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)
```

## Multiple imputation {#imputation}

```{r}
modelData_imputed <- mice(
  modelData,
  m = 5,
  method = "pmm") # predictive mean matching; can choose among many methods

imputedData_fit <- with(
  modelData_imputed,
  lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum))

imputedData_pooledEstimates <- pool(imputedData_fit)
summary(imputedData_pooledEstimates)
```

# Model Building Steps

1. Examine extent and type of missing data, consider [how to handle missing values](#missingness) ([multiple imputation](#imputation), [FIML](#fiml), [pairwise deletion](#pairwiseDeletion), [listwise deletion](#listwiseDeletion))
    - Little's MCAR test from `mcar_test()` function of the `njtierney/naniar` package
    - Bayesian handling of missing data: https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html
1. Examine descriptive statistics, consider variable transformations
1. Examine scatterplot matrix to examine whether associations between predictor and outcomes are linear or nonlinear
1. Model building: theory and empiricism (stepwise regression, likelihood ratio tests, k-fold cross validation to check for over-fitting compared to simpler model)
1. Test assumptions
    - Examine whether predictors have linear association with outcome (Residual Plots, Marginal Model Plots, Added-Variable Plots—check for non-horizontal lines)
    - Examine whether residuals have constant variance across levels of outcome and predictors Residual Plots, Spread-Level Plots—check for fan-shaped plot or other increasing/decreasing structure)
    - Examine whether predictors show multicollinearity (VIF)
    - Examine whether residuals are normally distributed (QQ plot and density plot)
    - Examine influence of influential observations (high outlyingness and leverage) on parameter stimates by iteratively removing influential observations and refitting (Added-Variable Plots)
1. Handle violated assumptions, select final set of predictors/outcomes and transformation of each
1. Use k-fold cross validation to identify the best estimation procedure (OLS, MM, or LTS) on the final model
1. Use identified estimation procedure to fit final model and determine the best parameter point estimates
1. Calculate bootstrapped estimates to determine the confidence intervals around the parameter estimates of the final model

# Bootstrapped Estimates

To determine the confidence intervals of parameter estimates

## Linear Regression

```{r}
multipleRegressionModelBootstrapped <- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(multipleRegressionModelBootstrapped)
confint(multipleRegressionModelBootstrapped, level = .95, type = "bca")
hist(multipleRegressionModelBootstrapped)
```

## Generalized Regression

```{r}
generalizedRegressionModelBootstrapped <- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(generalizedRegressionModelBootstrapped)
confint(generalizedRegressionModelBootstrapped, level = .95, type = "bca")
hist(generalizedRegressionModelBootstrapped)
```

# Cross Validation

To examine degree of prediction error and over-fitting to determine best model
https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best

## K-fold cross validation

```{r}
kFolds <- 10
replications <- 20

folds <- cvFolds(nrow(mydata), K = kFolds, R = replications)

fitLm <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = "na.exclude")

fitLmrob <- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                  data = mydata,
                  na.action = "na.exclude")

fitLts <- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                 data = mydata,
                 na.action = "na.exclude")

cvFitLm <- cvLm(fitLm, K = kFolds, R = replications)

cvFitLmrob <- cvLmrob(fitLmrob, K = kFolds, R = replications)
cvFitLts <- cvLts(fitLts, K = kFolds, R = replications)

cvFits <- cvSelect(OLS = cvFitLm, MM = cvFitLmrob, LTS = cvFitLts)
cvFits

bwplot(cvFits, xlab = "Root Mean Square Error", xlim= c(0, max(cvFits$cv$CV) + 0.2))
```

# Examining Model Fits

## Effect Plots

```{r}
allEffects(multipleRegressionModel)
plot(allEffects(multipleRegressionModel))
```

## Confidence Ellipses

```{r}
confidenceEllipse(multipleRegressionModel, levels = c(0.5, .95))
```

## Data Ellipse

```{r}
mydata_nomissing <- na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")])
dataEllipse(mydata_nomissing$bpi_antisocialT1Sum, mydata_nomissing$bpi_antisocialT2Sum, levels = c(0.5, .95))
```

# Diagnostics

## Assumptions

### 1. Linear relation between predictors and outcome {#linearAssociation}

#### Ways to Test

##### Before Model Fitting

- scatterplot matrix
- distance correlation

###### Scatterplot Matrix

```{r}
scatterplotMatrix(~ bpi_antisocialT2Sum + bpi_antisocialT1Sum + bpi_anxiousDepressedSum, data = mydata)
```

###### Distance correlation

The distance correlation is an index of the degree of the linear and non-linear association between two variables.

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "distance", p_adjust = "none")
```

##### After Model Fitting

Check for nonlinearities (non-horizontal line) in plots of:

- Residuals versus fitted values ([Residual Plots](#residualPlots))—best
- Residuals versus predictors ([Residual Plots](#residualPlots))
- Outcome versus fitted values ([Marginal Model Plots](#marginalModelPlots))
- Outcome versus predictors, ignoring other predictors ([Marginal Model Plots](#marginalModelPlots))
- Outcome versus predictors, controlling for other predictors ([Added-Variable Plots](#addedVariablePlots))

#### Ways to Handle

- Transform outcome/predictor variables (Box-Cox transformations)
- Semi-parametric regression models: Generalized additive models (GAM)
- Non-parametric regression models: Nearest-Neighbor Kernel Regression

##### Semi-parametric or non-parametric regression models

http://www.lisa.stat.vt.edu/?q=node/7517

Note: using semi-parametric or non-parametric models increases fit in context of nonlinearity at the expense of added complexity.
Make sure to avoid fitting an overly complex model (e.g., use k-fold cross validation).
Often, the simpler (generalized) linear model is preferable to semi-paremetric or non-parametric approaches

###### Semi-parametric: Generalized Additive Models

http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models

```{r}
generalizedAdditiveModel <- gam(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                family = gaussian(),
                                data = mydata, na.action = na.exclude)

summary(generalizedAdditiveModel)
confint(generalizedAdditiveModel)
```

###### Non-parametric: Nearest-Neighbor Kernel Regression

### 2. Exogeneity

Exogeneity means that the association between predictors and outcome is fully causal and unrelated to other variables.

#### Ways to Test

- Durbin-Wu-Hausman test of endogeneity

##### Durbin-Wu-Hausman test of endogeneity

The instrumental variables (2SLS) estimator is implemented in the `R` package `AER` as command:

```{r, eval = FALSE}
ivreg(y ~ x1 + x2 + w1 + w2 | z1 + z2 + z3 + w1 + w2)
```

where `x1` and `x2` are endogenous regressors, `w1` and `w2` exogeneous regressors, and `z1` to `z3` are excluded instruments.

Durbin-Wu-Hausman test:

```{r}
hsng2 <- read.dta("http://www.stata-press.com/data/r11/hsng2.dta")

fiv <- ivreg(rent ~ hsngval + pcturban | pcturban + faminc + reg2 + reg3 + reg4, data = hsng2) #Housing values are likely endogeneous and therefore instrumented by median family income (faminc) and 3 regional dummies (reg2, reg4, reg4)

summary(fiv, diagnostics = TRUE)
```

The Eicker-Huber-White covariance estimator which is robust to heteroscedastic error terms is reported after estimation with `vcov = sandwich` in `coeftest()`

First stage results are reported by explicitly estimating them.
For example:

```{r}
first <- lm(hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, data = hsng2)

summary(first)
```

In case of a single endogenous variable (K = 1), the F-statistic to assess weak instruments is reported after estimating the first stage with, for example:

```{r}
waldtest(first, . ~ . - faminc - reg2 - reg3 - reg4)
```

or in case of heteroscedatistic errors:

```{r}
waldtest(first, . ~ . - faminc - reg2 - reg3- reg4, vcov = sandwich)
```

#### Ways to Handle

- Conduct an experiment/RCT with random assignment
- Instrumental variables

### 3. Homoscedasticity of residuals

Homoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome/predictors (i.e., the residuals show constant variance as a function of outcome/predictors).

#### Ways to Test

- Plot residuals vs. outcome and predictor variables ([Residual Plots](#residualPlots))
- Plot residuals vs. fitted values ([Residual Plots](#residualPlots))
- Time Series data: Plot residuals vs. time
- Spread-level plot
- Breusch-Pagan test: `bptest()` function from `lmtest` package
- Goldfeld-Quandt Test

##### Residuals vs. outcome and predictor variables

Plot residuals, or perhaps their absolute values, versus the outcome and predictor variables ([Residual Plots](#residualPlots)).
Examine whether residual variance is constant at all levels of other variables or whether it increases/decreases as a function of another variable (or shows some others structure, e.g., small variance at low and high levels of a predictor and high variance in the middle).
Note that this is *different than whether the residuals show non-linearities*—i.e., a non-horizontal line, which would indicate a [nonlinear association between variables](#linearAssociation) (see Assumption #1, above).
Rather, here we are examining whether there is change in the variance as a function of another variable (e.g., a fan-shaped [Residual Plot](#residualPlots))

##### Spread-level plot

Examining whether level (e.g., mean) depends on spread (e.g., variance)—plot of log of the absolute Studentized residuals against the log of the fitted values

```{r}
spreadLevelPlot(multipleRegressionModel)
```

##### Breusch-Pagan test

http://www.inside-r.org/packages/cran/lmtest/docs/bptest

```{r}
bptest(multipleRegressionModel)
```

##### Goldfeld-Quandt Test

```{r}
gqtest(multipleRegressionModel)
```

##### Test of dependence of spread on level

```{r}
ncvTest(multipleRegressionModel)
```

##### Test of dependence of spread on predictors

```{r}
ncvTest(multipleRegressionModel, ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum)
```

#### Ways to Handle

- If residual variance increases as a function of the fitted values, consider a poisson regression model (especially for count data), for which the variance does increase with the mean
- If residual variance differs as a function of model predictors, consider adding interactions/product terms (the effect of one variable depends on the level of another variable)
- Try transforming the outcome variable to be normally distributed (log-transform if the errors seem consistent in percentage rather than absolute terms)
- If the error variance is proportional to a variable $z$, then fit the model using Weighted Least Squares (WLS), with the weights given be $1/z$
- Weighted least squares (WLS) using the "weights" argument of the `lm()` function; to get weights, see: https://stats.stackexchange.com/a/100410/20338
- Huber-White standard errors (a.k.a. "Sandwich" estimates) from a heteroscedasticity-corrected covariance matrix
    - `coeftest()` function from the `sandwich` package along with hccm sandwich estimates from the `car` package 
    - `robcov()` function from the `rms` package
- Time series data: ARCH (auto-regressive conditional heteroscedasticity) models
- Time series data: seasonal patterns can be addressed by applying log transformation to outcome variable

##### Huber-White standard errors

Standard errors (SEs) on the diagonal increase

```{r}
vcov(multipleRegressionModel)
hccm(multipleRegressionModel)

summary(multipleRegressionModel)
coeftest(multipleRegressionModel, vcov = sandwich)
coeftest(multipleRegressionModel, vcov = hccm)

robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
           data = mydata,
           x = TRUE,
           y = TRUE))

robcov(ols(t_ext ~ m_ext + age,
           data = mydata,
           x = TRUE,
           y = TRUE),
       cluster = mydata$tcid) #account for nested data within subject
```

### 4. Errors are independent

Independent errors means that the errors are uncorrelated with each other.

#### Ways to Test

- Plot residuals vs. predictors ([Residual Plots](#residualPlots))
- Time Series data: Residual time series plot (residuals vs. row number)
- Time Series data: Table or plot of residual autocorrelations
- Time Series data: Durbin-Watson statistic for test of significant residual autocorrelation at lag 1

#### Ways to Handle

- Generalized least squares (GLS) models are capable of handling correlated errors: https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html
- Regression with cluster variable
    - `robcov()` from `rms` package
- [Hierarchical linear modeling](hlm.html)
    - [Linear mixed effects models](hlm.html#linear)
    - [Generalized linear mixed effects models](hlm.html#generalized)
    - [Nonlinear mixed effects models](hlm.html#nonlinear)

### 5. No multicollinearity

Multicollinearity occurs when the predictors are correlated with each other.

#### Ways to Test

- Variance Inflation Factor (VIF)
- Generalized Variance Inflation Factor (GVIF)—when models have related regressors (multiple polynomial terms or contrasts from same predictor)
- Correlation
- Tolerance
- Condition Index

##### Variance Inflation Factor (VIF)

$$
\text{VIF} = 1/\text{Tolerance}
$$


If the variance inflation factor of a predictor variable were 5.27 ($\sqrt{5.27} = 2.3$), this means that the standard error for the coefficient of that predictor variable is 2.3 times as large (i.e., confidence interval is 2.3 times wider) as it would be if that predictor variable were uncorrelated with the other predictor variables.

VIF = 1: Not correlated\
1 < VIF < 5: Moderately correlated\
VIF > 5 to 10: Highly correlated (multicollinearity present)

```{r}
vif(multipleRegressionModel)
```

##### Generalized Variance Inflation Factor (GVIF)

Useful when models have related regressors (multiple polynomial terms or contrasts from same predictor)

##### Correlation

correlation among all independent variables the correlation coefficients should be smaller than .08

```{r}
cor(mydata[,c("bpi_antisocialT1Sum","bpi_anxiousDepressedSum")], use = "pairwise.complete.obs")
```

##### Tolerance

The tolerance is an index of the influence of one independent variable on all other independent variables.

$$
\text{tolerance} = 1/\text{VIF}
$$

T < 0.2: there might be multicollinearity in the data\
T < 0.01: there certainly is multicollinarity in the data

##### Condition Index

The condition index is calculated using a factor analysis on the independent variables.
Values of 10-30 indicate a mediocre multicollinearity in the regression variables.
Values > 30 indicate strong multicollinearity.

For how to interpret, see here: https://stats.stackexchange.com/a/87610/20338

```{r}
ols_eigen_cindex(multipleRegressionModel)
```

#### Ways to Handle

- Remove highly correlated (i.e., redundant) predictors
- Average the correlated predictors
- [Principal Component Analysis](pca.html) for data reduction
- Standardize predictors
- Center the data (deduct the mean)
- Singular-value decomposition of the model matrix or the mean-centered model matrix
- Conduct a [factor analysis](#factorAnalysis.html) and rotate the factors to ensure independence of the factors

### 6. Errors are normally distributed

#### Ways to Test

- Probability Plots
    - [Normal Quantile (QQ) Plots](#qqPlot) (based on non-cumulative distribution of residuals)
    - [Normal Probability (PP) Plots](#ppPlot) (based on cumulative distribution of residuals)
- [Density Plot of Residuals](#densityPlotResiduals)
- Statistical Tests
    - Kolmogorov-Smirnov test
    - Shapiro-Wilk test
    - Jarque-Bera test
    - Anderson-Darling test (best test)
- Examine influence of outliers

#### Ways to Handle

- Apply a transformation to the predictor or outcome variable
- Exclude outliers
- Robust regression
    - Best when no outliers: MM-type regression estimator
        - `lmrob()`/`glmrob()` function of `robustbase` package
        - Iteratively reweighted least squares (IRLS): `rlm(, method = "MM")` function of `MASS` package: http://www.ats.ucla.edu/stat/r/dae/rreg.htm
    - Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers
        - `ltsReg()` function of `robustbase` package (best)
    - Best when single predictor: Theil-Sen estimator
        - `mblm(, repeated = FALSE)` function of `mblm` package
    - Robust correlation
        - Spearman's rho: `cor(, method = "spearman")`
        - Percentage bend correlation
        - Minimum vollume ellipsoid
        - Minimum covariance determinant:
        - Winsorized correlation
        - Biweight midcorrelation
    - Not great options:
        - Quantile (L1) regression: `rq()` function of `quantreg` package

##### Transformations of Outcome Variable

###### Box-Cox Transformation

Useful if the outcome is strictly positive (or add a constant to outcome to make it strictly positive)

lambda = -1: inverse transformation\
lambda = -0.5: 1/sqrt(Y)\
lambda = 0: log transformation\
lambda = 0.5: square root\
lambda = 0.333: cube root\
lambda = 1: no transformation\
lambda = 2: squared\

####### Raw distribution

```{r}
plot(density(na.omit(mydata$bpi_antisocialT2Sum)))
```

####### Add constant to outcome to make it strictly positive

```{r}
strictlyPositiveDV <- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude)
```

####### Identify best power transformation (lambda)

Consider rounding the power to a common value (square root = .5; cube root = .333; squared = 2)

```{r}
boxCox(strictlyPositiveDV)
powerTransform(strictlyPositiveDV) 
transformedDV <- powerTransform(strictlyPositiveDV)

summary(transformedDV)
```

####### Transform the DV

```{r}
mydata$bpi_antisocialT2SumTransformed <- bcPower(mydata$bpi_antisocialT2Sum + 1, coef(transformedDV))

plot(density(na.omit(mydata$bpi_antisocialT2SumTransformed)))
```

####### Compare residuals from model with and without transformation

######## Model without transformation

```{r}
summary(modelWithoutTransformation <- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude))
plot(density(na.omit(studres(modelWithoutTransformation))))
```

######## Model with transformation

```{r}
summary(modelWithTransformation <- lm(bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      na.action = na.exclude))

plot(density(na.omit(studres(modelWithTransformation))))
```

####### Constructed Variable Test & Plot

A significant *p*-value indicates a strong need to transform variable:

```{r}
multipleRegressionModel_constructedVariable <- update(multipleRegressionModel, . ~ . + boxCoxVariable(bpi_antisocialT1Sum + 1))
summary(multipleRegressionModel_constructedVariable)$coef["boxCoxVariable(bpi_antisocialT1Sum + 1)", , drop = FALSE]
```

Plot allows us to see whether the need for transformation is spread through data or whether it is just dependent on a small fraction of observations:

```{r}
avPlots(multipleRegressionModel_constructedVariable, "boxCoxVariable(bpi_antisocialT1Sum + 1)")
```

####### Inverse Response Plot

The black line is the best-fitting power transformation:

```{r}
inverseResponsePlot(strictlyPositiveDV, lambda = c(-1,-0.5,0,1/3,.5,1,2))
```

###### Yeo-Johnson Transformations

Useful if the outcome is not strictly positive.

```{r, eval = FALSE}
yjPower(DV, lambda)
```

##### Transformations of Predictor Variable

###### Component-Plus-Residual Plots (Partial Residual Plots)

Linear model:

```{r}
crPlots(multipleRegressionModelNoMissing, order = 1)
```

Quadratic model:

```{r}
crPlots(multipleRegressionModelNoMissing, order = 2)
```


```{r}
multipleRegressionModel_quadratic <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + bpi_anxiousDepressedSum,
                                        data = mydata,
                                        na.action = na.exclude)

summary(multipleRegressionModel_quadratic)
anova(multipleRegressionModel_quadratic, multipleRegressionModel)
crPlots(multipleRegressionModel_quadratic, order = 1)
```

###### CERES Plot (Combining conditional Expectations and RESiduals)

Useful when nonlinear associations among the predictors are very strong (component-plus-residual plots may appear nonlinear even though the true partial regression is linear, a phenonomen called leakage)

```{r}
ceresPlots(multipleRegressionModel)
ceresPlots(multipleRegressionModel_quadratic)
```

###### Box-Tidwell Method for Choosing Predictor Transformations

predictors must be strictly positive (or add a constant to make it strictly positive)

```{r}
boxTidwell(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1),
           other.x = NULL, #list variables not to be transformed in other.x
           data = mydata,
           na.action = na.exclude)
```

###### Constructed-Variables Plot

```{r}
multipleRegressionModel_cv <- lm(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1) + I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) + I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)),
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel_cv)$coef["I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))", , drop = FALSE]
summary(multipleRegressionModel_cv)$coef["I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))", , drop = FALSE]

avPlots(multipleRegressionModel_cv, "I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))")
avPlots(multipleRegressionModel_cv, "I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))")
```

##### Robust models

Resources

- For comparison of methods, see Book: "Modern Methods for Robust Regression"
- http://www.ats.ucla.edu/stat/r/dae/rreg.htm
- http://stats.stackexchange.com/a/46234/20338
- http://cran.r-project.org/web/views/Robust.html


###### Robust correlation

####### Spearman's rho

Spearman's rho is a non-parametric correlation.

```{r}
cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum) #Pearson r, correlation that is sensitive to outliers
cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum, method = "spearman") #Spearman's rho, a rank correlation that is less sensitive to outliers
```

####### Minimum vollume ellipsoid

```{r}
cov.mve(na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")]), cor = TRUE)
```

####### Minimum covariance determinant

```{r}
cov.mcd(na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")]), cor = TRUE)
```

####### Winsorized correlation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], winsorize = 0.2, p_adjust = "none")
```

####### Percentage bend correlation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "percentage", p_adjust = "none")
```

####### Biweight midcorrelation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "biweight", p_adjust = "none")
```

###### Robust regression with a single predictor

####### Theil-Sen estimator

The Theil-Sen single median estimator is robust to outliers; have to remove missing values first

```{r}
mydata_subset <- na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")])[1:400,]
mblm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, repeated = FALSE)
```

###### Robust multiple regression

Best when no outliers: MM-type regression estimator

####### Robust linear regression

MM-type regression estimator (best):

```{r}
lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)
```

Iteratively reweighted least squares (IRLS): http://www.ats.ucla.edu/stat/r/dae/rreg.htm

```{r}
rlm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    na.action = na.exclude)
```

####### Robust generalized regression

```{r, warning = FALSE}
glmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    family = "poisson",
    na.action = "na.exclude")
```

Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers

```{r}
ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
       data = mydata,
       na.action = na.exclude)
```

######## Not great options:

Quantile (L1) regression: `rq()` function of quantreg package

```{r}
rq(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)
```

## Examining Model Assumptions

### Distribution of Residuals

#### QQ plots {#qqPlot}

http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html

```{r}
qqPlot(multipleRegressionModel, main = "QQ Plot", id = TRUE)

qqnorm(resid(multipleRegressionModel))
qqnorm(resid(rmsMultipleRegressionModel))
qqnorm(resid(robustLinearRegression))
qqnorm(resid(ltsRegression))

qqnorm(resid(generalizedRegressionModel))
qqnorm(resid(rmsGeneralizedRegressionModel))
qqnorm(resid(robustGeneralizedRegression))

qqnorm(resid(multilevelRegressionModel))
```

#### PP plots {#ppPlot}

```{r}
ppPlot(multipleRegressionModel)
ppPlot(rmsMultipleRegressionModel)
ppPlot(robustLinearRegression)
ppPlot(ltsRegression)

ppPlot(generalizedRegressionModel)
ppPlot(rmsGeneralizedRegressionModel)
ppPlot(robustGeneralizedRegression)
```

#### Density Plot of Residuals {#densityPlotResiduals}

```{r}
studentizedResiduals <- na.omit(rstudent(multipleRegressionModel))
plot(density(studentizedResiduals), col="red")
xfit <- seq(min(studentizedResiduals, na.rm=TRUE), max(studentizedResiduals, na.rm=TRUE), length=40)
lines(xfit, dnorm(xfit), col="gray") #compare to normal distribution
```

### Residual Plots {#residualPlots}

Residual plots are plots of the residuals versus observed/fitted values.

Includes plots of a) model residuals versus observed values on predictors and b) model residuals versus model fitted values.

Note: have to remove `na.action = na.exclude`

Tests include:

- lack-of-fit test for every numeric predictor, t-test for the regressor, added to the model, indicating no lack-of-fit for this type
- Tukey's test for nonadditivity: adding the squares of the fitted values to the model and refitting (evaluates adequacy of model fit)

```{r}
residualPlots(multipleRegressionModelNoMissing, id = TRUE)
```

### Marginal Model Plots {#marginalModelPlots}

Marginal model plots are plots of the outcome versus predictors/fitted values.

Includes plots of a) observed outcome values versus values on predictors (ignoring the other predictors) and b) observed outcome values versus model fitted values.

```{r}
marginalModelPlots(multipleRegressionModel, sd = TRUE, id = TRUE)
```

### Added-Variable Plots {#addedVariablePlots}

Added-variable plots are plots of the partial association between the outcome and each predictor, controlling for all other predictors.

Useful for identifying jointly influential observations and studying the impact of observations on the regression coefficients.

y-axis: residuals from model with all predictors excluding the predictor of interest

x-axis: residuals from model regressing predictor of interest on all other predictors

```{r}
avPlots(multipleRegressionModel, id = TRUE)
```

#### Refit model removing jointly influential observations

```{r}
multipleRegressionModel_removeJointlyInfluentialObs <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                                          data = mydata,
                                                          na.action = na.exclude,
                                                          subset = -c(6318,6023,4022,4040))

avPlots(multipleRegressionModel_removeJointlyInfluentialObs, id = TRUE)

compareCoefs(multipleRegressionModel, multipleRegressionModel_removeJointlyInfluentialObs)
```

### Outlier test

Locates the largest Studentized residuals in absolute value and computes the Bonferroni-corrected p-values based on a t-test for linear models.

Test of outlyingness, i.e., how likely one would have a residual of a given magnitude in a normal distribution with the same sample size

Note: it does not test how extreme an observation is relative to its distribution (i.e., leverage)

```{r}
outlierTest(multipleRegressionModel)
```

### Observations with high leverage

Identifies observations with high leverage (i.e., high hat values)

hat values are an index of leverage (observations that are far from the center of the regressor space and have greater influence on OLS regression coefficients)

```{r}
hist(hatvalues(multipleRegressionModel))
plot(hatvalues(multipleRegressionModel))

influenceIndexPlot(multipleRegressionModel, id = TRUE)

influencePlot(multipleRegressionModel, id = TRUE) # circle size is proportional to Cook's Distance

leveragePlots(multipleRegressionModel, id = TRUE)
```

### Observations with high influence (on OLS regression coefficients)

https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html

```{r}
head(influence.measures(multipleRegressionModel)$infmat)
```

### DFBETA {#dfbeta}

```{r}
head(dfbeta(multipleRegressionModel))

hist(dfbeta(multipleRegressionModel))
dfbetasPlots(multipleRegressionModel, id = TRUE)
```

### DFFITS {#dffits}

```{r}
head(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])

hist(dffits(multipleRegressionModel))
plot(dffits(multipleRegressionModel))
plot(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])
```

### Cook's Distance {#cooksDistance}

Observations that are both outlying (have a high residual from the regression line) and have high leverage (are far from the center of the regressor space) have high influence on the OLS regression coefficients.
An observation will have less influence if it lies on the regression line (not an outlier, i.e., has a low residual) or if it has low leverage (i.e., has a value near the center of a predictor's distribution).

```{r}
head(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])

hist(cooks.distance(multipleRegressionModel))
plot(cooks.distance(multipleRegressionModel))
plot(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])
```

#### Refit model removing values with high cook's distance

```{r}
multipleRegressionModel_2 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371))

multipleRegressionModel_3 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767))

multipleRegressionModel_4 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472))

multipleRegressionModel_5 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170))

multipleRegressionModel_6 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023))

multipleRegressionModel_7 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023,2766))
```

##### Examine how much regression coefficients change when excluding influential observations

```{r}
compareCoefs(multipleRegressionModel, multipleRegressionModel_2, multipleRegressionModel_3, multipleRegressionModel_4, multipleRegressionModel_5, multipleRegressionModel_6, multipleRegressionModel_7)
```

#### Examine how much regression coefficients change when using least trimmed squares (LTS) that is resistant to outliers

```{r}
coef(lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
        data = mydata,
        na.action="na.exclude"))

coef(ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = "na.exclude"))
```

### Resources

Book: An R Companion to Applied Regression

http://people.duke.edu/~rnau/testing.htm

http://www.statmethods.net/stats/rdiagnostics.html

http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html

https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions

# Session Info

```{r, class.source = "fold-hide"}
sessionInfo()
```
</div>
<script type="text/javascript" src="includes/external-links.js"></script>

<br>
<hr>
<br>

<!-- Add lab logo -->
<p style="text-align: center;">
    <a href="https://developmental-psychopathology.lab.uiowa.edu" target="_blank">
        <img alt="Developmental Psychopathology Lab" src="images/formalLogo.png" width="60%">
    </a>
</p>

<br>
<hr>
<br>

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/DevPsyLab" target="_blank" class="fa fa-github fa-fw fa-3x"></a>
    <a href="https://twitter.com/devpsylab" target="_blank" class="fa fa-twitter fa-fw fa-3x"></a>
    <a href="https://www.facebook.com/DevPsyLab" target="_blank" class="fa fa-facebook fa-fw fa-3x"></a>
	<a href="https://www.instagram.com/dev_psy_lab" target="_blank" class="fa fa-instagram fa-fw fa-3x"></a>
</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("regression.Rmd");
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
