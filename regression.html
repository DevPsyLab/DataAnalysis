<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Regression</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TYR0QMREEW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TYR0QMREEW');
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
#rmd-source-code {
  display: none;
}
</style>


<link rel="stylesheet" href="includes/custom.css" type="text/css" />
<link rel="stylesheet" href="font/css/roboto.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Analysis Guides for the Developmental Psychopathology Lab</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="bayesian.html">Bayesian Analysis</a>
    </li>
    <li>
      <a href="dataManagement.html">Data Management</a>
    </li>
    <li>
      <a href="developmentalScaling.html">Developmental Scaling</a>
    </li>
    <li>
      <a href="eegERP.html">Electroencaphalography/Event-Related Potentials</a>
    </li>
    <li>
      <a href="eda.html">Exploratory Data Analysis</a>
    </li>
    <li>
      <a href="factorAnalysis.html">Factor Analysis</a>
    </li>
    <li>
      <a href="figures.html">Figures</a>
    </li>
    <li>
      <a href="git.html">Git, GitLab, and GitHub</a>
    </li>
    <li>
      <a href="hlm.html">Hierarchical Linear Modeling</a>
    </li>
    <li>
      <a href="hpc.html">High-Performance Computing</a>
    </li>
    <li>
      <a href="irt.html">Item Response Theory</a>
    </li>
    <li>
      <a href="jamovi.html">jamovi</a>
    </li>
    <li>
      <a href="jsPsych.html">jsPsych</a>
    </li>
    <li>
      <a href="lamp.html">LAMP (Linux, Apache, MySQL, PHP)</a>
    </li>
    <li>
      <a href="lda.html">Longitudinal Data Analysis</a>
    </li>
    <li>
      <a href="markdown.html">Markdown</a>
    </li>
    <li>
      <a href="sem.html#mediation">Mediation</a>
    </li>
    <li>
      <a href="regression.html#moderation">Moderation/Interaction</a>
    </li>
    <li>
      <a href="mplus.html">Mplus</a>
    </li>
    <li>
      <a href="multipleImputation.html">Multiple Imputation</a>
    </li>
    <li>
      <a href="osf.html">Open Science Framework</a>
    </li>
    <li>
      <a href="pca.html">Principal Component Analysis</a>
    </li>
    <li>
      <a href="R.html">R</a>
    </li>
    <li>
      <a href="regression.html">Regression</a>
    </li>
    <li>
      <a href="SPSS.html">SPSS</a>
    </li>
    <li>
      <a href="sem.html">Structural Equation Modeling</a>
    </li>
  </ul>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://devpsylab.github.io/LabWiki">Lab Wiki</a>
</li>
<li>
  <a href="https://developmental-psychopathology.lab.uiowa.edu">Lab Website</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Regression</h1>

</div>


<div id="preamble" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Preamble</h1>
<div id="install-libraries" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Install
Libraries</h2>
<pre class="r fold-hide"><code>#install.packages(&quot;remotes&quot;)
#remotes::install_github(&quot;DevPsyLab/petersenlab&quot;)</code></pre>
</div>
<div id="load-libraries" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Load Libraries</h2>
<pre class="r fold-hide"><code>library(&quot;petersenlab&quot;)
library(&quot;MASS&quot;)
library(&quot;tidyverse&quot;)
library(&quot;psych&quot;)
library(&quot;rms&quot;)
library(&quot;robustbase&quot;)
library(&quot;brms&quot;)
library(&quot;cvTools&quot;)
library(&quot;car&quot;)
library(&quot;mgcv&quot;)
library(&quot;AER&quot;)
library(&quot;foreign&quot;)
library(&quot;olsrr&quot;)
library(&quot;quantreg&quot;)
library(&quot;mblm&quot;)
library(&quot;effects&quot;)
library(&quot;correlation&quot;)
library(&quot;interactions&quot;)
library(&quot;lavaan&quot;)
library(&quot;regtools&quot;)
library(&quot;mice&quot;)</code></pre>
</div>
</div>
<div id="import-data" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Import Data</h1>
<pre class="r fold-hide"><code>mydata &lt;- read.csv(&quot;https://osf.io/8syp5/download&quot;)</code></pre>
</div>
<div id="data-preparation" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Data Preparation</h1>
<pre class="r fold-hide"><code>mydata$countVariable &lt;- as.integer(mydata$bpi_antisocialT2Sum)
mydata$orderedVariable &lt;- factor(mydata$countVariable, ordered = TRUE)

mydata$female &lt;- NA
mydata$female[which(mydata$sex == &quot;male&quot;)] &lt;- 0
mydata$female[which(mydata$sex == &quot;female&quot;)] &lt;- 1</code></pre>
</div>
<div id="linear-regression" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Linear Regression</h1>
<div id="linear-regression-model" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear regression
model</h2>
<pre class="r fold-show"><code>multipleRegressionModel &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>confint(multipleRegressionModel)</code></pre>
<pre><code>                            2.5 %    97.5 %
(Intercept)             1.0809881 1.3156128
bpi_antisocialT1Sum     0.4290884 0.5019688
bpi_anxiousDepressedSum 0.1035825 0.2179258</code></pre>
<div id="remove-missing-data" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Remove missing
data</h3>
<pre class="r fold-show"><code>multipleRegressionModelNoMissing &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata,
                                       na.action = na.omit)

multipleRegressionModelNoMissing &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit)</code></pre>
</div>
</div>
<div
id="linear-regression-model-on-correlationcovariance-matrix-for-pairwise-deletion"
class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Linear regression
model on correlation/covariance matrix (for pairwise deletion)</h2>
<pre class="r fold-show"><code>multipleRegressionModelPairwise &lt;- setCor(
  y = &quot;bpi_antisocialT2Sum&quot;,
  x = c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;),
  data = cov(mydata[,c(&quot;bpi_antisocialT2Sum&quot;,&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;),
  n.obs = nrow(mydata))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r fold-show"><code>summary(multipleRegressionModelPairwise)</code></pre>
<pre><code>
Multiple Regression from raw data 
setCor(y = &quot;bpi_antisocialT2Sum&quot;, x = c(&quot;bpi_antisocialT1Sum&quot;, 
    &quot;bpi_anxiousDepressedSum&quot;), data = cov(mydata[, c(&quot;bpi_antisocialT2Sum&quot;, 
    &quot;bpi_antisocialT1Sum&quot;, &quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;), 
    n.obs = nrow(mydata))

Multiple Regression from matrix input 

Beta weights 
                        bpi_antisocialT2Sum
(Intercept)                           0.000
bpi_antisocialT1Sum                  -0.031
bpi_anxiousDepressedSum              -1.004

Multiple R 
bpi_antisocialT2Sum 
                  1 

Multiple R2 
bpi_antisocialT2Sum 
                  1 

Cohen&#39;s set correlation R2 
[1] 1

Squared Canonical Correlations
NULL</code></pre>
<pre class="r fold-show"><code>multipleRegressionModelPairwise[c(&quot;coefficients&quot;,&quot;se&quot;,&quot;Probability&quot;,&quot;R2&quot;,&quot;shrunkenR2&quot;)]</code></pre>
<pre><code>$coefficients
                        bpi_antisocialT2Sum
(Intercept)                       0.0000000
bpi_antisocialT1Sum              -0.0311043
bpi_anxiousDepressedSum          -1.0040177

$se
                        bpi_antisocialT2Sum
(Intercept)                             NaN
bpi_antisocialT1Sum                     NaN
bpi_anxiousDepressedSum                 NaN

$Probability
                        bpi_antisocialT2Sum
(Intercept)                             NaN
bpi_antisocialT1Sum                     NaN
bpi_anxiousDepressedSum                 NaN

$R2
bpi_antisocialT2Sum 
                  1 

$shrunkenR2
bpi_antisocialT2Sum 
                Inf </code></pre>
</div>
<div id="linear-regression-model-with-robust-covariance-matrix-rms"
class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Linear regression
model with robust covariance matrix (rms)</h2>
<pre class="r fold-show"><code>rmsMultipleRegressionModel &lt;- robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                         data = mydata,
                                         x = TRUE,
                                         y = TRUE))

rmsMultipleRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Linear Regression Model

ols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, x = TRUE, y = TRUE)

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs    2874    LR chi2    873.09    R2       0.262    
sigma1.9786    d.f.            2    R2 adj   0.261    
d.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    

Residuals

    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

                        Coef   S.E.   t     Pr(&gt;|t|)
Intercept               1.1983 0.0622 19.26 &lt;0.0001 
bpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 
bpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 </code></pre>
<pre class="r fold-show"><code>confint(rmsMultipleRegressionModel)</code></pre>
<pre><code>                             2.5 %    97.5 %
Intercept               1.07631713 1.3202837
bpi_antisocialT1Sum     0.42056957 0.5104877
bpi_anxiousDepressedSum 0.09606644 0.2254418</code></pre>
</div>
<div
id="robust-linear-regression-mm-type-iteratively-reweighted-least-squares-regression"
class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Robust linear
regression (MM-type iteratively reweighted least squares
regression)</h2>
<pre class="r fold-show"><code>robustLinearRegression &lt;- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude)

summary(robustLinearRegression)</code></pre>
<pre><code>
Call:
lmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)
 \--&gt; method = &quot;MM&quot;
Residuals:
     Min       1Q   Median       3Q      Max 
-8.43518 -1.06680 -0.06707  1.14090 13.05599 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              0.94401    0.05406  17.464  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.49135    0.02237  21.966  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.15766    0.03102   5.083 3.96e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Robust residual standard error: 1.628 
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.326, Adjusted R-squared:  0.3255 
Convergence in 14 IRWLS iterations

Robustness weights: 
 12 observations c(52,347,354,517,709,766,768,979,1618,2402,2403,2404)
     are outliers with |weight| = 0 ( &lt; 3.5e-05); 
 283 weights are ~= 1. The remaining 2579 ones are summarized as
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
0.0001311 0.8599000 0.9470000 0.8814000 0.9816000 0.9990000 
Algorithmic parameters: 
       tuning.chi                bb        tuning.psi        refine.tol 
        1.548e+00         5.000e-01         4.685e+00         1.000e-07 
          rel.tol         scale.tol         solve.tol          zero.tol 
        1.000e-07         1.000e-10         1.000e-07         1.000e-10 
      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw 
        3.479e-05         2.365e-11         5.000e-01         5.000e-01 
     nResample         max.it         groups        n.group       best.r.s 
           500             50              5            400              2 
      k.fast.s          k.max    maxit.scale      trace.lev            mts 
             1            200            200              0           1000 
    compute.rd fast.s.large.n 
             0           2000 
                  psi           subsampling                   cov 
           &quot;bisquare&quot;         &quot;nonsingular&quot;         &quot;.vcov.avar1&quot; 
compute.outlier.stats 
                 &quot;SM&quot; 
seed : int(0) </code></pre>
<pre class="r fold-show"><code>confint(robustLinearRegression)</code></pre>
<pre><code>                             2.5 %    97.5 %
(Intercept)             0.83801566 1.0499981
bpi_antisocialT1Sum     0.44749135 0.5352118
bpi_anxiousDepressedSum 0.09683728 0.2184779</code></pre>
</div>
<div id="least-trimmed-squares-regression-for-removing-outliers"
class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Least trimmed squares
regression (for removing outliers)</h2>
<pre class="r fold-show"><code>ltsRegression &lt;- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                        data = mydata,
                        na.action = na.exclude)

summary(ltsRegression)</code></pre>
<pre><code>
Call:
ltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals (from reweighted LS):
    Min      1Q  Median      3Q     Max 
-3.8455 -0.8887  0.0000  0.9983  3.9195 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
Intercept                0.73180    0.04791  15.274  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.54795    0.01523  35.974  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.13903    0.02343   5.935 3.31e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.534 on 2709 degrees of freedom
Multiple R-Squared: 0.4168, Adjusted R-squared: 0.4163 
F-statistic: 967.9 on 2 and 2709 DF,  p-value: &lt; 2.2e-16 </code></pre>
</div>
<div id="bayesian-linear-regression" class="section level2"
number="4.6">
<h2><span class="header-section-number">4.6</span> Bayesian linear
regression</h2>
<pre class="r fold-show"><code>bayesianRegularizedRegression &lt;- brm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     chains = 4,
                                     iter = 2000,
                                     seed = 52242)</code></pre>
<pre class="r fold-show"><code>summary(bayesianRegularizedRegression)</code></pre>
<pre><code> Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   1.20      0.06     1.08     1.32 1.00     4472
bpi_antisocialT1Sum         0.47      0.02     0.43     0.50 1.00     3350
bpi_anxiousDepressedSum     0.16      0.03     0.10     0.22 1.00     3184
                        Tail_ESS
Intercept                   3070
bpi_antisocialT1Sum         3292
bpi_anxiousDepressedSum     2958

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     1.98      0.03     1.93     2.03 1.00     4104     3454

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<div id="generalized-linear-regression" class="section level1"
number="5">
<h1><span class="header-section-number">5</span> Generalized Linear
Regression</h1>
<div id="generalized-regression-model" class="section level2"
number="5.1">
<h2><span class="header-section-number">5.1</span> Generalized
regression model</h2>
<p>In this example, we predict a count variable that has a poisson
distribution. We could change the distribution.</p>
<pre class="r fold-show"><code>generalizedRegressionModel &lt;- glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = &quot;poisson&quot;,
                                  na.action = na.exclude)

summary(generalizedRegressionModel)</code></pre>
<pre><code>
Call:
glm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    family = &quot;poisson&quot;, data = mydata, na.action = na.exclude)

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             0.459999   0.019650  23.410  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.141577   0.004891  28.948  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.047567   0.008036   5.919 3.23e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 5849.4  on 2873  degrees of freedom
Residual deviance: 4562.6  on 2871  degrees of freedom
  (8656 observations deleted due to missingness)
AIC: 11482

Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r fold-show"><code>confint(generalizedRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>                             2.5 %     97.5 %
(Intercept)             0.42136094 0.49838751
bpi_antisocialT1Sum     0.13196544 0.15113708
bpi_anxiousDepressedSum 0.03177425 0.06327445</code></pre>
</div>
<div id="generalized-regression-model-rms" class="section level2"
number="5.2">
<h2><span class="header-section-number">5.2</span> Generalized
regression model (rms)</h2>
<p>In this example, we predict a count variable that has a poisson
distribution. We could change the distribution.</p>
<pre class="r fold-show"><code>rmsGeneralizedRegressionModel &lt;- Glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE,
                                     family = &quot;poisson&quot;)

rmsGeneralizedRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
          countVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

General Linear Model

Glm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    family = &quot;poisson&quot;, data = mydata, x = TRUE, y = TRUE)

                       Model Likelihood    
                             Ratio Test    
          Obs2874    LR chi2    1286.72    
Residual d.f.2871    d.f.             2    
          g 0.387    Pr(&gt; chi2) &lt;0.0001    

                        Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept               0.4600 0.0196 23.41  &lt;0.0001 
bpi_antisocialT1Sum     0.1416 0.0049 28.95  &lt;0.0001 
bpi_anxiousDepressedSum 0.0476 0.0080  5.92  &lt;0.0001 </code></pre>
<pre class="r fold-show"><code>confint(rmsGeneralizedRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in summary.rms(fitted): adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum</code></pre>
</div>
<div id="bayesian-generalized-linear-model" class="section level2"
number="5.3">
<h2><span class="header-section-number">5.3</span> Bayesian generalized
linear model</h2>
<p>In this example, we predict a count variable that has a poisson
distribution. We could change the distribution. For example, we could
use Gamma regression, <code>family = Gamma</code>, when the response
variable is continuous and positive, and the coefficient of
variation–rather than the variance–is constant.</p>
<pre class="r fold-show"><code>bayesianGeneralizedLinearRegression &lt;- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                           data = mydata,
                                           family = poisson,
                                           chains = 4,
                                           seed = 52242,
                                           iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianGeneralizedLinearRegression)</code></pre>
<pre><code> Family: poisson 
  Links: mu = log 
Formula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.46      0.02     0.42     0.50 1.00     3095
bpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2776
bpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2620
                        Tail_ESS
Intercept                   2987
bpi_antisocialT1Sum         2762
bpi_anxiousDepressedSum     2689

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="robust-generalized-regression" class="section level2"
number="5.4">
<h2><span class="header-section-number">5.4</span> Robust generalized
regression</h2>
<pre class="r fold-show"><code>robustGeneralizedRegression &lt;- glmrob(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      family = &quot;poisson&quot;,
                                      na.action = na.exclude)

summary(robustGeneralizedRegression)</code></pre>
<pre><code>
Call:  glmrob(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,      family = &quot;poisson&quot;, data = mydata, na.action = na.exclude) 


Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             0.365934   0.020794  17.598  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.156275   0.005046  30.969  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.050526   0.008332   6.064 1.32e-09 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
Robustness weights w.r * w.x: 
 2273 weights are ~= 1. The remaining 601 ones are summarized as
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1286  0.5550  0.7574  0.7183  0.8874  0.9982 

Number of observations: 2874 
Fitted by method &#39;Mqle&#39;  (in 5 iterations)

(Dispersion parameter for poisson family taken to be 1)

No deviance values available 
Algorithmic parameters: 
   acc    tcc 
0.0001 1.3450 
maxit 
   50 
test.acc 
  &quot;coef&quot; </code></pre>
<pre class="r fold-show"><code>confint(robustGeneralizedRegression)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in glm.control(acc = 1e-04, test.acc = &quot;coef&quot;, maxit = 50, tcc = 1.345): unused arguments (acc = 1e-04, test.acc = &quot;coef&quot;, tcc = 1.345)</code></pre>
</div>
<div id="ordinal-regression-model" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Ordinal regression
model</h2>
<pre class="r fold-show"><code>ordinalRegressionModel1 &lt;- polr(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata)

ordinalRegressionModel2 &lt;- lrm(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  x = TRUE,
  y = TRUE)

ordinalRegressionModel3 &lt;- orm(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  x = TRUE,
  y = TRUE)

summary(ordinalRegressionModel1)</code></pre>
<pre><code>
Re-fitting to get Hessian</code></pre>
<pre><code>Call:
polr(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata)

Coefficients:
                         Value Std. Error t value
bpi_antisocialT1Sum     0.4404    0.01862  23.647
bpi_anxiousDepressedSum 0.1427    0.02615   5.457

Intercepts:
      Value   Std. Error t value
0|1   -0.6001  0.0630    -9.5185
1|2    0.6766  0.0585    11.5656
2|3    1.5801  0.0634    24.9191
3|4    2.4110  0.0720    33.4816
4|5    3.1617  0.0825    38.3006
5|6    3.8560  0.0949    40.6120
6|7    4.5367  0.1106    41.0316
7|8    5.1667  0.1291    40.0147
8|9    5.8129  0.1545    37.6263
9|10   6.5364  0.1962    33.3193
10|11  7.1835  0.2513    28.5820
11|12  7.8469  0.3331    23.5566
12|14  8.7818  0.5113    17.1741

Residual Deviance: 11088.49 
AIC: 11118.49 
(8656 observations deleted due to missingness)</code></pre>
<pre class="r fold-show"><code>confint(ordinalRegressionModel1)</code></pre>
<pre><code>Waiting for profiling to be done...

Re-fitting to get Hessian</code></pre>
<pre><code>                             2.5 %    97.5 %
bpi_antisocialT1Sum     0.40403039 0.4770422
bpi_anxiousDepressedSum 0.09149919 0.1940080</code></pre>
<pre class="r fold-show"><code>ordinalRegressionModel2</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
        orderedVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Logistic Regression Model

lrm(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, x = TRUE, y = TRUE)


Frequencies of Responses

  0   1   2   3   4   5   6   7   8   9  10  11  12  14 
455 597 527 444 313 205 133  80  52  33  16   9   6   4 

                       Model Likelihood       Discrimination    Rank Discrim.    
                             Ratio Test              Indexes          Indexes    
Obs          2874    LR chi2     881.26       R2       0.268    C       0.705    
max |deriv| 2e-12    d.f.             2      R2(2,2874)0.264    Dxy     0.410    
                     Pr(&gt; chi2) &lt;0.0001    R2(2,2803.4)0.269    gamma   0.426    
                                              Brier    0.056    tau-a   0.350    

                        Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=1                     0.6001 0.0630   9.52 &lt;0.0001 
y&gt;=2                    -0.6766 0.0585 -11.57 &lt;0.0001 
y&gt;=3                    -1.5801 0.0634 -24.92 &lt;0.0001 
y&gt;=4                    -2.4110 0.0720 -33.48 &lt;0.0001 
y&gt;=5                    -3.1617 0.0825 -38.30 &lt;0.0001 
y&gt;=6                    -3.8560 0.0949 -40.61 &lt;0.0001 
y&gt;=7                    -4.5367 0.1106 -41.03 &lt;0.0001 
y&gt;=8                    -5.1667 0.1291 -40.01 &lt;0.0001 
y&gt;=9                    -5.8129 0.1545 -37.63 &lt;0.0001 
y&gt;=10                   -6.5365 0.1962 -33.32 &lt;0.0001 
y&gt;=11                   -7.1835 0.2513 -28.58 &lt;0.0001 
y&gt;=12                   -7.8469 0.3331 -23.56 &lt;0.0001 
y&gt;=14                   -8.7818 0.5113 -17.17 &lt;0.0001 
bpi_antisocialT1Sum      0.4404 0.0186  23.65 &lt;0.0001 
bpi_anxiousDepressedSum  0.1427 0.0261   5.46 &lt;0.0001 </code></pre>
<pre class="r fold-show"><code>ordinalRegressionModel3</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
        orderedVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, x = TRUE, y = TRUE)


Frequencies of Responses

  0   1   2   3   4   5   6   7   8   9  10  11  12  14 
455 597 527 444 313 205 133  80  52  33  16   9   6   4 

                       Model Likelihood               Discrimination    Rank Discrim.    
                             Ratio Test                      Indexes          Indexes    
Obs          2874    LR chi2     881.26    R2                  0.268    rho     0.506    
Distinct Y     14    d.f.             2    R2(2,2874)          0.264                     
Median Y        3    Pr(&gt; chi2) &lt;0.0001    R2(2,2803.4)        0.269                     
max |deriv| 6e-06    Score chi2  916.79    |Pr(Y&gt;=median)-0.5| 0.188                     
                     Pr(&gt; chi2) &lt;0.0001                                                  

                        Coef   S.E.   Wald Z Pr(&gt;|Z|)
bpi_antisocialT1Sum     0.4404 0.0186 23.65  &lt;0.0001 
bpi_anxiousDepressedSum 0.1427 0.0261  5.46  &lt;0.0001 </code></pre>
<pre class="r fold-show"><code>confint(ordinalRegressionModel3)</code></pre>
<pre><code>                              2.5 %     97.5 %
y&gt;=1                             NA         NA
y&gt;=2                    -0.79129972 -0.5619711
y&gt;=3                             NA         NA
y&gt;=4                             NA         NA
y&gt;=5                             NA         NA
y&gt;=6                             NA         NA
y&gt;=7                             NA         NA
y&gt;=8                             NA         NA
y&gt;=9                             NA         NA
y&gt;=10                            NA         NA
y&gt;=11                            NA         NA
y&gt;=12                            NA         NA
y&gt;=14                            NA         NA
bpi_antisocialT1Sum      0.40390078  0.4769035
bpi_anxiousDepressedSum  0.09143921  0.1939303</code></pre>
</div>
<div id="bayesian-ordinal-regression-model" class="section level2"
number="5.6">
<h2><span class="header-section-number">5.6</span> Bayesian ordinal
regression model</h2>
<pre class="r fold-show"><code>bayesianOrdinalRegression &lt;- brm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                 data = mydata,
                                 family = cumulative(),
                                 chains = 4,
                                 seed = 52242,
                                 iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianOrdinalRegression)</code></pre>
<pre><code> Family: cumulative 
  Links: mu = logit; disc = identity 
Formula: orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept[1]               -0.60      0.06    -0.73    -0.48 1.00     4982
Intercept[2]                0.67      0.06     0.56     0.79 1.00     7230
Intercept[3]                1.57      0.06     1.45     1.70 1.00     5998
Intercept[4]                2.40      0.07     2.27     2.55 1.00     4945
Intercept[5]                3.15      0.08     2.99     3.31 1.00     4691
Intercept[6]                3.84      0.10     3.66     4.03 1.00     4206
Intercept[7]                4.52      0.11     4.30     4.74 1.00     4280
Intercept[8]                5.15      0.13     4.90     5.41 1.00     4535
Intercept[9]                5.79      0.15     5.50     6.10 1.00     4853
Intercept[10]               6.51      0.20     6.14     6.92 1.00     5189
Intercept[11]               7.16      0.25     6.70     7.66 1.00     5162
Intercept[12]               7.86      0.33     7.27     8.56 1.00     5183
Intercept[13]               8.91      0.53     8.01    10.07 1.00     4954
bpi_antisocialT1Sum         0.44      0.02     0.40     0.48 1.00     3659
bpi_anxiousDepressedSum     0.14      0.03     0.09     0.19 1.00     3614
                        Tail_ESS
Intercept[1]                3581
Intercept[2]                3613
Intercept[3]                3420
Intercept[4]                3509
Intercept[5]                3288
Intercept[6]                3069
Intercept[7]                3181
Intercept[8]                3274
Intercept[9]                3351
Intercept[10]               3060
Intercept[11]               3211
Intercept[12]               3204
Intercept[13]               3280
bpi_antisocialT1Sum         2854
bpi_anxiousDepressedSum     3250

Family Specific Parameters: 
     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
disc     1.00      0.00     1.00     1.00   NA       NA       NA

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="bayesian-count-regression-model" class="section level2"
number="5.7">
<h2><span class="header-section-number">5.7</span> Bayesian count
regression model</h2>
<pre class="r fold-show"><code>bayesianCountRegression &lt;- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                               data = mydata,
                               family = &quot;poisson&quot;,
                               chains = 4,
                               seed = 52242,
                               iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianCountRegression)</code></pre>
<pre><code> Family: poisson 
  Links: mu = log 
Formula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 2874) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.46      0.02     0.42     0.50 1.00     3095
bpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2776
bpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2620
                        Tail_ESS
Intercept                   2987
bpi_antisocialT1Sum         2762
bpi_anxiousDepressedSum     2689

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="logistic-regression-model-rms" class="section level2"
number="5.8">
<h2><span class="header-section-number">5.8</span> Logistic regression
model (rms)</h2>
<pre class="r fold-show"><code>logisticRegressionModel &lt;- robcov(lrm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      x = TRUE,
                                      y = TRUE))

logisticRegressionModel</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
                 female     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                      2                    7613                    7616 

Logistic Regression Model

lrm(formula = female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, x = TRUE, y = TRUE)

                       Model Likelihood       Discrimination    Rank Discrim.    
                             Ratio Test              Indexes          Indexes    
Obs          3914    LR chi2      66.63       R2       0.023    C       0.571    
 0           1965    d.f.             2      R2(2,3914)0.016    Dxy     0.142    
 1           1949    Pr(&gt; chi2) &lt;0.0001    R2(2,2935.5)0.022    gamma   0.147    
max |deriv| 1e-12                             Brier    0.246    tau-a   0.071    

                        Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept                0.3002 0.0529  5.67  &lt;0.0001 
bpi_antisocialT1Sum     -0.1244 0.0166 -7.48  &lt;0.0001 
bpi_anxiousDepressedSum  0.0382 0.0253  1.51  0.1314  </code></pre>
<pre class="r fold-show"><code>confint(logisticRegressionModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Error in summary.rms(fitted): adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum</code></pre>
</div>
<div id="bayesian-logistic-regression-model" class="section level2"
number="5.9">
<h2><span class="header-section-number">5.9</span> Bayesian logistic
regression model</h2>
<pre class="r fold-show"><code>bayesianLogisticRegression &lt;- brm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = bernoulli,
                                  chains = 4,
                                  seed = 52242,
                                  iter = 2000)</code></pre>
<pre class="r fold-show"><code>summary(bayesianLogisticRegression)</code></pre>
<pre><code> Family: bernoulli 
  Links: mu = logit 
Formula: female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
   Data: mydata (Number of observations: 3914) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Population-Level Effects: 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
Intercept                   0.30      0.05     0.20     0.41 1.00     4125
bpi_antisocialT1Sum        -0.13      0.02    -0.16    -0.09 1.00     2943
bpi_anxiousDepressedSum     0.04      0.03    -0.01     0.09 1.00     3084
                        Tail_ESS
Intercept                   3269
bpi_antisocialT1Sum         2857
bpi_anxiousDepressedSum     2730

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<div id="hierarchical-linear-regression" class="section level1"
number="6">
<h1><span class="header-section-number">6</span> Hierarchical Linear
Regression</h1>
<pre class="r fold-show"><code>model1 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum,
             data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit,
             na.action = na.exclude)

model2 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
             data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit,
             na.action = na.exclude)

summary(model2)$adj.r.squared</code></pre>
<pre><code>[1] 0.2614694</code></pre>
<pre class="r fold-show"><code>anova(model1, model2)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Sum of Sq"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2872","2":"11358.86","3":"NA","4":"NA","5":"NA","6":"NA","_rn_":"1"},{"1":"2871","2":"11239.86","3":"1","4":"119.0016","5":"30.39663","6":"3.834142e-08","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>summary(model2)$adj.r.squared - summary(model1)$adj.r.squared</code></pre>
<pre><code>[1] 0.007559301</code></pre>
</div>
<div id="moderation" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Moderated Multiple
Regression</h1>
<p><a
href="https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html"
class="uri">https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html</a>
(archived at <a href="https://perma.cc/P34H-7BH3"
class="uri">https://perma.cc/P34H-7BH3</a>)</p>
<pre class="r fold-show"><code>states &lt;- as.data.frame(state.x77)</code></pre>
<div id="mean-center-predictors" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Mean Center
Predictors</h2>
<p>Make sure to mean-center or orthogonalize predictors before computing
the interaction term.</p>
<pre class="r fold-show"><code>states$Illiteracy_centered &lt;- scale(states$Illiteracy, scale = FALSE)
states$Murder_centered &lt;- scale(states$Murder, scale = FALSE)</code></pre>
</div>
<div id="model" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Model</h2>
<pre class="r fold-show"><code>interactionModel &lt;- lm(
  Income ~ Illiteracy_centered + Murder_centered + Illiteracy_centered:Murder_centered + `HS Grad`,
  data = states)</code></pre>
</div>
<div id="plots" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Plots</h2>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, plot.points = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-31-2.png" width="672" /></p>
<pre class="r fold-show"><code>interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, interval = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-31-3.png" width="672" /></p>
<pre class="r fold-show"><code>johnson_neyman(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, alpha = .05)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of
Illiteracy_centered is p &lt; .05.

Note: The range of observed values of Murder_centered is [-5.98, 7.72]</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-31-4.png" width="672" /></p>
</div>
<div id="simple-slopes-analysis" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Simple Slopes
Analysis</h2>
<pre class="r fold-show"><code>sim_slopes(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, johnson_neyman = FALSE)</code></pre>
<pre><code>SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy_centered when Murder_centered = -3.69154e+00 (- 1 SD): 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  395.34   274.84     1.44   0.16

Slope of Illiteracy_centered when Murder_centered = -1.24345e-16 (Mean): 

   Est.     S.E.   t val.      p
------- -------- -------- ------
  37.12   192.56     0.19   0.85

Slope of Illiteracy_centered when Murder_centered =  3.69154e+00 (+ 1 SD): 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -321.10   183.49    -1.75   0.09</code></pre>
<pre class="r fold-show"><code>sim_slopes(interactionModel,
           pred = Illiteracy_centered,
           modx = Murder_centered,
           modx.values = c(0, 5, 10),
           johnson_neyman = FALSE)</code></pre>
<pre><code>SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy_centered when Murder_centered =  0.00: 

   Est.     S.E.   t val.      p
------- -------- -------- ------
  37.12   192.56     0.19   0.85

Slope of Illiteracy_centered when Murder_centered =  5.00: 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -448.07   202.17    -2.22   0.03

Slope of Illiteracy_centered when Murder_centered = 10.00: 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -933.27   330.10    -2.83   0.01</code></pre>
</div>
<div id="johnson-neyman-intervals" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Johnson-Neyman
intervals</h2>
<p>Indicates all the values of the moderator for which the slope of the
predictor is statistically significant.</p>
<pre class="r fold-show"><code>sim_slopes(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, johnson_neyman = TRUE)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of
Illiteracy_centered is p &lt; .05.

Note: The range of observed values of Murder_centered is [-5.98, 7.72]

SIMPLE SLOPES ANALYSIS 

Slope of Illiteracy_centered when Murder_centered = -3.69154e+00 (- 1 SD): 

    Est.     S.E.   t val.      p
-------- -------- -------- ------
  395.34   274.84     1.44   0.16

Slope of Illiteracy_centered when Murder_centered = -1.24345e-16 (Mean): 

   Est.     S.E.   t val.      p
------- -------- -------- ------
  37.12   192.56     0.19   0.85

Slope of Illiteracy_centered when Murder_centered =  3.69154e+00 (+ 1 SD): 

     Est.     S.E.   t val.      p
--------- -------- -------- ------
  -321.10   183.49    -1.75   0.09</code></pre>
<pre class="r fold-show"><code>probe_interaction(interactionModel,
                  pred = Illiteracy_centered,
                  modx = Murder_centered,
                  cond.int = TRUE,
                  interval = TRUE,
                  jnplot = TRUE)</code></pre>
<pre><code>JOHNSON-NEYMAN INTERVAL 

When Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of
Illiteracy_centered is p &lt; .05.

Note: The range of observed values of Murder_centered is [-5.98, 7.72]</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre><code>SIMPLE SLOPES ANALYSIS 

When Murder_centered = -3.69154e+00 (- 1 SD): 

                                        Est.     S.E.   t val.      p
---------------------------------- --------- -------- -------- ------
Slope of Illiteracy_centered          395.34   274.84     1.44   0.16
Conditional intercept                4523.23   134.99    33.51   0.00

When Murder_centered = -1.24345e-16 (Mean): 

                                        Est.     S.E.   t val.      p
---------------------------------- --------- -------- -------- ------
Slope of Illiteracy_centered           37.12   192.56     0.19   0.85
Conditional intercept                4586.22    85.52    53.63   0.00

When Murder_centered =  3.69154e+00 (+ 1 SD): 

                                        Est.     S.E.   t val.      p
---------------------------------- --------- -------- -------- ------
Slope of Illiteracy_centered         -321.10   183.49    -1.75   0.09
Conditional intercept                4649.22   118.97    39.08   0.00</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-33-2.png" width="672" /></p>
</div>
</div>
<div id="missingness" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Approaches to Handling
Missingness</h1>
<div id="listwiseDeletion" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Listwise
deletion</h2>
<p>Listwise deletion deletes every row in the data file that has a
missing value for one of the model variables.</p>
<pre class="r fold-show"><code>listwiseDeletionModel &lt;- lm(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  na.action = na.exclude)

summary(listwiseDeletionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>confint(listwiseDeletionModel)</code></pre>
<pre><code>                            2.5 %    97.5 %
(Intercept)             1.0809881 1.3156128
bpi_antisocialT1Sum     0.4290884 0.5019688
bpi_anxiousDepressedSum 0.1035825 0.2179258</code></pre>
</div>
<div id="pairwiseDeletion" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Pairwise
deletion</h2>
<p>Also see here:</p>
<ul>
<li><a
href="https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002"
class="uri">https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002</a>
(archived at <a href="https://perma.cc/GH5T-RXD9"
class="uri">https://perma.cc/GH5T-RXD9</a>)</li>
<li><a
href="https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure"
class="uri">https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure</a>
(archived at <a href="https://perma.cc/F7EL-AUFZ"
class="uri">https://perma.cc/F7EL-AUFZ</a>)</li>
<li><a
href="https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"
class="uri">https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re</a>
(archived at <a href="https://perma.cc/KU3X-FB2C"
class="uri">https://perma.cc/KU3X-FB2C</a>)</li>
<li><a
href="https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values"
class="uri">https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values</a>
(archived at <a href="https://perma.cc/QWQ5-2TLW"
class="uri">https://perma.cc/QWQ5-2TLW</a>)</li>
<li><a
href="https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps"
class="uri">https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps</a>
(archived at <a href="https://perma.cc/UC4K-2L9T"
class="uri">https://perma.cc/UC4K-2L9T</a>)</li>
</ul>
<p>Adapted from here: <a
href="https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise"
class="uri">https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise</a>
(archived at <a href="https://perma.cc/EGU6-3M3Q"
class="uri">https://perma.cc/EGU6-3M3Q</a>)</p>
<pre class="r fold-show"><code>modelData &lt;- mydata[,c(&quot;bpi_antisocialT2Sum&quot;,&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)]
varMeans &lt;- colMeans(modelData, na.rm = TRUE)
varCovariances &lt;- cov(modelData, use = &quot;pairwise&quot;)

pairwiseRegression_syntax &lt;- &#39;
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
&#39;

pairwiseRegression_fit &lt;- lavaan(
  pairwiseRegression_syntax,
  sample.mean = varMeans,
  sample.cov = varCovariances,
  sample.nobs = sum(complete.cases(modelData))
)

summary(
  pairwiseRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)</code></pre>
<pre><code>lavaan 0.6.17 ended normally after 1 iteration

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                         4

  Number of observations                          2874

Model Test User Model:
                                                      
  Test statistic                                 0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Regressions:
                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  bpi_antisocialT2Sum ~                                                      
    bpi_antsclT1Sm         0.441    0.018   24.611    0.000    0.441    0.456
    bp_nxsDprssdSm         0.137    0.028    4.854    0.000    0.137    0.090

Intercepts:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    1.175    0.058   20.125    0.000    1.175    0.523

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    3.755    0.099   37.908    0.000    3.755    0.743

R-Square:
                   Estimate
    bpi_antsclT2Sm    0.257</code></pre>
</div>
<div id="fiml" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Full-information
maximum likelihood (FIML)</h2>
<pre class="r fold-show"><code>fimlRegression_syntax &lt;- &#39;
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
&#39;

fimlRegression_fit &lt;- lavaan(
  fimlRegression_syntax,
  data = mydata,
  missing = &quot;ML&quot;,
)</code></pre>
<pre><code>Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 7616 cases were deleted due to missing values in 
          exogenous variable(s), while fixed.x = TRUE.</code></pre>
<pre class="r fold-show"><code>summary(
  fimlRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)</code></pre>
<pre><code>lavaan 0.6.17 ended normally after 11 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                         4

                                                  Used       Total
  Number of observations                          3914       11530
  Number of missing patterns                         2            

Model Test User Model:
                                                      
  Test statistic                                 0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Observed
  Observed information based on                Hessian

Regressions:
                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  bpi_antisocialT2Sum ~                                                      
    bpi_antsclT1Sm         0.466    0.019   25.062    0.000    0.466    0.466
    bp_nxsDprssdSm         0.161    0.029    5.516    0.000    0.161    0.102

Intercepts:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    1.198    0.060   20.039    0.000    1.198    0.516

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .bpi_antsclT2Sm    3.911    0.103   37.908    0.000    3.911    0.725

R-Square:
                   Estimate
    bpi_antsclT2Sm    0.275</code></pre>
</div>
<div id="imputation" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> Multiple
imputation</h2>
<pre class="r fold-show"><code>modelData_imputed &lt;- mice(
  modelData,
  m = 5,
  method = &quot;pmm&quot;) # predictive mean matching; can choose among many methods</code></pre>
<pre><code>
 iter imp variable
  1   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  1   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  2   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  3   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  4   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum
  5   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum</code></pre>
<pre class="r fold-show"><code>imputedData_fit &lt;- with(
  modelData_imputed,
  lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum))

imputedData_pooledEstimates &lt;- pool(imputedData_fit)
summary(imputedData_pooledEstimates)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["term"],"name":[1],"type":["fct"],"align":["left"]},{"label":["estimate"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["std.error"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["statistic"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["df"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["p.value"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"1.1812757","3":"0.14313890","4":"8.252653","5":"4.305769","6":"0.0008511883"},{"1":"bpi_antisocialT1Sum","2":"0.4348888","3":"0.03821409","4":"11.380327","5":"4.456219","6":"0.0001841263"},{"1":"bpi_anxiousDepressedSum","2":"0.1650603","3":"0.06220642","4":"2.653428","5":"4.473293","6":"0.0506154526"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="model-building-steps" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Model Building
Steps</h1>
<ol style="list-style-type: decimal">
<li>Examine extent and type of missing data, consider <a
href="#missingness">how to handle missing values</a> (<a
href="#imputation">multiple imputation</a>, <a href="#fiml">FIML</a>, <a
href="#pairwiseDeletion">pairwise deletion</a>, <a
href="#listwiseDeletion">listwise deletion</a>)
<ul>
<li>Little’s MCAR test from <code>mcar_test()</code> function of the
<code>njtierney/naniar</code> package</li>
<li>Bayesian handling of missing data: <a
href="https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html"
class="uri">https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html</a>
(archived at <a href="https://perma.cc/Z4HT-QXWR"
class="uri">https://perma.cc/Z4HT-QXWR</a>)</li>
</ul></li>
<li>Examine descriptive statistics, consider variable
transformations</li>
<li>Examine scatterplot matrix to examine whether associations between
predictor and outcomes are linear or nonlinear</li>
<li>Model building: theory and empiricism (stepwise regression,
likelihood ratio tests, k-fold cross validation to check for
over-fitting compared to simpler model)</li>
<li>Test assumptions
<ul>
<li>Examine whether predictors have linear association with outcome
(Residual Plots, Marginal Model Plots, Added-Variable Plots—check for
non-horizontal lines)</li>
<li>Examine whether residuals have constant variance across levels of
outcome and predictors Residual Plots, Spread-Level Plots—check for
fan-shaped plot or other increasing/decreasing structure)</li>
<li>Examine whether predictors show multicollinearity (VIF)</li>
<li>Examine whether residuals are normally distributed (QQ plot and
density plot)</li>
<li>Examine influence of influential observations (high outlyingness and
leverage) on parameter stimates by iteratively removing influential
observations and refitting (Added-Variable Plots)</li>
</ul></li>
<li>Handle violated assumptions, select final set of predictors/outcomes
and transformation of each</li>
<li>Use k-fold cross validation to identify the best estimation
procedure (OLS, MM, or LTS) on the final model</li>
<li>Use identified estimation procedure to fit final model and determine
the best parameter point estimates</li>
<li>Calculate bootstrapped estimates to determine the confidence
intervals around the parameter estimates of the final model</li>
</ol>
</div>
<div id="bootstrapped-estimates" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Bootstrapped
Estimates</h1>
<p>To determine the confidence intervals of parameter estimates</p>
<div id="linear-regression-1" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Linear
Regression</h2>
<pre class="r fold-show"><code>multipleRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(multipleRegressionModelBootstrapped)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["R"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["original"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["bootBias"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bootSE"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bootMed"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1000","2":"1.1983004","3":"0.0014493812","4":"0.06328970","5":"1.1972919","_rn_":"(Intercept)"},{"1":"1000","2":"0.4655286","3":"-0.0007233581","4":"0.02335905","5":"0.4650478","_rn_":"bpi_antisocialT1Sum"},{"1":"1000","2":"0.1607541","3":"0.0004515062","4":"0.03226340","5":"0.1624174","_rn_":"bpi_anxiousDepressedSum"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>confint(multipleRegressionModelBootstrapped, level = .95, type = &quot;bca&quot;)</code></pre>
<pre><code>Warning in confint.boot(multipleRegressionModelBootstrapped, level = 0.95, :
BCa method fails for this problem.  Using &#39;perc&#39; instead</code></pre>
<pre><code>Bootstrap percent confidence intervals

                             2.5 %    97.5 %
(Intercept)             1.07699943 1.3255073
bpi_antisocialT1Sum     0.41958572 0.5079732
bpi_anxiousDepressedSum 0.09828903 0.2261259</code></pre>
<pre class="r fold-show"><code>hist(multipleRegressionModelBootstrapped)</code></pre>
<pre><code>Warning in confint.boot(x, type = ci, level = level): BCa method fails for this
problem.  Using &#39;perc&#39; instead</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
<div id="generalized-regression" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Generalized
Regression</h2>
<pre class="r fold-show"><code>generalizedRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(generalizedRegressionModelBootstrapped)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["R"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["original"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["bootBias"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bootSE"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bootMed"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1000","2":"1.1983004","3":"-0.0002712900","4":"0.06213884","5":"1.1965101","_rn_":"(Intercept)"},{"1":"1000","2":"0.4655286","3":"-0.0002141366","4":"0.02263479","5":"0.4652356","_rn_":"bpi_antisocialT1Sum"},{"1":"1000","2":"0.1607541","3":"-0.0003051812","4":"0.03335354","5":"0.1613450","_rn_":"bpi_anxiousDepressedSum"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>confint(generalizedRegressionModelBootstrapped, level = .95, type = &quot;bca&quot;)</code></pre>
<pre><code>Warning in confint.boot(generalizedRegressionModelBootstrapped, level = 0.95, :
BCa method fails for this problem.  Using &#39;perc&#39; instead</code></pre>
<pre><code>Bootstrap percent confidence intervals

                             2.5 %    97.5 %
(Intercept)             1.08098776 1.3212744
bpi_antisocialT1Sum     0.41984922 0.5099598
bpi_anxiousDepressedSum 0.09307757 0.2275430</code></pre>
<pre class="r fold-show"><code>hist(generalizedRegressionModelBootstrapped)</code></pre>
<pre><code>Warning in confint.boot(x, type = ci, level = level): BCa method fails for this
problem.  Using &#39;perc&#39; instead</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="cross-validation" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Cross Validation</h1>
<p>To examine degree of prediction error and over-fitting to determine
best model</p>
<p><a
href="https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best"
class="uri">https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best</a>
(archived at <a href="https://perma.cc/38BL-KLRJ"
class="uri">https://perma.cc/38BL-KLRJ</a>)</p>
<div id="k-fold-cross-validation" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> K-fold cross
validation</h2>
<pre class="r fold-show"><code>kFolds &lt;- 10
replications &lt;- 20

folds &lt;- cvFolds(nrow(mydata), K = kFolds, R = replications)

fitLm &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = &quot;na.exclude&quot;)

fitLmrob &lt;- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                  data = mydata,
                  na.action = &quot;na.exclude&quot;)

fitLts &lt;- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                 data = mydata,
                 na.action = &quot;na.exclude&quot;)

cvFitLm &lt;- cvLm(fitLm, K = kFolds, R = replications)

cvFitLmrob &lt;- cvLmrob(fitLmrob, K = kFolds, R = replications)
cvFitLts &lt;- cvLts(fitLts, K = kFolds, R = replications)

cvFits &lt;- cvSelect(OLS = cvFitLm, MM = cvFitLmrob, LTS = cvFitLts)
cvFits</code></pre>
<pre><code>
10-fold CV results:
  Fit       CV
1 OLS 1.980610
2  MM 1.026687
3 LTS 1.006749

Best model:
   CV 
&quot;LTS&quot; </code></pre>
<pre class="r fold-show"><code>bwplot(cvFits, xlab = &quot;Root Mean Square Error&quot;, xlim= c(0, max(cvFits$cv$CV) + 0.2))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
</div>
<div id="examining-model-fits" class="section level1" number="12">
<h1><span class="header-section-number">12</span> Examining Model
Fits</h1>
<div id="effect-plots" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Effect Plots</h2>
<pre class="r fold-show"><code>allEffects(multipleRegressionModel)</code></pre>
<pre><code> model: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum

 bpi_antisocialT1Sum effect
bpi_antisocialT1Sum
       0      3.2      6.5      9.8       13 
1.394591 2.884283 4.420527 5.956772 7.446463 

 bpi_anxiousDepressedSum effect
bpi_anxiousDepressedSum
       0        2        4        6        8 
2.502636 2.824145 3.145653 3.467161 3.788669 </code></pre>
<pre class="r fold-show"><code>plot(allEffects(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
</div>
<div id="confidence-ellipses" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Confidence
Ellipses</h2>
<pre class="r fold-show"><code>confidenceEllipse(multipleRegressionModel, levels = c(0.5, .95))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
</div>
<div id="data-ellipse" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Data Ellipse</h2>
<pre class="r fold-show"><code>mydata_nomissing &lt;- na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)])
dataEllipse(mydata_nomissing$bpi_antisocialT1Sum, mydata_nomissing$bpi_antisocialT2Sum, levels = c(0.5, .95))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
</div>
</div>
<div id="diagnostics" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Diagnostics</h1>
<div id="assumptions" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Assumptions</h2>
<div id="linearAssociation" class="section level3" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> 1. Linear relation
between predictors and outcome</h3>
<div id="ways-to-test" class="section level4" number="13.1.1.1">
<h4><span class="header-section-number">13.1.1.1</span> Ways to
Test</h4>
<div id="before-model-fitting" class="section level5"
number="13.1.1.1.1">
<h5><span class="header-section-number">13.1.1.1.1</span> Before Model
Fitting</h5>
<ul>
<li>scatterplot matrix</li>
<li>distance correlation</li>
</ul>
<div id="scatterplot-matrix" class="section level6"
number="13.1.1.1.1.1">
<h6><span class="header-section-number">13.1.1.1.1.1</span> Scatterplot
Matrix</h6>
<pre class="r fold-show"><code>scatterplotMatrix(~ bpi_antisocialT2Sum + bpi_antisocialT1Sum + bpi_anxiousDepressedSum, data = mydata)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
</div>
<div id="distance-correlation" class="section level6"
number="13.1.1.1.1.2">
<h6><span class="header-section-number">13.1.1.1.1.2</span> Distance
correlation</h6>
<p>The distance correlation is an index of the degree of the linear and
non-linear association between two variables.</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;distance&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.231721","4":"0.95","5":"0.1968321","6":"0.2660239","7":"484.0006","8":"4128499","9":"0","10":"Distance (Bias Corrected) correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="after-model-fitting" class="section level5"
number="13.1.1.1.2">
<h5><span class="header-section-number">13.1.1.1.2</span> After Model
Fitting</h5>
<p>Check for nonlinearities (non-horizontal line) in plots of:</p>
<ul>
<li>Residuals versus fitted values (<a href="#residualPlots">Residual
Plots</a>)—best</li>
<li>Residuals versus predictors (<a href="#residualPlots">Residual
Plots</a>)</li>
<li>Outcome versus fitted values (<a href="#marginalModelPlots">Marginal
Model Plots</a>)</li>
<li>Outcome versus predictors, ignoring other predictors (<a
href="#marginalModelPlots">Marginal Model Plots</a>)</li>
<li>Outcome versus predictors, controlling for other predictors (<a
href="#addedVariablePlots">Added-Variable Plots</a>)</li>
</ul>
</div>
</div>
<div id="ways-to-handle" class="section level4" number="13.1.1.2">
<h4><span class="header-section-number">13.1.1.2</span> Ways to
Handle</h4>
<ul>
<li>Transform outcome/predictor variables (Box-Cox transformations)</li>
<li>Semi-parametric regression models: Generalized additive models
(GAM)</li>
<li>Non-parametric regression models: Nearest-Neighbor Kernel
Regression</li>
</ul>
<div id="semi-parametric-or-non-parametric-regression-models"
class="section level5" number="13.1.1.2.1">
<h5><span class="header-section-number">13.1.1.2.1</span>
Semi-parametric or non-parametric regression models</h5>
<p><a href="http://www.lisa.stat.vt.edu/?q=node/7517"
class="uri">http://www.lisa.stat.vt.edu/?q=node/7517</a> (archived at <a
href="https://web.archive.org/web/20180113065042/http://www.lisa.stat.vt.edu/?q=node/7517"
class="uri">https://web.archive.org/web/20180113065042/http://www.lisa.stat.vt.edu/?q=node/7517</a>)</p>
<p>Note: using semi-parametric or non-parametric models increases fit in
context of nonlinearity at the expense of added complexity. Make sure to
avoid fitting an overly complex model (e.g., use k-fold cross
validation). Often, the simpler (generalized) linear model is preferable
to semi-paremetric or non-parametric approaches</p>
<div id="semi-parametric-generalized-additive-models"
class="section level6" number="13.1.1.2.1.1">
<h6><span class="header-section-number">13.1.1.2.1.1</span>
Semi-parametric: Generalized Additive Models</h6>
<p><a
href="http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models"
class="uri">http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models</a>
(archived at <a
href="https://web.archive.org/web/20170213041653/http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models"
class="uri">https://web.archive.org/web/20170213041653/http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models</a>)</p>
<pre class="r fold-show"><code>generalizedAdditiveModel &lt;- gam(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                family = gaussian(),
                                data = mydata, na.action = na.exclude)

summary(generalizedAdditiveModel)</code></pre>
<pre><code>
Family: gaussian 
Link function: identity 

Formula:
bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum

Parametric coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1


R-sq.(adj) =  0.261   Deviance explained = 26.2%
GCV = 3.9191  Scale est. = 3.915     n = 2874</code></pre>
<pre class="r fold-show"><code>confint(generalizedAdditiveModel)</code></pre>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>Warning in X[, nonA, drop = FALSE] %*% B0[nonA] + O: longer object length is
not a multiple of shorter object length</code></pre>
<pre><code>Error in glm.control(nthreads = 1, ncv.threads = 1, irls.reg = 0, epsilon = 1e-07, : unused arguments (nthreads = 1, ncv.threads = 1, irls.reg = 0, mgcv.tol = 1e-07, mgcv.half = 15, rank.tol = 1.49011611938477e-08, nlm = list(7, 1e-06, 2, 1e-04, 200, FALSE), optim = list(1e+07), newton = list(1e-06, 5, 2, 30, FALSE), idLinksBases = TRUE, scalePenalty = TRUE, efs.lspmax = 15, efs.tol = 0.1, keepData = FALSE, scale.est = &quot;fletcher&quot;, edge.correct = FALSE)</code></pre>
</div>
<div id="non-parametric-nearest-neighbor-kernel-regression"
class="section level6" number="13.1.1.2.1.2">
<h6><span class="header-section-number">13.1.1.2.1.2</span>
Non-parametric: Nearest-Neighbor Kernel Regression</h6>
</div>
</div>
</div>
</div>
<div id="exogeneity" class="section level3" number="13.1.2">
<h3><span class="header-section-number">13.1.2</span> 2. Exogeneity</h3>
<p>Exogeneity means that the association between predictors and outcome
is fully causal and unrelated to other variables.</p>
<div id="ways-to-test-1" class="section level4" number="13.1.2.1">
<h4><span class="header-section-number">13.1.2.1</span> Ways to
Test</h4>
<ul>
<li>Durbin-Wu-Hausman test of endogeneity</li>
</ul>
<div id="durbin-wu-hausman-test-of-endogeneity" class="section level5"
number="13.1.2.1.1">
<h5><span class="header-section-number">13.1.2.1.1</span>
Durbin-Wu-Hausman test of endogeneity</h5>
<p>The instrumental variables (2SLS) estimator is implemented in the
<code>R</code> package <code>AER</code> as command:</p>
<pre class="r fold-show"><code>ivreg(y ~ x1 + x2 + w1 + w2 | z1 + z2 + z3 + w1 + w2)</code></pre>
<p>where <code>x1</code> and <code>x2</code> are endogenous regressors,
<code>w1</code> and <code>w2</code> exogeneous regressors, and
<code>z1</code> to <code>z3</code> are excluded instruments.</p>
<p>Durbin-Wu-Hausman test:</p>
<pre class="r fold-show"><code>hsng2 &lt;- read.dta(&quot;http://www.stata-press.com/data/r11/hsng2.dta&quot;) #archived at https://perma.cc/7P2Q-ARKR</code></pre>
<pre class="r fold-show"><code>fiv &lt;- ivreg(rent ~ hsngval + pcturban | pcturban + faminc + reg2 + reg3 + reg4, data = hsng2) #Housing values are likely endogeneous and therefore instrumented by median family income (faminc) and 3 regional dummies (reg2, reg4, reg4)

summary(fiv, diagnostics = TRUE)</code></pre>
<pre><code>
Call:
ivreg(formula = rent ~ hsngval + pcturban | pcturban + faminc + 
    reg2 + reg3 + reg4, data = hsng2)

Residuals:
     Min       1Q   Median       3Q      Max 
-84.1948 -11.6023  -0.5239   8.6583  73.6130 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 1.207e+02  1.571e+01   7.685 7.55e-10 ***
hsngval     2.240e-03  3.388e-04   6.612 3.17e-08 ***
pcturban    8.152e-02  3.082e-01   0.265    0.793    

Diagnostic tests:
                 df1 df2 statistic  p-value    
Weak instruments   4  44     13.30  3.5e-07 ***
Wu-Hausman         1  46     15.91 0.000236 ***
Sargan             3  NA     11.29 0.010268 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 22.86 on 47 degrees of freedom
Multiple R-Squared: 0.5989, Adjusted R-squared: 0.5818 
Wald test: 42.66 on 2 and 47 DF,  p-value: 2.731e-11 </code></pre>
<p>The Eicker-Huber-White covariance estimator which is robust to
heteroscedastic error terms is reported after estimation with
<code>vcov = sandwich</code> in <code>coeftest()</code></p>
<p>First stage results are reported by explicitly estimating them. For
example:</p>
<pre class="r fold-show"><code>first &lt;- lm(hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, data = hsng2)

summary(first)</code></pre>
<pre><code>
Call:
lm(formula = hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, 
    data = hsng2)

Residuals:
   Min     1Q Median     3Q    Max 
-10504  -5223  -1162   2939  46756 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.867e+04  1.200e+04  -1.557 0.126736    
pcturban     1.822e+02  1.150e+02   1.584 0.120289    
faminc       2.731e+00  6.819e-01   4.006 0.000235 ***
reg2        -5.095e+03  4.122e+03  -1.236 0.223007    
reg3        -1.778e+03  4.073e+03  -0.437 0.664552    
reg4         1.341e+04  4.048e+03   3.314 0.001849 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 9253 on 44 degrees of freedom
Multiple R-squared:  0.6908,    Adjusted R-squared:  0.6557 
F-statistic: 19.66 on 5 and 44 DF,  p-value: 3.032e-10</code></pre>
<p>In case of a single endogenous variable (K = 1), the F-statistic to
assess weak instruments is reported after estimating the first stage
with, for example:</p>
<pre class="r fold-show"><code>waldtest(first, . ~ . - faminc - reg2 - reg3 - reg4)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"44","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"48","2":"-4","3":"13.29778","4":"3.495112e-07","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>or in case of heteroscedatistic errors:</p>
<pre class="r fold-show"><code>waldtest(first, . ~ . - faminc - reg2 - reg3- reg4, vcov = sandwich)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"44","2":"NA","3":"NA","4":"NA","_rn_":"1"},{"1":"48","2":"-4","3":"12.97475","4":"4.643466e-07","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="ways-to-handle-1" class="section level4" number="13.1.2.2">
<h4><span class="header-section-number">13.1.2.2</span> Ways to
Handle</h4>
<ul>
<li>Conduct an experiment/RCT with random assignment</li>
<li>Instrumental variables</li>
</ul>
</div>
</div>
<div id="homoscedasticity-of-residuals" class="section level3"
number="13.1.3">
<h3><span class="header-section-number">13.1.3</span> 3.
Homoscedasticity of residuals</h3>
<p>Homoscedasticity of the residuals means that the variance of the
residuals does not differ as a function of the outcome/predictors (i.e.,
the residuals show constant variance as a function of
outcome/predictors).</p>
<div id="ways-to-test-2" class="section level4" number="13.1.3.1">
<h4><span class="header-section-number">13.1.3.1</span> Ways to
Test</h4>
<ul>
<li>Plot residuals vs. outcome and predictor variables (<a
href="#residualPlots">Residual Plots</a>)</li>
<li>Plot residuals vs. fitted values (<a href="#residualPlots">Residual
Plots</a>)</li>
<li>Time Series data: Plot residuals vs. time</li>
<li>Spread-level plot</li>
<li>Breusch-Pagan test: <code>bptest()</code> function from
<code>lmtest</code> package</li>
<li>Goldfeld-Quandt Test</li>
</ul>
<div id="residuals-vs.-outcome-and-predictor-variables"
class="section level5" number="13.1.3.1.1">
<h5><span class="header-section-number">13.1.3.1.1</span> Residuals
vs. outcome and predictor variables</h5>
<p>Plot residuals, or perhaps their absolute values, versus the outcome
and predictor variables (<a href="#residualPlots">Residual Plots</a>).
Examine whether residual variance is constant at all levels of other
variables or whether it increases/decreases as a function of another
variable (or shows some others structure, e.g., small variance at low
and high levels of a predictor and high variance in the middle). Note
that this is <em>different than whether the residuals show
non-linearities</em>—i.e., a non-horizontal line, which would indicate a
<a href="#linearAssociation">nonlinear association between variables</a>
(see Assumption #1, above). Rather, here we are examining whether there
is change in the variance as a function of another variable (e.g., a
fan-shaped <a href="#residualPlots">Residual Plot</a>)</p>
</div>
<div id="spread-level-plot" class="section level5" number="13.1.3.1.2">
<h5><span class="header-section-number">13.1.3.1.2</span> Spread-level
plot</h5>
<p>Examining whether level (e.g., mean) depends on spread (e.g.,
variance)—plot of log of the absolute Studentized residuals against the
log of the fitted values</p>
<pre class="r fold-show"><code>spreadLevelPlot(multipleRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<pre><code>
Suggested power transformation:  0.5304728 </code></pre>
</div>
<div id="breusch-pagan-test" class="section level5" number="13.1.3.1.3">
<h5><span class="header-section-number">13.1.3.1.3</span> Breusch-Pagan
test</h5>
<p><a
href="https://www.rdocumentation.org/packages/lmtest/versions/0.9-40/topics/bptest"
class="uri">https://www.rdocumentation.org/packages/lmtest/versions/0.9-40/topics/bptest</a>
(archived at <a href="https://perma.cc/K4WC-7TVW"
class="uri">https://perma.cc/K4WC-7TVW</a>)</p>
<pre class="r fold-show"><code>bptest(multipleRegressionModel)</code></pre>
<pre><code>
    studentized Breusch-Pagan test

data:  multipleRegressionModel
BP = 94.638, df = 2, p-value &lt; 2.2e-16</code></pre>
</div>
<div id="goldfeld-quandt-test" class="section level5"
number="13.1.3.1.4">
<h5><span class="header-section-number">13.1.3.1.4</span>
Goldfeld-Quandt Test</h5>
<pre class="r fold-show"><code>gqtest(multipleRegressionModel)</code></pre>
<pre><code>
    Goldfeld-Quandt test

data:  multipleRegressionModel
GQ = 1.0755, df1 = 1434, df2 = 1434, p-value = 0.08417
alternative hypothesis: variance increases from segment 1 to 2</code></pre>
</div>
<div id="test-of-dependence-of-spread-on-level" class="section level5"
number="13.1.3.1.5">
<h5><span class="header-section-number">13.1.3.1.5</span> Test of
dependence of spread on level</h5>
<pre class="r fold-show"><code>ncvTest(multipleRegressionModel)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 232.1196, Df = 1, p = &lt; 2.22e-16</code></pre>
</div>
<div id="test-of-dependence-of-spread-on-predictors"
class="section level5" number="13.1.3.1.6">
<h5><span class="header-section-number">13.1.3.1.6</span> Test of
dependence of spread on predictors</h5>
<pre class="r fold-show"><code>ncvTest(multipleRegressionModel, ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum)</code></pre>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum 
Chisquare = 233.2878, Df = 2, p = &lt; 2.22e-16</code></pre>
</div>
</div>
<div id="ways-to-handle-2" class="section level4" number="13.1.3.2">
<h4><span class="header-section-number">13.1.3.2</span> Ways to
Handle</h4>
<ul>
<li>If residual variance increases as a function of the fitted values,
consider a poisson regression model (especially for count data), for
which the variance does increase with the mean</li>
<li>If residual variance differs as a function of model predictors,
consider adding interactions/product terms (the effect of one variable
depends on the level of another variable)</li>
<li>Try transforming the outcome variable to be normally distributed
(log-transform if the errors seem consistent in percentage rather than
absolute terms)</li>
<li>If the error variance is proportional to a variable <span
class="math inline">\(z\)</span>, then fit the model using Weighted
Least Squares (WLS), with the weights given be <span
class="math inline">\(1/z\)</span></li>
<li>Weighted least squares (WLS) using the “weights” argument of the
<code>lm()</code> function; to get weights, see: <a
href="https://stats.stackexchange.com/a/100410/20338"
class="uri">https://stats.stackexchange.com/a/100410/20338</a> (archived
at <a href="https://perma.cc/C6BY-G9MS"
class="uri">https://perma.cc/C6BY-G9MS</a>)</li>
<li>Huber-White standard errors (a.k.a. “Sandwich” estimates) from a
heteroscedasticity-corrected covariance matrix
<ul>
<li><code>coeftest()</code> function from the <code>sandwich</code>
package along with hccm sandwich estimates from the <code>car</code>
package</li>
<li><code>robcov()</code> function from the <code>rms</code>
package</li>
</ul></li>
<li>Time series data: ARCH (auto-regressive conditional
heteroscedasticity) models</li>
<li>Time series data: seasonal patterns can be addressed by applying log
transformation to outcome variable</li>
</ul>
<div id="huber-white-standard-errors" class="section level5"
number="13.1.3.2.1">
<h5><span class="header-section-number">13.1.3.2.1</span> Huber-White
standard errors</h5>
<p>Standard errors (SEs) on the diagonal increase</p>
<pre class="r fold-show"><code>vcov(multipleRegressionModel)</code></pre>
<pre><code>                          (Intercept) bpi_antisocialT1Sum
(Intercept)              0.0035795220       -0.0006533375
bpi_antisocialT1Sum     -0.0006533375        0.0003453814
bpi_anxiousDepressedSum -0.0003167540       -0.0002574524
                        bpi_anxiousDepressedSum
(Intercept)                       -0.0003167540
bpi_antisocialT1Sum               -0.0002574524
bpi_anxiousDepressedSum            0.0008501565</code></pre>
<pre class="r fold-show"><code>hccm(multipleRegressionModel)</code></pre>
<pre><code>Error in V %*% t(X) %*% apply(X, 2, &quot;*&quot;, (e^2)/factor): non-conformable arguments</code></pre>
<pre class="r fold-show"><code>summary(multipleRegressionModel)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>coeftest(multipleRegressionModel, vcov = sandwich)</code></pre>
<pre><code>
t test of coefficients:

                        Estimate Std. Error t value  Pr(&gt;|t|)    
(Intercept)             1.198300   0.062211 19.2618 &lt; 2.2e-16 ***
bpi_antisocialT1Sum     0.465529   0.022929 20.3030 &lt; 2.2e-16 ***
bpi_anxiousDepressedSum 0.160754   0.032991  4.8727  1.16e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r fold-show"><code>coeftest(multipleRegressionModel, vcov = hccm)</code></pre>
<pre><code>Error in V %*% t(X) %*% apply(X, 2, &quot;*&quot;, (e^2)/factor): non-conformable arguments</code></pre>
<pre class="r fold-show"><code>robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
           data = mydata,
           x = TRUE,
           y = TRUE))</code></pre>
<pre><code>Frequencies of Missing Values Due to Each Variable
    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
                   7501                    7613                    7616 

Linear Regression Model

ols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, x = TRUE, y = TRUE)

                Model Likelihood    Discrimination    
                      Ratio Test           Indexes    
Obs    2874    LR chi2    873.09    R2       0.262    
sigma1.9786    d.f.            2    R2 adj   0.261    
d.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    

Residuals

    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

                        Coef   S.E.   t     Pr(&gt;|t|)
Intercept               1.1983 0.0622 19.26 &lt;0.0001 
bpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 
bpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 </code></pre>
<pre class="r fold-show"><code>robcov(ols(t_ext ~ m_ext + age,
           data = mydata,
           x = TRUE,
           y = TRUE),
       cluster = mydata$tcid) #account for nested data within subject</code></pre>
<pre><code>Error in eval(predvars, data, callenv): object &#39;t_ext&#39; not found</code></pre>
</div>
</div>
</div>
<div id="errors-are-independent" class="section level3" number="13.1.4">
<h3><span class="header-section-number">13.1.4</span> 4. Errors are
independent</h3>
<p>Independent errors means that the errors are uncorrelated with each
other.</p>
<div id="ways-to-test-3" class="section level4" number="13.1.4.1">
<h4><span class="header-section-number">13.1.4.1</span> Ways to
Test</h4>
<ul>
<li>Plot residuals vs. predictors (<a href="#residualPlots">Residual
Plots</a>)</li>
<li>Time Series data: Residual time series plot (residuals vs. row
number)</li>
<li>Time Series data: Table or plot of residual autocorrelations</li>
<li>Time Series data: Durbin-Watson statistic for test of significant
residual autocorrelation at lag 1</li>
</ul>
</div>
<div id="ways-to-handle-3" class="section level4" number="13.1.4.2">
<h4><span class="header-section-number">13.1.4.2</span> Ways to
Handle</h4>
<ul>
<li>Generalized least squares (GLS) models are capable of handling
correlated errors: <a
href="https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html"
class="uri">https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html</a>
(archived at <a href="https://perma.cc/RHZ6-5GT8"
class="uri">https://perma.cc/RHZ6-5GT8</a>)</li>
<li>Regression with cluster variable
<ul>
<li><code>robcov()</code> from <code>rms</code> package</li>
</ul></li>
<li><a href="hlm.html">Hierarchical linear modeling</a>
<ul>
<li><a href="hlm.html#linear">Linear mixed effects models</a></li>
<li><a href="hlm.html#generalized">Generalized linear mixed effects
models</a></li>
<li><a href="hlm.html#nonlinear">Nonlinear mixed effects models</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="no-multicollinearity" class="section level3" number="13.1.5">
<h3><span class="header-section-number">13.1.5</span> 5. No
multicollinearity</h3>
<p>Multicollinearity occurs when the predictors are correlated with each
other.</p>
<div id="ways-to-test-4" class="section level4" number="13.1.5.1">
<h4><span class="header-section-number">13.1.5.1</span> Ways to
Test</h4>
<ul>
<li>Variance Inflation Factor (VIF)</li>
<li>Generalized Variance Inflation Factor (GVIF)—when models have
related regressors (multiple polynomial terms or contrasts from same
predictor)</li>
<li>Correlation</li>
<li>Tolerance</li>
<li>Condition Index</li>
</ul>
<div id="variance-inflation-factor-vif" class="section level5"
number="13.1.5.1.1">
<h5><span class="header-section-number">13.1.5.1.1</span> Variance
Inflation Factor (VIF)</h5>
<p><span class="math display">\[
\text{VIF} = 1/\text{Tolerance}
\]</span></p>
<p>If the variance inflation factor of a predictor variable were 5.27
(<span class="math inline">\(\sqrt{5.27} = 2.3\)</span>), this means
that the standard error for the coefficient of that predictor variable
is 2.3 times as large (i.e., confidence interval is 2.3 times wider) as
it would be if that predictor variable were uncorrelated with the other
predictor variables.</p>
<p>VIF = 1: Not correlated<br />
1 &lt; VIF &lt; 5: Moderately correlated<br />
VIF &gt; 5 to 10: Highly correlated (multicollinearity present)</p>
<pre class="r fold-show"><code>vif(multipleRegressionModel)</code></pre>
<pre><code>    bpi_antisocialT1Sum bpi_anxiousDepressedSum 
               1.291545                1.291545 </code></pre>
</div>
<div id="generalized-variance-inflation-factor-gvif"
class="section level5" number="13.1.5.1.2">
<h5><span class="header-section-number">13.1.5.1.2</span> Generalized
Variance Inflation Factor (GVIF)</h5>
<p>Useful when models have related regressors (multiple polynomial terms
or contrasts from same predictor)</p>
</div>
<div id="correlation" class="section level5" number="13.1.5.1.3">
<h5><span class="header-section-number">13.1.5.1.3</span>
Correlation</h5>
<p>correlation among all independent variables the correlation
coefficients should be smaller than .08</p>
<pre class="r fold-show"><code>cor(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_anxiousDepressedSum&quot;)], use = &quot;pairwise.complete.obs&quot;)</code></pre>
<pre><code>                        bpi_antisocialT1Sum bpi_anxiousDepressedSum
bpi_antisocialT1Sum                1.000000                0.497279
bpi_anxiousDepressedSum            0.497279                1.000000</code></pre>
</div>
<div id="tolerance" class="section level5" number="13.1.5.1.4">
<h5><span class="header-section-number">13.1.5.1.4</span> Tolerance</h5>
<p>The tolerance is an index of the influence of one independent
variable on all other independent variables.</p>
<p><span class="math display">\[
\text{tolerance} = 1/\text{VIF}
\]</span></p>
<p>T &lt; 0.2: there might be multicollinearity in the data<br />
T &lt; 0.01: there certainly is multicollinarity in the data</p>
</div>
<div id="condition-index" class="section level5" number="13.1.5.1.5">
<h5><span class="header-section-number">13.1.5.1.5</span> Condition
Index</h5>
<p>The condition index is calculated using a factor analysis on the
independent variables. Values of 10-30 indicate a mediocre
multicollinearity in the regression variables. Values &gt; 30 indicate
strong multicollinearity.</p>
<p>For how to interpret, see here: <a
href="https://stats.stackexchange.com/a/87610/20338"
class="uri">https://stats.stackexchange.com/a/87610/20338</a> (archived
at <a href="https://perma.cc/Y4J8-MY7Q"
class="uri">https://perma.cc/Y4J8-MY7Q</a>)</p>
<pre class="r fold-show"><code>ols_eigen_cindex(multipleRegressionModel)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Eigenvalue"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Condition Index"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["intercept"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["bpi_antisocialT1Sum"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["bpi_anxiousDepressedSum"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"2.4393975","2":"1.000000","3":"0.05150939","4":"0.04435009","5":"0.05806627"},{"1":"0.3575629","2":"2.611951","3":"0.40330814","4":"0.01600701","5":"0.75801976"},{"1":"0.2030395","2":"3.466179","3":"0.54518247","4":"0.93964290","5":"0.18391397"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="ways-to-handle-4" class="section level4" number="13.1.5.2">
<h4><span class="header-section-number">13.1.5.2</span> Ways to
Handle</h4>
<ul>
<li>Remove highly correlated (i.e., redundant) predictors</li>
<li>Average the correlated predictors</li>
<li><a href="pca.html">Principal Component Analysis</a> for data
reduction</li>
<li>Standardize predictors</li>
<li>Center the data (deduct the mean)</li>
<li>Singular-value decomposition of the model matrix or the
mean-centered model matrix</li>
<li>Conduct a <a href="#factorAnalysis.html">factor analysis</a> and
rotate the factors to ensure independence of the factors</li>
</ul>
</div>
</div>
<div id="errors-are-normally-distributed" class="section level3"
number="13.1.6">
<h3><span class="header-section-number">13.1.6</span> 6. Errors are
normally distributed</h3>
<div id="ways-to-test-5" class="section level4" number="13.1.6.1">
<h4><span class="header-section-number">13.1.6.1</span> Ways to
Test</h4>
<ul>
<li>Probability Plots
<ul>
<li><a href="#qqPlot">Normal Quantile (QQ) Plots</a> (based on
non-cumulative distribution of residuals)</li>
<li><a href="#ppPlot">Normal Probability (PP) Plots</a> (based on
cumulative distribution of residuals)</li>
</ul></li>
<li><a href="#densityPlotResiduals">Density Plot of Residuals</a></li>
<li>Statistical Tests
<ul>
<li>Kolmogorov-Smirnov test</li>
<li>Shapiro-Wilk test</li>
<li>Jarque-Bera test</li>
<li>Anderson-Darling test (best test)</li>
</ul></li>
<li>Examine influence of outliers</li>
</ul>
</div>
<div id="ways-to-handle-5" class="section level4" number="13.1.6.2">
<h4><span class="header-section-number">13.1.6.2</span> Ways to
Handle</h4>
<ul>
<li>Apply a transformation to the predictor or outcome variable</li>
<li>Exclude outliers</li>
<li>Robust regression
<ul>
<li>Best when no outliers: MM-type regression estimator
<ul>
<li><code>lmrob()</code>/<code>glmrob()</code> function of
<code>robustbase</code> package</li>
<li>Iteratively reweighted least squares (IRLS):
<code>rlm(, method = "MM")</code> function of <code>MASS</code> package:
<a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a> (archived at
<a
href="https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>)</li>
</ul></li>
<li>Most resistant to outliers: Least trimmed squares (LTS)—but not good
as a standalone estimator, better for identifying outliers
<ul>
<li><code>ltsReg()</code> function of <code>robustbase</code> package
(best)</li>
</ul></li>
<li>Best when single predictor: Theil-Sen estimator
<ul>
<li><code>mblm(, repeated = FALSE)</code> function of <code>mblm</code>
package</li>
</ul></li>
<li>Robust correlation
<ul>
<li>Spearman’s rho: <code>cor(, method = "spearman")</code></li>
<li>Percentage bend correlation</li>
<li>Minimum vollume ellipsoid</li>
<li>Minimum covariance determinant:</li>
<li>Winsorized correlation</li>
<li>Biweight midcorrelation</li>
</ul></li>
<li>Not great options:
<ul>
<li>Quantile (L1) regression: <code>rq()</code> function of
<code>quantreg</code> package</li>
</ul></li>
</ul></li>
</ul>
<div id="transformations-of-outcome-variable" class="section level5"
number="13.1.6.2.1">
<h5><span class="header-section-number">13.1.6.2.1</span>
Transformations of Outcome Variable</h5>
<div id="box-cox-transformation" class="section level6"
number="13.1.6.2.1.1">
<h6><span class="header-section-number">13.1.6.2.1.1</span> Box-Cox
Transformation</h6>
<p>Useful if the outcome is strictly positive (or add a constant to
outcome to make it strictly positive)</p>
<p>lambda = -1: inverse transformation<br />
lambda = -0.5: 1/sqrt(Y)<br />
lambda = 0: log transformation<br />
lambda = 0.5: square root<br />
lambda = 0.333: cube root<br />
lambda = 1: no transformation<br />
lambda = 2: squared<br />
</p>
<div id="raw-distribution" class="section level7"
number="13.1.6.2.1.1.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.1</span> Raw distribution</p>
<pre class="r fold-show"><code>plot(density(na.omit(mydata$bpi_antisocialT2Sum)))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
</div>
<div id="add-constant-to-outcome-to-make-it-strictly-positive"
class="section level7" number="13.1.6.2.1.1.2">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.2</span> Add constant to
outcome to make it strictly positive</p>
<pre class="r fold-show"><code>strictlyPositiveDV &lt;- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude)</code></pre>
</div>
<div id="identify-best-power-transformation-lambda"
class="section level7" number="13.1.6.2.1.1.3">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.3</span> Identify best power
transformation (lambda)</p>
<p>Consider rounding the power to a common value (square root = .5; cube
root = .333; squared = 2)</p>
<pre class="r fold-show"><code>boxCox(strictlyPositiveDV)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<pre class="r fold-show"><code>powerTransform(strictlyPositiveDV) </code></pre>
<pre><code>Estimated transformation parameter 
      Y1 
0.234032 </code></pre>
<pre class="r fold-show"><code>transformedDV &lt;- powerTransform(strictlyPositiveDV)

summary(transformedDV)</code></pre>
<pre><code>bcPower Transformation to Normality 
   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
Y1     0.234        0.23       0.1825       0.2856

Likelihood ratio test that transformation parameter is equal to 0
 (log transformation)
                           LRT df       pval
LR test, lambda = (0) 78.04052  1 &lt; 2.22e-16

Likelihood ratio test that no transformation is needed
                           LRT df       pval
LR test, lambda = (1) 853.8675  1 &lt; 2.22e-16</code></pre>
</div>
<div id="transform-the-dv" class="section level7"
number="13.1.6.2.1.1.4">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.4</span> Transform the DV</p>
<pre class="r fold-show"><code>mydata$bpi_antisocialT2SumTransformed &lt;- bcPower(mydata$bpi_antisocialT2Sum + 1, coef(transformedDV))

plot(density(na.omit(mydata$bpi_antisocialT2SumTransformed)))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
</div>
<div id="compare-residuals-from-model-with-and-without-transformation"
class="section level7" number="13.1.6.2.1.1.5">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.5</span> Compare residuals
from model with and without transformation</p>
<div id="model-without-transformation" class="section level8"
number="13.1.6.2.1.1.5.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.5.1</span> Model without
transformation</p>
<pre class="r fold-show"><code>summary(modelWithoutTransformation &lt;- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude))</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.3755 -1.2337 -0.2212  0.9911 12.8017 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              2.19830    0.05983  36.743  &lt; 2e-16 ***
bpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***
bpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.979 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.262, Adjusted R-squared:  0.2615 
F-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>plot(density(na.omit(studres(modelWithoutTransformation))))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
</div>
<div id="model-with-transformation" class="section level8"
number="13.1.6.2.1.1.5.2">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.5.2</span> Model with
transformation</p>
<pre class="r fold-show"><code>summary(modelWithTransformation &lt;- lm(bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      na.action = na.exclude))</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.3429 -0.4868  0.0096  0.4922  2.9799 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)             0.800373   0.022024  36.342  &lt; 2e-16 ***
bpi_antisocialT1Sum     0.165476   0.006841  24.189  &lt; 2e-16 ***
bpi_anxiousDepressedSum 0.055910   0.010733   5.209 2.03e-07 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 0.7283 on 2871 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.2477,    Adjusted R-squared:  0.2472 
F-statistic: 472.7 on 2 and 2871 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>plot(density(na.omit(studres(modelWithTransformation))))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
</div>
</div>
<div id="constructed-variable-test-plot" class="section level7"
number="13.1.6.2.1.1.6">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.6</span> Constructed Variable
Test &amp; Plot</p>
<p>A significant <em>p</em>-value indicates a strong need to transform
variable:</p>
<pre class="r fold-show"><code>multipleRegressionModel_constructedVariable &lt;- update(multipleRegressionModel, . ~ . + boxCoxVariable(bpi_antisocialT1Sum + 1))
summary(multipleRegressionModel_constructedVariable)$coef[&quot;boxCoxVariable(bpi_antisocialT1Sum + 1)&quot;, , drop = FALSE]</code></pre>
<pre><code>                                           Estimate Std. Error   t value
boxCoxVariable(bpi_antisocialT1Sum + 1) -0.06064732 0.04683415 -1.294938
                                         Pr(&gt;|t|)
boxCoxVariable(bpi_antisocialT1Sum + 1) 0.1954457</code></pre>
<p>Plot allows us to see whether the need for transformation is spread
through data or whether it is just dependent on a small fraction of
observations:</p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_constructedVariable, &quot;boxCoxVariable(bpi_antisocialT1Sum + 1)&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
</div>
<div id="inverse-response-plot" class="section level7"
number="13.1.6.2.1.1.7">
<p class="heading"><span
class="header-section-number">13.1.6.2.1.1.7</span> Inverse Response
Plot</p>
<p>The black line is the best-fitting power transformation:</p>
<pre class="r fold-show"><code>inverseResponsePlot(strictlyPositiveDV, lambda = c(-1,-0.5,0,1/3,.5,1,2))</code></pre>
<img src="regression_files/figure-html/unnamed-chunk-71-1.png" width="672" />
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["lambda"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.8481126","2":"2940.938"},{"1":"-1.0000000","2":"3329.131"},{"1":"-0.5000000","2":"3189.376"},{"1":"0.0000000","2":"3052.679"},{"1":"0.3333333","2":"2984.003"},{"1":"0.5000000","2":"2960.827"},{"1":"1.0000000","2":"2944.657"},{"1":"2.0000000","2":"3116.590"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="yeo-johnson-transformations" class="section level6"
number="13.1.6.2.1.2">
<h6><span class="header-section-number">13.1.6.2.1.2</span> Yeo-Johnson
Transformations</h6>
<p>Useful if the outcome is not strictly positive.</p>
<pre class="r fold-show"><code>yjPower(DV, lambda)</code></pre>
</div>
</div>
<div id="transformations-of-predictor-variable" class="section level5"
number="13.1.6.2.2">
<h5><span class="header-section-number">13.1.6.2.2</span>
Transformations of Predictor Variable</h5>
<div id="component-plus-residual-plots-partial-residual-plots"
class="section level6" number="13.1.6.2.2.1">
<h6><span class="header-section-number">13.1.6.2.2.1</span>
Component-Plus-Residual Plots (Partial Residual Plots)</h6>
<p>Linear model:</p>
<pre class="r fold-show"><code>crPlots(multipleRegressionModelNoMissing, order = 1)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>Quadratic model:</p>
<pre class="r fold-show"><code>crPlots(multipleRegressionModelNoMissing, order = 2)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<pre class="r fold-show"><code>multipleRegressionModel_quadratic &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + bpi_anxiousDepressedSum,
                                        data = mydata,
                                        na.action = na.exclude)

summary(multipleRegressionModel_quadratic)</code></pre>
<pre><code>
Call:
lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + 
    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6238 -1.2610 -0.2484  0.9923 12.9008 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               1.099237   0.076509  14.367  &lt; 2e-16 ***
bpi_antisocialT1Sum       0.547680   0.043723  12.526  &lt; 2e-16 ***
I(bpi_antisocialT1Sum^2) -0.010221   0.004925  -2.075    0.038 *  
bpi_anxiousDepressedSum   0.161734   0.029144   5.549 3.13e-08 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1.977 on 2870 degrees of freedom
  (8656 observations deleted due to missingness)
Multiple R-squared:  0.2631,    Adjusted R-squared:  0.2623 
F-statistic: 341.5 on 3 and 2870 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r fold-show"><code>anova(multipleRegressionModel_quadratic, multipleRegressionModel)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Res.Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["RSS"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Sum of Sq"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["F"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Pr(>F)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"2870","2":"11223.01","3":"NA","4":"NA","5":"NA","6":"NA","_rn_":"1"},{"1":"2871","2":"11239.86","3":"-1","4":"-16.84449","5":"4.307551","6":"0.03803241","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>crPlots(multipleRegressionModel_quadratic, order = 1)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
</div>
<div id="ceres-plot-combining-conditional-expectations-and-residuals"
class="section level6" number="13.1.6.2.2.2">
<h6><span class="header-section-number">13.1.6.2.2.2</span> CERES Plot
(Combining conditional Expectations and RESiduals)</h6>
<p>Useful when nonlinear associations among the predictors are very
strong (component-plus-residual plots may appear nonlinear even though
the true partial regression is linear, a phenonomen called leakage)</p>
<pre class="r fold-show"><code>ceresPlots(multipleRegressionModel)</code></pre>
<pre><code>Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,
: pseudoinverse used at 1</code></pre>
<pre><code>Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,
: neighborhood radius 1</code></pre>
<pre><code>Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,
: reciprocal condition number 3.7939e-17</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<pre class="r fold-show"><code>ceresPlots(multipleRegressionModel_quadratic)</code></pre>
<pre><code>Warning in ceresPlots.default(multipleRegressionModel_quadratic): Factors
skipped in drawing CERES plots.</code></pre>
<pre><code>Warning in min(x): no non-missing arguments to min; returning Inf</code></pre>
<pre><code>Warning in max(x): no non-missing arguments to max; returning -Inf</code></pre>
<pre><code>Error in plot.window(...): need finite &#39;ylim&#39; values</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-76-2.png" width="672" /></p>
</div>
<div id="box-tidwell-method-for-choosing-predictor-transformations"
class="section level6" number="13.1.6.2.2.3">
<h6><span class="header-section-number">13.1.6.2.2.3</span> Box-Tidwell
Method for Choosing Predictor Transformations</h6>
<p>predictors must be strictly positive (or add a constant to make it
strictly positive)</p>
<pre class="r fold-show"><code>boxTidwell(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1),
           other.x = NULL, #list variables not to be transformed in other.x
           data = mydata,
           na.action = na.exclude)</code></pre>
<pre><code>Warning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include
     arithmetic operators in their names;
  the printed representation of the hypothesis will be omitted</code></pre>
<pre><code>                               MLE of lambda Score Statistic (t) Pr(&gt;|t|)
I(bpi_antisocialT1Sum + 1)           0.90820             -1.0591   0.2897
I(bpi_anxiousDepressedSum + 1)       0.36689             -1.0650   0.2870

iterations =  4 

Score test for null hypothesis that all lambdas = 1:
F = 1.4056, df = 2 and 2869, Pr(&gt;F) = 0.2454</code></pre>
</div>
<div id="constructed-variables-plot" class="section level6"
number="13.1.6.2.2.4">
<h6><span class="header-section-number">13.1.6.2.2.4</span>
Constructed-Variables Plot</h6>
<pre class="r fold-show"><code>multipleRegressionModel_cv &lt;- lm(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1) + I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) + I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)),
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel_cv)$coef[&quot;I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))&quot;, , drop = FALSE]</code></pre>
<pre><code>                                                    Estimate Std. Error
I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -0.1691003 0.06887107
                                                    t value   Pr(&gt;|t|)
I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -2.455317 0.01418393</code></pre>
<pre class="r fold-show"><code>summary(multipleRegressionModel_cv)$coef[&quot;I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))&quot;, , drop = FALSE]</code></pre>
<pre><code>                                                            Estimate Std. Error
I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.00828867  0.1384918
                                                             t value  Pr(&gt;|t|)
I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.05984953 0.9522831</code></pre>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_cv, &quot;I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel_cv, &quot;I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))&quot;)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-78-2.png" width="672" /></p>
</div>
</div>
<div id="robust-models" class="section level5" number="13.1.6.2.3">
<h5><span class="header-section-number">13.1.6.2.3</span> Robust
models</h5>
<p>Resources</p>
<ul>
<li>For comparison of methods, see Book: “Modern Methods for Robust
Regression”</li>
<li><a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a> (archived at
<a
href="https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>)</li>
<li><a href="http://stats.stackexchange.com/a/46234/20338"
class="uri">http://stats.stackexchange.com/a/46234/20338</a> (archived
at <a href="https://perma.cc/WC57-9GA7"
class="uri">https://perma.cc/WC57-9GA7</a>)</li>
<li><a href="http://cran.r-project.org/web/views/Robust.html"
class="uri">http://cran.r-project.org/web/views/Robust.html</a>
(archived at <a href="https://perma.cc/THN5-KNY3"
class="uri">https://perma.cc/THN5-KNY3</a>)</li>
</ul>
<div id="robust-correlation" class="section level6"
number="13.1.6.2.3.1">
<h6><span class="header-section-number">13.1.6.2.3.1</span> Robust
correlation</h6>
<div id="spearmans-rho" class="section level7" number="13.1.6.2.3.1.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.1</span> Spearman’s rho</p>
<p>Spearman’s rho is a non-parametric correlation.</p>
<pre class="r fold-show"><code>cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum) #Pearson r, correlation that is sensitive to outliers</code></pre>
<pre><code>
    Pearson&#39;s product-moment correlation

data:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum
t = 31.274, df = 2873, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.4761758 0.5307381
sample estimates:
      cor 
0.5039595 </code></pre>
<pre class="r fold-show"><code>cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum, method = &quot;spearman&quot;) #Spearman&#39;s rho, a rank correlation that is less sensitive to outliers</code></pre>
<pre><code>Warning in cor.test.default(mydata$bpi_antisocialT1Sum,
mydata$bpi_antisocialT2Sum, : Cannot compute exact p-value with ties</code></pre>
<pre><code>
    Spearman&#39;s rank correlation rho

data:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum
S = 1997347186, p-value &lt; 2.2e-16
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.4956973 </code></pre>
</div>
<div id="minimum-vollume-ellipsoid" class="section level7"
number="13.1.6.2.3.1.2">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.2</span> Minimum vollume
ellipsoid</p>
<pre class="r fold-show"><code>cov.mve(na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)]), cor = TRUE)</code></pre>
<pre><code>$center
bpi_antisocialT1Sum bpi_antisocialT2Sum 
           2.427726            2.152955 

$cov
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum            3.162659            1.643541
bpi_antisocialT2Sum            1.643541            2.632760

$msg
[1] &quot;168 singular samples of size 3 out of 1500&quot;

$crit
[1] 3.635635

$best
   [1]    1    3    7    8    9   10   12   13   14   15   16   17   18   19
  [15]   21   22   23   25   26   39   40   42   43   46   48   50   55   56
  [29]   57   60   61   65   67   70   76   79   80   81   86   87   88   93
  [43]   95   97   98   99  101  102  104  105  106  107  108  112  116  117
  [57]  119  121  123  124  129  130  132  133  134  140  141  142  143  145
  [71]  148  149  150  153  154  155  157  161  165  168  169  170  172  176
  [85]  177  178  179  180  182  183  184  185  187  188  190  191  192  193
  [99]  194  195  196  197  198  200  201  202  205  206  207  212  213  214
 [113]  217  227  228  232  233  234  235  237  238  239  241  242  243  244
 [127]  245  249  251  253  255  256  257  261  262  265  268  269  272  275
 [141]  282  284  289  290  293  294  295  296  297  298  299  300  301  304
 [155]  307  310  313  315  320  326  329  330  331  332  333  334  335  338
 [169]  339  341  342  345  346  352  355  356  357  361  364  367  370  371
 [183]  376  379  380  381  382  386  387  388  389  390  391  397  403  404
 [197]  407  408  409  410  411  416  419  421  422  423  425  426  430  431
 [211]  432  433  436  438  440  441  442  443  445  448  449  450  451  453
 [225]  455  459  461  463  464  466  468  469  473  477  478  483  484  486
 [239]  487  494  495  496  497  498  499  500  501  502  503  504  505  508
 [253]  509  510  513  518  519  521  522  523  524  525  526  527  528  529
 [267]  531  532  533  535  536  537  538  539  541  543  545  546  550  551
 [281]  553  554  555  557  564  566  568  572  575  579  582  583  585  599
 [295]  600  601  605  608  610  615  616  617  618  619  620  621  622  623
 [309]  625  626  627  631  633  634  635  636  637  639  641  642  643  646
 [323]  647  648  649  650  657  660  662  667  670  671  672  673  674  675
 [337]  676  679  680  681  682  683  685  687  691  693  694  696  701  704
 [351]  705  706  707  710  713  714  715  717  718  719  720  721  722  723
 [365]  728  730  732  733  734  736  737  740  741  742  743  744  745  746
 [379]  747  751  752  756  758  762  763  773  774  779  780  783  784  786
 [393]  789  791  798  799  801  802  803  805  809  810  811  812  813  814
 [407]  815  817  818  819  822  824  826  827  828  830  832  834  836  837
 [421]  838  839  840  842  847  848  849  855  857  858  859  860  862  865
 [435]  867  868  869  871  872  873  874  880  881  888  892  895  897  898
 [449]  899  900  901  902  903  905  906  909  912  913  918  920  923  925
 [463]  926  927  928  930  931  932  933  934  935  936  937  939  940  941
 [477]  943  944  945  950  951  954  955  956  957  959  960  962  964  965
 [491]  967  968  969  970  972  974  975  976  977  980  983  985  987  988
 [505]  989  992  995  998  999 1000 1001 1003 1004 1006 1007 1010 1011 1013
 [519] 1014 1015 1016 1017 1019 1021 1023 1024 1025 1026 1027 1029 1030 1032
 [533] 1033 1034 1037 1043 1044 1047 1050 1051 1053 1055 1057 1058 1059 1061
 [547] 1064 1066 1067 1070 1072 1074 1075 1076 1079 1086 1087 1091 1094 1095
 [561] 1097 1101 1105 1106 1109 1111 1112 1113 1115 1119 1121 1123 1124 1125
 [575] 1128 1129 1130 1132 1138 1139 1144 1145 1146 1147 1148 1149 1150 1152
 [589] 1154 1158 1160 1162 1164 1165 1170 1175 1176 1177 1178 1179 1180 1181
 [603] 1182 1184 1185 1186 1187 1188 1189 1197 1198 1200 1201 1203 1205 1206
 [617] 1207 1208 1214 1215 1216 1218 1219 1220 1223 1224 1229 1230 1232 1233
 [631] 1234 1235 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1254 1255
 [645] 1258 1259 1261 1262 1263 1265 1266 1267 1269 1271 1272 1273 1274 1276
 [659] 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293
 [673] 1294 1296 1299 1301 1302 1303 1304 1306 1311 1312 1315 1316 1318 1319
 [687] 1320 1322 1323 1325 1326 1333 1334 1336 1337 1338 1339 1343 1344 1347
 [701] 1349 1350 1351 1352 1353 1354 1355 1357 1358 1359 1360 1361 1362 1363
 [715] 1364 1365 1366 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1380
 [729] 1381 1383 1386 1387 1388 1389 1390 1391 1395 1396 1397 1398 1399 1401
 [743] 1402 1403 1404 1405 1408 1409 1412 1413 1414 1415 1416 1417 1419 1421
 [757] 1422 1423 1424 1428 1430 1432 1433 1434 1438 1439 1440 1444 1445 1448
 [771] 1449 1454 1456 1458 1459 1460 1464 1467 1468 1469 1470 1471 1483 1485
 [785] 1486 1489 1493 1495 1500 1502 1503 1504 1505 1506 1508 1513 1514 1515
 [799] 1516 1517 1518 1519 1521 1522 1524 1527 1531 1532 1533 1534 1535 1536
 [813] 1537 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1560 1561
 [827] 1562 1563 1565 1566 1569 1571 1572 1575 1576 1577 1579 1583 1584 1585
 [841] 1586 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1600 1602 1604
 [855] 1605 1606 1611 1612 1613 1614 1617 1621 1623 1624 1625 1626 1627 1634
 [869] 1640 1641 1644 1652 1653 1655 1656 1657 1664 1672 1674 1675 1686 1687
 [883] 1689 1690 1691 1692 1694 1695 1708 1710 1711 1714 1718 1719 1721 1722
 [897] 1723 1724 1729 1732 1738 1742 1750 1754 1757 1759 1760 1761 1763 1764
 [911] 1765 1766 1772 1775 1778 1781 1788 1791 1795 1797 1798 1801 1802 1803
 [925] 1804 1806 1807 1809 1810 1812 1817 1818 1820 1824 1827 1833 1834 1835
 [939] 1836 1837 1840 1846 1847 1848 1849 1850 1852 1855 1856 1859 1861 1863
 [953] 1864 1866 1868 1870 1879 1881 1884 1885 1888 1890 1892 1898 1902 1904
 [967] 1910 1915 1920 1921 1923 1924 1925 1926 1927 1932 1933 1934 1936 1938
 [981] 1941 1943 1945 1948 1950 1951 1954 1955 1956 1960 1961 1962 1963 1967
 [995] 1970 1971 1972 1973 1974 1984 1985 1986 1988 1989 1990 1992 1993 1994
[1009] 1995 1999 2002 2005 2006 2009 2011 2012 2017 2019 2021 2022 2024 2027
[1023] 2028 2029 2032 2034 2035 2036 2038 2042 2043 2045 2049 2053 2054 2055
[1037] 2059 2061 2062 2067 2068 2071 2073 2074 2077 2078 2079 2080 2081 2082
[1051] 2083 2084 2085 2086 2087 2088 2089 2094 2095 2096 2097 2099 2100 2101
[1065] 2102 2104 2105 2108 2110 2113 2114 2116 2117 2118 2119 2123 2124 2125
[1079] 2126 2127 2129 2130 2131 2137 2138 2139 2140 2143 2145 2151 2157 2158
[1093] 2160 2161 2165 2166 2167 2169 2170 2171 2174 2175 2176 2177 2178 2179
[1107] 2180 2181 2183 2188 2189 2190 2191 2197 2200 2201 2202 2203 2205 2206
[1121] 2207 2208 2210 2212 2215 2219 2221 2223 2225 2226 2227 2231 2232 2234
[1135] 2236 2237 2238 2239 2241 2248 2249 2253 2257 2258 2259 2260 2261 2262
[1149] 2264 2271 2272 2273 2274 2279 2280 2283 2297 2298 2299 2301 2302 2305
[1163] 2306 2307 2311 2314 2316 2317 2319 2320 2321 2322 2323 2324 2325 2326
[1177] 2327 2328 2329 2331 2332 2334 2336 2337 2338 2340 2341 2343 2345 2348
[1191] 2349 2350 2351 2353 2359 2362 2364 2367 2368 2369 2371 2373 2376 2377
[1205] 2378 2379 2383 2384 2385 2386 2388 2390 2398 2400 2402 2410 2411 2414
[1219] 2415 2416 2418 2420 2421 2424 2425 2428 2429 2431 2434 2437 2442 2445
[1233] 2447 2448 2449 2450 2451 2452 2454 2455 2462 2463 2464 2465 2466 2469
[1247] 2470 2472 2473 2477 2479 2481 2483 2489 2490 2491 2495 2498 2499 2500
[1261] 2503 2504 2506 2507 2508 2510 2511 2512 2520 2522 2523 2526 2527 2528
[1275] 2529 2530 2531 2532 2533 2535 2536 2539 2542 2543 2546 2554 2561 2562
[1289] 2563 2565 2566 2571 2573 2575 2576 2577 2578 2581 2582 2583 2584 2585
[1303] 2588 2591 2597 2598 2599 2607 2613 2614 2615 2617 2618 2619 2620 2621
[1317] 2627 2634 2635 2636 2638 2639 2642 2643 2644 2645 2646 2647 2650 2653
[1331] 2655 2656 2657 2658 2659 2663 2664 2667 2668 2669 2670 2673 2674 2676
[1345] 2677 2679 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698
[1359] 2700 2701 2703 2705 2709 2711 2713 2714 2717 2718 2721 2722 2723 2724
[1373] 2728 2729 2733 2736 2738 2739 2740 2746 2748 2753 2757 2759 2760 2763
[1387] 2764 2765 2766 2769 2771 2773 2776 2777 2778 2779 2781 2783 2784 2786
[1401] 2789 2790 2797 2799 2800 2803 2804 2806 2807 2809 2810 2813 2817 2819
[1415] 2821 2825 2826 2831 2832 2834 2838 2839 2844 2846 2847 2849 2850 2851
[1429] 2852 2855 2856 2857 2862 2863 2864 2867 2868 2869 2870 2871 2873

$cor
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum            1.000000            0.569572
bpi_antisocialT2Sum            0.569572            1.000000

$n.obs
[1] 2875</code></pre>
</div>
<div id="minimum-covariance-determinant" class="section level7"
number="13.1.6.2.3.1.3">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.3</span> Minimum covariance
determinant</p>
<pre class="r fold-show"><code>cov.mcd(na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)]), cor = TRUE)</code></pre>
<pre><code>$center
bpi_antisocialT1Sum bpi_antisocialT2Sum 
           2.181181            2.094954 

$cov
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum            2.379036            1.096437
bpi_antisocialT2Sum            1.096437            2.482409

$msg
[1] &quot;144 singular samples of size 3 out of 1500&quot;

$crit
[1] -0.3839279

$best
   [1]    1    3    8    9   10   12   13   14   15   16   17   18   19   21
  [15]   22   23   24   39   40   42   43   44   46   48   50   55   56   57
  [29]   60   61   63   65   67   69   70   76   79   80   81   82   86   87
  [43]   88   93   95   97   98   99  102  104  105  106  107  108  116  117
  [57]  119  120  121  122  123  124  129  130  131  132  133  134  138  139
  [71]  140  141  142  143  145  148  149  150  153  154  155  160  161  166
  [85]  168  169  170  176  177  178  179  180  181  182  183  184  187  188
  [99]  190  191  192  194  195  196  197  198  200  201  202  205  206  207
 [113]  209  212  213  214  227  228  231  232  233  234  235  237  238  239
 [127]  240  241  242  243  244  245  246  247  249  251  253  254  255  256
 [141]  261  262  265  268  269  272  275  281  282  284  289  290  291  293
 [155]  294  295  296  297  298  299  300  301  304  307  308  310  313  315
 [169]  329  330  331  332  333  334  335  339  341  342  343  345  346  350
 [183]  355  356  357  359  364  367  370  371  376  379  380  381  382  386
 [197]  387  388  389  390  391  397  399  401  403  404  407  408  409  410
 [211]  411  413  416  421  422  423  425  426  430  431  432  435  437  438
 [225]  440  441  442  443  445  448  449  450  451  453  455  456  458  459
 [239]  461  462  463  464  466  467  468  469  471  473  474  476  477  478
 [253]  480  482  483  484  486  493  494  495  496  497  498  499  500  501
 [267]  502  503  504  505  506  507  509  510  513  518  519  520  521  522
 [281]  523  524  525  527  528  529  531  532  533  534  535  536  537  538
 [295]  539  541  543  545  546  550  553  554  557  558  562  564  566  568
 [309]  572  575  578  579  582  583  585  599  600  601  605  608  609  610
 [323]  615  616  617  618  619  620  621  622  623  625  626  627  628  631
 [337]  633  634  635  636  637  639  641  642  643  646  647  648  649  650
 [351]  657  660  662  667  670  672  673  674  675  676  679  680  681  682
 [365]  683  685  686  687  690  691  693  696  701  704  705  706  707  710
 [379]  713  714  715  717  718  719  720  721  722  723  728  730  732  733
 [393]  734  736  737  740  741  742  743  744  745  746  747  751  752  758
 [407]  762  763  764  765  769  772  774  776  779  780  782  783  786  789
 [421]  791  798  799  801  802  803  805  806  808  809  810  811  812  813
 [435]  814  817  818  819  822  824  826  827  828  832  834  836  837  838
 [449]  839  840  842  843  847  848  849  852  855  857  858  859  860  861
 [463]  862  865  866  867  868  869  871  872  873  874  877  880  881  882
 [477]  888  889  892  893  895  898  899  900  901  902  903  905  906  907
 [491]  909  912  913  914  918  920  921  923  925  926  927  928  931  932
 [505]  933  934  935  936  937  939  940  941  943  944  945  946  949  950
 [519]  951  954  955  956  957  959  960  962  964  965  967  970  972  973
 [533]  974  975  976  977  980  983  985  987  988  990  992  995  998 1000
 [547] 1001 1002 1003 1004 1005 1006 1007 1008 1013 1014 1015 1016 1017 1019
 [561] 1021 1022 1023 1024 1025 1026 1027 1029 1030 1032 1034 1036 1037 1041
 [575] 1043 1044 1046 1047 1049 1050 1051 1053 1054 1055 1057 1058 1059 1061
 [589] 1064 1066 1067 1070 1072 1075 1076 1079 1083 1086 1087 1088 1091 1094
 [603] 1095 1097 1101 1105 1109 1111 1112 1113 1114 1115 1119 1121 1123 1124
 [617] 1125 1128 1129 1132 1135 1136 1138 1139 1144 1145 1146 1147 1148 1149
 [631] 1150 1152 1154 1158 1160 1162 1163 1164 1165 1171 1176 1177 1178 1179
 [645] 1180 1181 1182 1184 1185 1186 1187 1188 1189 1190 1191 1197 1198 1200
 [659] 1201 1203 1205 1206 1207 1208 1209 1214 1215 1216 1217 1218 1219 1220
 [673] 1221 1223 1224 1229 1230 1231 1232 1233 1234 1235 1242 1243 1244 1245
 [687] 1246 1247 1248 1249 1250 1251 1254 1255 1259 1261 1262 1263 1265 1266
 [701] 1267 1269 1271 1272 1273 1274 1275 1276 1280 1281 1282 1283 1284 1285
 [715] 1286 1287 1288 1289 1290 1291 1292 1293 1294 1296 1299 1301 1302 1303
 [729] 1304 1306 1309 1310 1311 1312 1314 1315 1316 1318 1319 1320 1322 1323
 [743] 1325 1326 1333 1334 1336 1337 1338 1339 1341 1343 1344 1347 1348 1349
 [757] 1350 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1365
 [771] 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1380 1381
 [785] 1382 1383 1387 1388 1389 1390 1391 1395 1396 1397 1398 1399 1400 1401
 [799] 1402 1405 1408 1412 1413 1414 1417 1419 1421 1422 1423 1424 1426 1428
 [813] 1430 1432 1433 1434 1438 1439 1440 1444 1445 1447 1448 1449 1456 1458
 [827] 1459 1464 1467 1468 1469 1470 1471 1483 1485 1486 1489 1495 1499 1500
 [841] 1502 1504 1505 1506 1508 1513 1514 1515 1516 1517 1518 1519 1521 1522
 [855] 1524 1525 1527 1531 1532 1533 1534 1535 1536 1537 1539 1540 1541 1542
 [869] 1543 1544 1545 1546 1547 1548 1549 1551 1553 1554 1560 1561 1563 1565
 [883] 1569 1571 1572 1575 1576 1577 1583 1584 1585 1586 1588 1589 1590 1591
 [897] 1592 1593 1594 1595 1596 1597 1598 1600 1602 1604 1605 1606 1611 1612
 [911] 1613 1614 1617 1621 1623 1624 1625 1626 1627 1634 1640 1641 1644 1652
 [925] 1656 1657 1661 1664 1665 1672 1674 1675 1686 1687 1689 1690 1691 1692
 [939] 1694 1699 1701 1705 1706 1708 1709 1710 1711 1712 1714 1716 1718 1719
 [953] 1721 1722 1723 1724 1731 1732 1738 1739 1742 1750 1753 1754 1757 1759
 [967] 1760 1761 1763 1764 1765 1766 1772 1774 1775 1776 1777 1778 1781 1791
 [981] 1795 1797 1798 1801 1802 1803 1804 1806 1807 1809 1810 1812 1813 1815
 [995] 1817 1818 1821 1824 1827 1834 1835 1836 1837 1840 1842 1846 1847 1848
[1009] 1849 1850 1851 1852 1855 1858 1859 1860 1861 1863 1864 1866 1867 1868
[1023] 1870 1881 1884 1885 1888 1890 1892 1902 1904 1910 1915 1921 1922 1923
[1037] 1924 1925 1926 1927 1932 1933 1935 1936 1938 1940 1941 1943 1947 1948
[1051] 1950 1951 1954 1955 1956 1957 1960 1961 1962 1963 1966 1967 1969 1970
[1065] 1971 1972 1973 1974 1979 1981 1984 1986 1988 1990 1992 1993 1994 1995
[1079] 1999 2002 2005 2006 2009 2011 2012 2017 2019 2021 2022 2027 2029 2032
[1093] 2034 2036 2038 2042 2043 2045 2049 2051 2053 2054 2055 2059 2061 2062
[1107] 2063 2068 2069 2070 2071 2073 2074 2077 2078 2079 2080 2081 2082 2083
[1121] 2084 2085 2086 2087 2088 2089 2094 2096 2097 2098 2099 2100 2101 2102
[1135] 2104 2105 2108 2110 2112 2113 2114 2116 2117 2118 2119 2123 2124 2125
[1149] 2126 2127 2129 2130 2131 2137 2138 2139 2143 2145 2151 2157 2158 2160
[1163] 2161 2163 2166 2167 2168 2169 2171 2174 2175 2176 2177 2178 2179 2180
[1177] 2181 2183 2185 2188 2189 2190 2191 2200 2201 2202 2203 2205 2206 2207
[1191] 2208 2210 2212 2215 2219 2221 2222 2223 2225 2226 2227 2228 2231 2232
[1205] 2234 2236 2237 2238 2239 2241 2242 2245 2248 2249 2253 2255 2257 2258
[1219] 2259 2260 2261 2262 2264 2269 2271 2272 2273 2274 2275 2280 2283 2285
[1233] 2290 2291 2297 2298 2299 2301 2302 2305 2306 2307 2311 2314 2316 2317
[1247] 2319 2320 2322 2323 2324 2325 2326 2328 2329 2330 2331 2332 2334 2336
[1261] 2337 2338 2340 2341 2342 2343 2344 2345 2348 2349 2350 2351 2352 2354
[1275] 2356 2359 2362 2364 2367 2368 2369 2371 2373 2377 2378 2379 2380 2384
[1289] 2385 2386 2388 2398 2400 2402 2410 2411 2414 2415 2416 2418 2420 2424
[1303] 2425 2428 2429 2437 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451
[1317] 2452 2453 2454 2462 2463 2464 2465 2466 2467 2469 2472 2473 2475 2477
[1331] 2481 2483 2485 2489 2490 2491 2492 2495 2496 2497 2498 2499 2500 2503
[1345] 2504 2506 2507 2508 2509 2510 2511 2512 2513 2520 2522 2523 2525 2526
[1359] 2527 2528 2529 2530 2531 2532 2533 2535 2536 2539 2541 2543 2545 2546
[1373] 2547 2554 2561 2562 2563 2564 2565 2566 2570 2571 2572 2573 2575 2577
[1387] 2578 2581 2582 2583 2584 2585 2588 2590 2591 2597 2598 2599 2603 2604
[1401] 2605 2606 2607 2608 2613 2614 2615 2617 2618 2619 2620 2621 2627 2634
[1415] 2635 2636 2638 2639 2640 2641 2642 2643 2644 2645 2647 2653 2655 2656
[1429] 2657 2658 2659 2663 2664 2667 2668 2669 2670 2672 2673 2674 2676 2677
[1443] 2678 2679 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698
[1457] 2700 2701 2702 2703 2704 2707 2709 2711 2713 2714 2717 2718 2721 2722
[1471] 2723 2724 2727 2728 2729 2731 2733 2736 2738 2739 2740 2741 2742 2746
[1485] 2748 2753 2757 2759 2760 2763 2764 2765 2766 2771 2773 2776 2777 2778
[1499] 2781 2783 2784 2786 2788 2789 2790 2792 2797 2799 2800 2803 2804 2806
[1513] 2807 2808 2809 2810 2813 2814 2817 2819 2821 2825 2826 2831 2832 2834
[1527] 2838 2839 2844 2846 2847 2848 2849 2850 2851 2852 2855 2856 2857 2862
[1541] 2863 2868 2869 2870 2871 2872 2873

$cor
                    bpi_antisocialT1Sum bpi_antisocialT2Sum
bpi_antisocialT1Sum           1.0000000           0.4511764
bpi_antisocialT2Sum           0.4511764           1.0000000

$n.obs
[1] 2875</code></pre>
</div>
<div id="winsorized-correlation" class="section level7"
number="13.1.6.2.3.1.4">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.4</span> Winsorized
correlation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], winsorize = 0.2, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["int"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4651836","4":"0.95","5":"0.4360424","6":"0.4933503","7":"28.16721","8":"2873","9":"2.369015e-154","10":"Winsorized Pearson correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="percentage-bend-correlation" class="section level7"
number="13.1.6.2.3.1.5">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.5</span> Percentage bend
correlation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;percentage&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4837302","4":"0.95","5":"0.4552238","6":"0.511246","7":"29.62478","8":"2873","9":"1.536055e-168","10":"Percentage Bend correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="biweight-midcorrelation" class="section level7"
number="13.1.6.2.3.1.6">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.1.6</span> Biweight
midcorrelation</p>
<pre class="r fold-show"><code>correlation(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)], method = &quot;biweight&quot;, p_adjust = &quot;none&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Parameter1"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Parameter2"],"name":[2],"type":["chr"],"align":["left"]},{"label":["r"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["CI"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["CI_low"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["CI_high"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["t"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["df_error"],"name":[8],"type":["int"],"align":["right"]},{"label":["p"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Method"],"name":[10],"type":["chr"],"align":["left"]},{"label":["n_Obs"],"name":[11],"type":["int"],"align":["right"]}],"data":[{"1":"bpi_antisocialT1Sum","2":"bpi_antisocialT2Sum","3":"0.4792883","4":"0.95","5":"0.465104","6":"0.4932265","7":"58.63389","8":"2873","9":"0","10":"Biweight correlation","11":"2875","_rn_":"1"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="robust-regression-with-a-single-predictor"
class="section level6" number="13.1.6.2.3.2">
<h6><span class="header-section-number">13.1.6.2.3.2</span> Robust
regression with a single predictor</h6>
<div id="theil-sen-estimator" class="section level7"
number="13.1.6.2.3.2.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.2.1</span> Theil-Sen
estimator</p>
<p>The Theil-Sen single median estimator is robust to outliers; have to
remove missing values first</p>
<pre class="r fold-show"><code>mydata_subset &lt;- na.omit(mydata[,c(&quot;bpi_antisocialT1Sum&quot;,&quot;bpi_antisocialT2Sum&quot;)])[1:400,]
mblm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, repeated = FALSE)</code></pre>
<pre><code>
Call:
mblm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, 
    repeated = FALSE)

Coefficients:
        (Intercept)  bpi_antisocialT1Sum  
                1.0                  0.6  </code></pre>
</div>
</div>
<div id="robust-multiple-regression" class="section level6"
number="13.1.6.2.3.3">
<h6><span class="header-section-number">13.1.6.2.3.3</span> Robust
multiple regression</h6>
<p>Best when no outliers: MM-type regression estimator</p>
<div id="robust-linear-regression" class="section level7"
number="13.1.6.2.3.3.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.3.1</span> Robust linear
regression</p>
<p>MM-type regression estimator (best):</p>
<pre class="r fold-show"><code>lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)</code></pre>
<pre><code>
Call:
lmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,     data = mydata, na.action = na.exclude)
 \--&gt; method = &quot;MM&quot;
Coefficients:
            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                 0.9440                   0.4914                   0.1577  </code></pre>
<p>Iteratively reweighted least squares (IRLS): <a
href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a> (archived at
<a
href="https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm"
class="uri">https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>)</p>
<pre class="r fold-show"><code>rlm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    na.action = na.exclude)</code></pre>
<pre><code>Call:
rlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    data = mydata, na.action = na.exclude)
Converged in 6 iterations

Coefficients:
            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              0.9838033               0.4873118               0.1620020 

Degrees of freedom: 2874 total; 2871 residual
  (8656 observations deleted due to missingness)
Scale estimate: 1.62 </code></pre>
</div>
<div id="robust-generalized-regression-1" class="section level7"
number="13.1.6.2.3.3.2">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.3.2</span> Robust generalized
regression</p>
<pre class="r fold-show"><code>glmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    family = &quot;poisson&quot;,
    na.action = &quot;na.exclude&quot;)</code></pre>
<pre><code>
Call:  glmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +      bpi_anxiousDepressedSum, family = &quot;poisson&quot;, data = mydata,      na.action = &quot;na.exclude&quot;) 

Coefficients:
            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                0.37281                  0.15589                  0.05026  

Number of observations: 2874 
Fitted by method  &#39;Mqle&#39; </code></pre>
<p>Most resistant to outliers: Least trimmed squares (LTS)—but not good
as a standalone estimator, better for identifying outliers</p>
<pre class="r fold-show"><code>ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
       data = mydata,
       na.action = na.exclude)</code></pre>
<pre><code>
Call:
ltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +     bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)

Coefficients:
              Intercept      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  
                 0.7501                   0.5477                   0.1104  

Scale estimate 1.752 </code></pre>
<div id="not-great-options" class="section level8"
number="13.1.6.2.3.3.2.1">
<p class="heading"><span
class="header-section-number">13.1.6.2.3.3.2.1</span> Not great
options:</p>
<p>Quantile (L1) regression: <code>rq()</code> function of quantreg
package</p>
<p><a
href="https://data.library.virginia.edu/getting-started-with-quantile-regression/"
class="uri">https://data.library.virginia.edu/getting-started-with-quantile-regression/</a>
(archived at <a href="https://perma.cc/FSV4-5DCR"
class="uri">https://perma.cc/FSV4-5DCR</a>)</p>
<pre class="r fold-show"><code>quantiles &lt;- 1:9/10

quantileRegressionModel &lt;- rq(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  tau = quantiles,
  na.action = na.exclude)

quantileRegressionModel</code></pre>
<pre><code>Call:
rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

Coefficients:
                           tau= 0.1  tau= 0.2   tau= 0.3 tau= 0.4 tau= 0.5
(Intercept)             -0.19607843 0.0000000 0.09090909      0.5     0.88
bpi_antisocialT1Sum      0.19607843 0.3333333 0.45454545      0.5     0.52
bpi_anxiousDepressedSum  0.09803922 0.1111111 0.13636364      0.1     0.12
                         tau= 0.6 tau= 0.7 tau= 0.8  tau= 0.9
(Intercept)             1.0000000   1.3750   2.1250 3.1000000
bpi_antisocialT1Sum     0.5913978   0.6250   0.6250 0.6428571
bpi_anxiousDepressedSum 0.2043011   0.1875   0.1875 0.2285714

Degrees of freedom: 11530 total; 11527 residual</code></pre>
<pre class="r fold-show"><code>summary(quantileRegressionModel)</code></pre>
<pre><code>Warning in summary.rq(xi, U = U, ...): 349 non-positive fis</code></pre>
<pre><code>Warning in summary.rq(xi, U = U, ...): 291 non-positive fis</code></pre>
<pre><code>
Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.1

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)             -0.19608  0.05341   -3.67101  0.00025
bpi_antisocialT1Sum      0.19608  0.03492    5.61452  0.00000
bpi_anxiousDepressedSum  0.09804  0.03536    2.77276  0.00559

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.2

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              0.00000  0.04196    0.00000  1.00000
bpi_antisocialT1Sum      0.33333  0.02103   15.85299  0.00000
bpi_anxiousDepressedSum  0.11111  0.02669    4.16376  0.00003

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.3

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              0.09091  0.05830    1.55945  0.11900
bpi_antisocialT1Sum      0.45455  0.02584   17.59380  0.00000
bpi_anxiousDepressedSum  0.13636  0.03601    3.78723  0.00016

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.4

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              0.50000  0.06410    7.80035  0.00000
bpi_antisocialT1Sum      0.50000  0.02374   21.06425  0.00000
bpi_anxiousDepressedSum  0.10000  0.03372    2.96584  0.00304

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.5

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              0.88000  0.06584   13.36547  0.00000
bpi_antisocialT1Sum      0.52000  0.02491   20.87561  0.00000
bpi_anxiousDepressedSum  0.12000  0.03771    3.18245  0.00148

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.6

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              1.00000  0.04246   23.55356  0.00000
bpi_antisocialT1Sum      0.59140  0.02146   27.55553  0.00000
bpi_anxiousDepressedSum  0.20430  0.03156    6.47422  0.00000

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.7

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              1.37500  0.07716   17.82011  0.00000
bpi_antisocialT1Sum      0.62500  0.02397   26.07659  0.00000
bpi_anxiousDepressedSum  0.18750  0.03760    4.98623  0.00000

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.8

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              2.12500  0.09498   22.37224  0.00000
bpi_antisocialT1Sum      0.62500  0.03524   17.73509  0.00000
bpi_anxiousDepressedSum  0.18750  0.05040    3.72020  0.00020

Call: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, 
    tau = quantiles, data = mydata, na.action = na.exclude)

tau: [1] 0.9

Coefficients:
                        Value    Std. Error t value  Pr(&gt;|t|)
(Intercept)              3.10000  0.11730   26.42749  0.00000
bpi_antisocialT1Sum      0.64286  0.04797   13.40136  0.00000
bpi_anxiousDepressedSum  0.22857  0.06439    3.54960  0.00039</code></pre>
<p>Plot to examine the association between the predictor and the outcome
at different levels (i.e., quantiles) of the predictor:</p>
<pre class="r fold-show"><code>ggplot(
  mydata,
  aes(bpi_antisocialT1Sum, bpi_antisocialT2Sum)) + 
  geom_point() + 
  geom_quantile(
    quantiles = quantiles,
    aes(color = factor(after_stat(quantile))))</code></pre>
<pre><code>Warning: Removed 8655 rows containing non-finite values (`stat_quantile()`).</code></pre>
<pre><code>Smoothing formula not specified. Using: y ~ x</code></pre>
<pre><code>Warning: Removed 8655 rows containing missing values (`geom_point()`).</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p>Below is a plot that examines the difference in quantile coefficients
(and associated confidence intervals) at different levels (i.e.,
quantiles) of the predictor. The x-axis is the quantile of the
predictor. The y-axis is the slope coefficient. Each black dot is the
slope coefficient for the given quantile of the predictor. The red lines
are the least squares estimate of the slope coefficient and its
confidence interval.</p>
<pre class="r fold-show"><code>plot(summary(quantileRegressionModel))</code></pre>
<pre><code>Warning in summary.rq(xi, U = U, ...): 349 non-positive fis</code></pre>
<pre><code>Warning in summary.rq(xi, U = U, ...): 291 non-positive fis</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(
  summary(quantileRegressionModel),
  parm = &quot;bpi_antisocialT1Sum&quot;)</code></pre>
<pre><code>Warning in summary.rq(xi, U = U, ...): 349 non-positive fis

Warning in summary.rq(xi, U = U, ...): 291 non-positive fis</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-92-2.png" width="672" /></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="examining-model-assumptions" class="section level2"
number="13.2">
<h2><span class="header-section-number">13.2</span> Examining Model
Assumptions</h2>
<div id="distribution-of-residuals" class="section level3"
number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Distribution of
Residuals</h3>
<div id="qqPlot" class="section level4" number="13.2.1.1">
<h4><span class="header-section-number">13.2.1.1</span> QQ plots</h4>
<p><a
href="http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html"
class="uri">http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html</a>
(archived at <a href="https://perma.cc/9UC3-3WRX"
class="uri">https://perma.cc/9UC3-3WRX</a>)</p>
<pre class="r fold-show"><code>qqPlot(multipleRegressionModel, main = &quot;QQ Plot&quot;, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<pre><code>[1] 8955 8956</code></pre>
<pre class="r fold-show"><code>qqnorm(resid(multipleRegressionModel))
qqnorm(resid(rmsMultipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-2.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(robustLinearRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-3.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(ltsRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-4.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(generalizedRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-5.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(rmsGeneralizedRegressionModel))</code></pre>
<pre><code>Error in residuals.Glm(object, ...): argument &quot;type&quot; is missing, with no default</code></pre>
<pre class="r fold-show"><code>qqnorm(resid(robustGeneralizedRegression))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-93-6.png" width="672" /></p>
<pre class="r fold-show"><code>qqnorm(resid(multilevelRegressionModel))</code></pre>
<pre><code>Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;resid&#39;: object &#39;multilevelRegressionModel&#39; not found</code></pre>
</div>
<div id="ppPlot" class="section level4" number="13.2.1.2">
<h4><span class="header-section-number">13.2.1.2</span> PP plots</h4>
<pre class="r fold-show"><code>ppPlot(multipleRegressionModel)
ppPlot(rmsMultipleRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-1.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(robustLinearRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-2.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(ltsRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-3.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(generalizedRegressionModel)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-4.png" width="672" /></p>
<pre class="r fold-show"><code>ppPlot(rmsGeneralizedRegressionModel)</code></pre>
<pre><code>Error in residuals.Glm(model): argument &quot;type&quot; is missing, with no default</code></pre>
<pre class="r fold-show"><code>ppPlot(robustGeneralizedRegression)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-94-5.png" width="672" /></p>
</div>
<div id="densityPlotResiduals" class="section level4" number="13.2.1.3">
<h4><span class="header-section-number">13.2.1.3</span> Density Plot of
Residuals</h4>
<pre class="r fold-show"><code>studentizedResiduals &lt;- na.omit(rstudent(multipleRegressionModel))
plot(density(studentizedResiduals), col=&quot;red&quot;)
xfit &lt;- seq(min(studentizedResiduals, na.rm=TRUE), max(studentizedResiduals, na.rm=TRUE), length=40)
lines(xfit, dnorm(xfit), col=&quot;gray&quot;) #compare to normal distribution</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
</div>
</div>
<div id="residualPlots" class="section level3" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Residual
Plots</h3>
<p>Residual plots are plots of the residuals versus observed/fitted
values.</p>
<p>Includes plots of a) model residuals versus observed values on
predictors and b) model residuals versus model fitted values.</p>
<p>Note: have to remove <code>na.action = na.exclude</code></p>
<p>Tests include:</p>
<ul>
<li>lack-of-fit test for every numeric predictor, t-test for the
regressor, added to the model, indicating no lack-of-fit for this
type</li>
<li>Tukey’s test for nonadditivity: adding the squares of the fitted
values to the model and refitting (evaluates adequacy of model fit)</li>
</ul>
<pre class="r fold-show"><code>residualPlots(multipleRegressionModelNoMissing, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<pre><code>                        Test stat Pr(&gt;|Test stat|)  
bpi_antisocialT1Sum       -2.0755          0.03803 *
bpi_anxiousDepressedSum   -1.2769          0.20175  
Tukey test                -2.3351          0.01954 *
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="marginalModelPlots" class="section level3" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Marginal Model
Plots</h3>
<p>Marginal model plots are plots of the outcome versus
predictors/fitted values.</p>
<p>Includes plots of a) observed outcome values versus values on
predictors (ignoring the other predictors) and b) observed outcome
values versus model fitted values.</p>
<pre class="r fold-show"><code>marginalModelPlots(multipleRegressionModel, sd = TRUE, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
</div>
<div id="addedVariablePlots" class="section level3" number="13.2.4">
<h3><span class="header-section-number">13.2.4</span> Added-Variable
Plots</h3>
<p>Added-variable plots are plots of the partial association between the
outcome and each predictor, controlling for all other predictors.</p>
<p>Useful for identifying jointly influential observations and studying
the impact of observations on the regression coefficients.</p>
<p>y-axis: residuals from model with all predictors excluding the
predictor of interest</p>
<p>x-axis: residuals from model regressing predictor of interest on all
other predictors</p>
<pre class="r fold-show"><code>avPlots(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<div id="refit-model-removing-jointly-influential-observations"
class="section level4" number="13.2.4.1">
<h4><span class="header-section-number">13.2.4.1</span> Refit model
removing jointly influential observations</h4>
<pre class="r fold-show"><code>multipleRegressionModel_removeJointlyInfluentialObs &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                                          data = mydata,
                                                          na.action = na.exclude,
                                                          subset = -c(6318,6023,4022,4040))

avPlots(multipleRegressionModel_removeJointlyInfluentialObs, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<pre class="r fold-show"><code>compareCoefs(multipleRegressionModel, multipleRegressionModel_removeJointlyInfluentialObs)</code></pre>
<pre><code>Calls:
1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)
2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(6318, 6023, 4022, 4040),
   na.action = na.exclude)

                        Model 1 Model 2
(Intercept)              1.1983  1.1949
SE                       0.0598  0.0599
                                       
bpi_antisocialT1Sum      0.4655  0.4649
SE                       0.0186  0.0188
                                       
bpi_anxiousDepressedSum  0.1608  0.1665
SE                       0.0292  0.0295
                                       </code></pre>
</div>
</div>
<div id="outlier-test" class="section level3" number="13.2.5">
<h3><span class="header-section-number">13.2.5</span> Outlier test</h3>
<p>Locates the largest Studentized residuals in absolute value and
computes the Bonferroni-corrected p-values based on a t-test for linear
models.</p>
<p>Test of outlyingness, i.e., how likely one would have a residual of a
given magnitude in a normal distribution with the same sample size</p>
<p>Note: it does not test how extreme an observation is relative to its
distribution (i.e., leverage)</p>
<pre class="r fold-show"><code>outlierTest(multipleRegressionModel)</code></pre>
<pre><code>     rstudent unadjusted p-value Bonferroni p
8955 6.519574         8.2999e-11   2.3854e-07
8956 6.519574         8.2999e-11   2.3854e-07
8957 6.182195         7.2185e-10   2.0746e-06
2560 4.914941         9.3800e-07   2.6958e-03
1385 4.573975         4.9884e-06   1.4337e-02</code></pre>
</div>
<div id="observations-with-high-leverage" class="section level3"
number="13.2.6">
<h3><span class="header-section-number">13.2.6</span> Observations with
high leverage</h3>
<p>Identifies observations with high leverage (i.e., high hat
values)</p>
<p>hat values are an index of leverage (observations that are far from
the center of the regressor space and have greater influence on OLS
regression coefficients)</p>
<pre class="r fold-show"><code>hist(hatvalues(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(hatvalues(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-101-2.png" width="672" /></p>
<pre class="r fold-show"><code>influenceIndexPlot(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-101-3.png" width="672" /></p>
<pre class="r fold-show"><code>influencePlot(multipleRegressionModel, id = TRUE) # circle size is proportional to Cook&#39;s Distance</code></pre>
<img src="regression_files/figure-html/unnamed-chunk-101-4.png" width="672" />
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["StudRes"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Hat"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["CookD"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"-3.940128","2":"0.006990052","3":"0.03624388","_rn_":"5286"},{"1":"-4.264903","2":"0.009024093","3":"0.05488391","_rn_":"10898"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r fold-show"><code>leveragePlots(multipleRegressionModel, id = TRUE)</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-101-5.png" width="672" /></p>
</div>
<div
id="observations-with-high-influence-on-ols-regression-coefficients"
class="section level3" number="13.2.7">
<h3><span class="header-section-number">13.2.7</span> Observations with
high influence (on OLS regression coefficients)</h3>
<p><a
href="https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html"
class="uri">https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html</a>
(archived at <a href="https://perma.cc/T2TS-PZZ4"
class="uri">https://perma.cc/T2TS-PZZ4</a>)</p>
<pre class="r fold-show"><code>head(influence.measures(multipleRegressionModel)$infmat)</code></pre>
<pre><code>        dfb.1_     dfb.b_T1     dfb.b_DS       dffit     cov.r       cook.d
1  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
2  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
3 -0.006546394  0.008357248 -0.006308529 -0.01004886 1.0024362 0.0000336708
4  0.042862749 -0.025185789 -0.007782877  0.04286275 0.9998621 0.0006121902
5  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA
6 -0.018327367  0.010769006  0.003327823 -0.01832737 1.0015775 0.0001119888
           hat
1 0.0000000000
2 0.0000000000
3 0.0014593072
4 0.0009143185
5 0.0000000000
6 0.0009143185</code></pre>
</div>
<div id="dfbeta" class="section level3" number="13.2.8">
<h3><span class="header-section-number">13.2.8</span> DFBETA</h3>
<pre class="r fold-show"><code>head(dfbeta(multipleRegressionModel))</code></pre>
<pre><code>    (Intercept) bpi_antisocialT1Sum bpi_anxiousDepressedSum
1  0.0000000000        0.0000000000            0.000000e+00
2  0.0000000000        0.0000000000            0.000000e+00
3 -0.0003917284        0.0001553400           -1.839704e-04
4  0.0025639901       -0.0004679817           -2.268890e-04
5  0.0000000000        0.0000000000            0.000000e+00
6 -0.0010966309        0.0002001580            9.704151e-05</code></pre>
<pre class="r fold-show"><code>hist(dfbeta(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<pre class="r fold-show"><code>dfbetasPlots(multipleRegressionModel, id = TRUE)</code></pre>
<pre><code>Error in dfbetasPlots.lm(multipleRegressionModel, id = TRUE): argument 2 matches multiple formal arguments</code></pre>
</div>
<div id="dffits" class="section level3" number="13.2.9">
<h3><span class="header-section-number">13.2.9</span> DFFITS</h3>
<pre class="r fold-show"><code>head(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<pre><code>     8472      4468      1917      8955      8956      1986 
0.2664075 0.2428065 0.2237760 0.1972271 0.1972271 0.1923034 </code></pre>
<pre class="r fold-show"><code>hist(dffits(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(dffits(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-104-2.png" width="672" /></p>
<pre class="r fold-show"><code>plot(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-104-3.png" width="672" /></p>
</div>
<div id="cooksDistance" class="section level3" number="13.2.10">
<h3><span class="header-section-number">13.2.10</span> Cook’s
Distance</h3>
<p>Observations that are both outlying (have a high residual from the
regression line) and have high leverage (are far from the center of the
regressor space) have high influence on the OLS regression coefficients.
An observation will have less influence if it lies on the regression
line (not an outlier, i.e., has a low residual) or if it has low
leverage (i.e., has a value near the center of a predictor’s
distribution).</p>
<pre class="r fold-show"><code>head(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<pre><code>      2765       1371       2767       8472       6170       6023 
0.05488391 0.03624388 0.03624388 0.02358305 0.02323282 0.02315496 </code></pre>
<pre class="r fold-show"><code>hist(cooks.distance(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
<pre class="r fold-show"><code>plot(cooks.distance(multipleRegressionModel))</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-105-2.png" width="672" /></p>
<pre class="r fold-show"><code>plot(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])</code></pre>
<p><img src="regression_files/figure-html/unnamed-chunk-105-3.png" width="672" /></p>
<div id="refit-model-removing-values-with-high-cooks-distance"
class="section level4" number="13.2.10.1">
<h4><span class="header-section-number">13.2.10.1</span> Refit model
removing values with high cook’s distance</h4>
<pre class="r fold-show"><code>multipleRegressionModel_2 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371))

multipleRegressionModel_3 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767))

multipleRegressionModel_4 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472))

multipleRegressionModel_5 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170))

multipleRegressionModel_6 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023))

multipleRegressionModel_7 &lt;- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023,2766))</code></pre>
<div
id="examine-how-much-regression-coefficients-change-when-excluding-influential-observations"
class="section level5" number="13.2.10.1.1">
<h5><span class="header-section-number">13.2.10.1.1</span> Examine how
much regression coefficients change when excluding influential
observations</h5>
<pre class="r fold-show"><code>compareCoefs(multipleRegressionModel, multipleRegressionModel_2, multipleRegressionModel_3, multipleRegressionModel_4, multipleRegressionModel_5, multipleRegressionModel_6, multipleRegressionModel_7)</code></pre>
<pre><code>Calls:
1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)
2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371), na.action =
   na.exclude)
3: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767), 
  na.action = na.exclude)
4: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472),
   na.action = na.exclude)
5: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170), na.action = na.exclude)
6: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170, 6023), na.action = na.exclude)
7: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + 
  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, 
  6170, 6023, 2766), na.action = na.exclude)

                        Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7
(Intercept)              1.1983  1.1704  1.1577  1.1676  1.1620  1.1537  1.1439
SE                       0.0598  0.0597  0.0596  0.0596  0.0596  0.0595  0.0595
                                                                               
bpi_antisocialT1Sum      0.4655  0.4739  0.4779  0.4748  0.4744  0.4793  0.4836
SE                       0.0186  0.0185  0.0185  0.0185  0.0185  0.0185  0.0185
                                                                               
bpi_anxiousDepressedSum  0.1608  0.1691  0.1726  0.1699  0.1770  0.1742  0.1743
SE                       0.0292  0.0290  0.0290  0.0289  0.0290  0.0290  0.0289
                                                                               </code></pre>
</div>
</div>
<div
id="examine-how-much-regression-coefficients-change-when-using-least-trimmed-squares-lts-that-is-resistant-to-outliers"
class="section level4" number="13.2.10.2">
<h4><span class="header-section-number">13.2.10.2</span> Examine how
much regression coefficients change when using least trimmed squares
(LTS) that is resistant to outliers</h4>
<pre class="r fold-show"><code>coef(lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
        data = mydata,
        na.action=&quot;na.exclude&quot;))</code></pre>
<pre><code>            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              1.1983004               0.4655286               0.1607541 </code></pre>
<pre class="r fold-show"><code>coef(ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = &quot;na.exclude&quot;))</code></pre>
<pre><code>              Intercept     bpi_antisocialT1Sum bpi_anxiousDepressedSum 
              0.7317979               0.5479517               0.1390324 </code></pre>
</div>
</div>
<div id="resources" class="section level3" number="13.2.11">
<h3><span class="header-section-number">13.2.11</span> Resources</h3>
<p>Book: An R Companion to Applied Regression</p>
<p><a href="http://people.duke.edu/~rnau/testing.htm"
class="uri">http://people.duke.edu/~rnau/testing.htm</a> (archived at <a
href="https://perma.cc/9CXH-GBSN"
class="uri">https://perma.cc/9CXH-GBSN</a>)</p>
<p><a href="http://www.statmethods.net/stats/rdiagnostics.html"
class="uri">http://www.statmethods.net/stats/rdiagnostics.html</a>
(archived at <a href="https://perma.cc/7JX8-K6BY"
class="uri">https://perma.cc/7JX8-K6BY</a>)</p>
<p><a
href="http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html"
class="uri">http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html</a>
(archived at <a href="https://perma.cc/4A2P-9S49"
class="uri">https://perma.cc/4A2P-9S49</a>)</p>
<p><a
href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions"
class="uri">https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions</a>
(archived at <a href="https://perma.cc/Q4NS-X8TJ"
class="uri">https://perma.cc/Q4NS-X8TJ</a>)</p>
</div>
</div>
</div>
<div id="session-info" class="section level1" number="14">
<h1><span class="header-section-number">14</span> Session Info</h1>
<pre class="r fold-hide"><code>sessionInfo()</code></pre>
<pre><code>R version 4.3.2 (2023-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 22.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0

locale:
 [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       
 [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   
 [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          
[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   

time zone: UTC
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] mice_3.16.0            regtools_1.7.0         gtools_3.9.5          
 [4] FNN_1.1.4              lavaan_0.6-17          interactions_1.1.5    
 [7] correlation_0.8.4      effects_4.2-2          mblm_0.12.1           
[10] quantreg_5.97          SparseM_1.81           olsrr_0.6.0           
[13] foreign_0.8-85         AER_1.2-12             survival_3.5-7        
[16] sandwich_3.1-0         lmtest_0.9-40          zoo_1.8-12            
[19] mgcv_1.9-0             nlme_3.1-163           car_3.1-2             
[22] carData_3.0-5          cvTools_0.3.2          lattice_0.21-9        
[25] brms_2.20.4            Rcpp_1.0.12            robustbase_0.99-2     
[28] rms_6.7-1              Hmisc_5.1-1            psych_2.4.1           
[31] lubridate_1.9.3        forcats_1.0.0          stringr_1.5.1         
[34] dplyr_1.1.4            purrr_1.0.2            readr_2.1.5           
[37] tidyr_1.3.1            tibble_3.2.1           ggplot2_3.4.4         
[40] tidyverse_2.0.0        MASS_7.3-60            petersenlab_0.1.2-9031

loaded via a namespace (and not attached):
  [1] shinythemes_1.2.0    splines_4.3.2        later_1.3.2         
  [4] polspline_1.1.24     datawizard_0.9.1     xts_0.13.2          
  [7] rpart_4.1.21         lifecycle_1.0.4      StanHeaders_2.32.5  
 [10] processx_3.8.3       insight_0.19.8       crosstalk_1.2.1     
 [13] backports_1.4.1      survey_4.2-1         magrittr_2.0.3      
 [16] sass_0.4.8           rmarkdown_2.25       jquerylib_0.1.4     
 [19] yaml_2.3.8           httpuv_1.6.14        text2vec_0.6.4      
 [22] pkgbuild_1.4.3       DBI_1.2.2            minqa_1.2.6         
 [25] RColorBrewer_1.1-3   multcomp_1.4-25      abind_1.4-5         
 [28] quadprog_1.5-8       nnet_7.3-19          TH.data_1.1-2       
 [31] tensorA_0.36.2.1     float_0.3-2          jtools_2.2.2        
 [34] inline_0.3.19        nortest_1.0-4        MatrixModels_0.5-3  
 [37] goftest_1.2-3        bridgesampling_1.1-2 codetools_0.2-19    
 [40] DT_0.31              shape_1.4.6          tidyselect_1.2.0    
 [43] farver_2.1.1         bayesplot_1.11.1     lme4_1.1-35.1       
 [46] matrixStats_1.2.0    stats4_4.3.2         base64enc_0.1-3     
 [49] jsonlite_1.8.8       rsparse_0.5.1        mitml_0.4-5         
 [52] ellipsis_0.3.2       Formula_1.2-5        iterators_1.0.14    
 [55] foreach_1.5.2        tools_4.3.2          glue_1.7.0          
 [58] pan_1.9              mnormt_2.1.1         gridExtra_2.3       
 [61] xfun_0.42            distributional_0.4.0 rje_1.12.1          
 [64] loo_2.6.0            withr_3.0.0          fastmap_1.1.1       
 [67] mitools_2.4          boot_1.3-28.1        fansi_1.0.6         
 [70] shinyjs_2.1.0        callr_3.7.3          digest_0.6.34       
 [73] estimability_1.4.1   timechange_0.3.0     R6_2.5.1            
 [76] mime_0.12            colorspace_2.1-0     mix_1.0-11          
 [79] markdown_1.12        threejs_0.3.3        RhpcBLASctl_0.23-42 
 [82] utf8_1.2.4           generics_0.1.3       data.table_1.15.0   
 [85] htmlwidgets_1.6.4    pkgconfig_2.0.3      dygraphs_1.1.1.6    
 [88] gtable_0.3.4         htmltools_0.5.7      scales_1.3.0        
 [91] posterior_1.5.0      lgr_0.4.4            knitr_1.45          
 [94] rstudioapi_0.15.0    tzdb_0.4.0           reshape2_1.4.4      
 [97] coda_0.19-4.1        checkmate_2.3.1      nloptr_2.0.3        
[100] cachem_1.0.8         parallel_4.3.2       miniUI_0.1.1.1      
[103] mlapi_0.1.1          pillar_1.9.0         grid_4.3.2          
[106] vctrs_0.6.5          shinystan_2.6.0      promises_1.2.1      
[109] jomo_2.7-6           xtable_1.8-4         cluster_2.1.4       
[112] htmlTable_2.4.2      evaluate_0.23        pbivnorm_0.6.0      
[115] mvtnorm_1.2-4        cli_3.6.2            compiler_4.3.2      
[118] rlang_1.1.3          crayon_1.5.2         rstantools_2.4.0    
[121] labeling_0.4.3       ps_1.7.6             plyr_1.8.9          
[124] pander_0.6.5         stringi_1.8.3        rstan_2.32.5        
[127] viridisLite_0.4.2    QuickJSR_1.1.3       munsell_0.5.0       
[130] colourpicker_1.3.0   glmnet_4.1-8         Brobdingnag_1.2-9   
[133] bayestestR_0.13.2    Matrix_1.6-1.1       hms_1.1.3           
[136] shiny_1.8.0          highr_0.10           broom_1.0.5         
[139] igraph_2.0.1.1       RcppParallel_5.1.7   bslib_0.6.1         
[142] DEoptimR_1.1-3      </code></pre>
</div>

<div id="rmd-source-code">---
title: "Regression"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  comment = "",
  class.source = "fold-show")
```

# Preamble

## Install Libraries

```{r, class.source = "fold-hide"}
#install.packages("remotes")
#remotes::install_github("DevPsyLab/petersenlab")
```

## Load Libraries

```{r, message = FALSE, warning = FALSE, class.source = "fold-hide"}
library("petersenlab")
library("MASS")
library("tidyverse")
library("psych")
library("rms")
library("robustbase")
library("brms")
library("cvTools")
library("car")
library("mgcv")
library("AER")
library("foreign")
library("olsrr")
library("quantreg")
library("mblm")
library("effects")
library("correlation")
library("interactions")
library("lavaan")
library("regtools")
library("mice")
```

# Import Data

```{r, eval = FALSE, class.source = "fold-hide"}
mydata <- read.csv("https://osf.io/8syp5/download")
```

```{r, include = FALSE}
mydata <- read.csv("./data/cnlsy.csv") #https://osf.io/8syp5/download
```

# Data Preparation

```{r, class.source = "fold-hide"}
mydata$countVariable <- as.integer(mydata$bpi_antisocialT2Sum)
mydata$orderedVariable <- factor(mydata$countVariable, ordered = TRUE)

mydata$female <- NA
mydata$female[which(mydata$sex == "male")] <- 0
mydata$female[which(mydata$sex == "female")] <- 1
```

# Linear Regression

## Linear regression model

```{r}
multipleRegressionModel <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel)
confint(multipleRegressionModel)
```

### Remove missing data

```{r}
multipleRegressionModelNoMissing <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata,
                                       na.action = na.omit)

multipleRegressionModelNoMissing <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                       data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit)
```

## Linear regression model on correlation/covariance matrix (for pairwise deletion)

```{r, warning = FALSE}
multipleRegressionModelPairwise <- setCor(
  y = "bpi_antisocialT2Sum",
  x = c("bpi_antisocialT1Sum","bpi_anxiousDepressedSum"),
  data = cov(mydata[,c("bpi_antisocialT2Sum","bpi_antisocialT1Sum","bpi_anxiousDepressedSum")], use = "pairwise.complete.obs"),
  n.obs = nrow(mydata))

summary(multipleRegressionModelPairwise)
multipleRegressionModelPairwise[c("coefficients","se","Probability","R2","shrunkenR2")]
```

## Linear regression model with robust covariance matrix (rms)

```{r}
rmsMultipleRegressionModel <- robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                         data = mydata,
                                         x = TRUE,
                                         y = TRUE))

rmsMultipleRegressionModel
confint(rmsMultipleRegressionModel)
```

## Robust linear regression (MM-type iteratively reweighted least squares regression)

```{r}
robustLinearRegression <- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude)

summary(robustLinearRegression)
confint(robustLinearRegression)
```

## Least trimmed squares regression (for removing outliers)

```{r}
ltsRegression <- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                        data = mydata,
                        na.action = na.exclude)

summary(ltsRegression)
```

## Bayesian linear regression

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianRegularizedRegression <- brm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     chains = 4,
                                     iter = 2000,
                                     seed = 52242)
```

```{r}
summary(bayesianRegularizedRegression)
```

# Generalized Linear Regression

## Generalized regression model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
generalizedRegressionModel <- glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = "poisson",
                                  na.action = na.exclude)

summary(generalizedRegressionModel)
confint(generalizedRegressionModel)
```

## Generalized regression model (rms)

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.

```{r}
rmsGeneralizedRegressionModel <- Glm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                     data = mydata,
                                     x = TRUE,
                                     y = TRUE,
                                     family = "poisson")

rmsGeneralizedRegressionModel
confint(rmsGeneralizedRegressionModel)
```

## Bayesian generalized linear model

In this example, we predict a count variable that has a poisson distribution.
We could change the distribution.
For example, we could use Gamma regression, `family = Gamma`, when the response variable is continuous and positive, and the coefficient of variation--rather than the variance--is constant.

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianGeneralizedLinearRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                           data = mydata,
                                           family = poisson,
                                           chains = 4,
                                           seed = 52242,
                                           iter = 2000)
```

```{r}
summary(bayesianGeneralizedLinearRegression)
```

## Robust generalized regression

```{r}
robustGeneralizedRegression <- glmrob(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      family = "poisson",
                                      na.action = na.exclude)

summary(robustGeneralizedRegression)
confint(robustGeneralizedRegression)
```

## Ordinal regression model

```{r}
ordinalRegressionModel1 <- polr(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata)

ordinalRegressionModel2 <- lrm(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  x = TRUE,
  y = TRUE)

ordinalRegressionModel3 <- orm(
  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  x = TRUE,
  y = TRUE)

summary(ordinalRegressionModel1)
confint(ordinalRegressionModel1)

ordinalRegressionModel2

ordinalRegressionModel3
confint(ordinalRegressionModel3)
```

## Bayesian ordinal regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianOrdinalRegression <- brm(orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                 data = mydata,
                                 family = cumulative(),
                                 chains = 4,
                                 seed = 52242,
                                 iter = 2000)
```

```{r}
summary(bayesianOrdinalRegression)
```

## Bayesian count regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianCountRegression <- brm(countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                               data = mydata,
                               family = "poisson",
                               chains = 4,
                               seed = 52242,
                               iter = 2000)
```

```{r}
summary(bayesianCountRegression)
```

## Logistic regression model (rms)

```{r}
logisticRegressionModel <- robcov(lrm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      x = TRUE,
                                      y = TRUE))

logisticRegressionModel
confint(logisticRegressionModel)
```

## Bayesian logistic regression model

```{r, message = FALSE, warning = FALSE, results = FALSE}
bayesianLogisticRegression <- brm(female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                  data = mydata,
                                  family = bernoulli,
                                  chains = 4,
                                  seed = 52242,
                                  iter = 2000)
```

```{r}
summary(bayesianLogisticRegression)
```

# Hierarchical Linear Regression

```{r}
model1 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum,
             data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit,
             na.action = na.exclude)

model2 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
             data = mydata %>% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %>% na.omit,
             na.action = na.exclude)

summary(model2)$adj.r.squared

anova(model1, model2)
summary(model2)$adj.r.squared - summary(model1)$adj.r.squared
```

# Moderated Multiple Regression {#moderation}

https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html (archived at https://perma.cc/P34H-7BH3)

```{r}
states <- as.data.frame(state.x77)
```

## Mean Center Predictors

Make sure to mean-center or orthogonalize predictors before computing the interaction term.

```{r}
states$Illiteracy_centered <- scale(states$Illiteracy, scale = FALSE)
states$Murder_centered <- scale(states$Murder, scale = FALSE)
```

## Model

```{r}
interactionModel <- lm(
  Income ~ Illiteracy_centered + Murder_centered + Illiteracy_centered:Murder_centered + `HS Grad`,
  data = states)
```

## Plots

```{r}
interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered)
interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, plot.points = TRUE)
interact_plot(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, interval = TRUE)

johnson_neyman(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, alpha = .05)
```

## Simple Slopes Analysis

```{r}
sim_slopes(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, johnson_neyman = FALSE)
sim_slopes(interactionModel,
           pred = Illiteracy_centered,
           modx = Murder_centered,
           modx.values = c(0, 5, 10),
           johnson_neyman = FALSE)
```

## Johnson-Neyman intervals

Indicates all the values of the moderator for which the slope of the predictor is statistically significant.

```{r}
sim_slopes(interactionModel, pred = Illiteracy_centered, modx = Murder_centered, johnson_neyman = TRUE)

probe_interaction(interactionModel,
                  pred = Illiteracy_centered,
                  modx = Murder_centered,
                  cond.int = TRUE,
                  interval = TRUE,
                  jnplot = TRUE)
```

# Approaches to Handling Missingness {#missingness}

## Listwise deletion {#listwiseDeletion}

Listwise deletion deletes every row in the data file that has a missing value for one of the model variables.

```{r}
listwiseDeletionModel <- lm(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  na.action = na.exclude)

summary(listwiseDeletionModel)
confint(listwiseDeletionModel)
```

## Pairwise deletion {#pairwiseDeletion}

Also see here:

- https://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002 (archived at https://perma.cc/GH5T-RXD9)
- https://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure (archived at https://perma.cc/F7EL-AUFZ)
- https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re (archived at https://perma.cc/KU3X-FB2C)
- https://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values (archived at https://perma.cc/QWQ5-2TLW)
- https://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps (archived at https://perma.cc/UC4K-2L9T)

Adapted from here: https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise (archived at https://perma.cc/EGU6-3M3Q)

```{r}
modelData <- mydata[,c("bpi_antisocialT2Sum","bpi_antisocialT1Sum","bpi_anxiousDepressedSum")]
varMeans <- colMeans(modelData, na.rm = TRUE)
varCovariances <- cov(modelData, use = "pairwise")

pairwiseRegression_syntax <- '
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
'

pairwiseRegression_fit <- lavaan(
  pairwiseRegression_syntax,
  sample.mean = varMeans,
  sample.cov = varCovariances,
  sample.nobs = sum(complete.cases(modelData))
)

summary(
  pairwiseRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)
```

## Full-information maximum likelihood (FIML) {#fiml}

```{r}
fimlRegression_syntax <- '
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum
  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum
  bpi_antisocialT2Sum ~ 1
'

fimlRegression_fit <- lavaan(
  fimlRegression_syntax,
  data = mydata,
  missing = "ML",
)

summary(
  fimlRegression_fit,
  standardized = TRUE,
  rsquare = TRUE)
```

## Multiple imputation {#imputation}

```{r}
modelData_imputed <- mice(
  modelData,
  m = 5,
  method = "pmm") # predictive mean matching; can choose among many methods

imputedData_fit <- with(
  modelData_imputed,
  lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum))

imputedData_pooledEstimates <- pool(imputedData_fit)
summary(imputedData_pooledEstimates)
```

# Model Building Steps

1. Examine extent and type of missing data, consider [how to handle missing values](#missingness) ([multiple imputation](#imputation), [FIML](#fiml), [pairwise deletion](#pairwiseDeletion), [listwise deletion](#listwiseDeletion))
    - Little's MCAR test from `mcar_test()` function of the `njtierney/naniar` package
    - Bayesian handling of missing data: https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html (archived at https://perma.cc/Z4HT-QXWR)
1. Examine descriptive statistics, consider variable transformations
1. Examine scatterplot matrix to examine whether associations between predictor and outcomes are linear or nonlinear
1. Model building: theory and empiricism (stepwise regression, likelihood ratio tests, k-fold cross validation to check for over-fitting compared to simpler model)
1. Test assumptions
    - Examine whether predictors have linear association with outcome (Residual Plots, Marginal Model Plots, Added-Variable Plots—check for non-horizontal lines)
    - Examine whether residuals have constant variance across levels of outcome and predictors Residual Plots, Spread-Level Plots—check for fan-shaped plot or other increasing/decreasing structure)
    - Examine whether predictors show multicollinearity (VIF)
    - Examine whether residuals are normally distributed (QQ plot and density plot)
    - Examine influence of influential observations (high outlyingness and leverage) on parameter stimates by iteratively removing influential observations and refitting (Added-Variable Plots)
1. Handle violated assumptions, select final set of predictors/outcomes and transformation of each
1. Use k-fold cross validation to identify the best estimation procedure (OLS, MM, or LTS) on the final model
1. Use identified estimation procedure to fit final model and determine the best parameter point estimates
1. Calculate bootstrapped estimates to determine the confidence intervals around the parameter estimates of the final model

# Bootstrapped Estimates

To determine the confidence intervals of parameter estimates

## Linear Regression

```{r}
multipleRegressionModelBootstrapped <- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(multipleRegressionModelBootstrapped)
confint(multipleRegressionModelBootstrapped, level = .95, type = "bca")
hist(multipleRegressionModelBootstrapped)
```

## Generalized Regression

```{r}
generalizedRegressionModelBootstrapped <- Boot(multipleRegressionModelNoMissing, R = 1000)
summary(generalizedRegressionModelBootstrapped)
confint(generalizedRegressionModelBootstrapped, level = .95, type = "bca")
hist(generalizedRegressionModelBootstrapped)
```

# Cross Validation

To examine degree of prediction error and over-fitting to determine best model

https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best (archived at https://perma.cc/38BL-KLRJ)

## K-fold cross validation

```{r}
kFolds <- 10
replications <- 20

folds <- cvFolds(nrow(mydata), K = kFolds, R = replications)

fitLm <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = "na.exclude")

fitLmrob <- lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                  data = mydata,
                  na.action = "na.exclude")

fitLts <- ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                 data = mydata,
                 na.action = "na.exclude")

cvFitLm <- cvLm(fitLm, K = kFolds, R = replications)

cvFitLmrob <- cvLmrob(fitLmrob, K = kFolds, R = replications)
cvFitLts <- cvLts(fitLts, K = kFolds, R = replications)

cvFits <- cvSelect(OLS = cvFitLm, MM = cvFitLmrob, LTS = cvFitLts)
cvFits

bwplot(cvFits, xlab = "Root Mean Square Error", xlim= c(0, max(cvFits$cv$CV) + 0.2))
```

# Examining Model Fits

## Effect Plots

```{r}
allEffects(multipleRegressionModel)
plot(allEffects(multipleRegressionModel))
```

## Confidence Ellipses

```{r}
confidenceEllipse(multipleRegressionModel, levels = c(0.5, .95))
```

## Data Ellipse

```{r}
mydata_nomissing <- na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")])
dataEllipse(mydata_nomissing$bpi_antisocialT1Sum, mydata_nomissing$bpi_antisocialT2Sum, levels = c(0.5, .95))
```

# Diagnostics

## Assumptions

### 1. Linear relation between predictors and outcome {#linearAssociation}

#### Ways to Test

##### Before Model Fitting

- scatterplot matrix
- distance correlation

###### Scatterplot Matrix

```{r}
scatterplotMatrix(~ bpi_antisocialT2Sum + bpi_antisocialT1Sum + bpi_anxiousDepressedSum, data = mydata)
```

###### Distance correlation

The distance correlation is an index of the degree of the linear and non-linear association between two variables.

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "distance", p_adjust = "none")
```

##### After Model Fitting

Check for nonlinearities (non-horizontal line) in plots of:

- Residuals versus fitted values ([Residual Plots](#residualPlots))—best
- Residuals versus predictors ([Residual Plots](#residualPlots))
- Outcome versus fitted values ([Marginal Model Plots](#marginalModelPlots))
- Outcome versus predictors, ignoring other predictors ([Marginal Model Plots](#marginalModelPlots))
- Outcome versus predictors, controlling for other predictors ([Added-Variable Plots](#addedVariablePlots))

#### Ways to Handle

- Transform outcome/predictor variables (Box-Cox transformations)
- Semi-parametric regression models: Generalized additive models (GAM)
- Non-parametric regression models: Nearest-Neighbor Kernel Regression

##### Semi-parametric or non-parametric regression models

http://www.lisa.stat.vt.edu/?q=node/7517 (archived at https://web.archive.org/web/20180113065042/http://www.lisa.stat.vt.edu/?q=node/7517)

Note: using semi-parametric or non-parametric models increases fit in context of nonlinearity at the expense of added complexity.
Make sure to avoid fitting an overly complex model (e.g., use k-fold cross validation).
Often, the simpler (generalized) linear model is preferable to semi-paremetric or non-parametric approaches

###### Semi-parametric: Generalized Additive Models

http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models (archived at https://web.archive.org/web/20170213041653/http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models)

```{r}
generalizedAdditiveModel <- gam(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                family = gaussian(),
                                data = mydata, na.action = na.exclude)

summary(generalizedAdditiveModel)
confint(generalizedAdditiveModel)
```

###### Non-parametric: Nearest-Neighbor Kernel Regression

### 2. Exogeneity

Exogeneity means that the association between predictors and outcome is fully causal and unrelated to other variables.

#### Ways to Test

- Durbin-Wu-Hausman test of endogeneity

##### Durbin-Wu-Hausman test of endogeneity

The instrumental variables (2SLS) estimator is implemented in the `R` package `AER` as command:

```{r, eval = FALSE}
ivreg(y ~ x1 + x2 + w1 + w2 | z1 + z2 + z3 + w1 + w2)
```

where `x1` and `x2` are endogenous regressors, `w1` and `w2` exogeneous regressors, and `z1` to `z3` are excluded instruments.

Durbin-Wu-Hausman test:

```{r, eval = FALSE}
hsng2 <- read.dta("http://www.stata-press.com/data/r11/hsng2.dta") #archived at https://perma.cc/7P2Q-ARKR
```

```{r, include = FALSE}
hsng2 <- read.dta("./data/hsng2.dta") #http://www.stata-press.com/data/r11/hsng2.dta archived at https://perma.cc/7P2Q-ARKR
```

```{r}
fiv <- ivreg(rent ~ hsngval + pcturban | pcturban + faminc + reg2 + reg3 + reg4, data = hsng2) #Housing values are likely endogeneous and therefore instrumented by median family income (faminc) and 3 regional dummies (reg2, reg4, reg4)

summary(fiv, diagnostics = TRUE)
```

The Eicker-Huber-White covariance estimator which is robust to heteroscedastic error terms is reported after estimation with `vcov = sandwich` in `coeftest()`

First stage results are reported by explicitly estimating them.
For example:

```{r}
first <- lm(hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, data = hsng2)

summary(first)
```

In case of a single endogenous variable (K = 1), the F-statistic to assess weak instruments is reported after estimating the first stage with, for example:

```{r}
waldtest(first, . ~ . - faminc - reg2 - reg3 - reg4)
```

or in case of heteroscedatistic errors:

```{r}
waldtest(first, . ~ . - faminc - reg2 - reg3- reg4, vcov = sandwich)
```

#### Ways to Handle

- Conduct an experiment/RCT with random assignment
- Instrumental variables

### 3. Homoscedasticity of residuals

Homoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome/predictors (i.e., the residuals show constant variance as a function of outcome/predictors).

#### Ways to Test

- Plot residuals vs. outcome and predictor variables ([Residual Plots](#residualPlots))
- Plot residuals vs. fitted values ([Residual Plots](#residualPlots))
- Time Series data: Plot residuals vs. time
- Spread-level plot
- Breusch-Pagan test: `bptest()` function from `lmtest` package
- Goldfeld-Quandt Test

##### Residuals vs. outcome and predictor variables

Plot residuals, or perhaps their absolute values, versus the outcome and predictor variables ([Residual Plots](#residualPlots)).
Examine whether residual variance is constant at all levels of other variables or whether it increases/decreases as a function of another variable (or shows some others structure, e.g., small variance at low and high levels of a predictor and high variance in the middle).
Note that this is *different than whether the residuals show non-linearities*—i.e., a non-horizontal line, which would indicate a [nonlinear association between variables](#linearAssociation) (see Assumption #1, above).
Rather, here we are examining whether there is change in the variance as a function of another variable (e.g., a fan-shaped [Residual Plot](#residualPlots))

##### Spread-level plot

Examining whether level (e.g., mean) depends on spread (e.g., variance)—plot of log of the absolute Studentized residuals against the log of the fitted values

```{r}
spreadLevelPlot(multipleRegressionModel)
```

##### Breusch-Pagan test

https://www.rdocumentation.org/packages/lmtest/versions/0.9-40/topics/bptest (archived at https://perma.cc/K4WC-7TVW)

```{r}
bptest(multipleRegressionModel)
```

##### Goldfeld-Quandt Test

```{r}
gqtest(multipleRegressionModel)
```

##### Test of dependence of spread on level

```{r}
ncvTest(multipleRegressionModel)
```

##### Test of dependence of spread on predictors

```{r}
ncvTest(multipleRegressionModel, ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum)
```

#### Ways to Handle

- If residual variance increases as a function of the fitted values, consider a poisson regression model (especially for count data), for which the variance does increase with the mean
- If residual variance differs as a function of model predictors, consider adding interactions/product terms (the effect of one variable depends on the level of another variable)
- Try transforming the outcome variable to be normally distributed (log-transform if the errors seem consistent in percentage rather than absolute terms)
- If the error variance is proportional to a variable $z$, then fit the model using Weighted Least Squares (WLS), with the weights given be $1/z$
- Weighted least squares (WLS) using the "weights" argument of the `lm()` function; to get weights, see: https://stats.stackexchange.com/a/100410/20338 (archived at https://perma.cc/C6BY-G9MS)
- Huber-White standard errors (a.k.a. "Sandwich" estimates) from a heteroscedasticity-corrected covariance matrix
    - `coeftest()` function from the `sandwich` package along with hccm sandwich estimates from the `car` package 
    - `robcov()` function from the `rms` package
- Time series data: ARCH (auto-regressive conditional heteroscedasticity) models
- Time series data: seasonal patterns can be addressed by applying log transformation to outcome variable

##### Huber-White standard errors

Standard errors (SEs) on the diagonal increase

```{r}
vcov(multipleRegressionModel)
hccm(multipleRegressionModel)

summary(multipleRegressionModel)
coeftest(multipleRegressionModel, vcov = sandwich)
coeftest(multipleRegressionModel, vcov = hccm)

robcov(ols(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
           data = mydata,
           x = TRUE,
           y = TRUE))

robcov(ols(t_ext ~ m_ext + age,
           data = mydata,
           x = TRUE,
           y = TRUE),
       cluster = mydata$tcid) #account for nested data within subject
```

### 4. Errors are independent

Independent errors means that the errors are uncorrelated with each other.

#### Ways to Test

- Plot residuals vs. predictors ([Residual Plots](#residualPlots))
- Time Series data: Residual time series plot (residuals vs. row number)
- Time Series data: Table or plot of residual autocorrelations
- Time Series data: Durbin-Watson statistic for test of significant residual autocorrelation at lag 1

#### Ways to Handle

- Generalized least squares (GLS) models are capable of handling correlated errors: https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html (archived at https://perma.cc/RHZ6-5GT8)
- Regression with cluster variable
    - `robcov()` from `rms` package
- [Hierarchical linear modeling](hlm.html)
    - [Linear mixed effects models](hlm.html#linear)
    - [Generalized linear mixed effects models](hlm.html#generalized)
    - [Nonlinear mixed effects models](hlm.html#nonlinear)

### 5. No multicollinearity

Multicollinearity occurs when the predictors are correlated with each other.

#### Ways to Test

- Variance Inflation Factor (VIF)
- Generalized Variance Inflation Factor (GVIF)—when models have related regressors (multiple polynomial terms or contrasts from same predictor)
- Correlation
- Tolerance
- Condition Index

##### Variance Inflation Factor (VIF)

$$
\text{VIF} = 1/\text{Tolerance}
$$


If the variance inflation factor of a predictor variable were 5.27 ($\sqrt{5.27} = 2.3$), this means that the standard error for the coefficient of that predictor variable is 2.3 times as large (i.e., confidence interval is 2.3 times wider) as it would be if that predictor variable were uncorrelated with the other predictor variables.

VIF = 1: Not correlated\
1 < VIF < 5: Moderately correlated\
VIF > 5 to 10: Highly correlated (multicollinearity present)

```{r}
vif(multipleRegressionModel)
```

##### Generalized Variance Inflation Factor (GVIF)

Useful when models have related regressors (multiple polynomial terms or contrasts from same predictor)

##### Correlation

correlation among all independent variables the correlation coefficients should be smaller than .08

```{r}
cor(mydata[,c("bpi_antisocialT1Sum","bpi_anxiousDepressedSum")], use = "pairwise.complete.obs")
```

##### Tolerance

The tolerance is an index of the influence of one independent variable on all other independent variables.

$$
\text{tolerance} = 1/\text{VIF}
$$

T < 0.2: there might be multicollinearity in the data\
T < 0.01: there certainly is multicollinarity in the data

##### Condition Index

The condition index is calculated using a factor analysis on the independent variables.
Values of 10-30 indicate a mediocre multicollinearity in the regression variables.
Values > 30 indicate strong multicollinearity.

For how to interpret, see here: https://stats.stackexchange.com/a/87610/20338 (archived at https://perma.cc/Y4J8-MY7Q)

```{r}
ols_eigen_cindex(multipleRegressionModel)
```

#### Ways to Handle

- Remove highly correlated (i.e., redundant) predictors
- Average the correlated predictors
- [Principal Component Analysis](pca.html) for data reduction
- Standardize predictors
- Center the data (deduct the mean)
- Singular-value decomposition of the model matrix or the mean-centered model matrix
- Conduct a [factor analysis](#factorAnalysis.html) and rotate the factors to ensure independence of the factors

### 6. Errors are normally distributed

#### Ways to Test

- Probability Plots
    - [Normal Quantile (QQ) Plots](#qqPlot) (based on non-cumulative distribution of residuals)
    - [Normal Probability (PP) Plots](#ppPlot) (based on cumulative distribution of residuals)
- [Density Plot of Residuals](#densityPlotResiduals)
- Statistical Tests
    - Kolmogorov-Smirnov test
    - Shapiro-Wilk test
    - Jarque-Bera test
    - Anderson-Darling test (best test)
- Examine influence of outliers

#### Ways to Handle

- Apply a transformation to the predictor or outcome variable
- Exclude outliers
- Robust regression
    - Best when no outliers: MM-type regression estimator
        - `lmrob()`/`glmrob()` function of `robustbase` package
        - Iteratively reweighted least squares (IRLS): `rlm(, method = "MM")` function of `MASS` package: http://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)
    - Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers
        - `ltsReg()` function of `robustbase` package (best)
    - Best when single predictor: Theil-Sen estimator
        - `mblm(, repeated = FALSE)` function of `mblm` package
    - Robust correlation
        - Spearman's rho: `cor(, method = "spearman")`
        - Percentage bend correlation
        - Minimum vollume ellipsoid
        - Minimum covariance determinant:
        - Winsorized correlation
        - Biweight midcorrelation
    - Not great options:
        - Quantile (L1) regression: `rq()` function of `quantreg` package

##### Transformations of Outcome Variable

###### Box-Cox Transformation

Useful if the outcome is strictly positive (or add a constant to outcome to make it strictly positive)

lambda = -1: inverse transformation\
lambda = -0.5: 1/sqrt(Y)\
lambda = 0: log transformation\
lambda = 0.5: square root\
lambda = 0.333: cube root\
lambda = 1: no transformation\
lambda = 2: squared\

####### Raw distribution

```{r}
plot(density(na.omit(mydata$bpi_antisocialT2Sum)))
```

####### Add constant to outcome to make it strictly positive

```{r}
strictlyPositiveDV <- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude)
```

####### Identify best power transformation (lambda)

Consider rounding the power to a common value (square root = .5; cube root = .333; squared = 2)

```{r}
boxCox(strictlyPositiveDV)
powerTransform(strictlyPositiveDV) 
transformedDV <- powerTransform(strictlyPositiveDV)

summary(transformedDV)
```

####### Transform the DV

```{r}
mydata$bpi_antisocialT2SumTransformed <- bcPower(mydata$bpi_antisocialT2Sum + 1, coef(transformedDV))

plot(density(na.omit(mydata$bpi_antisocialT2SumTransformed)))
```

####### Compare residuals from model with and without transformation

######## Model without transformation

```{r}
summary(modelWithoutTransformation <- lm(bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                         data = mydata,
                         na.action = na.exclude))
plot(density(na.omit(studres(modelWithoutTransformation))))
```

######## Model with transformation

```{r}
summary(modelWithTransformation <- lm(bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                      data = mydata,
                                      na.action = na.exclude))

plot(density(na.omit(studres(modelWithTransformation))))
```

####### Constructed Variable Test & Plot

A significant *p*-value indicates a strong need to transform variable:

```{r}
multipleRegressionModel_constructedVariable <- update(multipleRegressionModel, . ~ . + boxCoxVariable(bpi_antisocialT1Sum + 1))
summary(multipleRegressionModel_constructedVariable)$coef["boxCoxVariable(bpi_antisocialT1Sum + 1)", , drop = FALSE]
```

Plot allows us to see whether the need for transformation is spread through data or whether it is just dependent on a small fraction of observations:

```{r}
avPlots(multipleRegressionModel_constructedVariable, "boxCoxVariable(bpi_antisocialT1Sum + 1)")
```

####### Inverse Response Plot

The black line is the best-fitting power transformation:

```{r}
inverseResponsePlot(strictlyPositiveDV, lambda = c(-1,-0.5,0,1/3,.5,1,2))
```

###### Yeo-Johnson Transformations

Useful if the outcome is not strictly positive.

```{r, eval = FALSE}
yjPower(DV, lambda)
```

##### Transformations of Predictor Variable

###### Component-Plus-Residual Plots (Partial Residual Plots)

Linear model:

```{r}
crPlots(multipleRegressionModelNoMissing, order = 1)
```

Quadratic model:

```{r}
crPlots(multipleRegressionModelNoMissing, order = 2)
```


```{r}
multipleRegressionModel_quadratic <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + bpi_anxiousDepressedSum,
                                        data = mydata,
                                        na.action = na.exclude)

summary(multipleRegressionModel_quadratic)
anova(multipleRegressionModel_quadratic, multipleRegressionModel)
crPlots(multipleRegressionModel_quadratic, order = 1)
```

###### CERES Plot (Combining conditional Expectations and RESiduals)

Useful when nonlinear associations among the predictors are very strong (component-plus-residual plots may appear nonlinear even though the true partial regression is linear, a phenonomen called leakage)

```{r}
ceresPlots(multipleRegressionModel)
ceresPlots(multipleRegressionModel_quadratic)
```

###### Box-Tidwell Method for Choosing Predictor Transformations

predictors must be strictly positive (or add a constant to make it strictly positive)

```{r}
boxTidwell(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1),
           other.x = NULL, #list variables not to be transformed in other.x
           data = mydata,
           na.action = na.exclude)
```

###### Constructed-Variables Plot

```{r}
multipleRegressionModel_cv <- lm(bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1) + I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) + I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)),
                              data = mydata,
                              na.action = na.exclude)

summary(multipleRegressionModel_cv)$coef["I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))", , drop = FALSE]
summary(multipleRegressionModel_cv)$coef["I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))", , drop = FALSE]

avPlots(multipleRegressionModel_cv, "I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))")
avPlots(multipleRegressionModel_cv, "I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))")
```

##### Robust models

Resources

- For comparison of methods, see Book: "Modern Methods for Robust Regression"
- http://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)
- http://stats.stackexchange.com/a/46234/20338 (archived at https://perma.cc/WC57-9GA7)
- http://cran.r-project.org/web/views/Robust.html (archived at https://perma.cc/THN5-KNY3)

###### Robust correlation

####### Spearman's rho

Spearman's rho is a non-parametric correlation.

```{r}
cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum) #Pearson r, correlation that is sensitive to outliers
cor.test(mydata$bpi_antisocialT1Sum, mydata$bpi_antisocialT2Sum, method = "spearman") #Spearman's rho, a rank correlation that is less sensitive to outliers
```

####### Minimum vollume ellipsoid

```{r}
cov.mve(na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")]), cor = TRUE)
```

####### Minimum covariance determinant

```{r}
cov.mcd(na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")]), cor = TRUE)
```

####### Winsorized correlation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], winsorize = 0.2, p_adjust = "none")
```

####### Percentage bend correlation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "percentage", p_adjust = "none")
```

####### Biweight midcorrelation

```{r}
correlation(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")], method = "biweight", p_adjust = "none")
```

###### Robust regression with a single predictor

####### Theil-Sen estimator

The Theil-Sen single median estimator is robust to outliers; have to remove missing values first

```{r}
mydata_subset <- na.omit(mydata[,c("bpi_antisocialT1Sum","bpi_antisocialT2Sum")])[1:400,]
mblm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, repeated = FALSE)
```

###### Robust multiple regression

Best when no outliers: MM-type regression estimator

####### Robust linear regression

MM-type regression estimator (best):

```{r}
lmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
   data = mydata,
   na.action = na.exclude)
```

Iteratively reweighted least squares (IRLS): http://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)

```{r}
rlm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    na.action = na.exclude)
```

####### Robust generalized regression

```{r, warning = FALSE}
glmrob(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
    data = mydata,
    family = "poisson",
    na.action = "na.exclude")
```

Most resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers

```{r}
ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
       data = mydata,
       na.action = na.exclude)
```

######## Not great options:

Quantile (L1) regression: `rq()` function of quantreg package

https://data.library.virginia.edu/getting-started-with-quantile-regression/ (archived at https://perma.cc/FSV4-5DCR)

```{r}
quantiles <- 1:9/10

quantileRegressionModel <- rq(
  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
  data = mydata,
  tau = quantiles,
  na.action = na.exclude)

quantileRegressionModel
summary(quantileRegressionModel)
```

Plot to examine the association between the predictor and the outcome at different levels (i.e., quantiles) of the predictor:

```{r}
ggplot(
  mydata,
  aes(bpi_antisocialT1Sum, bpi_antisocialT2Sum)) + 
  geom_point() + 
  geom_quantile(
    quantiles = quantiles,
    aes(color = factor(after_stat(quantile))))
```

Below is a plot that examines the difference in quantile coefficients (and associated confidence intervals) at different levels (i.e., quantiles) of the predictor.
The x-axis is the quantile of the predictor.
The y-axis is the slope coefficient.
Each black dot is the slope coefficient for the given quantile of the predictor.
The red lines are the least squares estimate of the slope coefficient and its confidence interval.

```{r}
plot(summary(quantileRegressionModel))

plot(
  summary(quantileRegressionModel),
  parm = "bpi_antisocialT1Sum")
```

## Examining Model Assumptions

### Distribution of Residuals

#### QQ plots {#qqPlot}

http://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html (archived at https://perma.cc/9UC3-3WRX)

```{r}
qqPlot(multipleRegressionModel, main = "QQ Plot", id = TRUE)

qqnorm(resid(multipleRegressionModel))
qqnorm(resid(rmsMultipleRegressionModel))
qqnorm(resid(robustLinearRegression))
qqnorm(resid(ltsRegression))

qqnorm(resid(generalizedRegressionModel))
qqnorm(resid(rmsGeneralizedRegressionModel))
qqnorm(resid(robustGeneralizedRegression))

qqnorm(resid(multilevelRegressionModel))
```

#### PP plots {#ppPlot}

```{r}
ppPlot(multipleRegressionModel)
ppPlot(rmsMultipleRegressionModel)
ppPlot(robustLinearRegression)
ppPlot(ltsRegression)

ppPlot(generalizedRegressionModel)
ppPlot(rmsGeneralizedRegressionModel)
ppPlot(robustGeneralizedRegression)
```

#### Density Plot of Residuals {#densityPlotResiduals}

```{r}
studentizedResiduals <- na.omit(rstudent(multipleRegressionModel))
plot(density(studentizedResiduals), col="red")
xfit <- seq(min(studentizedResiduals, na.rm=TRUE), max(studentizedResiduals, na.rm=TRUE), length=40)
lines(xfit, dnorm(xfit), col="gray") #compare to normal distribution
```

### Residual Plots {#residualPlots}

Residual plots are plots of the residuals versus observed/fitted values.

Includes plots of a) model residuals versus observed values on predictors and b) model residuals versus model fitted values.

Note: have to remove `na.action = na.exclude`

Tests include:

- lack-of-fit test for every numeric predictor, t-test for the regressor, added to the model, indicating no lack-of-fit for this type
- Tukey's test for nonadditivity: adding the squares of the fitted values to the model and refitting (evaluates adequacy of model fit)

```{r}
residualPlots(multipleRegressionModelNoMissing, id = TRUE)
```

### Marginal Model Plots {#marginalModelPlots}

Marginal model plots are plots of the outcome versus predictors/fitted values.

Includes plots of a) observed outcome values versus values on predictors (ignoring the other predictors) and b) observed outcome values versus model fitted values.

```{r}
marginalModelPlots(multipleRegressionModel, sd = TRUE, id = TRUE)
```

### Added-Variable Plots {#addedVariablePlots}

Added-variable plots are plots of the partial association between the outcome and each predictor, controlling for all other predictors.

Useful for identifying jointly influential observations and studying the impact of observations on the regression coefficients.

y-axis: residuals from model with all predictors excluding the predictor of interest

x-axis: residuals from model regressing predictor of interest on all other predictors

```{r}
avPlots(multipleRegressionModel, id = TRUE)
```

#### Refit model removing jointly influential observations

```{r}
multipleRegressionModel_removeJointlyInfluentialObs <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                                          data = mydata,
                                                          na.action = na.exclude,
                                                          subset = -c(6318,6023,4022,4040))

avPlots(multipleRegressionModel_removeJointlyInfluentialObs, id = TRUE)

compareCoefs(multipleRegressionModel, multipleRegressionModel_removeJointlyInfluentialObs)
```

### Outlier test

Locates the largest Studentized residuals in absolute value and computes the Bonferroni-corrected p-values based on a t-test for linear models.

Test of outlyingness, i.e., how likely one would have a residual of a given magnitude in a normal distribution with the same sample size

Note: it does not test how extreme an observation is relative to its distribution (i.e., leverage)

```{r}
outlierTest(multipleRegressionModel)
```

### Observations with high leverage

Identifies observations with high leverage (i.e., high hat values)

hat values are an index of leverage (observations that are far from the center of the regressor space and have greater influence on OLS regression coefficients)

```{r}
hist(hatvalues(multipleRegressionModel))
plot(hatvalues(multipleRegressionModel))

influenceIndexPlot(multipleRegressionModel, id = TRUE)

influencePlot(multipleRegressionModel, id = TRUE) # circle size is proportional to Cook's Distance

leveragePlots(multipleRegressionModel, id = TRUE)
```

### Observations with high influence (on OLS regression coefficients)

https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html (archived at https://perma.cc/T2TS-PZZ4)

```{r}
head(influence.measures(multipleRegressionModel)$infmat)
```

### DFBETA {#dfbeta}

```{r}
head(dfbeta(multipleRegressionModel))

hist(dfbeta(multipleRegressionModel))
dfbetasPlots(multipleRegressionModel, id = TRUE)
```

### DFFITS {#dffits}

```{r}
head(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])

hist(dffits(multipleRegressionModel))
plot(dffits(multipleRegressionModel))
plot(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])
```

### Cook's Distance {#cooksDistance}

Observations that are both outlying (have a high residual from the regression line) and have high leverage (are far from the center of the regressor space) have high influence on the OLS regression coefficients.
An observation will have less influence if it lies on the regression line (not an outlier, i.e., has a low residual) or if it has low leverage (i.e., has a value near the center of a predictor's distribution).

```{r}
head(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])

hist(cooks.distance(multipleRegressionModel))
plot(cooks.distance(multipleRegressionModel))
plot(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])
```

#### Refit model removing values with high cook's distance

```{r}
multipleRegressionModel_2 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371))

multipleRegressionModel_3 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767))

multipleRegressionModel_4 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472))

multipleRegressionModel_5 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170))

multipleRegressionModel_6 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023))

multipleRegressionModel_7 <- lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
                                data = mydata,
                                na.action = na.exclude,
                                subset = -c(2765,1371,2767,8472,6170,6023,2766))
```

##### Examine how much regression coefficients change when excluding influential observations

```{r}
compareCoefs(multipleRegressionModel, multipleRegressionModel_2, multipleRegressionModel_3, multipleRegressionModel_4, multipleRegressionModel_5, multipleRegressionModel_6, multipleRegressionModel_7)
```

#### Examine how much regression coefficients change when using least trimmed squares (LTS) that is resistant to outliers

```{r}
coef(lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
        data = mydata,
        na.action="na.exclude"))

coef(ltsReg(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,
            data = mydata,
            na.action = "na.exclude"))
```

### Resources

Book: An R Companion to Applied Regression

http://people.duke.edu/~rnau/testing.htm (archived at https://perma.cc/9CXH-GBSN)

http://www.statmethods.net/stats/rdiagnostics.html (archived at https://perma.cc/7JX8-K6BY)

http://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html (archived at https://perma.cc/4A2P-9S49)

https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions (archived at https://perma.cc/Q4NS-X8TJ)

# Session Info

```{r, class.source = "fold-hide"}
sessionInfo()
```
</div>
<script type="text/javascript" src="includes/external-links.js"></script>

<br>
<hr>
<br>

<!-- Add lab logo -->
<p style="text-align: center;">
    <a href="https://developmental-psychopathology.lab.uiowa.edu" target="_blank">
        <img alt="Developmental Psychopathology Lab" src="images/formalLogo.png" width="60%">
    </a>
</p>

<br>
<hr>
<br>

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<!-- Add font awesome icons -->
<p style="text-align: center;">
    <a href="https://github.com/DevPsyLab" target="_blank" class="fa fa-github fa-fw fa-3x"></a>
    <a href="https://twitter.com/devpsylab" target="_blank" class="fa fa-twitter fa-fw fa-3x"></a>
    <a href="https://www.facebook.com/DevPsyLab" target="_blank" class="fa fa-facebook fa-fw fa-3x"></a>
	<a href="https://www.instagram.com/dev_psy_lab" target="_blank" class="fa fa-instagram fa-fw fa-3x"></a>
</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("regression.Rmd");
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
