[
  {
    "objectID": "SPSS.html",
    "href": "SPSS.html",
    "title": "SPSS",
    "section": "",
    "text": "We use R for almost all data analysis. However, R is code-based and has a steep learning curve. As a result, undergraduate students in the lab most often use SPSS or jamovi for their research projects (e.g., Honors projects and SROP projects), because SPSS and jamovi have a point-and-click interface.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#averageVars",
    "href": "SPSS.html#averageVars",
    "title": "SPSS",
    "section": "4.1 Average Across Variables",
    "text": "4.1 Average Across Variables\nCOMPUTE cbcl_externalizingPOM_Average = MEAN(cbcl_externalizingPOM_Father,cbcl_externalizingPOM_Mother,cbcl_externalizingPOM_Secondary).\nCOMPUTE cbq_inhibitoryControl_Average = MEAN(cbq_inhibitoryControl_Father,cbq_inhibitoryControl_Mother,cbq_inhibitoryControl_Secondary).\nEXECUTE.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#renameVars",
    "href": "SPSS.html#renameVars",
    "title": "SPSS",
    "section": "4.2 Rename Variables",
    "text": "4.2 Rename Variables\nRENAME VARIABLES (ses_hollingsheadSES = SES).",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#meanCenter",
    "href": "SPSS.html#meanCenter",
    "title": "SPSS",
    "section": "4.3 Mean-Center Variables",
    "text": "4.3 Mean-Center Variables\nCOMPUTE ses_hollingsheadSES_mc = ses_hollingsheadSES - mean_ses.\nCOMPUTE adi_nationalRank_mc = adi_nationalRank - mean_adiN.\nEXECUTE.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#interaction",
    "href": "SPSS.html#interaction",
    "title": "SPSS",
    "section": "4.4 Compute Interaction Terms",
    "text": "4.4 Compute Interaction Terms\nCOMPUTE ses_adi_interaction = ses_hollingsheadSES_mc * adi_nationalRank_mc.\nEXECUTE.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#saveData",
    "href": "SPSS.html#saveData",
    "title": "SPSS",
    "section": "4.5 Save Data File",
    "text": "4.5 Save Data File\nSAVE OUTFILE='\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Members\\HAWKID\\Thesis\\Data\\srs_idWave.sav'\n  /COMPRESSED.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#frequencies",
    "href": "SPSS.html#frequencies",
    "title": "SPSS",
    "section": "5.1 Frequencies",
    "text": "5.1 Frequencies\nFREQUENCIES VARIABLES=tc_sex tc_hispanic tc_race tc_ethnicity \n  /ORDER=ANALYSIS.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#descriptives",
    "href": "SPSS.html#descriptives",
    "title": "SPSS",
    "section": "5.2 Descriptives",
    "text": "5.2 Descriptives\nDESCRIPTIVES VARIABLES=ICsim IClim IChg ICcbq ses_hollingsheadSES ADINat EXTBx \n  /STATISTICS=MEAN STDDEV MIN MAX KURTOSIS SKEWNESS.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#histograms",
    "href": "SPSS.html#histograms",
    "title": "SPSS",
    "section": "5.3 Histograms",
    "text": "5.3 Histograms\nGRAPH\n  /HISTOGRAM=ses_hollingsheadSES.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#correlations",
    "href": "SPSS.html#correlations",
    "title": "SPSS",
    "section": "5.4 Correlations",
    "text": "5.4 Correlations\nCORRELATIONS\n  /VARIABLES=tc_ageLV1 ses_hollingsheadParent cbq_inhibitoryControl_Average \n    lim_scoreExcludingPreferencePOM hg_scorePOM sim_goXnoGoPOM cbcl_externalizingPOM_Average\n  /PRINT=TWOTAIL NOSIG FULL\n  /MISSING=PAIRWISE.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#multipleRegression",
    "href": "SPSS.html#multipleRegression",
    "title": "SPSS",
    "section": "5.5 Multiple Regression",
    "text": "5.5 Multiple Regression\nREGRESSION\n/MISSING PAIRWISE\n/STATISTICS COEFF OUTS R ANOVA\n/CRITERIA=PIN(.05) POUT(.10)\n/NOORIGIN \n/DEPENDENT outcome\n/METHOD=ENTER predictor1 predictor2 predictor3.",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#mediation",
    "href": "SPSS.html#mediation",
    "title": "SPSS",
    "section": "5.6 Mediation",
    "text": "5.6 Mediation\nFirst, download and run the PROCESS macro.\nBelow is the syntax for a model without covariates—replace outcome with name of outcome variable, predictor with name of predictor variable, and mediator with name of mediating variable:\nPROCESS \n  y=outcome\n  /x=predictor\n  /m=mediator1 mediator2 mediator3\n  /model=4\n  /total=1\n  /effsize=1\n  /stand=1\n  /boot=10000\n  .\nModel with covariates (p. 126):\nPROCESS \n  y=outcome\n  /x=predictor\n  /m=mediator1 mediator2 mediator3\n  /cov=covariate1 covariate2 covariate3\n  /model=4\n  /total=1\n  /effsize=1\n  /stand=1\n  /boot=10000\n  .",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "SPSS.html#moderation",
    "href": "SPSS.html#moderation",
    "title": "SPSS",
    "section": "5.7 Moderation",
    "text": "5.7 Moderation\n\nCalculate means of the predictor and moderator\naggregate outfile * mode addvariables\n/predictor_mean = mean(predictor).\n\naggregate outfile * mode addvariables\n/moderator_mean = mean(moderator).\nMean-center the predictor and moderator\ncompute predictor_MC = predictor - predictor_mean.\ncompute moderator_MC = moderator - moderator_mean.\nexecute.\nVerify that predictor and moderator are mean-centered (i.e., have a mean of zero)\nDESCRIPTIVES VARIABLES=predictor moderator predictor_MC moderator_MC\n/STATISTICS=MEAN STDDEV MIN MAX.\nCreate the interaction terms\ncompute predictorXmoderator = predictor_MC * moderator_MC.\nexecute.\nRun moderated multiple regression model\nREGRESSION\n/MISSING PAIRWISE\n/STATISTICS COEFF OUTS R ANOVA\n/CRITERIA=PIN(.05) POUT(.10)\n/NOORIGIN \n/DEPENDENT outcome\n/METHOD=ENTER predictor_MC moderator_MC predictorXmoderator covariate1 covariate2.\nCreate plot of interaction\n\nsee McCabe et al. (2018) and following links:\n\nhttps://github.com/connorjmccabe/InterActive\nhttps://connorjmccabe.shinyapps.io/interactive/ (archived at https://perma.cc/93G6-ALDP)",
    "crumbs": [
      "About",
      "SPSS"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory data analysis is important for understanding your data, checking for data issues/errors, and checking assumptions for different statistical models.\nLOOK AT YOUR DATA—this is one of the most overlooked steps in data analysis!",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#install-libraries",
    "href": "eda.html#install-libraries",
    "title": "Exploratory Data Analysis",
    "section": "2.1 Install Libraries",
    "text": "2.1 Install Libraries\n\n\nCode\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#load-libraries",
    "href": "eda.html#load-libraries",
    "title": "Exploratory Data Analysis",
    "section": "2.2 Load Libraries",
    "text": "2.2 Load Libraries\n\n\nCode\nlibrary(\"petersenlab\")\nlibrary(\"car\")\nlibrary(\"vioplot\")\nlibrary(\"ellipse\")\nlibrary(\"nlme\")\nlibrary(\"effects\")\nlibrary(\"corrplot\")\nlibrary(\"ggplot2\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\nlibrary(\"purrr\")\nlibrary(\"naniar\")\nlibrary(\"mvnormtest\")\nlibrary(\"ggExtra\")\nlibrary(\"XICOR\")",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#sample",
    "href": "eda.html#sample",
    "title": "Exploratory Data Analysis",
    "section": "4.1 Sample",
    "text": "4.1 Sample\n\nCheck the sample size (N)\n\nIs the sample size in the data the expected sample size? Are there cases (participants) that are missing? Are there cases that should not be there?\nHere is the sample size:\n\n\n\n\nCode\nlength(unique(mydata$ID))\n\n\n[1] 100\n\n\n\nCheck the extent of missingness\n\nHow much data are missing in the model variables—including the predictor, outcome, and covariates?\nHere are the proportion of missing data in each variable:\n\n\n\n\nCode\nmap(mydata, ~mean(is.na(.))) %&gt;% t %&gt;% t\n\n\n                  [,1]\nID                0   \npredictor         0.01\noutcome           0.01\npredictorOverplot 0.01\noutcomeOverplot   0.01\ncategorical1      0.03\ncategorical2      0.07",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#distribution",
    "href": "eda.html#distribution",
    "title": "Exploratory Data Analysis",
    "section": "4.2 Distribution",
    "text": "4.2 Distribution\n\nFrequencies\n\nExamine the frequencies of categorical variables:\n\n\n\n\nCode\nmydata %&gt;% \n  select(categorical1, categorical2) %&gt;%\n  sapply(function(x) table(x, useNA = \"always\")) %&gt;% \n  t()\n\n\n               1   2   3   4   5 &lt;NA&gt;\ncategorical1 196 204 191 199 180   30\ncategorical2 195 179 193 202 161   70",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#central-tendency",
    "href": "eda.html#central-tendency",
    "title": "Exploratory Data Analysis",
    "section": "4.3 Central Tendency",
    "text": "4.3 Central Tendency\n\nMean\n\n\n\nCode\nround(colMeans(mydata, na.rm = TRUE), 2)\n\n\n               ID         predictor           outcome predictorOverplot \n            50.50             22.64             73.36             25.54 \n  outcomeOverplot      categorical1      categorical2 \n            63.53              2.96              2.95 \n\n\nCode\nround(apply(mydata, 2, function(x) mean(x, na.rm = TRUE)), 2)\n\n\n               ID         predictor           outcome predictorOverplot \n            50.50             22.64             73.36             25.54 \n  outcomeOverplot      categorical1      categorical2 \n            63.53              2.96              2.95 \n\n\nCode\nmydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(mean = ~ mean(., na.rm = TRUE)))) %&gt;% \n  round(., 2)\n\n\n\n  \n\n\n\n\nMedian\n\n\n\nCode\nround(apply(mydata, 2, function(x) median(x, na.rm = TRUE)), 2)\n\n\n               ID         predictor           outcome predictorOverplot \n            50.50             19.61             73.17             26.00 \n  outcomeOverplot      categorical1      categorical2 \n            63.00              3.00              3.00 \n\n\nCode\nmydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(median = ~ median(., na.rm = TRUE)))) %&gt;% \n  round(., 2)\n\n\n\n  \n\n\n\n\nMode\n\n\n\nCode\nround(apply(mydata, 2, function(x) Mode(x, multipleModes = \"mean\")), 2)\n\n\n               ID         predictor           outcome predictorOverplot \n            50.50             22.64             73.36             35.00 \n  outcomeOverplot      categorical1      categorical2 \n            63.33              2.00              4.00 \n\n\nCode\nmydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(mode = ~ Mode(., multipleModes = \"mean\")))) %&gt;% \n  round(., 2)\n\n\n\n  \n\n\n\nCompute all of these measures of central tendency:\n\n\nCode\nmydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(mean = ~ mean(., na.rm = TRUE),\n                               median = ~ median(., na.rm = TRUE),\n                               mode = ~ Mode(., multipleModes = \"mean\")),\n                   .names = \"{.col}.{.fn}\")) %&gt;% \n  round(., 2) %&gt;% \n  pivot_longer(cols = everything(),\n               names_to = c(\"variable\",\"index\"),\n               names_sep = \"\\\\.\") %&gt;% \n  pivot_wider(names_from = index,\n              values_from = value)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#dispersion",
    "href": "eda.html#dispersion",
    "title": "Exploratory Data Analysis",
    "section": "4.4 Dispersion",
    "text": "4.4 Dispersion\n\nStandard deviation\nObserved minimum and maximum (vis-à-vis possible minimum and maximum)\nSkewness\nKurtosis\n\nCompute all of these measures of dispersion:\n\n\nCode\nmydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(SD = ~ sd(., na.rm = TRUE),\n                               min = ~ min(., na.rm = TRUE),\n                               max = ~ max(., na.rm = TRUE),\n                               skewness = ~ skew(., na.rm = TRUE),\n                               kurtosis = ~ kurtosi(., na.rm = TRUE)),\n                   .names = \"{.col}.{.fn}\")) %&gt;% \n  round(., 2) %&gt;% \n  pivot_longer(cols = everything(),\n               names_to = c(\"variable\",\"index\"),\n               names_sep = \"\\\\.\") %&gt;% \n  pivot_wider(names_from = index,\n              values_from = value)\n\n\n\n  \n\n\n\nConsider transforming data if skewness &gt; |0.8| or if kurtosis &gt; |3.0|.",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#summaryStats",
    "href": "eda.html#summaryStats",
    "title": "Exploratory Data Analysis",
    "section": "4.5 Summary Statistics",
    "text": "4.5 Summary Statistics\nAdd summary statistics to the bottom of correlation matrices in papers:\n\n\nCode\ncor.table(mydata, type = \"manuscript\")\n\n\n\n  \n\n\n\nCode\nsummaryTable &lt;- mydata %&gt;% \n  summarise(across(everything(),\n                   .fns = list(n = ~ length(na.omit(.)),\n                               missingness = ~ mean(is.na(.)) * 100,\n                               M = ~ mean(., na.rm = TRUE),\n                               SD = ~ sd(., na.rm = TRUE),\n                               min = ~ min(., na.rm = TRUE),\n                               max = ~ max(., na.rm = TRUE),\n                               skewness = ~ skew(., na.rm = TRUE),\n                               kurtosis = ~ kurtosi(., na.rm = TRUE)),\n                   .names = \"{.col}.{.fn}\")) %&gt;%  \n  pivot_longer(cols = everything(),\n               names_to = c(\"variable\",\"index\"),\n               names_sep = \"\\\\.\") %&gt;% \n  pivot_wider(names_from = index,\n              values_from = value)\n\nsummaryTableTransposed &lt;- summaryTable[-1] %&gt;% \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  setNames(summaryTable$variable) %&gt;% \n  round(., digits = 2)\n\nsummaryTableTransposed",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#distribution-plots",
    "href": "eda.html#distribution-plots",
    "title": "Exploratory Data Analysis",
    "section": "4.6 Distribution Plots",
    "text": "4.6 Distribution Plots\nSee here for resources for creating figures in R.\n\n4.6.1 Histogram\n\n4.6.1.1 Base R\n\n\nCode\nhist(mydata$outcome)\n\n\n\n\n\n\n\n\n\n\n\n4.6.1.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = outcome)) +\n  geom_histogram(color = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 Histogram overlaid with density plot and rug plot\n\n4.6.2.1 Base R\n\n\nCode\nhist(mydata$outcome, prob = TRUE)\nlines(density(mydata$outcome, na.rm = TRUE))\nrug(mydata$outcome)\n\n\n\n\n\n\n\n\n\n\n\n4.6.2.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = outcome)) +\n  geom_histogram(aes(y = after_stat(density)), color = 1) +\n  geom_density() +\n  geom_rug()\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.3 Density Plot\n\n4.6.3.1 Base R\n\n\nCode\nplot(density(mydata$outcome, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n4.6.3.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = outcome)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.4 Box and whisker plot (boxplot)\n\n4.6.4.1 Base R\n\n\nCode\nboxplot(mydata$outcome, horizontal = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n4.6.4.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = outcome)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.5 Violin plot\n\n4.6.5.1 Base R\n\n\nCode\nvioplot(na.omit(mydata$outcome), horizontal = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n4.6.5.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = \"\", y = outcome)) +\n  geom_violin()",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#correlation-coefficients",
    "href": "eda.html#correlation-coefficients",
    "title": "Exploratory Data Analysis",
    "section": "5.1 Correlation Coefficients",
    "text": "5.1 Correlation Coefficients\n\n5.1.1 Pearson Correlation\n\n\nCode\ncor(mydata, use = \"pairwise.complete.obs\")\n\n\n                            ID    predictor     outcome predictorOverplot\nID                 1.000000000 -0.019094589 -0.02501796        0.02984671\npredictor         -0.019094589  1.000000000  0.63517610        0.04350431\noutcome           -0.025017962  0.635176098  1.00000000        0.02976820\npredictorOverplot  0.029846711  0.043504314  0.02976820        1.00000000\noutcomeOverplot    0.003150872  0.043344473  0.06289224        0.56564445\ncategorical1       0.040442534 -0.002422711 -0.01532232        0.02498938\ncategorical2       0.008920415 -0.046927177 -0.01868628        0.03880135\n                  outcomeOverplot categorical1 categorical2\nID                    0.003150872  0.040442534  0.008920415\npredictor             0.043344473 -0.002422711 -0.046927177\noutcome               0.062892238 -0.015322318 -0.018686282\npredictorOverplot     0.565644447  0.024989385  0.038801346\noutcomeOverplot       1.000000000  0.011871644  0.043682632\ncategorical1          0.011871644  1.000000000 -0.086310192\ncategorical2          0.043682632 -0.086310192  1.000000000\n\n\nCode\ncor.test( ~ predictor + outcome, data = mydata)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  predictor and outcome\nt = 25.718, df = 978, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5962709 0.6711047\nsample estimates:\n      cor \n0.6351761 \n\n\nCode\ncor.table(mydata)\n\n\n\n  \n\n\n\n\n\n5.1.2 Spearman Correlation\n\n\nCode\ncor(mydata, use = \"pairwise.complete.obs\", method = \"spearman\")\n\n\n                            ID   predictor     outcome predictorOverplot\nID                 1.000000000 -0.01181085 -0.02805450        0.02925155\npredictor         -0.011810853  1.00000000  0.60003760        0.02672008\noutcome           -0.028054497  0.60003760  1.00000000        0.02460831\npredictorOverplot  0.029251546  0.02672008  0.02460831        1.00000000\noutcomeOverplot    0.001416626  0.03278463  0.07073642        0.54841297\ncategorical1       0.040888369 -0.01162260 -0.01906631        0.02350886\ncategorical2       0.009137467 -0.03763251 -0.02273699        0.04062487\n                  outcomeOverplot categorical1 categorical2\nID                    0.001416626   0.04088837  0.009137467\npredictor             0.032784628  -0.01162260 -0.037632515\noutcome               0.070736421  -0.01906631 -0.022736992\npredictorOverplot     0.548412967   0.02350886  0.040624873\noutcomeOverplot       1.000000000   0.01505255  0.044184959\ncategorical1          0.015052553   1.00000000 -0.086290083\ncategorical2          0.044184959  -0.08629008  1.000000000\n\n\nCode\ncor.test( ~ predictor + outcome, data = mydata, method = \"spearman\")\n\n\n\n    Spearman's rank correlation rho\n\ndata:  predictor and outcome\nS = 62740170, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6000376 \n\n\nCode\ncor.table(mydata, correlation = \"spearman\")\n\n\n\n  \n\n\n\n\n\n5.1.3 Xi (\\(\\xi\\))\nXi (\\(\\xi\\)) is an index of the degree of dependence between two variables, which is useful as an index of nonlinear correlation.\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022. https://doi.org/10.1080/01621459.2020.1758115\n\n\nCode\ncalculateXI(\n  mydata$predictor,\n  mydata$outcome)\n\n\n[1] 0.2319062",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#scatterplot",
    "href": "eda.html#scatterplot",
    "title": "Exploratory Data Analysis",
    "section": "5.2 Scatterplot",
    "text": "5.2 Scatterplot\n\n5.2.1 Base R\n\n\nCode\nplot(\n  mydata$predictor,\n  mydata$outcome)\nabline(lm(\n  outcomeOverplot ~ predictorOverplot,\n  data = mydata,\n  na.action = \"na.exclude\"))\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 ggplot2\n\n\nCode\nggplot(mydata, aes(x = predictor, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#scatterplot-with-marginal-density-plot",
    "href": "eda.html#scatterplot-with-marginal-density-plot",
    "title": "Exploratory Data Analysis",
    "section": "5.3 Scatterplot with Marginal Density Plot",
    "text": "5.3 Scatterplot with Marginal Density Plot\n\n\nCode\nscatterplot &lt;- \n  ggplot(mydata, aes(x = predictor, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\nCode\ndensityMarginal &lt;- ggMarginal(\n  scatterplot,\n  type = \"density\",\n  xparams = list(fill = \"gray\"),\n  yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(densityMarginal, newpage = TRUE)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#high-density-scatterplot",
    "href": "eda.html#high-density-scatterplot",
    "title": "Exploratory Data Analysis",
    "section": "5.4 High Density Scatterplot",
    "text": "5.4 High Density Scatterplot\n\n\nCode\nggplot(mydata, aes(x = predictorOverplot, y = outcomeOverplot)) + \n  geom_point(position = \"jitter\", alpha = 0.3) + \n  geom_density2d()\n\n\n\n\n\n\n\n\n\nCode\nsmoothScatter(mydata$predictorOverplot, mydata$outcomeOverplot)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#data-ellipse",
    "href": "eda.html#data-ellipse",
    "title": "Exploratory Data Analysis",
    "section": "5.5 Data Ellipse",
    "text": "5.5 Data Ellipse\n\n\nCode\nmydata_nomissing &lt;- na.omit(mydata[,c(\"predictor\",\"outcome\")])\ndataEllipse(mydata_nomissing$predictor, mydata_nomissing$outcome, levels = c(0.5, .95))",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#visually-weighted-regression",
    "href": "eda.html#visually-weighted-regression",
    "title": "Exploratory Data Analysis",
    "section": "5.6 Visually Weighted Regression",
    "text": "5.6 Visually Weighted Regression\n\n\nCode\nvwReg(outcome ~ predictor, data = mydata)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#tests-of-normality",
    "href": "eda.html#tests-of-normality",
    "title": "Exploratory Data Analysis",
    "section": "6.1 Tests of Normality",
    "text": "6.1 Tests of Normality\n\n6.1.1 Shapiro-Wilk test of normality\nThe Shapiro-Wilk test of normality does not accept more than 5000 cases because it will reject the hypothesis that data come from a normal distribution with even slight deviations from normality.\n\n\nCode\nshapiro.test(na.omit(mydata$outcome)) #subset to keep only the first 5000 rows: mydata$outcome[1:5000]\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  na.omit(mydata$outcome)\nW = 0.9971, p-value = 0.06998\n\n\n\n\n6.1.2 Test of multivariate normality\n\n\nCode\nmydata %&gt;% \n  na.omit %&gt;% \n  t %&gt;% \n  mshapiro.test\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Z\nW = 0.98542, p-value = 1.339e-07",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#statistical-decision-tree",
    "href": "eda.html#statistical-decision-tree",
    "title": "Exploratory Data Analysis",
    "section": "6.2 Statistical decision tree",
    "text": "6.2 Statistical decision tree\nhttps://upload.wikimedia.org/wikipedia/commons/7/74/InferentialStatisticalDecisionMakingTrees.pdf (archived at https://perma.cc/L2QR-ALFA)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#tests-of-systematic-missingness-i.e.-whether-missingness-on-a-variable-depends-on-other-variables",
    "href": "eda.html#tests-of-systematic-missingness-i.e.-whether-missingness-on-a-variable-depends-on-other-variables",
    "title": "Exploratory Data Analysis",
    "section": "6.3 Tests of systematic missingness (i.e., whether missingness on a variable depends on other variables)",
    "text": "6.3 Tests of systematic missingness (i.e., whether missingness on a variable depends on other variables)\n\nGenerally test:\n\nWhether data are consistent with a missing completely at random (MCAR) pattern—Little’s MCAR Test\nWhether outcome variable(s) differ as a function of any model variables (predictors and covariates) and as a function of any key demographic characteristics (e.g., sex, ethnicity, socioeconomic status)\nWhether focal predictor variable(s) differ as a function of any model variables (including outcome variable) and as a function of any key demographic characteristics\n\nFor instance:\n\nWhether males are more likely than girls to be missing scores on the dependent variable\nWhether longitudinal attrition is greater in lower socioeconomic status families\n\nIf missingness differs systematically as a function of other variables, you can include that variable as a control variable in models, and/or can include that variable in multiple imputation to inform imputed scores for missing values\n\n\n6.3.1 Little’s MCAR Test\n\n\nCode\nmcar_test(mydata)",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#multivariate-associations",
    "href": "eda.html#multivariate-associations",
    "title": "Exploratory Data Analysis",
    "section": "6.4 Multivariate Associations",
    "text": "6.4 Multivariate Associations\n\n6.4.1 Correlation Matrix\n\n6.4.1.1 Pearson Correlations\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")])\n\n\n\n  \n\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], type = \"manuscript\")\n\n\n\n  \n\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], type = \"manuscriptBig\")\n\n\n\n  \n\n\n\n\n\n6.4.1.2 Spearman Correlations\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], correlation = \"spearman\")\n\n\n\n  \n\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], type = \"manuscript\", correlation = \"spearman\")\n\n\n\n  \n\n\n\nCode\ncor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], type = \"manuscriptBig\", correlation = \"spearman\")\n\n\n\n  \n\n\n\n\n\n6.4.1.3 Partial Correlations\nExamine the associations among variables controlling for a covariate (outcomeOverplot).\n\n\nCode\npartialcor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\")], z = mydata[,c(\"outcomeOverplot\")])\n\n\n\n  \n\n\n\nCode\npartialcor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\")], z = mydata[,c(\"outcomeOverplot\")], type = \"manuscript\")\n\n\n\n  \n\n\n\nCode\npartialcor.table(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\")], z = mydata[,c(\"outcomeOverplot\")], type = \"manuscriptBig\")\n\n\n\n  \n\n\n\n\n\n\n6.4.2 Correlogram\n\n\nCode\ncorrplot(cor(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")], use = \"pairwise.complete.obs\"))\n\n\n\n\n\n\n\n\n\n\n\n6.4.3 Scatterplot matrix\n\n\nCode\nscatterplotMatrix(~ predictor + outcome + predictorOverplot + outcomeOverplot, data = mydata, use = \"pairwise.complete.obs\")\n\n\n\n\n\n\n\n\n\n\n\n6.4.4 Pairs panels\n\n\nCode\npairs.panels(mydata[,c(\"predictor\",\"outcome\",\"predictorOverplot\",\"outcomeOverplot\")])",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "eda.html#effect-plots",
    "href": "eda.html#effect-plots",
    "title": "Exploratory Data Analysis",
    "section": "6.5 Effect Plots",
    "text": "6.5 Effect Plots\n\n6.5.1 Multiple Regression Model\n\n\nCode\nmultipleRegressionModel &lt;- lm(outcome ~ predictor + predictorOverplot,\n                              data = mydata,\n                              na.action = \"na.exclude\")\n\nallEffects(multipleRegressionModel)\n\n\n model: outcome ~ predictor + predictorOverplot\n\n predictor effect\npredictor\n      0.1        20        40        60        80 \n 49.23335  70.37778  91.62847 112.87915 134.12983 \n\n predictorOverplot effect\npredictorOverplot\n       1       10       30       40       50 \n73.13937 73.15070 73.17586 73.18845 73.20103 \n\n\nCode\nplot(allEffects(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\n\n\n6.5.2 Multilevel Regression Model\n\n\nCode\nmultilevelRegressionModel &lt;- lme(outcome ~ predictor + predictorOverplot, random = ~ 1|ID,\n                                 method = \"ML\",\n                                 data = mydata,\n                                 na.action = \"na.exclude\")\n\nallEffects(multilevelRegressionModel)\n\n\n model: outcome ~ predictor + predictorOverplot\n\n predictor effect\npredictor\n      0.1        20        40        60        80 \n 49.23335  70.37778  91.62847 112.87915 134.12983 \n\n predictorOverplot effect\npredictorOverplot\n       1       10       30       40       50 \n73.13937 73.15070 73.17586 73.18845 73.20103 \n\n\nCode\nplot(allEffects(multilevelRegressionModel))",
    "crumbs": [
      "About",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "Adapted from brms workshop by Paul-Christian Bürkner",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#install-libraries",
    "href": "bayesian.html#install-libraries",
    "title": "Bayesian Analysis",
    "section": "1.1 Install Libraries",
    "text": "1.1 Install Libraries\n\n\nCode\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#load-libraries",
    "href": "bayesian.html#load-libraries",
    "title": "Bayesian Analysis",
    "section": "1.2 Load Libraries",
    "text": "1.2 Load Libraries\n\n\nCode\nlibrary(\"lme4\")\nlibrary(\"rstan\")\nlibrary(\"brms\")\nlibrary(\"bayestestR\")\nlibrary(\"mice\")",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#simulate-data",
    "href": "bayesian.html#simulate-data",
    "title": "Bayesian Analysis",
    "section": "1.3 Simulate Data",
    "text": "1.3 Simulate Data\n\n\nCode\nset.seed(52242)\n\nsampleSize &lt;- 1000\n\nid &lt;- rep(1:100, each = 10)\nX &lt;- rnorm(sampleSize)\nM &lt;- 0.5*X + rnorm(sampleSize)\nY &lt;- 0.7*M + rnorm(sampleSize)\n\nX[sample(1:length(X), size = 10)] &lt;- NA\nM[sample(1:length(M), size = 10)] &lt;- NA\nY[sample(1:length(Y), size = 10)] &lt;- NA\n\nmydata &lt;- data.frame(\n  id = id,\n  X = X,\n  Y = Y,\n  M = M)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#load-data",
    "href": "bayesian.html#load-data",
    "title": "Bayesian Analysis",
    "section": "1.4 Load Data",
    "text": "1.4 Load Data\n\n\nCode\ndata(\"sleepstudy\", package = \"lme4\")",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#prepare-data",
    "href": "bayesian.html#prepare-data",
    "title": "Bayesian Analysis",
    "section": "1.5 Prepare Data",
    "text": "1.5 Prepare Data\n\n\nCode\nconditions &lt;- make_conditions(sleepstudy, \"Subject\")",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#post-processing-methods",
    "href": "bayesian.html#post-processing-methods",
    "title": "Bayesian Analysis",
    "section": "2.1 Post-Processing Methods",
    "text": "2.1 Post-Processing Methods\n\n\nCode\nmethods(class = \"brmsfit\")\n\n\n  [1] add_criterion           add_ic                  as_draws_array         \n  [4] as_draws_df             as_draws_list           as_draws_matrix        \n  [7] as_draws_rvars          as_draws                as.array               \n [10] as.data.frame           as.matrix               as.mcmc                \n [13] autocor                 bayes_factor            bayes_R2               \n [16] bayesfactor_models      bayesfactor_parameters  bayesfactor_restricted \n [19] bci                     bridge_sampler          check_prior            \n [22] ci                      coef                    conditional_effects    \n [25] conditional_smooths     control_params          default_prior          \n [28] describe_posterior      describe_prior          diagnostic_draws       \n [31] diagnostic_posterior    effective_sample        equivalence_test       \n [34] estimate_density        eti                     expose_functions       \n [37] family                  fitted                  fixef                  \n [40] formula                 getCall                 hdi                    \n [43] hypothesis              inits                   kfold                  \n [46] log_lik                 log_posterior           logLik                 \n [49] loo_compare             loo_epred               loo_linpred            \n [52] loo_model_weights       loo_moment_match        loo_predict            \n [55] loo_predictive_interval loo_R2                  loo_subsample          \n [58] loo                     LOO                     map_estimate           \n [61] marginal_effects        marginal_smooths        mcmc_plot              \n [64] mcse                    mediation               model_to_priors        \n [67] model_weights           model.frame             nchains                \n [70] ndraws                  neff_ratio              ngrps                  \n [73] niterations             nobs                    nsamples               \n [76] nuts_params             nvariables              p_direction            \n [79] p_map                   p_rope                  p_significance         \n [82] pairs                   parnames                plot                   \n [85] point_estimate          post_prob               posterior_average      \n [88] posterior_epred         posterior_interval      posterior_linpred      \n [91] posterior_predict       posterior_samples       posterior_smooths      \n [94] posterior_summary       pp_average              pp_check               \n [97] pp_mixture              predict                 predictive_error       \n[100] predictive_interval     prepare_predictions     print                  \n[103] prior_draws             prior_summary           psis                   \n[106] ranef                   reloo                   residuals              \n[109] restructure             rhat                    rope                   \n[112] sexit_thresholds        si                      simulate_prior         \n[115] spi                     stancode                standata               \n[118] stanplot                summary                 unupdate               \n[121] update                  VarCorr                 variables              \n[124] vcov                    waic                    WAIC                   \n[127] weighted_posteriors    \nsee '?methods' for accessing help and source code",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#fit-the-models",
    "href": "bayesian.html#fit-the-models",
    "title": "Bayesian Analysis",
    "section": "3.1 Fit the Models",
    "text": "3.1 Fit the Models\n\n3.1.1 Complete Pooling\n\n\nCode\nfit_sleep1 &lt;- brm(\n  Reaction ~ 1 + Days,\n  data = sleepstudy,\n  seed  = 52242)\n\n\n\n\n3.1.2 Random Intercepts\n\n\nCode\nfit_sleep2 &lt;- brm(\n  Reaction ~ 1 + Days + (1 | Subject), \n  data = sleepstudy,\n  seed  = 52242)\n\n\n\n\n3.1.3 Random Intercepts and Slopes\n\n\nCode\nfit_sleep3 &lt;- brm(\n  Reaction ~ 1 + Days + (1 + Days | Subject), \n  data = sleepstudy,\n  seed  = 52242\n)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#summarize-results",
    "href": "bayesian.html#summarize-results",
    "title": "Bayesian Analysis",
    "section": "3.2 Summarize Results",
    "text": "3.2 Summarize Results\nFor convergence, Rhat values should not be above 1.00.\n\n\nCode\nsummary(fit_sleep3)\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: Reaction ~ 1 + Days + (1 + Days | Subject) \n   Data: sleepstudy (Number of observations: 180) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~Subject (Number of levels: 18) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)          26.76      6.70    15.60    41.96 1.00     1836     2290\nsd(Days)                6.58      1.55     4.17    10.23 1.00     1265     1938\ncor(Intercept,Days)     0.08      0.30    -0.49     0.66 1.00     1041     1736\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   251.38      7.39   236.80   266.10 1.00     1912     2086\nDays         10.43      1.76     7.03    13.97 1.00     1294     1865\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    25.96      1.55    23.13    29.23 1.00     3535     2807\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#model-priors",
    "href": "bayesian.html#model-priors",
    "title": "Bayesian Analysis",
    "section": "3.3 Model Priors",
    "text": "3.3 Model Priors\n\n\nCode\nprior_summary(fit_sleep3)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#model-parameters",
    "href": "bayesian.html#model-parameters",
    "title": "Bayesian Analysis",
    "section": "3.4 Model Parameters",
    "text": "3.4 Model Parameters\n\n\nCode\nvariables(fit_sleep3)\n\n\n [1] \"b_Intercept\"                  \"b_Days\"                      \n [3] \"sd_Subject__Intercept\"        \"sd_Subject__Days\"            \n [5] \"cor_Subject__Intercept__Days\" \"sigma\"                       \n [7] \"Intercept\"                    \"r_Subject[308,Intercept]\"    \n [9] \"r_Subject[309,Intercept]\"     \"r_Subject[310,Intercept]\"    \n[11] \"r_Subject[330,Intercept]\"     \"r_Subject[331,Intercept]\"    \n[13] \"r_Subject[332,Intercept]\"     \"r_Subject[333,Intercept]\"    \n[15] \"r_Subject[334,Intercept]\"     \"r_Subject[335,Intercept]\"    \n[17] \"r_Subject[337,Intercept]\"     \"r_Subject[349,Intercept]\"    \n[19] \"r_Subject[350,Intercept]\"     \"r_Subject[351,Intercept]\"    \n[21] \"r_Subject[352,Intercept]\"     \"r_Subject[369,Intercept]\"    \n[23] \"r_Subject[370,Intercept]\"     \"r_Subject[371,Intercept]\"    \n[25] \"r_Subject[372,Intercept]\"     \"r_Subject[308,Days]\"         \n[27] \"r_Subject[309,Days]\"          \"r_Subject[310,Days]\"         \n[29] \"r_Subject[330,Days]\"          \"r_Subject[331,Days]\"         \n[31] \"r_Subject[332,Days]\"          \"r_Subject[333,Days]\"         \n[33] \"r_Subject[334,Days]\"          \"r_Subject[335,Days]\"         \n[35] \"r_Subject[337,Days]\"          \"r_Subject[349,Days]\"         \n[37] \"r_Subject[350,Days]\"          \"r_Subject[351,Days]\"         \n[39] \"r_Subject[352,Days]\"          \"r_Subject[369,Days]\"         \n[41] \"r_Subject[370,Days]\"          \"r_Subject[371,Days]\"         \n[43] \"r_Subject[372,Days]\"          \"lprior\"                      \n[45] \"lp__\"",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#model-coefficients",
    "href": "bayesian.html#model-coefficients",
    "title": "Bayesian Analysis",
    "section": "3.5 Model Coefficients",
    "text": "3.5 Model Coefficients\n\n\nCode\ncoef(fit_sleep3)\n\n\n$Subject\n, , Intercept\n\n    Estimate Est.Error     Q2.5    Q97.5\n308 253.8036  13.42340 227.2528 279.4458\n309 211.5798  13.53907 184.5701 237.5476\n310 212.9776  13.44786 186.3989 238.6100\n330 274.5239  13.43628 248.1815 301.9403\n331 273.0398  13.23918 248.1694 299.4477\n332 260.2840  12.23798 236.3152 284.9809\n333 267.8093  12.62690 244.2342 293.6989\n334 244.3311  12.62822 219.1096 269.0711\n335 250.7611  13.16495 224.1951 276.3869\n337 286.1682  13.38914 259.9519 312.2581\n349 226.5570  12.82480 200.4294 251.4542\n350 238.6786  13.13686 212.3307 263.6997\n351 255.7790  12.53802 230.2905 280.3204\n352 272.2132  12.54778 247.9377 297.7906\n369 254.4293  12.50311 230.2014 278.3378\n370 226.7179  13.30644 200.0374 251.3793\n371 252.3647  12.07777 229.1029 275.8134\n372 263.5458  12.38755 239.3714 287.7161\n\n, , Days\n\n      Estimate Est.Error        Q2.5     Q97.5\n308 19.6319154  2.542400 14.91127989 24.921229\n309  1.7458810  2.561539 -3.16997162  6.728358\n310  4.9887767  2.536991  0.03313305 10.034349\n330  5.7537881  2.555568  0.62257977 10.503648\n331  7.5170261  2.469967  2.42232385 12.196072\n332 10.2271380  2.325028  5.69008473 14.750818\n333 10.2869556  2.337880  5.51782984 14.730549\n334 11.5339766  2.405073  6.85791261 16.290102\n335 -0.2252846  2.582433 -5.24670583  4.873086\n337 19.1100826  2.537751 14.24044379 24.060945\n349 11.5705306  2.403886  7.01645714 16.487190\n350 17.0331885  2.543700 12.24508630 22.034614\n351  7.4875668  2.395344  2.89592486 12.176265\n352 14.0213129  2.421787  9.20693205 18.780621\n369 11.3270352  2.379922  6.54155107 15.929727\n370 15.1243321  2.534860 10.24501333 20.291155\n371  9.4030963  2.306262  4.87040862 14.017797\n372 11.7617904  2.361364  7.07438198 16.475036",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#plots",
    "href": "bayesian.html#plots",
    "title": "Bayesian Analysis",
    "section": "3.6 Plots",
    "text": "3.6 Plots\n\n3.6.1 Trace Plots\n\n\nCode\nplot(fit_sleep3, ask = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.2 Visualize Predictions\n\n3.6.2.1 Sample-Level\n\n\nCode\nplot(conditional_effects(fit_sleep1), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n3.6.2.2 Person-Level\n\n\nCode\n# re_formula = NULL ensures that group-level effects are included\nce2 &lt;- conditional_effects(\n  fit_sleep3,\n  conditions = conditions,\n  re_formula = NULL)\n\nplot(ce2, ncol = 6, points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.3 Check Model Fit\n\n3.6.3.1 Posterior Predictive Check\nEvaluate how closely the posterior predictions match the observed values. If they do not match the general pattern of the observed values, a different response distribution may be necessary.\n\n\nCode\npp_check(fit_sleep3)\n\n\n\n\n\n\n\n\n\nCode\npp_check(fit_sleep3, type = \"dens_overlay\")\n\n\n\n\n\n\n\n\n\nCode\npp_check(fit_sleep3, \"error_scatter_avg\")",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#fitted-values",
    "href": "bayesian.html#fitted-values",
    "title": "Bayesian Analysis",
    "section": "3.7 Fitted Values",
    "text": "3.7 Fitted Values\n\n\nCode\nfitted(fit_sleep3)\n\n\n       Estimate Est.Error     Q2.5    Q97.5\n  [1,] 253.8036 13.423405 227.2528 279.4458\n  [2,] 273.4355 11.520427 251.1066 295.5886\n  [3,] 293.0674  9.908596 273.5416 312.1998\n  [4,] 312.6994  8.750308 295.6254 329.6635\n  [5,] 332.3313  8.239121 316.1146 348.5817\n  [6,] 351.9632  8.492698 335.3675 368.3632\n  [7,] 371.5951  9.449673 352.8905 389.8877\n  [8,] 391.2270 10.926771 369.8542 412.5090\n  [9,] 410.8589 12.744407 385.4375 435.8825\n [10,] 430.4909 14.777452 401.3990 459.9801\n [11,] 211.5798 13.539067 184.5701 237.5476\n [12,] 213.3257 11.594215 190.0978 235.9280\n [13,] 215.0716  9.933190 195.1221 234.5327\n [14,] 216.8174  8.719730 199.3095 234.0045\n [15,] 218.5633  8.156106 203.0775 234.6862\n [16,] 220.3092  8.374570 204.7937 236.6225\n [17,] 222.0551  9.320287 204.3521 240.2189\n [18,] 223.8010 10.803936 202.8558 244.8068\n [19,] 225.5468 12.637456 200.9486 249.8845\n [20,] 227.2927 14.690423 198.2139 255.6742\n [21,] 212.9776 13.447858 186.3989 238.6100\n [22,] 217.9664 11.512588 195.5189 240.0293\n [23,] 222.9552  9.854294 203.7593 241.8930\n [24,] 227.9439  8.634072 211.1470 244.6109\n [25,] 232.9327  8.053566 217.1679 248.8342\n [26,] 237.9215  8.248957 221.8949 254.3320\n [27,] 242.9103  9.170786 225.2566 260.7676\n [28,] 247.8990 10.631744 227.3514 268.4746\n [29,] 252.8878 12.443363 228.9194 276.8797\n [30,] 257.8766 14.474573 229.8574 285.8047\n [31,] 274.5239 13.436278 248.1815 301.9403\n [32,] 280.2777 11.472291 257.6589 303.5975\n [33,] 286.0315  9.785459 266.5387 305.5182\n [34,] 291.7853  8.541593 274.8651 308.8530\n [35,] 297.5391  7.951370 281.6089 313.1176\n [36,] 303.2929  8.157916 287.2294 319.3005\n [37,] 309.0467  9.107182 291.3656 327.0262\n [38,] 314.8004 10.601500 294.3065 335.9972\n [39,] 320.5542 12.446071 296.7421 345.2748\n [40,] 326.3080 14.507909 298.5109 355.0400\n [41,] 273.0398 13.239183 248.1694 299.4477\n [42,] 280.5569 11.353368 258.9471 302.8042\n [43,] 288.0739  9.732595 269.2880 307.5558\n [44,] 295.5909  8.529322 279.0331 312.7310\n [45,] 303.1080  7.935787 287.8983 318.7171\n [46,] 310.6250  8.087372 295.1102 326.6103\n [47,] 318.1420  8.946279 300.3617 335.9235\n [48,] 325.6590 10.337684 304.9808 346.4615\n [49,] 333.1761 12.078947 308.6069 357.2433\n [50,] 340.6931 14.040502 311.9999 368.6084\n [51,] 260.2840 12.237982 236.3152 284.9809\n [52,] 270.5112 10.596908 249.4092 291.4664\n [53,] 280.7383  9.253769 262.2199 298.9000\n [54,] 290.9654  8.353534 274.5185 306.8564\n [55,] 301.1926  8.046262 285.4025 316.5144\n [56,] 311.4197  8.397301 294.8811 327.6733\n [57,] 321.6468  9.332657 303.7138 339.8676\n [58,] 331.8740 10.700180 311.4909 352.7807\n [59,] 342.1011 12.357213 318.5939 365.9429\n [60,] 352.3283 14.202784 325.0814 379.7613\n [61,] 267.8093 12.626904 244.2342 293.6989\n [62,] 278.0962 10.899411 257.6470 300.7191\n [63,] 288.3832  9.438589 270.4678 307.7862\n [64,] 298.6701  8.384995 282.4267 315.2729\n [65,] 308.9571  7.903206 293.5251 324.2920\n [66,] 319.2440  8.095960 303.3288 334.8122\n [67,] 329.5310  8.919632 311.9167 346.4874\n [68,] 339.8179 10.222841 319.7585 358.8223\n [69,] 350.1049 11.848396 326.8274 372.1452\n [70,] 360.3919 13.681880 333.3820 386.2789\n [71,] 244.3311 12.628215 219.1096 269.0711\n [72,] 255.8651 10.843243 234.8299 277.5367\n [73,] 267.3991  9.340705 249.6269 286.1411\n [74,] 278.9331  8.275892 262.7474 294.9786\n [75,] 290.4670  7.829478 275.3135 305.1521\n [76,] 302.0010  8.104309 286.2364 317.3663\n [77,] 313.5350  9.034803 296.2280 330.8457\n [78,] 325.0690 10.447213 304.2386 345.2764\n [79,] 336.6030 12.174958 312.7848 360.3547\n [80,] 348.1369 14.102613 320.6016 376.0334\n [81,] 250.7611 13.164951 224.1951 276.3869\n [82,] 250.5358 11.184087 228.1236 272.3726\n [83,] 250.3105  9.496820 231.2769 269.0982\n [84,] 250.0852  8.284523 233.2194 266.2534\n [85,] 249.8599  7.772707 234.3995 264.8948\n [86,] 249.6346  8.095341 233.6417 265.8702\n [87,] 249.4093  9.164717 231.5820 267.5963\n [88,] 249.1841 10.760458 228.4438 270.7762\n [89,] 248.9588 12.685456 224.6143 274.6212\n [90,] 248.7335 14.811890 220.4098 278.3679\n [91,] 286.1682 13.389144 259.9519 312.2581\n [92,] 305.2783 11.475844 282.8305 327.6996\n [93,] 324.3884  9.848919 305.2016 343.8880\n [94,] 343.4985  8.671087 326.7256 360.5936\n [95,] 362.6085  8.139696 346.8772 378.4415\n [96,] 381.7186  8.378659 365.1972 397.5078\n [97,] 400.8287  9.328963 382.4937 418.4049\n [98,] 419.9388 10.804515 398.5223 440.7333\n [99,] 439.0489 12.622436 413.9451 463.7273\n[100,] 458.1590 14.655872 429.1228 486.4694\n[101,] 226.5570 12.824803 200.4294 251.4542\n[102,] 238.1276 11.032094 215.9740 259.2405\n[103,] 249.6981  9.512936 230.3887 267.9422\n[104,] 261.2686  8.416777 244.4129 277.6187\n[105,] 272.8392  7.921215 257.1578 288.3382\n[106,] 284.4097  8.136738 268.2892 300.1678\n[107,] 295.9802  9.012474 278.6256 313.8360\n[108,] 307.5508 10.382689 287.7080 327.8511\n[109,] 319.1213 12.080277 295.9433 342.9261\n[110,] 330.6918 13.986540 303.8621 358.5722\n[111,] 238.6786 13.136865 212.3307 263.6997\n[112,] 255.7118 11.212769 233.3976 276.9796\n[113,] 272.7450  9.582065 253.7956 291.4491\n[114,] 289.7782  8.417041 273.2937 306.1856\n[115,] 306.8113  7.925781 291.4421 322.7084\n[116,] 323.8445  8.229839 307.6099 340.5938\n[117,] 340.8777  9.251125 322.8653 359.5570\n[118,] 357.9109 10.787827 336.8251 379.3801\n[119,] 374.9441 12.653534 350.5318 399.9437\n[120,] 391.9773 14.723703 363.9737 420.5865\n[121,] 255.7790 12.538023 230.2905 280.3204\n[122,] 263.2665 10.819083 241.2003 284.4985\n[123,] 270.7541  9.400980 251.5079 289.1507\n[124,] 278.2417  8.436803 261.1905 294.5546\n[125,] 285.7292  8.090501 269.3469 301.2611\n[126,] 293.2168  8.438489 276.5729 309.7038\n[127,] 300.7044  9.404007 282.1637 319.0133\n[128,] 308.1919 10.823028 286.8555 329.7530\n[129,] 315.6795 12.542562 291.0335 340.4165\n[130,] 323.1671 14.455765 294.8664 351.5085\n[131,] 272.2132 12.547779 247.9377 297.7906\n[132,] 286.2346 10.786103 265.4433 307.7685\n[133,] 300.2559  9.325415 282.4138 318.7572\n[134,] 314.2772  8.325673 298.3476 330.6147\n[135,] 328.2985  7.962436 313.0277 344.1861\n[136,] 342.3198  8.319499 325.8859 358.6065\n[137,] 356.3411  9.314389 338.3377 374.7585\n[138,] 370.3624 10.771802 349.3693 391.7183\n[139,] 384.3838 12.531389 359.3285 409.3472\n[140,] 398.4051 14.483432 369.4604 426.8266\n[141,] 254.4293 12.503112 230.2014 278.3378\n[142,] 265.7563 10.773693 244.8015 286.6302\n[143,] 277.0834  9.335157 259.1086 295.2843\n[144,] 288.4104  8.339419 272.0860 304.6397\n[145,] 299.7374  7.954541 284.2035 314.8833\n[146,] 311.0645  8.266292 294.9191 327.2119\n[147,] 322.3915  9.204157 304.1501 340.6066\n[148,] 333.7185 10.603277 312.6269 354.5394\n[149,] 345.0456 12.307335 320.8454 369.4530\n[150,] 356.3726 14.207026 328.3684 384.8325\n[151,] 226.7179 13.306439 200.0374 251.3793\n[152,] 241.8423 11.394588 219.0042 263.3217\n[153,] 256.9666  9.770516 237.4683 275.6759\n[154,] 272.0909  8.598859 254.9541 288.7202\n[155,] 287.2153  8.078910 271.2685 302.8427\n[156,] 302.3396  8.333561 286.0381 318.3356\n[157,] 317.4639  9.299394 299.2168 335.7071\n[158,] 332.5883 10.787041 311.4962 354.1105\n[159,] 347.7126 12.613200 323.3014 372.7489\n[160,] 362.8369 14.651839 334.4412 391.6530\n[161,] 252.3647 12.077768 229.1029 275.8134\n[162,] 261.7678 10.423329 241.2570 281.8510\n[163,] 271.1709  9.058519 253.5544 288.7477\n[164,] 280.5740  8.130526 264.9742 296.7426\n[165,] 289.9771  7.796910 274.9358 305.3184\n[166,] 299.3802  8.131166 283.7186 315.9440\n[167,] 308.7833  9.059669 291.1535 326.7062\n[168,] 318.1864 10.424828 298.0406 338.9303\n[169,] 327.5895 12.079493 304.4257 351.3125\n[170,] 336.9925 13.920810 310.2639 364.4948\n[171,] 263.5458 12.387551 239.3714 287.7161\n[172,] 275.3076 10.657490 254.9976 296.0264\n[173,] 287.0694  9.212211 269.2020 305.0324\n[174,] 298.8312  8.203637 282.4993 314.8825\n[175,] 310.5930  7.802984 295.2651 325.9087\n[176,] 322.3548  8.100958 306.6305 338.2362\n[177,] 334.1166  9.028653 316.6219 351.7410\n[178,] 345.8784 10.419199 325.6658 366.2850\n[179,] 357.6402 12.114244 334.1252 380.9293\n[180,] 369.4019 14.003649 342.1829 396.2329",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#residuals",
    "href": "bayesian.html#residuals",
    "title": "Bayesian Analysis",
    "section": "3.8 Residuals",
    "text": "3.8 Residuals\n\n\nCode\nresiduals(fit_sleep3)\n\n\n           Estimate Est.Error        Q2.5      Q97.5\n  [1,]   -4.2244673  29.28706  -63.002265  53.128273\n  [2,]  -15.2258734  28.02991  -70.891336  39.747922\n  [3,]  -42.7137771  27.79352  -96.701875  12.326794\n  [4,]    9.0896556  27.38952  -45.543725  63.650947\n  [5,]   24.0863878  27.11413  -29.988776  77.641748\n  [6,]   62.8630668  27.10382    9.918941 115.616064\n  [7,]   10.7967209  28.18829  -43.506810  67.776789\n  [8,] -101.0242464  27.95897 -156.796686 -47.135290\n  [9,]   19.0289570  29.02182  -37.947663  75.968144\n [10,]   35.9679828  30.05818  -22.619714  95.283838\n [11,]   10.8079797  28.88379  -46.164272  65.434946\n [12,]   -7.5680787  29.01514  -64.114191  48.280094\n [13,]  -12.3264170  27.91637  -67.023547  42.299361\n [14,]  -12.0506382  28.02378  -68.333068  42.724023\n [15,]  -10.2508166  27.13701  -63.741910  41.896208\n [16,]   -4.1352820  27.20537  -58.260765  48.432132\n [17,]   -7.9827051  27.85208  -64.137881  44.781207\n [18,]   -6.9447045  27.78456  -60.836789  46.859561\n [19,]   -1.4023237  28.82651  -58.466019  54.085573\n [20,]    9.9275054  29.53783  -47.990517  68.778058\n [21,]  -13.7318990  28.66353  -68.839142  42.144727\n [22,]  -23.4081843  28.49820  -77.953568  32.985362\n [23,]   11.7845766  27.91344  -42.374344  67.332793\n [24,]    5.0480395  27.67880  -49.935871  59.923446\n [25,]   -3.3787437  27.21590  -56.685213  48.561840\n [26,]  -17.6586327  27.97261  -72.606340  37.791079\n [27,]   -7.7210286  27.36769  -60.081905  47.976011\n [28,]    7.2544990  28.09069  -47.343929  61.584794\n [29,]    7.9954282  28.75324  -49.895246  63.742358\n [30,]  -10.0755777  29.70183  -66.939758  48.042172\n [31,]   47.1350729  29.48763  -11.565006 104.661624\n [32,]   20.2182528  28.48794  -35.113909  75.782897\n [33,]   -1.8660173  27.46365  -55.344197  52.649728\n [34,]   -6.9236967  27.02505  -59.299748  47.098047\n [35,]  -12.2506201  26.89874  -64.312813  39.945039\n [36,]   -5.3127676  27.15819  -59.464006  47.637983\n [37,]  -28.4792129  27.63301  -82.248162  25.985264\n [38,]    3.6543684  27.83417  -50.353751  57.904114\n [39,]  -15.1484514  29.26645  -72.252116  41.788958\n [40,]   28.0658928  29.35092  -30.583225  84.633817\n [41,]   14.8717128  29.08505  -42.214918  70.484262\n [42,]    4.3242094  28.50086  -51.341599  61.153497\n [43,]   13.8504402  28.05522  -40.913331  68.398581\n [44,]   24.3362194  27.28591  -30.788101  78.914085\n [45,]   13.1889431  27.54427  -40.715730  67.631888\n [46,]  -17.7137627  27.80068  -72.569896  36.414923\n [47,]  -28.3472950  27.29476  -81.678244  25.608079\n [48,]    9.6170948  28.06588  -45.830459  65.058308\n [49,]  -39.4598317  28.66948  -94.986989  17.393661\n [50,]   30.9086055  29.55650  -26.572501  88.329012\n [51,]  -25.8076366  28.94518  -83.383272  30.483731\n [52,]  -27.9344955  28.28366  -83.500392  27.486197\n [53,]   -8.0502355  27.42836  -61.613145  45.863473\n [54,]   19.2579856  27.03447  -33.502671  70.986982\n [55,]   16.2909217  27.66439  -38.052394  69.143371\n [56,]   -1.3228837  27.23727  -55.037646  53.096391\n [57,]  132.6726978  27.93106   77.206137 187.357331\n [58,]   15.3854189  27.44064  -38.977512  68.151968\n [59,]  -11.1021018  28.82826  -68.121326  47.100381\n [60,]  -98.4821229  29.81761 -157.161360 -39.718893\n [61,]   15.9789884  29.07316  -42.057333  72.926161\n [62,]   11.1301601  27.74470  -41.371425  65.286299\n [63,]  -11.2582348  27.79820  -65.845164  42.115609\n [64,]    0.7878905  27.34122  -52.032957  55.202097\n [65,]  -11.2076660  27.25256  -64.511382  41.487374\n [66,]   19.3969010  27.21057  -34.020029  74.136133\n [67,]    2.7413390  27.37918  -51.426456  55.531692\n [68,]    8.2943051  27.90299  -45.690878  62.797717\n [69,]  -17.1992905  28.29974  -72.534877  38.919021\n [70,]    1.5071670  29.70570  -57.437486  59.401577\n [71,]   20.8939676  28.92899  -35.987943  77.979178\n [72,]   20.7384532  28.22225  -33.222491  77.164287\n [73,]  -23.7536273  27.31639  -79.600621  30.224124\n [74,]  -24.3197072  27.21669  -78.018696  29.064264\n [75,]  -10.9198895  26.94082  -64.727842  42.597232\n [76,]  -18.3519567  27.38319  -71.581832  36.963853\n [77,]   -7.0780084  27.72428  -62.094219  45.695043\n [78,]    6.1788854  27.86481  -47.264481  61.252139\n [79,]   -0.7823376  29.00939  -58.323713  56.949851\n [80,]   29.4033174  30.11908  -30.421139  91.266906\n [81,]   -8.2632425  29.26668  -66.610496  48.867494\n [82,]   23.8145000  27.58684  -30.942961  77.722409\n [83,]    4.8575626  27.66325  -48.351131  59.644099\n [84,]   20.3904477  27.07922  -32.030167  72.499148\n [85,]    1.7451560  26.68396  -50.377078  53.120733\n [86,]    5.0312298  27.48604  -49.511514  57.622505\n [87,]   -3.5468474  27.34797  -55.915356  51.679646\n [88,]  -14.2663646  28.29239  -67.847265  42.089247\n [89,]  -12.4371538  28.87968  -69.821446  43.482297\n [90,]  -11.8790136  29.81158  -71.208520  45.480586\n [91,]   26.0171585  29.20766  -30.038091  85.375120\n [92,]    7.9445807  28.47354  -47.037698  65.102815\n [93,]  -33.6100632  27.38287  -88.355109  19.776385\n [94,]    2.1611300  27.13725  -51.694605  56.458443\n [95,]    3.0344026  26.94435  -49.562906  56.141762\n [96,]   10.2042187  27.03990  -41.849247  62.926880\n [97,]    3.2323238  27.99663  -51.220682  58.432583\n [98,]   -3.1560171  28.43919  -57.916122  52.620312\n [99,]   16.8474105  29.12410  -40.608396  74.864152\n[100,]    1.0091687  30.05855  -58.433140  58.681566\n[101,]    9.7313939  29.08582  -47.555087  66.634698\n[102,]   -8.1613084  27.98578  -62.902932  46.795870\n[103,]  -11.0084064  27.63072  -65.334627  43.825662\n[104,]   -6.3686903  27.45658  -60.350031  48.049654\n[105,]  -21.9359022  27.54636  -76.421955  33.382814\n[106,]  -14.3629928  27.16031  -65.278820  38.392060\n[107,]  -14.5449835  27.69595  -68.122893  40.408575\n[108,]    0.6573357  28.24338  -55.091787  56.585500\n[109,]   17.1863718  28.80868  -38.919359  71.919224\n[110,]   20.5621092  29.24914  -37.219896  78.646516\n[111,]   17.9965429  29.47544  -41.464296  74.103829\n[112,]  -11.7668505  28.50034  -68.575339  42.664311\n[113,]  -16.4576884  28.04687  -72.912191  37.544355\n[114,]  -34.5885129  27.58706  -88.300127  18.468037\n[115,]  -37.7808870  27.24016  -91.964538  15.866419\n[116,]    5.8793799  27.11760  -47.287881  59.747935\n[117,]   38.5737262  27.15999  -15.540225  91.749533\n[118,]    5.5372407  28.20525  -50.153684  60.059953\n[119,]   19.6188928  28.44534  -37.454506  76.496327\n[120,]   -3.3657784  30.05201  -63.089191  54.815028\n[121,]   -5.7588610  28.26386  -60.117322  47.793813\n[122,]   36.3992500  28.41437  -20.941171  93.045153\n[123,]   -0.7592382  27.57961  -53.859534  52.501323\n[124,]    2.6689136  27.10773  -49.976338  57.604008\n[125,]  -13.7773247  27.10203  -67.322177  39.475178\n[126,]   11.2033790  27.21120  -41.010565  65.306322\n[127,]  -13.1617072  27.39834  -67.210756  40.749569\n[128,]  -41.7794969  27.96592  -95.664816  13.047176\n[129,]    6.1565976  28.43068  -50.436851  63.030017\n[130,]   24.2307695  29.33433  -34.562678  81.750961\n[131,]  -50.5287655  28.75460 -107.766298   5.502855\n[132,]   11.8878467  28.00367  -41.800030  67.161674\n[133,]   26.3445173  27.88222  -28.062456  81.011281\n[134,]   32.2314300  27.63113  -21.515342  85.469826\n[135,]   19.8834068  27.58353  -34.795368  74.287228\n[136,]   10.3634070  28.00825  -43.891951  64.427959\n[137,]   -2.1253800  27.61106  -56.317242  52.471969\n[138,]   -9.9925668  28.56169  -65.409750  47.065316\n[139,]   -8.9106062  28.45719  -65.225630  46.066217\n[140,]   -9.3813569  29.96843  -68.884786  49.232889\n[141,]   17.3575607  28.92836  -40.066329  73.778632\n[142,]    3.4131616  28.26150  -51.218879  58.355724\n[143,]  -19.1739905  27.84646  -73.553085  35.209401\n[144,]   -9.4444957  27.44893  -62.862543  44.960354\n[145,]   15.3734319  26.88666  -36.844027  69.230582\n[146,]    6.0053845  27.56009  -49.185780  59.829004\n[147,]  -25.2752645  27.89850  -79.656786  30.357560\n[148,]   14.7457663  27.94262  -38.924833  70.236543\n[149,]   -4.3719206  28.74177  -60.030737  49.742929\n[150,]   10.7447807  29.98415  -48.151526  70.239442\n[151,]   -2.1547111  28.84685  -59.700674  54.220042\n[152,]   -7.5099338  28.44010  -63.560652  48.292218\n[153,]  -17.5507261  27.98482  -73.264030  36.060693\n[154,]  -30.9786194  27.79869  -86.288731  23.161655\n[155,]  -19.0839475  27.32837  -73.305653  34.583350\n[156,]   41.7623717  27.66888  -12.092815  94.875495\n[157,]  -36.4474394  27.40064  -89.271537  17.970059\n[158,]   14.9040041  27.71451  -41.485350  69.539959\n[159,]   17.6758065  28.72776  -37.630394  75.476323\n[160,]    9.3395327  30.20087  -50.037040  68.902599\n[161,]   17.6074905  28.96742  -40.245647  74.093538\n[162,]   11.1078854  28.11767  -43.567214  64.468572\n[163,]    6.6902202  27.88459  -47.499881  60.814755\n[164,]    0.8499879  27.08400  -53.106702  54.184408\n[165,]  -11.1672150  27.11328  -64.701485  42.438264\n[166,]  -14.5162212  27.23590  -67.592466  40.089936\n[167,]  -49.5763850  27.32816 -102.762919   3.495567\n[168,]  -13.0714634  28.48988  -68.945979  43.583314\n[169,]   22.8183510  28.52828  -32.677928  77.757969\n[170,]   32.5603981  28.99731  -24.633516  89.478661\n[171,]    6.4484739  28.52375  -50.038578  63.126532\n[172,]   -2.0101903  27.94278  -56.639697  51.203558\n[173,]   10.0258693  27.51549  -43.561998  63.677171\n[174,]   11.8882142  26.55038  -38.738214  64.488492\n[175,]  -23.5377678  27.08766  -76.857261  30.636991\n[176,]    7.6821194  27.16770  -44.069064  62.063792\n[177,]    0.7254750  26.93533  -51.140271  55.049178\n[178,]   -2.3682298  27.88377  -58.034041  52.294497\n[179,]   12.1459358  29.01541  -44.404463  69.565984\n[180,]   -5.3515491  29.73252  -62.975318  52.234002",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#compare-models",
    "href": "bayesian.html#compare-models",
    "title": "Bayesian Analysis",
    "section": "3.9 Compare Models",
    "text": "3.9 Compare Models\nelpd values: higher is better looic values: lower is better\nelpd_diff values that are greater than ~2 standard errors of the elpd_diff values indicate a significantly better model (i.e., if elpd_diff value is greater than 2 times the se_diff value).\n\n\nCode\nloo(fit_sleep1, fit_sleep2, fit_sleep3)\n\n\nOutput of model 'fit_sleep1':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -953.1 10.5\np_loo         3.0  0.5\nlooic      1906.3 21.0\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.9, 1.2]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit_sleep2':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -884.7 14.3\np_loo        19.2  3.3\nlooic      1769.4 28.7\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.8]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit_sleep3':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -861.2 22.1\np_loo        34.0  8.2\nlooic      1722.4 44.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     177   98.3%   49      \n   (0.7, 1]   (bad)        3    1.7%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   0    0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n           elpd_diff se_diff\nfit_sleep3   0.0       0.0  \nfit_sleep2 -23.5      11.4  \nfit_sleep1 -91.9      20.7  \n\n\nCode\nprint(loo(fit_sleep1, fit_sleep2, fit_sleep3), simplify = FALSE)\n\n\nOutput of model 'fit_sleep1':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -953.1 10.5\np_loo         3.0  0.5\nlooic      1906.3 21.0\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.9, 1.2]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit_sleep2':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -884.7 14.3\np_loo        19.2  3.3\nlooic      1769.4 28.7\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.8]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit_sleep3':\n\nComputed from 4000 by 180 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -861.2 22.1\np_loo        34.0  8.2\nlooic      1722.4 44.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     177   98.3%   49      \n   (0.7, 1]   (bad)        3    1.7%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   0    0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n           elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic \nfit_sleep3    0.0       0.0  -861.2     22.1        34.0    8.2   1722.4\nfit_sleep2  -23.5      11.4  -884.7     14.3        19.2    3.3   1769.4\nfit_sleep1  -91.9      20.7  -953.1     10.5         3.0    0.5   1906.3\n           se_looic\nfit_sleep3   44.1  \nfit_sleep2   28.7  \nfit_sleep1   21.0  \n\n\n\n3.9.1 Compute Model Weights\n\n\nCode\nmodel_weights(fit_sleep1, fit_sleep2, fit_sleep3, weights = \"loo\")\n\n\n  fit_sleep1   fit_sleep2   fit_sleep3 \n1.194774e-40 6.125789e-11 1.000000e+00 \n\n\nCode\nround(model_weights(fit_sleep1, fit_sleep2, fit_sleep3, weights = \"loo\"))\n\n\nfit_sleep1 fit_sleep2 fit_sleep3 \n         0          0          1",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#parameters-for-which-to-set-priors",
    "href": "bayesian.html#parameters-for-which-to-set-priors",
    "title": "Bayesian Analysis",
    "section": "5.1 Parameters for Which to Set Priors",
    "text": "5.1 Parameters for Which to Set Priors\n\n\nCode\nget_prior(\n  Reaction ~ 1 + Days + (1 + Days | Subject), \n  data = sleepstudy)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#define-priors",
    "href": "bayesian.html#define-priors",
    "title": "Bayesian Analysis",
    "section": "5.2 Define Priors",
    "text": "5.2 Define Priors\n\n\nCode\nbprior &lt;- c(\n  set_prior(\"normal(5, 5)\", coef = \"Days\"),\n  set_prior(\"cauchy(0, 10)\", class = \"sd\"),\n  set_prior(\"lkj(2)\", class = \"cor\"))\n\nbprior",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#fit-the-model",
    "href": "bayesian.html#fit-the-model",
    "title": "Bayesian Analysis",
    "section": "5.3 Fit the Model",
    "text": "5.3 Fit the Model\nFit the model with these priors, and sample from these priors:\n\n\nCode\nfit_sleep4 &lt;- brm(\n  Reaction ~ 1 + Days + (1 + Days | Subject), \n  data = sleepstudy,\n  prior = bprior, \n  sample_prior = TRUE,\n  seed  = 52242\n)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#between-chain-parallelization",
    "href": "bayesian.html#between-chain-parallelization",
    "title": "Bayesian Analysis",
    "section": "7.1 Between-Chain Parallelization",
    "text": "7.1 Between-Chain Parallelization\n\n\nCode\nfit_sleep4 &lt;- brm(\n  Reaction ~ 1 + Days + (1 + Days | Subject), \n  data = sleepstudy,\n  prior = bprior, \n  sample_prior = TRUE,\n  cores = 4,\n  seed  = 52242\n)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#within-chain-parallelization",
    "href": "bayesian.html#within-chain-parallelization",
    "title": "Bayesian Analysis",
    "section": "7.2 Within-Chain Parallelization",
    "text": "7.2 Within-Chain Parallelization\nhttps://paul-buerkner.github.io/brms/articles/brms_threading.html (archived at https://perma.cc/NCG3-KV4G)",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#multiply-imputed-datasets-from-mice",
    "href": "bayesian.html#multiply-imputed-datasets-from-mice",
    "title": "Bayesian Analysis",
    "section": "8.1 Multiply Imputed Datasets from mice",
    "text": "8.1 Multiply Imputed Datasets from mice\n\n\nCode\n?brm_multiple\n\n\n\n\nCode\nimp &lt;- mice::mice(\n  mydata,\n  m = 5,\n  print = FALSE)\n\nfit_imp &lt;- brm_multiple(\n  bayesianMediationSyntax,\n  data = imp,\n  chains = 2)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000245 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 4.732 seconds (Warm-up)\nChain 1:                2.47 seconds (Sampling)\nChain 1:                7.202 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000113 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 5.271 seconds (Warm-up)\nChain 2:                2.463 seconds (Sampling)\nChain 2:                7.734 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000127 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.27 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 5.309 seconds (Warm-up)\nChain 1:                3.605 seconds (Sampling)\nChain 1:                8.914 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000116 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 5.574 seconds (Warm-up)\nChain 2:                1.824 seconds (Sampling)\nChain 2:                7.398 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000125 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 5.026 seconds (Warm-up)\nChain 1:                3.535 seconds (Sampling)\nChain 1:                8.561 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000114 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.14 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 5.118 seconds (Warm-up)\nChain 2:                3.577 seconds (Sampling)\nChain 2:                8.695 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.00014 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.4 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 5.054 seconds (Warm-up)\nChain 1:                3.537 seconds (Sampling)\nChain 1:                8.591 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000115 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.15 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 4.661 seconds (Warm-up)\nChain 2:                2.177 seconds (Sampling)\nChain 2:                6.838 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000123 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.23 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 4.739 seconds (Warm-up)\nChain 1:                3.62 seconds (Sampling)\nChain 1:                8.359 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000116 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 4.782 seconds (Warm-up)\nChain 2:                3.619 seconds (Sampling)\nChain 2:                8.401 seconds (Total)\nChain 2:",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "bayesian.html#imputation-on-the-fly-during-model-fitting",
    "href": "bayesian.html#imputation-on-the-fly-during-model-fitting",
    "title": "Bayesian Analysis",
    "section": "8.2 Imputation on the Fly During Model Fitting",
    "text": "8.2 Imputation on the Fly During Model Fitting\nhttps://paul-buerkner.github.io/brms/articles/brms_missings.html (archived at https://perma.cc/4Y9L-USQR)\n\n\nCode\n?mi\n\n\n\n\nCode\nbayesianRegressionImputationSyntax &lt;-\n  bf(X | mi() ~ (1 |i| id)) +\n  bf(M | mi() ~ mi(X) + (1 |i| id)) +\n  bf(Y | mi() ~ mi(X) + mi(M) + (1 |i| id)) +\n  set_rescor(FALSE) # don't add a residual correlation between X, M, and Y\n\nbayesianRegressionModel &lt;- brm(\n  bayesianRegressionImputationSyntax,\n  data = mydata,\n  seed = 52242\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000509 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 21.281 seconds (Warm-up)\nChain 1:                13.422 seconds (Sampling)\nChain 1:                34.703 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000423 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.23 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 20.124 seconds (Warm-up)\nChain 2:                13.411 seconds (Sampling)\nChain 2:                33.535 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000426 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 4.26 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 19.295 seconds (Warm-up)\nChain 3:                13.416 seconds (Sampling)\nChain 3:                32.711 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000416 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 4.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 18.811 seconds (Warm-up)\nChain 4:                12.406 seconds (Sampling)\nChain 4:                31.217 seconds (Total)\nChain 4: \n\n\nCode\nsummary(bayesianRegressionModel)\n\n\n Family: MV(gaussian, gaussian, gaussian) \n  Links: mu = identity\n         mu = identity\n         mu = identity \nFormula: X | mi() ~ (1 | i | id) \n         M | mi() ~ mi(X) + (1 | i | id) \n         Y | mi() ~ mi(X) + mi(M) + (1 | i | id) \n   Data: mydata (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 100) \n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(X_Intercept)                  0.09      0.06     0.00     0.21 1.00     1007\nsd(M_Intercept)                  0.07      0.05     0.00     0.18 1.00     1534\nsd(Y_Intercept)                  0.10      0.06     0.01     0.22 1.00     1310\ncor(X_Intercept,M_Intercept)     0.08      0.49    -0.84     0.90 1.00     3556\ncor(X_Intercept,Y_Intercept)     0.06      0.47    -0.82     0.88 1.00     2320\ncor(M_Intercept,Y_Intercept)     0.00      0.48    -0.88     0.86 1.00     2474\n                             Tail_ESS\nsd(X_Intercept)                  1428\nsd(M_Intercept)                  2177\nsd(Y_Intercept)                  2087\ncor(X_Intercept,M_Intercept)     2922\ncor(X_Intercept,Y_Intercept)     2480\ncor(M_Intercept,Y_Intercept)     2693\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nX_Intercept    -0.02      0.03    -0.08     0.05 1.00     7564     3001\nM_Intercept     0.01      0.03    -0.06     0.07 1.00     7728     3049\nY_Intercept     0.06      0.03    -0.01     0.12 1.00     6781     2915\nM_miX           0.51      0.03     0.45     0.58 1.00     8400     2801\nY_miX           0.04      0.04    -0.04     0.11 1.00     5567     2993\nY_miM           0.68      0.03     0.62     0.74 1.00     5954     3270\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_X     0.98      0.02     0.94     1.03 1.00     6184     2817\nsigma_M     1.01      0.02     0.96     1.05 1.00     6661     2907\nsigma_Y     1.01      0.02     0.96     1.05 1.00     5554     2605\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nhypothesis(\n  bayesianRegressionModel,\n  \"bsp_M_miX * bsp_Y_miM = 0\", # indirect effect = a path * b path\n  class = NULL,\n  seed = 52242\n)\n\n\nHypothesis Tests for class :\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (bsp_M_miX*bsp_Y_... = 0     0.35      0.03      0.3      0.4         NA\n  Post.Prob Star\n1        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.",
    "crumbs": [
      "About",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "Longitudinal Data Analysis",
    "section": "",
    "text": "1 Approaches for Modeling Longitudinal Data\n\nGrowth curve model\nLatent growth curve model\nLatent change score model\nCross-lagged panel model\nLatent curve model with structured residuals\n\n\n\n2 Estimating Nonlinear Growth\nThere are a variety of ways to estimate nonlinear growth in a growth curve model using a mixed-effects or structural equation model:\n\npolynomial growth model\n\nfractional polynomial model (more parsimonious than traditional polynomials because can capture nonlinear growth with fewer parameters, thus reducing overfitting)\n\npiecewise/spline model\n\ncan have fixed or random knots\nlocation of knots can be estimated for the data\neach individual can have a different numbers of knots and different location for the knots\n\nlatent basis growth model\n\ncan specify the rate of change between T1 and T2 to be one; can allow the rate of change to freely vary between remaining timepoints\n\nexponential growth model\nlogistic growth model\nlogarithmic growth model\ngeneralized additive model\nnonparametric growth model (e.g., kernel smoothing)\nGompertz growth model\nRichards growth model\nTaylor series approximation model\nlatent change score model\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Longitudinal Data Analysis"
    ]
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "Markdown",
    "section": "",
    "text": "This is an RMarkdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see https://rmarkdown.rstudio.com (archived at https://perma.cc/4MP3-5RT8)\nYou can embed an R code chunk like this:\n```{r}\nsummary(cars)\n```\n\n\nCode\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\n\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf (archived at https://perma.cc/8P7X-9XPV)",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#overview",
    "href": "markdown.html#overview",
    "title": "Markdown",
    "section": "",
    "text": "This is an RMarkdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see https://rmarkdown.rstudio.com (archived at https://perma.cc/4MP3-5RT8)\nYou can embed an R code chunk like this:\n```{r}\nsummary(cars)\n```\n\n\nCode\nsummary(cars)\n\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#referenceGuide",
    "href": "markdown.html#referenceGuide",
    "title": "Markdown",
    "section": "",
    "text": "https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf (archived at https://perma.cc/8P7X-9XPV)",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#bestPractices",
    "href": "markdown.html#bestPractices",
    "title": "Markdown",
    "section": "2.1 Best Practices",
    "text": "2.1 Best Practices\nAdapted from here (archived at https://perma.cc/P75L-3UM9):\n\nUse a new line for every sentence. This helps with version control—individually changed sentences will be highlighted upon commits (which are easy to parse to identify the specific changes) rather than whole paragraphs (which are a nightmare to parse to identify the specific changes).\n\nThis formatting is incompatible with a double space after a period. Instead, use a single space after a period. To replace all double spaces with single spaces, do a find and replace for \".  \" → \". \" Be careful not to blanket remove all double spaces because tabbed bullets have 3 spaces.\n\nAdd protocol entries as links to the parent .Rmd file (or to README.md)\nAdd linked content (e.g., images) to sub-directories\nUse no spaces in file or folder names. Use a dash (-) instead of a space.\nIf you use Visual Studio Code to edit the Lab Wiki, you can view the source code side-by-side with the markdown Preview (after selecting the code type as markdown): \nWhen making numbered (ordered) lists, just use 1. for all numbers. Markdown will make them sequential automatically. This is helpful if you go back and add additional entries later.\nFor all quotation marks, use straight quotes (', \") rather than curly quotes (‘, ’, “, ”).\nUse \"[XXX]\" to indicate an area that needs editing. This will allow the lab to CTRL+F for \"[XXX]\" and find areas that need editing.\nTo update the list of R packages that are installed for running the .Rmd files, update the DESCRIPTION file.\nWhen adding a link to an external (non-lab) URL, add both the original link and the perma.cc link to prevent link rot (websites will go offline and disappear over time). For example: http://www.cookbook-r.com/Graphs/Scatterplots_(ggplot2)/ (archived at https://perma.cc/N4TL-JQS4) You can get a perma.cc account through the UI Libraries.\nAs noted in the links section, link to the heading label, and do not use the website-derived link for a given heading because it is not a stable link; the heading path can change if other headings are added.",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#updateWiki",
    "href": "markdown.html#updateWiki",
    "title": "Markdown",
    "section": "2.2 Updating",
    "text": "2.2 Updating\nTo update the lab wiki, perform the following steps:\n\n(Perform this step if you do not already have the the LabWiki GitHub repo on your local machine:) Clone the GitHub repo to your local machine.\nFetch the latest version of the repo. It is important to do this to make sure you have the latest version locally before making any changes.\nEdit the relevant .Rmd files using RStudio or Visual Studio Code.\nPreview the wiki before you commit changes to the site; create the .html files locally (in a folder called _site) by running the following command:\n\nrmarkdown::render_site()\n\nIf the .html files look correct, commit the changes.\nPush the changes to the GitHub repo. After pushing the changes to the GitHub repo, GitHub Actions will automatically re-run the code and deploy the updated website, assuming there were no errors. You can track the workflow for errors here: https://github.com/DevPsyLab/LabWiki/actions.\n\nFor more information about the rmarkdown package that was used to generate the site, see the following links:\n\nhttps://bookdown.org/yihui/rmarkdown/rmarkdown-site.html (archived at https://perma.cc/5C4T-SY9V)\nhttps://bookdown.org/yihui/rmarkdown/html-document.html (archived at https://perma.cc/WQS5-KH4S)",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#vsCode",
    "href": "markdown.html#vsCode",
    "title": "Markdown",
    "section": "3.1 Microsoft Visual Studio Code",
    "text": "3.1 Microsoft Visual Studio Code\nIf you use Visual Studio Code to create/edit .Rmd documents, you can view the source code side-by-side with the markdown Preview (after selecting the code type as markdown):",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#rstudio",
    "href": "markdown.html#rstudio",
    "title": "Markdown",
    "section": "3.2 RStudio",
    "text": "3.2 RStudio\n\n3.2.1 Creating a Code Chunk\nAdd a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Ctrl+Alt+I.\n\n\n3.2.2 Running a Code Chunk\nYou can execute a code chunk in an R Markdown Notebook by clicking the Run button within the chunk or by placing your cursor inside it and pressing Ctrl+Shift+Enter. To run more than one code chunk, you have several options:\n\nclick the Run button within the chunks you want to run\nselect “Run All Chunks Above”\nselect “Run All Chunks Below”\nselect “Run All”\n\n\n\n3.2.3 Creating the HTML Output\nWhen you click the Knit button, a document will be generated that includes both content as well as the output of any embedded Rcode chunks within the document.\nWhen you save the computational notebook (.Rmd), an HTML (.html) file containing the code and output will be saved alongside it (click the Preview button or press Ctrl+Shift+K to preview the HTML file).\nThe preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#general",
    "href": "markdown.html#general",
    "title": "Markdown",
    "section": "4.1 General Info",
    "text": "4.1 General Info\nYou can use an R Markdown Notebook to create a computational notebook. For more information on the value of computational notebooks, see the article titled “The Scientific Paper Is Obsolete” (https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/; archived at https://perma.cc/XHW4-K89V)“. For more information about how to create R Notebooks, see here:\nhttps://garrettgman.github.io/rmarkdown/r_notebooks (archived at https://perma.cc/L78P-G73Z)\nhttps://bookdown.org/yihui/rmarkdown/notebook.html (archived at https://perma.cc/P5YT-395F)",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#files",
    "href": "markdown.html#files",
    "title": "Markdown",
    "section": "4.2 Files",
    "text": "4.2 Files\nConsistent with the Petersen Lab template for how to structure your repo, you should have two computational notebooks: 1) a “Prepare Data” script (prepareData.Rmd) and 2) a “Computational Notebook” script (computationalNotebook.Rmd). The “Prepare Data” script is used to update the data file by re-exporting, merging, re-running calculations, etc. This script should be separate from the “Computational Notebook” file that is used for running analyses on the data. Every time you run the “Computational Notebook” file with the exact same data, you should get the exact same findings (i.e., reproducibility). Thus, we keep the “Prepare Data script” separate from the “Computational Notebook” file so that you update the data only when you intend to.",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#structure",
    "href": "markdown.html#structure",
    "title": "Markdown",
    "section": "4.3 Structure",
    "text": "4.3 Structure\nWhen creating a computational notebook, follow the structure of the computational notebooks in the Petersen Lab template.\nTo use the lab templates for the “Prepare Data” and “Computational Notebook” scripts, first install the petersenlab package. Then, in RStudio, select “File” &gt; “New File” &gt; “R Markdown”. Then, select “From Template”, and select either the “prepareData” or “computationalNotebook” template.\nIf you want to create a computational notebook manually (instead of using the lab template), create a .Rmd file with the following YAML header at the top of the file:\n---\ntitle: \"INSERT TITLE\"\nauthor: \"INSERT AUTHOR NAME(S)\"\ndate: \"`r format(Sys.time(), '%d %B, %Y')`\"\noutput: \n  html_document:\n    toc: true\n    toc_depth: 4\n    toc_float: true\n    number_sections: true\n    code_folding: show\n    df_print: paged\n---\nIf this is your “Computational Notebook” script, include the following as the first code chunk:\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(\n  echo = TRUE, # print code\n  error = TRUE # do not interrupt in case of errors\n)\n\n# Use for higher quality figures on Windows devices\ntrace(grDevices::png, quote({\n  if (missing(type) && missing(antialias)) {\n    type &lt;- \"cairo-png\"\n    antialias &lt;- \"subpixel\"\n  }\n}), print = FALSE)\n```\nIf this is your “Prepare Data” script, include the following as the first code chunk:\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(\n  echo = TRUE # print code\n)\n```\nThen, include whatever text and/or code chunks.",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#css",
    "href": "markdown.html#css",
    "title": "Markdown",
    "section": "4.4 Creating CSS Commands",
    "text": "4.4 Creating CSS Commands\nPersonalized commands can be created in Visual Code through the CSS file within the Wiki folder. To create a new CSS command:\n\nIn Visual Studio Code, open the css tab within the Wiki Folder\nOpen the # srs.css file under the css tab\nEnter in your personalized command according to the following structure:\n\nCOMMAND_NAME {\n  command-specification1: value1;\n  command-specification2: value2;\n  command-specification3: value3;\n  quotes: \"\"\" \"\"\" \"‘\" \"'\";\n}\n\nThe following specifications may be added to your command:\n\nfont-size: Value = percentage\n\ne.g., font-size: 150%\n\nfont-style: Value = bold, italic, underline\n\ne.g., font-style: bold, font-style: italic\n\ncolor: Value = color name or color code\n\ne.g., color: green, color: #0e6b1c\n\nTo edit the command to add quotations, include the following after the end bracket of the command: COMMAND_NAME:before { content: open-quote; }  COMMAND_NAME:after  { content: close-quote; }\n\nAn example of a command is presented here:\n\nexample {\n  font-size: 150%;\n  font-style: italic;\n  color:#C800FC;\n  quotes: \"\"\" \"\"\" \"‘\" \"'\";\n}\nexample:before { content: open-quote; }\nexample:after  { content: close-quote; }",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#websiteSteps",
    "href": "markdown.html#websiteSteps",
    "title": "Markdown",
    "section": "5.1 Steps to Create",
    "text": "5.1 Steps to Create\n\nCreate a new GitHub repo under the DevPsyLab organization in GitHub\nCreate the repository using one of the following templates (and select “Include all branches”):\n\nrmarkdown: https://github.com/DevPsyLab/MarkdownWebsite\n\nfor local rendering: https://github.com/DevPsyLab/MarkdownWebsite-RenderLocal\n\nQuarto: https://github.com/DevPsyLab/QuartoWebsite\n\nfor local rendering: https://github.com/DevPsyLab/QuartoWebsite-RenderLocal\n\n\n\nIf you do not create the repository from a template, the steps for creating the repo manually are below:\n\nCreate a README.md file\n\nthis is the file that people see when navigating to the repo\n\nAdd a .gitignore file\n\nthis file says what file types to ignore and not add to the repo\n\nCreate a new branch called gh-pages\nDelete all files in the gh-pages branch\nSwitch back the main branch of the repo, and add the following contents (to the main branch, not to the gh-pages branch):\n\n/.github/workflows/deploy_markdown.yml file\n\nthis file tells GitHub Actions was steps to run\nadd the following contents to the file:\n\nif using rmarkdown:\n\nif the website is intended to be public: https://github.com/DevPsyLab/LabWiki/blob/main/.github/workflows/deploy_rmarkdown.yml\nif the website is intended to be private: https://github.com/DevPsyLab/SchoolReadinessStudy/blob/main/.github/workflows/deploy_rmarkdown.yml\n\nif using Quarto:\n\nif the website is intended to be public: https://github.com/DevPsyLab/QuartoWebsite/blob/main/.github/workflows/publish.yml\nif the website is intended to be private: https://github.com/DevPsyLab/QuartoWebsite-RenderLocal/blob/main/.github/workflows/publish.yml\n\n\n\nYAML file—this file creates the navbar:\n\nif using rmarkdown:\n\n_site.yml file\n\nif using Quarto:\n\n_quarto.yml file\n\n\nLICENSE file\n\nthis file is the license\n\nDESCRIPTION file\n\nthis file tells GitHub Actions what R packages to install\n\n\nAdd each .Rmd file (for rmarkdown) or .qmd file (for Quarto)\nAdd folders for the font, images, and includes\n\nincludes contains css files, javascript files, footers, and Google Analytics\n\nIf the rmarkdown website is intended to be public, create a new Google Analytics property, and replace the URL and gtag in the google_analytics.html file\nCommit the repo to GitHub\nTo deploy the website on GitHub Pages:\n\nMake the repo public (Settings &gt; General &gt; Change Visibility)\nActivate GitHub Pages (Settings &gt; Pages)\n\nSource: “Deploy from a Branch”\nBranch: gh-pages/root\nClick “Save”\n\nMake sure “Read and write permissions” are enabled in Settings -&gt; Actions -&gt; General -&gt; Workflow permissions",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#quarto-manuscripts",
    "href": "markdown.html#quarto-manuscripts",
    "title": "Markdown",
    "section": "5.2 Quarto Manuscripts",
    "text": "5.2 Quarto Manuscripts\n\nQuarto Journal Templates: https://github.com/quarto-journals\nSample Manuscripts: https://quarto.org/docs/manuscripts\n\nExample:\n\nhttps://github.com/mine-cetinkaya-rundel/indo-rct\n\nhttps://mine-cetinkaya-rundel.github.io/indo-rct/\n\n\nTo prevent code from re-running unless code is altered:\nexecute:\n    freeze: auto",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#troubleshootingMarkdown",
    "href": "markdown.html#troubleshootingMarkdown",
    "title": "Markdown",
    "section": "5.3 Troubleshooting",
    "text": "5.3 Troubleshooting\nIf you get the following error in GitHub Actions:\nremote: Permission to GITHUB_USERNAME/REPO_NAME.git denied to github-actions[bot].\nfatal: unable to access 'https://github.com/GITHUB_USERNAME/REPO_NAME.git/': The requested URL returned error: 403\nCheck if “Read and write permissions” are enabled in Settings -&gt; Actions -&gt; General -&gt; Workflow permissions: https://stackoverflow.com/a/75308228/2029527 (archived at https://perma.cc/R57Z-9JUS)",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#subheading",
    "href": "markdown.html#subheading",
    "title": "Markdown",
    "section": "7.1 Subheading",
    "text": "7.1 Subheading\n\n7.1.1 Subsubheading\n\n7.1.1.1 Subsubsubheading\n# Headings {#heading-label1}\n\n## Subheading {#heading-label2}\n\n### Subsubheading {#heading-label3}\n\n#### Subsubsubheading {#heading-label4}",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#unorderedLists",
    "href": "markdown.html#unorderedLists",
    "title": "Markdown",
    "section": "8.1 Unordered Lists",
    "text": "8.1 Unordered Lists\nTo create an unordered list, add a dash (-) in front of line items. Adding an asterisk (*) or plus sign (+) in front of line items will also create an unordered list; however, the lab convention is to use dashes for unordered lists. We use asterisks for adding emphasis to text.\n- one item\n- one item\n- one item\n    - one more item\n    - one more item\n    - one more item\n        - lower level\n            - sub-item 1\n            - sub-item 2\n\none item\none item\none item\n\none more item\none more item\none more item\n\nlower level\n\nsub-item 1\nsub-item 2\n\n\n\n\n\n8.1.1 Checkboxes\n- &lt;input type=\"checkbox\" unchecked&gt; Box 1&lt;/input&gt;\n- &lt;input type=\"checkbox\" checked&gt; Box 2&lt;/input&gt;\n\n Box 1\n Box 2",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#orderedLists",
    "href": "markdown.html#orderedLists",
    "title": "Markdown",
    "section": "8.2 Ordered Lists",
    "text": "8.2 Ordered Lists\n1. the first item\n    1. test\n    1. test\n        1. new test\n        1. new test\n1. the second item\n    1. test\n    1. test\n1. the third item\n    - one unordered item\n        - subitem\n    - one unordered item\n1. fourth item\n    1. test\n    1. test\n    1. test\n\nthe first item\n\ntest\ntest\n\nnew test\nnew test\n\n\nthe second item\n\ntest\ntest\n\nthe third item\n\none unordered item\n\nsubitem\n\none unordered item\n\nfourth item\n\ntest\ntest\ntest",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#markdownComments",
    "href": "markdown.html#markdownComments",
    "title": "Markdown",
    "section": "10.1 Markdown",
    "text": "10.1 Markdown\n```markdown\n[This is how to write a hidden comment in Markdown.]: # \n```\n\n[This comment will be hidden.]: # \n[This is how to write a hidden comment in Markdown.]: #",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#htmlComments",
    "href": "markdown.html#htmlComments",
    "title": "Markdown",
    "section": "10.2 HTML",
    "text": "10.2 HTML\n&lt;!-- This text will be hidden --&gt;\n\n&lt;!--\nThis multiline\ntext comment will be\nhidden\n--&gt;",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#arrows",
    "href": "markdown.html#arrows",
    "title": "Markdown",
    "section": "17.1 Arrows",
    "text": "17.1 Arrows\n- left arrow: ← &#8592;\n- upward arrow: ↑ &#8593;\n- right arrow: → &#8594;\n- downward arrow: ↓ &#8595;\n\nleft arrow: ← ←\nupward arrow: ↑ ↑\nright arrow: → →\ndownward arrow: ↓ ↓",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#dashes",
    "href": "markdown.html#dashes",
    "title": "Markdown",
    "section": "17.2 Dashes and Hyphens",
    "text": "17.2 Dashes and Hyphens\n- hyphen: -\n- en dash: –\n- em dash: —\n\nhyphen: -\nen dash: –\nem dash: —",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#subSuperscript",
    "href": "markdown.html#subSuperscript",
    "title": "Markdown",
    "section": "17.3 Subscript/Superscript",
    "text": "17.3 Subscript/Superscript\n\n17.3.1 Markdown\nH~2O\nX^2\nH~2O\nX^2\n\n\n17.3.2 HTML\nH&lt;sub&gt;2&lt;/sub&gt;O\nX&lt;sup&gt;2&lt;/sup&gt;\nH2O\nX2",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#definitions",
    "href": "markdown.html#definitions",
    "title": "Markdown",
    "section": "17.4 Definition List",
    "text": "17.4 Definition List\nterm\n: definition\n\nterm\n\ndefinition",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#taskList",
    "href": "markdown.html#taskList",
    "title": "Markdown",
    "section": "17.5 Task list",
    "text": "17.5 Task list\n- [x] Create the website\n- [ ] Conduct the study\n- [ ] Write it up\n\nCreate the website\nConduct the study\nWrite it up",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#lowerFileSize",
    "href": "markdown.html#lowerFileSize",
    "title": "Markdown",
    "section": "25.1 Lowering File Size",
    "text": "25.1 Lowering File Size\n\nOpen Handbrake and drag and drop the video that you would like to lower the file size of.\nSelect the video tab and lower the Constant Quality to 30, and the Framerate (FPS): to 20\n\nWe have found this lowers file size, but saves video quality",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "markdown.html#footnotes",
    "href": "markdown.html#footnotes",
    "title": "Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy reference.↩︎\nEvery new line should be prefixed with 2 spaces.\nThis allows you to have a footnote with multiple lines.↩︎\nNamed footnotes will still render with numbers instead of the text but allow easier identification and linking.\nThis footnote also has been made with a different syntax using 4 spaces for new lines.↩︎",
    "crumbs": [
      "About",
      "Markdown"
    ]
  },
  {
    "objectID": "lamp.html",
    "href": "lamp.html",
    "title": "LAMP (Linux, Apache, MySQL, PHP)",
    "section": "",
    "text": "1 Logging In\nTo log in to the lab’s LAMP server, use WinSCP.\nOur LAMP domain is: pbslamp.divms.uiowa.edu\nOur credentials are in our LastPass account.\n\n\n2 LAMP Server\nHere are access instructions for the Shared LAMP service: https://iowa.sharepoint.com/sites/sharedlamp/SitePages/User-Access.aspx (archived at https://perma.cc/QN98-PDVT)\nFor making URLs/online tasks available to the public, put your files in the /data/www/devpsylab.psychology.uiowa.edu/ folder.\nFor saving results, save your files to the /data/apache-rw/devpsylab.psychology.uiowa.edu/ folder.\n\n\n3 MariaDB\nHere are instructions for MariaDB: https://iowa.sharepoint.com/sites/sharedlamp/SitePages/MariaDB.aspx (archived at https://perma.cc/BS2Q-HQ4W)\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "LAMP (Linux, Apache, MySQL, PHP)"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "1 Learning Statistics\nHere are resources for learning statistics:\n\nOpenIntro Statistics: https://www.openintro.org/book/os/\nIntroductory Statistics: https://openstax.org/details/books/introductory-statistics-2e\n\n\n\n2 Workshops\nHere are potential sources for workshops on various statistical topics:\n\nhttps://centerstat.org\nhttps://smart-workshops.com\nhttps://www.statscamp.org\nhttps://statisticalhorizons.com\nhttps://stats.oarc.ucla.edu/other/mult-pkg/seminars/\nhttps://datascience-workshop.uiowa.edu/resources\n\n\n\n3 Statistical Decision Tree\n\n\n\nA Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests.\n\n\n\n\n\nA Statistical Decision Tree For Choosing an Appropriate Statistical Procedure, Re-Formulated in a Regression Framework. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including re-formulating the tests in a regression framework.\n\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Statistics"
    ]
  },
  {
    "objectID": "jatos.html",
    "href": "jatos.html",
    "title": "JATOS (Just Another Tool for Online Studies)",
    "section": "",
    "text": "1 Logging In\nAfter creating a JATOS account with the help of the department IT (pbs-help@uiowa.edu), log in here:\nhttps://jatos.psychology.uiowa.edu\n\n\n2 jsPsych\nTo convert jsPsych tasks to JATOS, see here: https://www.jatos.org/3.6.x/jsPsych-and-JATOS.html (archived at https://perma.cc/W82S-6FU9)\n\n\n3 SONA\n\nhttps://osdoc.cogsci.nl/4.0/manual/osweb/sonasystems/\nhttps://www.andywills.info/jatos-sona-notes/\n\n\n\n4 Parse JSON Data in R\nhttps://github.com/becky-gilbert/r-jatos-json-parser/blob/master/parse_JSON_data_examples.R\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "JATOS (Just Another Tool for Online Studies)"
    ]
  },
  {
    "objectID": "developmentalScaling.html",
    "href": "developmentalScaling.html",
    "title": "Developmental Scaling",
    "section": "",
    "text": "To be added…\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Developmental Scaling"
    ]
  },
  {
    "objectID": "figures.html",
    "href": "figures.html",
    "title": "Figures in R",
    "section": "",
    "text": "https://www.data-to-viz.com",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#preamble",
    "href": "figures.html#preamble",
    "title": "Figures in R",
    "section": "5.1 Preamble",
    "text": "5.1 Preamble\n\n5.1.1 Install Libraries\n\n\nCode\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n\n\n\n\n5.1.2 Load Libraries\n\n\nCode\nlibrary(\"petersenlab\")\nlibrary(\"ellipse\")\nlibrary(\"ggplot2\")\nlibrary(\"grid\")\nlibrary(\"reshape\")\nlibrary(\"plyr\")\nlibrary(\"RColorBrewer\")\nlibrary(\"reshape2\")\nlibrary(\"ggExtra\")\nlibrary(\"viridis\")\nlibrary(\"ggthemes\")\nlibrary(\"ggpubr\")",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#simulate-data",
    "href": "figures.html#simulate-data",
    "title": "Figures in R",
    "section": "5.2 Simulate Data",
    "text": "5.2 Simulate Data\n\n\nCode\nset.seed(52242)\n\nn &lt;- 1000\n\npredictor &lt;- rbeta(n, 1.5, 5) * 100\noutcome &lt;- predictor + rnorm(n, mean = 0, sd = 20) + 50\nnumber &lt;- sample(1:1000, replace = TRUE)\n\npredictorOverplot &lt;- sample(1:50, n, replace = TRUE)\noutcomeOverplot &lt;- predictorOverplot + sample(1:75, n, replace = TRUE)\n\ndf &lt;- data.frame(predictor = predictor,\n                 outcome = outcome,\n                 predictorOverplot = predictorOverplot,\n                 outcomeOverplot = outcomeOverplot)\n\ndf[sample(1:n, size = 10), \"predictor\"] &lt;- NA\ndf[sample(1:n, size = 10), \"outcome\"] &lt;- NA\ndf[sample(1:n, size = 10), \"predictorOverplot\"] &lt;- NA\ndf[sample(1:n, size = 10), \"outcomeOverplot\"] &lt;- NA",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#line",
    "href": "figures.html#line",
    "title": "Figures in R",
    "section": "5.3 Line",
    "text": "5.3 Line\n\n\nCode\nplot.new()\nlines(\n  x = seq(from = -10, to = 10, length.out = 100),\n  y = seq(from = -25, to = 25, length.out = 100))",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#curve",
    "href": "figures.html#curve",
    "title": "Figures in R",
    "section": "5.4 Curve",
    "text": "5.4 Curve\n\n\nCode\ncurve(x^3 - 3*x, from = -2, to = 2)\ncurve(x^2 - 2, add = TRUE, col = \"violet\")",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#basic-scatterplot",
    "href": "figures.html#basic-scatterplot",
    "title": "Figures in R",
    "section": "5.5 Basic Scatterplot",
    "text": "5.5 Basic Scatterplot\n\n5.5.1 Base R\n\n\nCode\nplot(outcome ~ predictor, data = df)\n\n\n\n\n\n\n\n\n\nCode\nplot(df$predictor, df$outcome)\n\n\n\n\n\n\n\n\n\n\n5.5.1.1 Best-fit line\n\n\nCode\nplot(outcome ~ predictor, data = df)\nabline(lm(outcome ~ predictor, data = df), col = \"red\") #regression line (y~x)\n\n\n\n\n\n\n\n\n\n\n\n5.5.1.2 Best-fit line with correlation coefficient\n\n\nCode\nplot(outcome ~ predictor, data = df)\nabline(lm(outcome ~ predictor, data = df), col = \"red\") #regression line (y~x)\naddText(x = df$predictor, y = df$outcome)\n\n\n\n\n\n\n\n\n\n\n\n5.5.1.3 Loess line\n\n\nCode\nplot(outcome ~ predictor, data = df)\nlines(loess.smooth(df$predictor, df$outcome)) #loess line (x,y)\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 ggplot2\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n5.5.2.1 Best-fit line\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n5.5.2.2 Best-fit line with correlation coefficient\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = y ~ x) +\n  stat_cor(\n    cor.coef.name = \"r\",\n    p.accuracy = 0.001,\n    r.accuracy = 0.01) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n5.5.2.3 Loess line\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  stat_smooth(method = \"loess\", formula = y ~ x) +\n  theme_classic()",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#change-plot-style",
    "href": "figures.html#change-plot-style",
    "title": "Figures in R",
    "section": "5.6 Change Plot Style",
    "text": "5.6 Change Plot Style\n\n5.6.1 Change Theme\n\n\nCode\nbasePlot &lt;- ggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point()\n\n\n\n5.6.1.1 Default Theme\n\n\nCode\nbasePlot\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.2 Grayscale: theme_gray()\n\n\nCode\nbasePlot + theme_gray() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.3 Black-and-White: theme_bw()\n\n\nCode\nbasePlot + theme_bw() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.4 Line Drawing: theme_linedraw()\nA theme with only black lines of various widths on white backgrounds, reminiscent of a line drawing. Note that this theme has some very thin lines (&lt;&lt; 1 pt) which some journals may refuse.\n\n\nCode\nbasePlot + theme_linedraw() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.5 Light: theme_light()\n\n\nCode\nbasePlot + theme_light() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.6 Dark: theme_dark()\n\n\nCode\nbasePlot + theme_dark() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.7 Minimal: theme_minimal()\n\n\nCode\nbasePlot + theme_minimal() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.8 Classic: theme_classic()\n\n\nCode\nbasePlot + theme_classic() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.9 A Completely Empty Theme: theme_void()\n\n\nCode\nbasePlot + theme_void() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.10 Visual Unit Tests: theme_test()\n\n\nCode\nbasePlot + theme_test() + theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.11 Edward Tufte: theme_tufte()\nTheme based on Edward Tufte.\n\n\nCode\nbasePlot + theme_tufte()\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.12 Wall Street Journal: theme_wsj()\nTheme based on the publication, the Wall Street Journal.\n\n\nCode\nbasePlot + theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.13 FiveThirtyEight: theme_fivethirtyeight()\nTheme based on the publication, FiveThirtyEight.\n\n\nCode\nbasePlot + theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.14 The Economist: theme_economist()\nTheme based on the publication, The Economist.\n\n\nCode\nbasePlot + theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n5.6.1.15 Stephen Few: theme_few()\nTheme based on the rules and examples from Stephen Few’s Show Me the Numbers and “Practical Rules for Using Color in Charts”.\n\n\nCode\nbasePlot + theme_few()",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#marginalDistributions",
    "href": "figures.html#marginalDistributions",
    "title": "Figures in R",
    "section": "5.7 Add Marginal Distributions",
    "text": "5.7 Add Marginal Distributions\n\n\nCode\nscatterplot &lt;- ggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n5.7.1 Density Plot\n\n\nCode\ndensityMarginal &lt;- ggMarginal(scatterplot, type = \"density\", xparams = list(fill = \"gray\"), yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(densityMarginal, newpage = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Histogram\n\n\nCode\nhistogramMarginal &lt;- ggMarginal(scatterplot, type = \"histogram\", xparams = list(fill = \"gray\"), yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(histogramMarginal, newpage = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n5.7.3 Boxplot\n\n\nCode\nboxplotMarginal &lt;- ggMarginal(scatterplot, type = \"boxplot\", xparams = list(fill = \"gray\"), yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(boxplotMarginal, newpage = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n5.7.4 Violin Plot\n\n\nCode\nviolinMarginal &lt;- ggMarginal(scatterplot, type = \"violin\", xparams = list(fill = \"gray\"), yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(violinMarginal, newpage = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n5.7.5 Density Plot and Histogram\n\n\nCode\ndensigramMarginal &lt;- ggMarginal(scatterplot, type = \"densigram\", xparams = list(fill = \"gray\"), yparams = list(fill = \"gray\"))\n\n\n\n\nCode\nprint(densigramMarginal, newpage = TRUE)",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#ellipse",
    "href": "figures.html#ellipse",
    "title": "Figures in R",
    "section": "5.8 Ellipse",
    "text": "5.8 Ellipse\n\n5.8.1 Basic Ellipse\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  stat_ellipse(alpha = 0.4, level = 0.95, geom = \"polygon\", fill = \"red\", color = \"red\") +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.8.2 Align Coordinates\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point() +\n  stat_ellipse(alpha = 0.4, level = 0.95, geom = \"polygon\", fill = \"red\", color = \"red\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed(ratio = (max(predictor, na.rm = TRUE) - min(predictor, na.rm = TRUE))/(max(outcome, na.rm = TRUE) - min(outcome, na.rm = TRUE)),\n              xlim = c(0, max(predictor, na.rm = TRUE)), \n              ylim = c(0, max(outcome, na.rm = TRUE))) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.8.3 Reduce Dot Size\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point(size = 0.5) +\n  stat_ellipse(alpha = 0.4, level = 0.95, geom = \"polygon\", fill = \"red\", color = \"red\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed(ratio = (max(predictor, na.rm = TRUE) - min(predictor, na.rm = TRUE))/(max(outcome, na.rm = TRUE) - min(outcome, na.rm = TRUE)),\n              xlim = c(0, max(predictor, na.rm = TRUE)), \n              ylim = c(0, max(outcome, na.rm = TRUE))) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.8.4 Transparency\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  geom_point(alpha = 0.3) +\n  stat_ellipse(alpha = 0.4, level = 0.95, geom = \"polygon\", fill = \"red\", color = \"red\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed(ratio = (max(predictor, na.rm = TRUE) - min(predictor, na.rm = TRUE))/(max(outcome, na.rm = TRUE) - min(outcome, na.rm = TRUE)),\n              xlim = c(0, max(predictor, na.rm = TRUE)), \n              ylim = c(0, max(outcome, na.rm = TRUE))) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#bubble-chart",
    "href": "figures.html#bubble-chart",
    "title": "Figures in R",
    "section": "5.9 Bubble Chart",
    "text": "5.9 Bubble Chart\n\n5.9.1 Basic Bubble Chart\n\n\nCode\nggplot(df, aes(x = predictorOverplot, y = outcomeOverplot)) +\n  geom_count(aes(size = ..n..)) +\n  scale_size_area() +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.9.2 Specify Sizes\n\n\nCode\nggplot(df, aes(x = predictorOverplot, y = outcomeOverplot)) +\n  geom_count(aes(size = ..n..)) +\n  scale_size_continuous(breaks = c(1, 2, 3, 4), range = c(1, 7)) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#dimensional-density",
    "href": "figures.html#dimensional-density",
    "title": "Figures in R",
    "section": "5.10 2-Dimensional Density",
    "text": "5.10 2-Dimensional Density\n\n\nCode\nggplot(df, aes(x = predictor, y = outcome)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_viridis() +\n  theme(\n    legend.position = \"none\",\n    text = element_text(family = \"Gotham\")\n  )",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#combined-ellipse-and-bubble-chart",
    "href": "figures.html#combined-ellipse-and-bubble-chart",
    "title": "Figures in R",
    "section": "5.11 Combined Ellipse and Bubble Chart",
    "text": "5.11 Combined Ellipse and Bubble Chart\n\n5.11.1 ggplot2\n\n\nCode\nggplot(df, aes(x = predictorOverplot, y = outcomeOverplot)) +\n  geom_count(alpha = .6, color = rgb(0,0,.7,.5)) +\n  scale_size_continuous(breaks = c(1, 2, 3, 4), range = c(1, 7)) +\n  stat_smooth(method = \"loess\", se = TRUE, color = \"green\") + \n  stat_smooth(method = \"lm\") +\n  stat_ellipse(alpha = 0.4, level = 0.95, geom = \"polygon\", fill = \"red\", color = \"red\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed(ratio = (max(predictorOverplot, na.rm = TRUE) - min(predictorOverplot, na.rm = TRUE))/(max(outcomeOverplot, na.rm = TRUE) - min(outcomeOverplot, na.rm = TRUE)),\n              xlim = c(0, max(predictorOverplot, na.rm = TRUE)), \n              ylim = c(0, max(outcomeOverplot, na.rm = TRUE))) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = predictorOverplot, y = outcomeOverplot)) +\n  geom_count(alpha = .6, color = rgb(0,0,.7,.5)) +\n  scale_size_continuous(breaks = c(1, 2, 3, 4), range = c(1, 7)) +\n  stat_smooth(method = \"loess\", se = TRUE, color = \"green\") + \n  stat_smooth(method = \"lm\") +\n  stat_ellipse(color = \"red\", size = 1.5) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed(ratio = (max(predictorOverplot, na.rm = TRUE) - min(predictorOverplot, na.rm = TRUE))/(max(outcomeOverplot, na.rm = TRUE) - min(outcomeOverplot, na.rm = TRUE)),\n              xlim = c(0, max(predictorOverplot, na.rm = TRUE)), \n              ylim = c(0, max(outcomeOverplot, na.rm = TRUE))) +\n  theme_classic() +\n  theme(text = element_text(family = \"Gotham\"))\n\n\n\n\n\n\n\n\n\n\n\n5.11.2 Other implementation\nFrom: https://stats.stackexchange.com/questions/7899/complex-regression-plot-in-r\n\n5.11.2.1 ggplot2\n\n\nCode\ndf$x &lt;- df$predictorOverplot\ndf$y &lt;- df$outcomeOverplot\n\nxc &lt;- with(df, xyTable(x, y))\ndf2 &lt;- cbind.data.frame(x = xc$x, y = xc$y, number = xc$number)\ndf2$n &lt;- cut(df2$number, c(0,1.5,2.5,Inf), labels = c(1,2,4))\ndf.ell &lt;- as.data.frame(with(df, ellipse(cor(df$x, df$y, use = \"pairwise.complete.obs\"),\n                                         scale = c(sd(df$x, na.rm = TRUE), sd(df$y, na.rm = TRUE)),\n                                         centre = c(mean(df$x, na.rm = TRUE), mean(df$y, na.rm = TRUE)),\n                                         level = .95)))\n\nggplot(data = na.omit(df2), aes(x = x, y = y)) + \n  geom_point(aes(size = n), alpha = .6, color = rgb(0,0,.7,.5)) + \n  stat_smooth(data = df, method = \"loess\", se = FALSE, color = \"green\") + \n  stat_smooth(data = df, method = \"lm\", col = \"red\") +\n  geom_path(data = df.ell, colour = \"green\", size = 1) +\n  coord_cartesian(xlim = c(-1,60), ylim = c(-1,130))\n\n\n\n\n\n\n\n\n\n\n\n5.11.2.2 Base R\n\n\nCode\ndo.it &lt;- function(df, type=\"confidence\", ...) {\n  require(ellipse)\n  lm0 &lt;- lm(y ~ x, data=df)\n  xc &lt;- with(df, xyTable(x, y))\n  df.new &lt;- data.frame(x = seq(min(df$x), max(df$x), 0.1))\n  pred.ulb &lt;- predict(lm0, df.new, interval = type)\n  pred.lo &lt;- predict(loess(y ~ x, data = df), df.new)\n  plot(xc$x, xc$y, cex = xc$number*1/4, xlab = \"x\", ylab = \"y\", ...) #change number*X to change dot size\n  abline(lm0, col = \"red\")\n  lines(df.new$x, pred.lo, col=\"green\", lwd = 2)\n  lines(df.new$x, pred.ulb[,\"lwr\"], lty = 2, col = \"red\")\n  lines(df.new$x, pred.ulb[,\"upr\"], lty = 2, col = \"red\")    \n  lines(ellipse(cor(df$x, df$y), scale=c(sd(df$x),sd(df$y)), \n                centre = c(mean(df$x), mean(df$y)), level = .95), lwd = 2, col = \"green\")\n  invisible(lm0)\n}\n\ndf3 &lt;- na.omit(df[sample(nrow(df), nrow(df), rep = TRUE),])\ndf3$x &lt;- df3$predictorOverplot\ndf3$y &lt;- df3$outcomeOverplot\n\ndo.it(df3, pch = 19, col = rgb(0,0,.7,.5))",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#visually-weighted-regression",
    "href": "figures.html#visually-weighted-regression",
    "title": "Figures in R",
    "section": "5.12 Visually-Weighted Regression",
    "text": "5.12 Visually-Weighted Regression\n\n5.12.1 Default\n\n\nCode\nvwReg(outcome ~ predictor, data = df)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\n\n\n\n5.12.2 Shade\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = TRUE, spag = FALSE, show.lm = TRUE, show.CI = TRUE, bw = FALSE, B = 1000, quantize = \"continuous\")\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = TRUE, spag = FALSE, show.lm = TRUE, show.CI = TRUE, bw = FALSE, B = 1000, quantize = \"SD\")\n\n\n\n\n\n\n\n\n\n\n\n5.12.3 Spaghetti\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = FALSE, spag = TRUE, show.lm = TRUE, show.CI = TRUE, bw = FALSE, B = 1000)\n\n\n\n\n\n\n\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = FALSE, spag = TRUE, show.lm = FALSE, show.CI = FALSE, bw = FALSE, B = 1000)\n\n\n\n\n\n\n\n\n\n\n\n5.12.4 Black/white\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = TRUE, spag = FALSE, show.lm = TRUE, show.CI = TRUE, bw = TRUE, B = 1000, quantize = \"continuous\")\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\n\n\n\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = TRUE, spag = FALSE, show.lm = TRUE, show.CI = TRUE, bw = TRUE, B = 1000, quantize = \"SD\")\n\n\n\n\n\n\n\n\n\nCode\nvwReg(outcome ~ predictor, data = df, shade = FALSE, spag = TRUE, show.lm = TRUE, show.CI = TRUE, bw = TRUE, B = 1000, quantize = \"SD\")",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#univariate-distribution",
    "href": "figures.html#univariate-distribution",
    "title": "Figures in R",
    "section": "7.1 Univariate Distribution",
    "text": "7.1 Univariate Distribution\nUsed for: distribution of one numeric variable\n\n7.1.1 Gallery\n\nViolin chart: https://r-graph-gallery.com/violin.html\nDensity chart: https://r-graph-gallery.com/density-plot.html\nHistogram: https://r-graph-gallery.com/histogram.html\nBoxplot: https://r-graph-gallery.com/boxplot.html\nRidgeline chart: https://r-graph-gallery.com/ridgeline-plot.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#bivariate-scatterplots",
    "href": "figures.html#bivariate-scatterplots",
    "title": "Figures in R",
    "section": "7.2 Bivariate Scatterplots",
    "text": "7.2 Bivariate Scatterplots\nUsed for: association between two numeric variables\n\n7.2.1 Base R\nplot(x, y)\n\n\n7.2.2 ggplot2 package\nhttp://www.cookbook-r.com/Graphs/Scatterplots_(ggplot2)/\nggplot(data, aes(x, y)) +\n    geom_point()\n\n\n7.2.3 Gallery\n\nScatterplot: https://r-graph-gallery.com/scatterplot.html\nBubble plot: https://r-graph-gallery.com/bubble-chart.html\n2D density chart: https://r-graph-gallery.com/2d-density-chart.html\nHeatmap: https://r-graph-gallery.com/heatmap.html\n\n\n\n7.2.4 Add lines\n\nLine chart: https://r-graph-gallery.com/line-plot.html\nConnected scatterplot: https://r-graph-gallery.com/connected-scatterplot.html\nVisually-weighted regression: https://www.nicebread.de/visually-weighted-watercolor-plots-new-variants-please-vote/\nUse the vwReg() function from the petersenlab package: https://github.com/DevPsyLab/petersenlab/blob/main/R/vwReg.R\n\n\n\n7.2.5 Area\n\nArea chart: https://r-graph-gallery.com/area-chart.html\nStacked area chart: https://r-graph-gallery.com/stacked-area-graph.html\nStreamgraph: https://r-graph-gallery.com/streamgraph.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#bivariate-barplots",
    "href": "figures.html#bivariate-barplots",
    "title": "Figures in R",
    "section": "7.3 Bivariate Barplots",
    "text": "7.3 Bivariate Barplots\nUsed for: association between one categorical variable and one numeric variable (or for depicting the frequency of categories of a categorical variable)\n\n7.3.1 Gallery\n\nBarplot: https://r-graph-gallery.com/barplot.html\nLollipop plot: https://r-graph-gallery.com/lollipop-plot.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#multivariate-correlation-matrices",
    "href": "figures.html#multivariate-correlation-matrices",
    "title": "Figures in R",
    "section": "7.4 Multivariate Correlation Matrices",
    "text": "7.4 Multivariate Correlation Matrices\nUsed for: association between multiple numeric variables\nFor correlation matrices, I do the following:\n\nI use the lab’s cor.table() function (with type = \"manuscript\") from the petersenlab package to create a correlation matrix.\nI save the correlation matrix to a .csv file.\n\nFor example: https://research-git.uiowa.edu/PetersenLab/SRS/SRS-SelfRegulationDevelopment/-/blob/master/Analyses/selfRegulationDevelopment.Rmd#self-regulation-measures\n\nI open the .csv file in Excel and create the table in Excel that can be copied and pasted to Word/Powerpoint/etc.\n\n\n7.4.1 Correlograms\n\ncorrplot package: https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\ncorrgram package: https://cran.r-project.org/web/packages/corrgram/vignettes/corrgram_examples.html\n\n\n7.4.1.1 Gallery\nhttps://r-graph-gallery.com/correlogram.html\n\n\n\n7.4.2 Pairs panels\npsych package: https://personality-project.org/r/psych/help/pairs.panels.html\nI depict examples of correlograms and pairs panels here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#correlations-factorAnalysis",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#pathDiagrams",
    "href": "figures.html#pathDiagrams",
    "title": "Figures in R",
    "section": "7.5 Path Diagrams",
    "text": "7.5 Path Diagrams\nUsed for: SEM/CFA/path analysis\nIf you are just trying to visualize the results of a SEM model fitted using the lavaan package, I recommend the semPlot, lavaanPlot, or lavaangui packages in R. You can access a web application version of lavaangui here: https://lavaangui.org. You can see examples of semPlot here: http://sachaepskamp.com/semPlot/examples (archived at: https://perma.cc/W2V4-C9C8). You can see examples of lavaanPlot here: https://lavaanplot.alexlishinski.com/articles/intro_to_lavaanplot (archived at: https://perma.cc/ARZ7-MV24). You can see examples of lavaangui here: https://doi.org/10.1080/10705511.2024.2420678. You can see examples of my implementation here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/sem.html#semModelPathDiagram-sem\nIf you are trying to create a figure for a paper or poster, you might want something that you can draw and customize yourself. I use Adobe Illustrator for hand-drawn figures.\nYou can look at various options below:\n\nsemPlot package: https://doi.org/10.32614/CRAN.package.semPlot\nlavaanPlot package: https://doi.org/10.32614/CRAN.package.lavaanPlot\nlavaangui package: https://doi.org/10.32614/CRAN.package.lavaangui\nAdobe Illustrator\nDraw.io: https://www.drawio.com (formerly https://www.diagrams.net)\nOnyx: https://onyx-sem.com\nyworks: https://www.yworks.com\nMicrosoft Visio: https://www.microsoft.com/en-us/microsoft-365/visio/flowchart-software\nMicrosoft Powerpoint\nAMOS\nWarppls\nGraphviz: https://graphviz.org\n\nhas an R port—this is what we use for our study flowchart via the DiagrammeR package: https://rich-iannone.github.io/DiagrammeR/index.html\n\nhttps://app.diagrams.net\nhttps://github.com/jgraph/drawio-desktop/releases",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#interactive",
    "href": "figures.html#interactive",
    "title": "Figures in R",
    "section": "7.6 Interactive",
    "text": "7.6 Interactive\nGallery: https://r-graph-gallery.com/interactive-charts.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#animation",
    "href": "figures.html#animation",
    "title": "Figures in R",
    "section": "7.7 Animation",
    "text": "7.7 Animation\nGallery: https://r-graph-gallery.com/animation.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#d",
    "href": "figures.html#d",
    "title": "Figures in R",
    "section": "7.8 3D",
    "text": "7.8 3D\nGallery: https://r-graph-gallery.com/3d.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#sequential",
    "href": "figures.html#sequential",
    "title": "Figures in R",
    "section": "8.1 Sequential",
    "text": "8.1 Sequential\n\nhttps://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3\nviridis, cividis, etc.: https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#diverging",
    "href": "figures.html#diverging",
    "title": "Figures in R",
    "section": "8.2 Diverging",
    "text": "8.2 Diverging\n\nhttps://colorbrewer2.org/#type=diverging&scheme=BrBG&n=3",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#qualitativecategorical",
    "href": "figures.html#qualitativecategorical",
    "title": "Figures in R",
    "section": "8.3 Qualitative/Categorical",
    "text": "8.3 Qualitative/Categorical\n\nhttps://colorbrewer2.org/#type=qualitative&scheme=Accent&n=3\nPolychrome package: https://stackoverflow.com/a/62939405 (archived at https://perma.cc/3HWM-MMFS)\npals package: https://stackoverflow.com/a/41230685 (archived at https://perma.cc/WH56-HMVD)\n\nColor palettes for color-blindness:\n\nSafe palette from the rcartocolor package: https://stackoverflow.com/a/56066712 (archived at https://perma.cc/WUH5-F4Z7)\nOkabe Ito scale: https://stackoverflow.com/a/56066712 (archived at https://perma.cc/WUH5-F4Z7)\n\n\n8.3.1 8 Colors\n\n\nCode\n# From here: https://github.com/clauswilke/colorblindr/blob/master/R/palettes.R\n# Two color palettes taken from the article [\"Color Universal Design\" by Okabe and Ito](https://web.archive.org/web/20210108233739/http://jfly.iam.u-tokyo.ac.jp/color/)\n# The variant `palette_OkabeIto` contains a gray color, while `palette_OkabeIto_black` contains black instead\n\npalette_OkabeIto &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#999999\")\n\npie(rep(1, 8), col = palette_OkabeIto)\n\n\n\n\n\n\n\n\n\nCode\npalette_OkabeIto_black &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#000000\")\n\npie(rep(1, 8), col = palette_OkabeIto_black)\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 12 Colors\nSafe palette from the rcartocolor package: https://stackoverflow.com/a/56066712 (archived at https://perma.cc/WUH5-F4Z7)\n\n\nCode\n#from: scales::show_col(carto_pal(12, \"Safe\"))\n\nc12 &lt;- c(\n  \"#88CCEE\",\n  \"#CC6677\",\n  \"#DDCC77\",\n  \"#117733\",\n  \"#332288\",\n  \"#AA4499\",\n  \"#44AA99\",\n  \"#999933\",\n  \"#882255\",\n  \"#661100\",\n  \"#6699CC\",\n  \"#888888\"\n)\n\npie(rep(1, 12), col = c12)\n\n\n\n\n\n\n\n\n\n\n\n8.3.3 25 Colors\nhttps://stackoverflow.com/a/9568659 (archived at https://perma.cc/5ALZ-3AQD)\n\n\nCode\nc25 &lt;- c(\n  \"dodgerblue2\", \"#E31A1C\", # red\n  \"green4\",\n  \"#6A3D9A\", # purple\n  \"#FF7F00\", # orange\n  \"black\", \"gold1\",\n  \"skyblue2\", \"#FB9A99\", # lt pink\n  \"palegreen2\",\n  \"#CAB2D6\", # lt purple\n  \"#FDBF6F\", # lt orange\n  \"gray70\", \"khaki2\",\n  \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \"steelblue4\",\n  \"darkturquoise\", \"green1\", \"yellow4\", \"yellow3\",\n  \"darkorange4\", \"brown\"\n)\n\npie(rep(1, 25), col = c25)\n\n\n\n\n\n\n\n\n\n\n\n8.3.4 36 Colors\n\n\nCode\n# from: Polychrome::palette36.colors(36)\n\nc36 &lt;- c(\"#5A5156\",\"#E4E1E3\",\"#F6222E\",\"#FE00FA\",\"#16FF32\",\"#3283FE\",\"#FEAF16\",\"#B00068\",\"#1CFFCE\",\"#90AD1C\",\"#2ED9FF\",\"#DEA0FD\",\"#AA0DFE\",\"#F8A19F\",\"#325A9B\",\"#C4451C\",\"#1C8356\",\"#85660D\",\"#B10DA1\",\"#FBE426\",\"#1CBE4F\",\"#FA0087\",\n\"#FC1CBF\",\"#F7E1A0\",\"#C075A6\",\"#782AB6\",\"#AAF400\",\"#BDCDFF\",\"#822E1C\",\"#B5EFB5\",\"#7ED7D1\",\"#1C7F93\",\"#D85FF7\",\"#683B79\",\"#66B0FF\",\"#3B00FB\")\n\npie(rep(1, 36), col = c36)",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#maps",
    "href": "figures.html#maps",
    "title": "Figures in R",
    "section": "8.4 Maps",
    "text": "8.4 Maps\n\nhttps://github.com/thomasp85/scico",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "figures.html#footnotes",
    "href": "figures.html#footnotes",
    "title": "Figures in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAsk me to give you access.↩︎",
    "crumbs": [
      "About",
      "Figures"
    ]
  },
  {
    "objectID": "eegERP.html",
    "href": "eegERP.html",
    "title": "EEG and ERP Processing and Analysis",
    "section": "",
    "text": "Download MATLAB: https://its.uiowa.edu/matlab\n\nClick How to Install MatLab and follow the relevant instructions\nCreate a MatLab folder in your local directory. You will keep all of your MatLab related files in this folder.\n\nInstall the HAPPE pipeline: https://github.com/PINE-Lab/HAPPE\n\nClone the HAPPE repository to your GitHub account\nOpen the HAPPE folder\nOpen the HAPPE User Guide document\nRead through the HAPPE User Guide\nNavigate to the Setting up HAPPE section in the user guide and follow the instructions for setting up the HAPPE pipeline, including installation of add-ons and eeglab\nNote: This version of the HAPPE pipeline includes functionality from the HAPPE 2.0, HAPPE+ER, and HAPPILEE pipelines, so it is suitable for standard EEG processing as well as ERP and low-density EEG/ERP processing.\n\nInstall EP Toolkit: https://sourceforge.net/projects/erppcatoolkit/\n\nAfter downloading, copy the EP_Toolkit folder to your Matlab folder (in your local directory)\nIn the EP_toolkit folder:\n\nOpen EP_Toolkit\nOpen Documentation\nOpen tutorial\n\nIn the tutorial document, navigate to the Set Up section and follow the instructions for installing and setting up EP Toolkit and FieldTrip. Do NOT follow instructions for setting up EEGLAB. You have already set up your path to EEGLAB when you set up the HAPPE pipeline.\nYou should have the following subfolders in your MatLab folder:\n\nEP_Toolkit\nFieldtrip-[version number]\n\n\n\n\n\n\nWe have scripts for each task that can prepare files for the HAPPE Pipeline and/or manage the files outputted from HAPPE. These actions can be done manually as well, but the MATLAB scripts make the process more efficient. The scripts will also generate a “log” of all of the files processed through HAPPE to facilitate tracking of EEG data processing. The sections below detail the code used to perform these actions as well as the instructions for using the current scripts.\nNote: Before using the scripts/code detailed below, ensure that all filepaths used are in your MATLAB path collection. These may include:\n\nThe location where the automatic scripts are stored (for our lab, this is under /Data Processing/6. MATLAB EEG Pipeline)\nThe location where the HAPPE pre-processing script is stored\nThe location of the raw data (to be processed)\nThe location(s) of any intermediate files for processing (e.g., the updated .mff files that contain accuracy information in FishShark)\nThe location(s) for any files outputted by HAPPE and/or places you wish to use the script to move them to\n\nTo add a file path, click on Home/Set Path/Add Folder in MATLAB. You don’t need to specify the file name, just its containing folder.\n\n\n\n\n\n\n1a - Files for HAPPE\n1b - Manual Processing\n2 - Processed Files\n3 - Files for PCA\n4 - EPT Averages\n5 - PCA\n6 - PCA Components\n\n\n\n\nThe following section describes the MATLAB scripts used to manage HAPPE output files for the Oddball task data. If you are running HAPPE manually (i.e., without integrating these scripts), skip to the Oddball HAPPE inputs section.\n\nOpen the eegProccessingOddball.m file (accessible via Git here) in MATLAB\nUpdate all thresholds and filepaths in script file (must be done BEFORE running the script)\n\nIn the second section of our script file, we set our “threshold” for the minimum number of trials that need to be retained after pre-processing for a subject’s data to be eligible for PCA. Additional thresholds can also be set for things like number of channels retained, but these are not currently in use.\n\n% Set quality threshold parameters\ntrialCutoff = 10;\n\nWe also set environment variables with all of the filepaths that are relevant for managing HAPPE output files and tracking processed data. The following paths should be checked and updated as necessary to reflect the organization of processing on your computer.\n\npassPath is the location you wish to have files that meet or exceed the above-defined thresholds to be saved\nallPath is the location you wish to have ALL files outputted from HAPPE saved to (regardless of whether threshold is met or not)\nfailPath is the location you wish to have files that do not meet the above-defined thresholds to be copied to\n\n.mff files that do not meet threshold will be copied here as an indication that they should be processed manually to see if they meet threshold afterward\n\nsummPath is the location you wish to save the file that lists all files processed through HAPPE in the current batch\n\nWe currently use this to save the “processing log” to a location that all team members/computers have access to so it is easier to determine which files require processing when EEG data are not stored on a shared server\n\nmanPath is the location you wish to save the file that lists all files that do not meet the above-defined thresholds\n\nWe currently use this to save the “to process manually” list to a location that all team members/computers have access to so it is easier to determine which files require manual processing because the copied .mff file would not be acessible via our shared server\n\n\n\n% Set paths for file sorting\npassPath = 'V:\\Processing-Repo\\Folder Structure\\3 - Files for PCA'; %location for .txt output files\nallPath = 'V:\\Processing-Repo\\Folder Structure\\2 - Processed Files'; %location for all processed files to end up\nfailPath = 'V:\\Processing-Repo\\Folder Structure\\1b - Manual Processing'; %location to copy unsuccessful .mff to for manual process\n\n% Set path for processing summary\nsummPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processed Data Logs';\n\n% Set path for summary of files to process manually \nmanPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processing To Do\\Cases to Process Manually\\Lists of Cases by Batch';\nRun the HAPPE Pipeline\n\nThis first section is designed to rely on user input. Click “Run” on the MATLAB editor window with the file open to begin the process.\nA message will appear in the console prompting you to enter the filepath to the location of the HAPPE pre-processing file you wish to run\nOnce the path is entered, MATLAB will run the file\nNote: the filepath inputted should NOT be in quotes\n\n% Set path to HAPPE pre-processing script\nhappeRun = input('Enter the full path to the HAPPE pre-processing file:\\n&gt; ','s') ;\n\n% Call and run HAPPE pre-processing script\nrun(happeRun);\nEnter HAPPE inputs\n\nSee the following section for HAPPE user inputs for Oddball\n\n\nAfter the HAPPE inputs are entered, no more user interaction is required for the script to do its job. The HAPPE pipeline will run, and the remaining MATLAB code in the script file will evaluate the files outputted by HAPPE and move them to the appropriate locations based on this evaluation. See Oddball Post-HAPPE Steps for a detailed explanation of the code used to accomplish this.\n\n\n\n\nIf you are not using the MATLAB scripts described above, you must first open the HAPPE pipeline V4 script in MATLAB and click “Run”\n\nThis will be stored in the location to which you mapped the HAPPE Git repo (e.g. Documents/GitHub/HAPPE/1. pre-process/HAPPE_v4.m)\n\nUser Inputs\n\nEnter the path to the folder containing the dataset.\nSelect raw\nLoad pre-existing set of input parameter:\n\nN if this is your first time running data through the pipeline.\nY if you have decided on a set of parameters. Enter the path to the folder containing the input parameters.\n\nLow density data: N\nData type: task\nPerforming event-related potential (ERP) analysis: Y\nEnter the task onset tags\n\nTarget: tgt+\nFrequent: frq+\ndone\n\nDo multiple onset tags belong to a single condition? N\nFile format: 5\nAcquisition layout type: 2\nNumber of channels: 128\nDo you have additional type fields besides “code”? N\nSelect channels of interest: all\nFrequency of electrical noise in Hz: 60\nAre there any additional frequencies, (e.g., harmonics) to reduce? N\nLine Noise reduction method: notch\n\nLow cutoff: 59\nhigh cutoff: 61\n\nResample: N\nFilter\n\nLow Pass Cutoff: 30\nHigh Pass Cutoff: .1\n\nChoose a filter: fir\nBad Channel Detection: Y\n\nafter wavelet thresholding\n\nECGone: N\nWavelet Thresholding\n\ndefault\nThreshold rule: hard\n\nMuscIL: N\nSegmentation: Y\n\nStarting parameter for stimulus: -200\nEnding parameter for stimulus: 1000\nTask offset: 2\n\nBaseline Correction: Y\n\nBaseline Correction start: -200\nBaseline Correction end: 0\n\nInterpolation: Y\nSegment Rejection: Y\n\nSegment Rejection Method: amplitude\n\nminimum segment rejection threshold: -150\nmaximum segment rejection threshold: 150\nsegment rejection based on all channels or ROI: all\n\n\nRe-referencing: Y\n\nDoes your data contain a flatline or all zero reference channel? N\nre-referencing method: average\n\nSave format: 1\nVisualizations: N\nParameter file save name: default\n\n\n\n\n\nThis section details the actions performed by the MATLAB scripts once HAPPE is completed. These actions will take place automatically upon completion of the HAPPE pipeline. No user inputs or actions are necessary at this stage.\n\nExclude files that don’t have any output data from the dataset (containing filename and quality reports from HAPPE pipeline) that will be used to assess file quality\n\nThis step is important because “empty” files don’t play nicely with the code used to evaluate files that have some data in them (even if the data do not meet threshold)\nThis code relies on HAPPE’s quality data that remains in the MATLAB environment after the pipeline has finished.\n\n% Create a list of files that received some kind of error message\nnoTags = any(strcmp(dataQC, 'NO_TAGS'), 2);\noneRej = any(strcmp(dataQC, 'REJ_ONE_SEG'), 2);\nallRej = any(strcmp(dataQC, 'ALL_SEG_REJ'), 2);\nerror = any(strcmp(dataQC, 'ERROR'), 2);\nloadFail = any(strcmp(dataQC, 'LOAD_FAIL'), 2);\n\n% Combine filenames with quality data (for some reason, they are not automatically connected by HAPPE)\ndataQCNew = [FileNames', dataQC];\n\n% Remove all files in the above lists (those receiving errors) from the quality data\ndataQCNew(noTags | allRej | error | loadFail | oneRej, :) = [];\n\n% Create list of variable names for quality data\ndataQCnamesNew = [\"File\", dataQCnames];\n\n% Save the data as a table for ease of use in subsequent steps\nqcTable = cell2table(dataQCNew, 'VariableNames', dataQCnamesNew);\nIdentify the files that meet (or don’t meet) the threshold\n% Create a list of files (i.e., rows in the table) that meet threshold\nthresholdTest = qcTable.(\"Number_tgt+_Segs_Post-Seg_Rej\") &gt;= trialCutoff & qcTable.(\"Number_frq+_Segs_Post-Seg_Rej\") &gt;= trialCutoff;\n\n% Add a variable to the quality data table that include whether or not the file meet threshold\nqcTable.Test = thresholdTest;\nAdd an identifying variable to be used for data joining down the line\n\nThis variable is generated using its expected location in the file name (i.e., how many text characters “in” it is)\n\n% Generate IDs based on File variable\nidWaveQC = extractBefore(qcTable.File, 8);\n\n% Append ID variable to quality data\nqcTable.idWave = idWaveQC;\nCreate a subsetted dataset conataining only the idWave, file name, and “test results” (i.e., whether a given file meets the specified cutoff threshold for inclusion) variables\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\n\nfprintf('Assessment Complete \\n');\nGenerate a list of files outputted by HAPPE\n% Generate path for HAPPE pre-processing output (using the HAPPE environment variable from user's input of location of raw data for processing)\ninputPath = strcat(srcDir, \"\\5 - processed\");\n\n% Read in list of files outputted from HAPPE \npreprocessingOutput = dir(inputPath);\n\n% Remove \"empty\" rows \npreprocessingOutput = preprocessingOutput(~ismember({preprocessingOutput.name}, {'.', '..'}));\n\n% Save data as a table for ease of later use\npreprocessingOutput = struct2table(preprocessingOutput);\n\n% Subset to file info\nfileInfo = preprocessingOutput(:, [\"name\", \"folder\"]);\nSelect only desired files to be moved/copied\n\nCurrently, we don’t do anything with the “Individual Trial” files outputted by HAPPE. These files are quite large and take a long time to move, so it is more efficient to just remove them from the list of filenames and not worry about moving them anywhere.\n\n% Subset to desired files (AveOverTrial)\nfileSubset = fileInfo(contains(fileInfo.name, \"AveOverTrials\"), :);\nAdd condition, ID, and threshold-related variables to the file data\n\nNOTE: The value for Condition variable (i.e., “Target” or “Frequent”) should match the name of the condition-specific folders you wish the files to save to within the processing repo.\n\n% Generate list of IDs based on file name variable\nidWaveFS = extractBefore(fileSubset.name, 8);\n\n% Add ID list to file data\nfileSubset.idWave = idWaveFS;\n\n% Generate list of files belonging to each condition based on file name variable\ntarget = contains(fileSubset.name, \"tgt+\");\nfrequent = contains(fileSubset.name, \"frq+\");\n\n% Create empty variable for condition\nfileSubset.cond = cell(size(fileSubset, 1), 1);\n\n% Fill in condition variable based on the lists generated above\nfileSubset.cond(target) = {'Target'};\nfileSubset.cond(frequent) = {'Frequent'};\nfileSubset.cond(~target & ~frequent) = {'All'};\n\n% Join threshold test information\nfileTest = join(fileSubset, testInfo);\nPrepare data table with information about files that met the threshold\n\nThe data generated here are preparing to copy the .txt files outputted by HAPPE into a folder containing all files that are suitable for PCA\n\n% Create a separate table for only files that meet threshold\nmovingInfo = fileTest(fileTest.Test, :);\n\n% Create empty columns for filepath variables\nmovingInfo.destination = cell(size(movingInfo, 1), 1);\nmovingInfo.origin = cell(size(movingInfo, 1), 1);\nmovingInfo.processedTo = cell(size(movingInfo, 1), 1);\nmovingInfo.processedFrom = cell(size(movingInfo, 1), 1);\n\n% Generate file paths based on condition \nmovingInfo.destination = strcat({passPath}, \"\\\", movingInfo.cond, \"\\\", movingInfo.name);\nmovingInfo.origin = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nmovingInfo.processedTo = strcat({allPath}, \"\\\", movingInfo.name);\nmovingInfo.processedFrom = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nPrepare data table with information about files that do NOT meet the threshold\n\nThe data generated here are preparing to copy .mff files from the location of the raw files into a folder indicating the need for manual processing\n\n% Create a separate table for only files that did not meet threshold\nfailFiles = fileTest(~fileTest.Test, [\"File\", \"folder\", \"name\"]);\n\n% Create empty columns for filepath variables\nfailFiles.destination = cell(size(failFiles, 1), 1);\nfailFiles.origin = cell(size(failFiles, 1), 1);\nfailFiles.processedTo = cell(size(failFiles, 1), 1);\nfailFiles.processedFrom = cell(size(failFiles, 1), 1);\n\n% Generate filepaths based on ID and task\nfailFiles.destination = strcat({failPath}, \"\\\", failFiles.File);\nfailFiles.origin = strcat({srcDir}, \"\\\", failFiles.File);\nfailFiles.processedFrom = strcat(failFiles.folder, \"\\\", failFiles.name);\nfailFiles.processedTo = strcat({allPath}, \"\\\", failFiles.name);\n\nGenerate environment variables that correspond to the column index of relevant variables for file sorting\n\nNote that the very last line of this code defines the varaible(s) to exclude from the HAPPE outputted files. This variable must be stripped from the data before saving them, because the presence of the extra variable makes the file incompatible with EP Toolkit’s PCA process.\n\n% Define column locations for each filepath variable\n\n% For files that meet threshold:\ntoCol = find(strcmp(movingInfo.Properties.VariableNames, \"destination\"));\nfromCol = find(strcmp(movingInfo.Properties.VariableNames, \"origin\"));\nprocColto = find(strcmp(movingInfo.Properties.VariableNames, \"processedTo\"));\nprocColfrom = find(strcmp(movingInfo.Properties.VariableNames, \"processedFrom\"));\n\n% For files that do not meet threshold\nrawCol = find(strcmp(failFiles.Properties.VariableNames, \"origin\"));\nmanCol = find(strcmp(failFiles.Properties.VariableNames, \"destination\"));\nfailProcColto = find(strcmp(failFiles.Properties.VariableNames, \"processedTo\"));\nfailProcColFrom = find(strcmp(failFiles.Properties.VariableNames, \"processedFrom\"));\n\n% Define variable to exclude\nextraVar = 'Time';\nUse a loop to process all files that met threshold\n\nFor each row in the “movingInfo” dataset, the loop will:\n\nIdentify the origin and destination paths\nRead in the HAPPE output file\nRemove the extra variable\nSave the “cleaned” data in the appropriate folder (without variable names, as required by EP Toolkit)\n\n\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, fromCol};\n    pathTo = movingInfo{row, toCol};\n    % Read in the data\n    rawTable = readtable(pathFrom);\n    % Remove extra column (Time)\n    cleanTable = rawTable{:, ~strcmp(rawTable.Properties.VariableNames, extraVar)};\n    % Save without headers\n    writematrix(cleanTable, pathTo, 'Delimiter', '\\t')\nend\n\nUse a loop to copy raw (.mff) files into a location that stores files requiring manual processing\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, rawCol};\n    pathTo = failFiles{row, manCol};\n    % Copy file \n    copyfile(pathFrom, pathTo)\nend\nUse a set of loops to copy all HAPPE output files into a folder intended to house all output (whether threshold is met or not)\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, procColfrom};\n    pathTo = movingInfo{row, procColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\n\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, failProcColFrom};\n    pathTo = failFiles{row, failProcColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\nGenerate a .txt file listing all processed .mff files\n\nThis file will contain a list of all raw files (e.g., 1111_22_oddball.mff) and save the list to the specified location (summPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past logs\nThe list of processed files is generated using an environment object (FileNames) that the HAPPE pipeline creates that lists all files inputted to the pipeline\n\n% Create a table from HAPPE FileNames cell array\nprocessedList = cell2table(FileNames(:));\n\n% Rename file variable from default\nprocessedList = renamevars(processedList, {'Var1'}, {'File'});\n\n% Save current date as a string variable \ntoday = string(date());\n\n% Save time as a string variable, replacing \":\" with \"_\" so that file can be written \ntime = strrep(datestr(now, 'HH:MM:SS:FFF'), ':', \"_\");\n\n% Generate file name to include current date and time \nlistFile = strcat(\"\\oddballProcessed_\", today, \"_\", time);\n\n% Generate full path including file name\nsummPathFull = strcat(summPath, listFile);\n\n% Write table to specified location\nwritetable(processedList, summPathFull);\nGenerate a .txt file listing all of the files that did not meet threshold to go through subsequent processing\n\nThis file will contain a list of all raw files (e.g., 1111_22_oddball.mff) that did not meet threshold and save the list to the specified location (manPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past lists\nThis list of file to process manually is generated using the environment object FileNames that HAPPE creates; any files listed in FileNames but not in the dataset of files that pass threshold make up the manualList\n\n% Compare movingInfo with FileNames to isolate files that don't meet threshold\nmanualList = cell2table(FileNames(~ismember(FileNames', movingInfo.File)));\nmanualList = renamevars(manualList, {'Var1'}, {'File'});\n\n% Generate file name to include current date\nmanFile = strcat(\"\\oddballManual_\", today, \"_\", time);\n\n% Generate full path including filename\nmanPathFull = strcat(manPath, manFile);\n\n% Write table to specified location\nwritetable(manualList, manPathFull);\n\n\n\n\n\n\n\n\n0 - Raw\n1a - Files for HAPPE\n1b - Manual Processing\n2 - Processed Files\n3 - Files for PCA\n4 - EPT Averages\n5 - PCA\n6 - PCA Components\n\n\n\n\nThe following section describes the MATLAB scripts used to manage HAPPE input and output files for the FishShark task data. If you are running HAPPE manually (i.e., without integrating these scripts), skip to the FishShark HAPPE inputs section.\nThe first two steps described in this section are the same as those described for Oddball processing. Processing actions specific to FishSharks task begin in Step 3.\n\nOpen the eegProccessingFishshark.m file (accessible via Git here) in MATLAB\nUpdate all thresholds and filepaths in script file (must be done BEFORE running the script)\n\nIn the fifth section of our script file, we set our “threshold” for the minimum number of trials that need to be retained after pre-processing for a subject’s data to be eligible for PCA. Additional thresholds can also be set for things like number of channels retained, but these are not currently in use.\nNote that setting these parameters occurs later in the FishSharks script than the one for Oddball; this is because FishSharks files require additional processing before they are ready for HAPPE (see step 3 for details)\n\n% Set quality threshold parameters\ntrialCutoff = 10;\n\nWe also set environment variables with all of the filepaths that are relevant for managing HAPPE output files and tracking processed data. The following paths should be checked and updated as necessary to reflect the organization of processing on your computer.\n\npassPath is the location you wish to have files that meet or exceed the above-defined thresholds to be saved\nallPath is the location you wish to have ALL files outputted from HAPPE saved to (regardless of whether threshold is met or not)\nfailPath is the location you wish to have files that do not meet the above-defined thresholds to be copied to\n\n.mff files that do not meet threshold will be copied here as an indication that they should be processed manually to see if they meet threshold afterward\n\nsummPath is the location you wish to save the file that lists all files processed through HAPPE in the current batch\n\nWe currently use this to save the “processing log” to a location that all team members/computers have access to so it is easier to determine which files require processing when EEG data are not stored on a shared server\n\nmanPath is the location you wish to save the file that lists all files that do not meet the above-defined thresholds\n\nWe currently use this to save the “to process manually” list to a location that all team members/computers have access to so it is easier to determine which files require manual processing because the copied .mff file would not be acessible via our shared server\n\n\n\n% Set paths for file sorting\npassPath = 'V:\\Processing-Repo\\Folder Structure\\3 - Files for PCA'; %location for .txt output files\nallPath = 'V:\\Processing-Repo\\Folder Structure\\2 - Processed Files'; %location for all processed files to end up\nfailPath = 'V:\\Processing-Repo\\Folder Structure\\1b - Manual Processing'; %location to copy unsuccessful .mff to for manual process\n\n% Set path for processing summary\nsummPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processed Data Logs';\n\n% Set path for summary of files to process manually \nmanPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processing To Do\\Cases to Process Manually\\Lists of Cases by Batch';\nUpdate raw (.mff) files’ condition tags with accuracy information\n\nUnlike the Passive Oddball task, FishSharks trials can be either correct or incorrect. Whether or not a trial was “responded to” correctly is relevant to the nature of the extracted ERP. Because the event tags in our .mff files do not inherently contain information about whether each trial was responded to correctly, we need to add it ourselves. This process can be done manually in NetStation, but it can also be automated using the MATLAB code detailed below.\nNOTE: This code requires eeglab. Before running the code, open eeglab in your MATLAB session by typing eeglab into the console. You can close it as soon as it opens, but this step ensures that eeglab is loaded into your current session and helps prevent the subsequent code from erroring out.\n\n\nSet the filepaths for raw and updated .mff files\n\nThis section is set to rely on user inputs. Click “Run” in the MATLAB editor to start the processing.\nA message will appear in the console prompting you to enter two filepaths: the first is the location of the raw .mff files, and the second is the location you would like the updated files to save in.\nNote: the filepaths inputted should NOT be in quotes\n\n% User input for location of raw files\npathRaw = input('Enter the full path to the folder containing the raw files:\\n&gt; ','s');\n\n% User input for destination of subsetted files\npathSub = input('Enter the full path to the folder in which to save the updated files:\\n&gt; ','s');\nAt this point, there will be no user input/actions necessary until all of the .mff files in the pathRaw directory have been updated and saved into the pathSub directory. The code that asks the user for the path to HAPPE will run when that process has finished. The following section will describe the code used to automate the process of updating .mff event tags to include accuracy information at the trial level. Move on to Step 4 (Run the HAPPE Pipeline) when the process has completed.\nGather and manage information from the directory housing the raw (.mff) files\n% Have MATLAB gather a list of raw files housed in specified location (pathRaw)\ndirInfo = struct2table(dir(pathRaw));\n\n% Remove blank rows\nnoName = strcmp(dirInfo.name, '.') | strcmp(dirInfo.name, '..');\ndirInfo(noName, :) = [];\nGenerate variables necessary for managing raw and updated files\n\nThis code will generate the full filepaths (including file name) necessary for reading in each .mff file and saving its updated counterpart\nIt will also generate an ID variable for joining purposes based on the expected location of the subject name in the name of each file\n\n% Generate ID variable\ndataFiles = dirInfo(:, \"name\");\n\n% Add ID variable to file data\ndataFiles.ID = extractBefore(dataFiles.name, 8);\n\n% Generate path to read raw data\nrawPaths = dataFiles;\nrawPaths.path = strcat({pathRaw}, \"/\", dirInfo.name);\nrawPaths = rawPaths(:, [\"ID\", \"path\"]);\n\n% Generate path to save updated versions of the data (containing accuracy info at trial level)\nsubPaths = dataFiles;\nsubPaths.path = strcat({pathSub}, \"/\", subPaths.ID, \"_sub_fishshark.mff\");\nsubPaths = subPaths(:, [\"ID\", \"path\"]);\n\n% Join filepath datatables\nmergePaths = join(rawPaths, subPaths, 'Keys', {'ID'})\nUse a loop to update the event tags in each .mff file to reflect accuracy of response\n\nFor every file included in the mergePaths dataset, MATLAB will perform the following actions:\n\nSet environment variables representing the path to read in the original .mff file and save its updated counterpart (updates with each iteration of the loop)\nRead in the .mff file\nExtract the “event” information from the .mff file\nEvaluate whether there is usable data in the file\n\nThis step prevents the code from erroring out if a subject did not make it past the practice trials\nIf the evaluation determines that there is NOT usable data in the present file, the loop will jump to the next file\n\nCreate a table containing response information (response vs no response and reaction time) and condition information (go vs no-go) at the trial level\nEvaluate each trial to determine whether the response was correct or incorrect\n\nFor No-Go trials, responses are CORRECT if there was no response (incorrect if subject did respond)\nFor Go trials, responses are CORRECT if there was a response AND subject’s reaction time was at least 200 ms (incorrect if subject did not respond OR if subject responded too quickly for response to be considered “valid”)\n\nUpdate the event tags in the .mff file to contain a “c” for correct trials and an “x” for incorrect trials\n\nThe c/x indicator will be appended to the front of the existing event tag (e.g., Go++ will become cGo++)\n\nExport an updated version of the .mff file with accuracy information to the specified location (pathSub)\n\nNOTE: When exporting EEG data from MATLAB, it is absolutely necessary for the data to be organized in order of event latency. If the rows are out of order, latency errors may be introduced.\n\nCorrect row order can be achieved in .mff filetypes by sorting according to either the latency column or the urevent column (as shown below)\n\n\nfor row = 1:height(mergePaths)\n    % Specify paths\n    rawFolder = mergePaths{row, \"path_rawPaths\"}\n    subFolder = mergePaths{row, \"path_subPaths\"}\n\n    % Read in EEG data\n    EEGraw = pop_mffimport(char(rawFolder), 'code')\n\n    % Create table from \"event\" field of raw data\n    EEGevent = struct2table(EEGraw.event)\n\n    % Check for the existence of usable rows\n    checkVars = strcmp(EEGevent.Properties.VariableNames, 'mffkey_cel')\n\n    % Skip files without necessary variables\n    if max(checkVars) == 0\n        continue\n    end\n\n    % Create table without practice/training trials\n    keepRows = strcmp(EEGevent.mffkey_cel, '4')\n    EEGsub  = EEGevent(keepRows, :)\n\n    % Check for the existence of usable rows\n    checkRows = max(keepRows)\n\n    % Skip files with no usable rows\n    if checkRows == 0\n        continue\n    end\n\n    % Get response info at trial level\n    EEGresp = table(EEGsub.mffkey_obs, EEGsub.mffkey_eval, EEGsub.mffkey_rtim)\n    EEGresp = rmmissing(EEGresp)\n    EEGresp = renamevars(EEGresp, [\"Var1\", \"Var2\", \"Var3\"], [\"Trial\", \"Eval\", \"RTime\"])\n\n    % Get condition info at trial level\n    EEGconds = table(EEGsub.mffkey_obs, EEGsub.type)\n    EEGconds = renamevars(EEGconds, [\"Var1\", \"Var2\"], [\"Trial\", \"Cond\"])\n    keepConds = strcmp(EEGconds.Cond, 'Go++') | strcmp(EEGconds.Cond, 'NG++')\n    EEGcond = EEGconds(keepConds, :)\n\n    % Merge datasets\n    EEGtrials = join(EEGcond, EEGresp)\n\n    EEGtrials.RTime = cellfun(@str2num, EEGtrials.RTime)\n    % Evaluate trials for correct-ness of response\n    correct = strcmp(EEGtrials.Cond, 'Go++') & strcmp(EEGtrials.Eval, '1') & EEGtrials.RTime &gt; 200 | strcmp(EEGtrials.Cond, 'NG++') & strcmp(EEGtrials.Eval, '0')\n    EEGtrials.Acc = correct\n\n\n    % Create new code tags including accuracy information\n    EEGtrials.newCode(EEGtrials.Acc & strcmp(EEGtrials.Cond, 'Go++')) = {'cGo++'}\n    EEGtrials.newCode(~EEGtrials.Acc & strcmp(EEGtrials.Cond, 'Go++')) = {'xGo++'}\n    EEGtrials.newCode(EEGtrials.Acc & strcmp(EEGtrials.Cond, 'NG++')) = {'cNG++'}\n    EEGtrials.newCode(~EEGtrials.Acc & strcmp(EEGtrials.Cond, 'NG++')) = {'xNG++'}\n\n    % Subset information for merge\n    EEGmerge = EEGtrials(:, {'Trial', 'Cond', 'newCode'})\n\n    % Prep key in original data\n    EEGevent.key = strcat(EEGevent.mffkey_obs, EEGevent.type)\n\n    % Prep key in merge data\n    EEGmerge.key = strcat(EEGmerge.Trial, EEGmerge.Cond)\n    EEGmerge = EEGmerge(:, {'key', 'newCode'})\n\n    % Merge new codes with event table\n    EEGnew = outerjoin(EEGevent, EEGmerge)\n\n    % Replace codes where new code is needed\n    EEGnew.code(~strcmp(EEGnew.newCode, '')) = EEGnew.newCode(~strcmp(EEGnew.newCode, ''))\n    EEGnew.type(~strcmp(EEGnew.newCode, '')) = EEGnew.newCode(~strcmp(EEGnew.newCode, ''))\n\n    % Arrange table in order of event (CRUCIAL for correct export)\n    EEGnew = sortrows(EEGnew, 'urevent')\n\n    % Convert table back to struct and restore original dimensions\n    EEGnew = table2struct(EEGnew(:, 1:29))\n    EEGnew = reshape(EEGnew, [1, height(EEGnew)])\n\n    % Replace event table(s) in original struct\n    EEGraw.event = EEGnew\n    EEGraw.urevent = table2struct(struct2table(EEGnew(:, 1:28)))\n\n    % Export updated file\n    pop_mffexport(EEGraw, char(subFolder))\nend \n\nRun the HAPPE Pipeline\n\nThis section is designed to rely on user input.\nWhen the prior steps have finsihed, a message will appear in the console prompting you to enter the filepath to the location of the HAPPE pre-processing file you wish to run\nOnce the path is entered, MATLAB will run the file\nNote: the filepath inputted should NOT be in quotes\n\n% Set path to HAPPE pre-processing script\nhappeRun = input('Enter the full path to the HAPPE pre-processing file:\\n&gt; ','s') ;\n\n% Call and run HAPPE pre-processing script\nrun(happeRun);\nEnter HAPPE inputs\n\nSee the following section for HAPPE inputs for the Fish/Shark task\n\n\nAfter the HAPPE inputs are entered, no more user interaction is required for the script to do its job. The HAPPE pipeline will run, and the remaining MATLAB code in the script file will evaluate the files outputted by HAPPE and move them to the appropriate locations based on this evaluation. See FishShark Post-HAPPE Steps for a detailed explanation of the code used to accomplish this.\n\n\n\n\nIf you are not using the MATLAB scripts described above, you must first open the HAPPE pipeline V4 script in MATLAB and click “Run”\n\nThis will be stored in the location to which you mapped the HAPPE Git repo (e.g. Documents/GitHub/HAPPE/1. pre-process/HAPPE_v4.m)\n\nUser Inputs\n\nEnter the path to the folder containing the dataset.\nSelect raw\nLoad pre-existing set of input parameter:\n\nN if this is your first time running data through the pipeline.\nY if you have decided on a set of parameters. Enter the path to the folder containing the input parameters.\n\nLow density data: N\nData type: task\nPerforming event-related potential (ERP) analysis: Y\nEnter the task onset tags\n\nCorrect Go: cGo++\nIncorrect Go: xGo++\nCorrect NoGo: cNG++\nIncorrect NoGo: xNG++\ndone\n\nDo multiple onset tags belong to a single condition? N\nFile format: 5\nAcquisition layout type: 2\nNumber of channels: 128\nDo you have additional type fields besides “code”? N\nSelect channels of interest: all\nFrequency of electrical noice in Hz: 60\nAre there any additional frequencies, (e.g., harmonics) to reduce? N\nLine Noise reduction method: notch\n\nLow cutoff: 59\nhigh cutoff: 61\n\nResample: N\nFilter\n\nLow Pass Cutoff: 30\nHigh Pass Cutoff: .1\n\nChoose a filter: fir\nBad Channel Detection: Y\n\nafter wavelet thresholding\n\nECGone: N\nWavelet Thresholding\n\ndefault\nThreshold rule: hard\n\nMuscIL: N\nSegmentation: Y\n\nStarting parameter for stimulus: -200\nEnding parameter for stimulus: 1000\nTask offset: 17\n\nBaseline Correction: Y\n\nBaseline Correction start: -200\nBaseline Correction end: 0\n\nInterpolation: Y\nSegment Rejection: Y\n\nSegment Rejection Method: amplitude\n\nminimum segment rejection threshold: -150\nmaximum segment rejection threshold: 150\nsegment rejection based on all channels or ROI: all\n\n\nRe-referencing: Y\n\nDoes your data contain a flatline or all zero reference channel? N\nre-referencing method: average\n\nSave format: 1\nVisualizations: N\nParameter file save name: default\n\n\n\n\n\nThis section details the actions performed by the MATLAB scripts once HAPPE is completed. These actions will take place automatically upon completion of the HAPPE pipeline. No user inputs or actions are necessary at this stage.\n\nExclude files that don’t have any output data from the dataset (containing filename and quality reports from HAPPE pipeline) that will be used to assess file quality\n\nThis step is important because “empty” files don’t play nicely with the code used to evaluate files that have some data in them (even if the data do not meet threshold)\nThis code relies on HAPPE’s quality data that remains in the MATLAB environment after the pipeline has finished.\n\n% Create a list of files that received some kind of error message\nnoTags = any(strcmp(dataQC, 'NO_TAGS'), 2);\nallRej = any(strcmp(dataQC, 'ALL_SEG_REJ'), 2);\noneRej = any(strcmp(dataQC, 'REJ_ONE_SEG'), 2);\nerror = any(strcmp(dataQC, 'ERROR'), 2);\nloadFail = any(strcmp(dataQC, 'LOAD_FAIL'), 2);\n\n% Combine filenames with quality data (for some reason, they are not automatically connected by HAPPE)\ndataQCNew = [FileNames', dataQC];\n\n% Remove all files in the above lists (those receiving errors) from the quality data\ndataQCNew(noTags | allRej | error | loadFail | oneRej, :) = [];\n\n% Create list of variable names for quality data\ndataQCnamesNew = [\"File\", dataQCnames];\n\n% Save the data as a table for ease of use in subsequent steps\nqcTable = cell2table(dataQCNew, 'VariableNames', dataQCnamesNew);\n\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\nIdentify the files that meet (or don’t meet) the threshold\n% Create a list of files (i.e., rows in the table) that meet threshold\nthresholdTest = qcTable.(\"Number_cNG++_Segs_Post-Seg_Rej\") &gt;= trialCutoff & qcTable.(\"Number_cGo++_Segs_Post-Seg_Rej\") &gt;= trialCutoff;\n\n% Add a variable to the quality data table that include whether or not the file meet threshold\nqcTable.Test = thresholdTest;\nAdd an identifying variable to be used for data joining down the line\n\nThis variable is generated using its expected location in the file name (i.e., how many text characters “in” it is)\n\n% Generate IDs based on File variable\nidWaveQC = extractBefore(qcTable.File, 8);\n\n% Append ID variable to quality data\nqcTable.idWave = idWaveQC;\nCreate a subsetted dataset conataining only the idWave, file name, and “test” results (i.e., whether a given file meets the specified cutoff threshold for inclusion)\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\n\nfprintf('Assessment Complete \\n');\nGenerate a list of files outputted by HAPPE\n% Generate path for HAPPE pre-processing output (using the HAPPE environment variable from user's input of location of raw data for processing)\ninputPath = strcat(srcDir, \"\\5 - processed\");\n\n% Read in list of files outputted from HAPPE \npreprocessingOutput = dir(inputPath);\n\n% Remove \"empty\" rows \npreprocessingOutput = preprocessingOutput(~ismember({preprocessingOutput.name}, {'.', '..'}));\n\n% Save data as a table for ease of later use\npreprocessingOutput = struct2table(preprocessingOutput);\n\n% Subset to file info\nfileInfo = preprocessingOutput(:, [\"name\", \"folder\"]);\nSelect only desired files to be moved/copied\n\nCurrently, we don’t do anything with the “Individual Trial” files outputted by HAPPE. These files are quite large and take a long time to move, so it is more efficient to just remove them from the list of filenames and not worry about moving them anywhere.\n\n% Subset to desired files (AveOverTrial)\nfileSubset = fileInfo(contains(fileInfo.name, \"AveOverTrials\"), :);\nAdd condition, ID, and threshold-related variables to the file data\n\nNOTE: The value for Condition variable (i.e., “cGo” or “cNoGo”) should match the name of the condition-specific folders you wish the files to save to within the processing repo.\n\n% Generate list of IDs based on file name variable\nidWaveFS = extractBefore(fileSubset.name, 8);\n\n% Add ID list to file data\nfileSubset.idWave = idWaveFS;\n\n% Generate list of files belonging to each condition based on file name variable\ncGo = contains(fileSubset.name, \"cGo++\");\ncNoGo = contains(fileSubset.name, \"cNG++\");\nxGo = contains(fileSubset.name, \"xGo++\");\nxNoGo = contains(fileSubset.name, \"xNG++\");\n\n% Create empty variable for condition\nfileSubset.cond = cell(size(fileSubset, 1), 1);\n\n% Fill in condition variable based on the lists generated above\nfileSubset.cond(cGo) = {'cGo'};\nfileSubset.cond(cNoGo) = {'cNoGo'};\nfileSubset.cond(xGo) = {'xGo'};\nfileSubset.cond(xNoGo) = {'xNoGo'};\nfileSubset.cond(~cGo & ~cNoGo & ~xGo & ~xNoGo) = {'All'};\n\n% Join threshold test information\nfileTest = join(fileSubset, testInfo);\nPrepare data table with information about files that met the threshold\n\nThe data generated here are preparing to copy the .txt files outputted by HAPPE into a folder containing all files that are suitable for PCA\n\n% Create a separate table for only files that meet threshold\nmovingInfo = fileTest(fileTest.Test, :);\n\n% Create empty columns for filepath variables\nmovingInfo.destination = cell(size(movingInfo, 1), 1);\nmovingInfo.origin = cell(size(movingInfo, 1), 1);\nmovingInfo.processedTo = cell(size(movingInfo, 1), 1);\nmovingInfo.processedFrom = cell(size(movingInfo, 1), 1);\n\n% Generate file paths based on condition \nmovingInfo.destination = strcat({passPath}, \"\\\", movingInfo.cond, \"\\\", movingInfo.name);\nmovingInfo.origin = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nmovingInfo.processedTo = strcat({allPath}, \"\\\", movingInfo.name);\nmovingInfo.processedFrom = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nPrepare data table with information about files that do NOT meet the threshold\n\nThe data generated here are preparing to copy .mff files from the location of the raw files into a folder indicating the need for manual processing\n\n% Create a separate table for only files that did not meet threshold\nfailFiles = fileTest(~fileTest.Test, [\"File\", \"folder\", \"name\"]);\n\n% Create empty columns for filepath variables\nfailFiles.destination = cell(size(failFiles, 1), 1);\nfailFiles.origin = cell(size(failFiles, 1), 1);\nfailFiles.processedTo = cell(size(failFiles, 1), 1);\nfailFiles.processedFrom = cell(size(failFiles, 1), 1);\n\n% Generate filepaths based on ID and task\nfailFiles.destination = strcat({failPath}, \"\\\", failFiles.File);\nfailFiles.origin = strcat({srcDir}, \"\\\", failFiles.File);\nfailFiles.processedFrom = strcat(failFiles.folder, \"\\\", failFiles.name);\nfailFiles.processedTo = strcat({allPath}, \"\\\", failFiles.name);\n\nGenerate environment variables that correspond to the column index of relevant variables for file sorting\n\nNote that the very last line of this code defines the varaible(s) to exclude from the HAPPE outputted files. This variable must be stripped from the data before saving them, because the presence of the extra variable makes the file incompatible with EP Toolkit’s PCA process.\n\n% Define column locations for each filepath variable\n\n% For files that meet threshold:\ntoCol = find(strcmp(movingInfo.Properties.VariableNames, \"destination\"));\nfromCol = find(strcmp(movingInfo.Properties.VariableNames, \"origin\"));\nprocColto = find(strcmp(movingInfo.Properties.VariableNames, \"processedTo\"));\nprocColfrom = find(strcmp(movingInfo.Properties.VariableNames, \"processedFrom\"));\n\n% For files that do not meet threshold\nrawCol = find(strcmp(failFiles.Properties.VariableNames, \"origin\"));\nmanCol = find(strcmp(failFiles.Properties.VariableNames, \"destination\"));\nfailProcColto = find(strcmp(failFiles.Properties.VariableNames, \"processedTo\"));\nfailProcColFrom = find(strcmp(failFiles.Properties.VariableNames, \"processedFrom\"));\n\n% Define variable to exclude\nextraVar = 'Time';\nUse a loop to process all files that met threshold\n\nFor each row in the “movingInfo” dataset, the loop will:\n\nIdentify the origin and destination paths\nRead in the HAPPE output file\nRemove the extra variable\nSave the “cleaned” data in the appropriate folder (without variable names, as required by EP Toolkit)\n\n\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, fromCol};\n    pathTo = movingInfo{row, toCol};\n    % Read in the data\n    rawTable = readtable(pathFrom);\n    % Remove extra column (Time)\n    cleanTable = rawTable{:, ~strcmp(rawTable.Properties.VariableNames, extraVar)};\n    % Save without headers\n    writematrix(cleanTable, pathTo, 'Delimiter', '\\t')\nend\n\nUse a loop to copy raw (.mff) files into a location that stores files requiring manual processing\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, rawCol};\n    pathTo = failFiles{row, manCol};\n    % Copy file \n    copyfile(pathFrom, pathTo)\nend\nUse a set of loops to copy all HAPPE output files into a folder intended to house all output (whether threshold is met or not)\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, procColfrom};\n    pathTo = movingInfo{row, procColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\n\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, failProcColFrom};\n    pathTo = failFiles{row, failProcColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\nGenerate a .txt file listing all processed .mff files\n\nThis file will contain a list of all raw files (e.g., 1111_22_fishshark.mff) and save the list to the specified location (summPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past logs\nThe list of processed files is generated using an environment variable that the HAPPE pipeline creates that lists all files inputted to the pipeline\n\n% Create a table from HAPPE FileNames cell array\nprocessedList = cell2table(FileNames(:));\n\n% Rename file variable from default\nprocessedList = renamevars(processedList, {'Var1'}, {'File'});\n\n% Save current date as a string variable \ntoday = string(date());\n\n% Save time as a string variable, replacing \":\" with \"_\" so that file can be written \ntime = strrep(datestr(now, 'HH:MM:SS:FFF'), ':', \"_\");\n\n% Generate file name to include current date and time \nlistFile = strcat(\"\\oddballProcessed_\", today, \"_\", time);\n\n% Generate full path including file name\nsummPathFull = strcat(summPath, listFile);\n\n% Write table to specified location\nwritetable(processedList, summPathFull);\nGenerate a .txt file listing all of the files that did not meet threshold to go through subsequent processing\n\nThis file will contain a list of all raw files (e.g., 1111_22_fishshark.mff) that did not meet threshold and save the list to the specified location (manPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past lists\nThis list of file to process manually is generated using the environment object FileNames that HAPPE creates; any files listed in FileNames but not in the dataset of files that pass threshold make up the manualList\n\n% Compare movingInfo with FileNames to isolate files that don't meet threshold\nmanualList = cell2table(FileNames(~ismember(FileNames', movingInfo.File)));\nmanualList = renamevars(manualList, {'Var1'}, {'File'});\n\n% Generate file name to include current date\nmanFile = strcat(\"\\fishSharkManual_\", today, \"_\", time);\n\n% Generate full path including filename\nmanPathFull = strcat(manPath, manFile);\n\n% Write table to specified location\nwritetable(manualList, manPathFull);",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#initial",
    "href": "eegERP.html#initial",
    "title": "EEG and ERP Processing and Analysis",
    "section": "",
    "text": "Download MATLAB: https://its.uiowa.edu/matlab\n\nClick How to Install MatLab and follow the relevant instructions\nCreate a MatLab folder in your local directory. You will keep all of your MatLab related files in this folder.\n\nInstall the HAPPE pipeline: https://github.com/PINE-Lab/HAPPE\n\nClone the HAPPE repository to your GitHub account\nOpen the HAPPE folder\nOpen the HAPPE User Guide document\nRead through the HAPPE User Guide\nNavigate to the Setting up HAPPE section in the user guide and follow the instructions for setting up the HAPPE pipeline, including installation of add-ons and eeglab\nNote: This version of the HAPPE pipeline includes functionality from the HAPPE 2.0, HAPPE+ER, and HAPPILEE pipelines, so it is suitable for standard EEG processing as well as ERP and low-density EEG/ERP processing.\n\nInstall EP Toolkit: https://sourceforge.net/projects/erppcatoolkit/\n\nAfter downloading, copy the EP_Toolkit folder to your Matlab folder (in your local directory)\nIn the EP_toolkit folder:\n\nOpen EP_Toolkit\nOpen Documentation\nOpen tutorial\n\nIn the tutorial document, navigate to the Set Up section and follow the instructions for installing and setting up EP Toolkit and FieldTrip. Do NOT follow instructions for setting up EEGLAB. You have already set up your path to EEGLAB when you set up the HAPPE pipeline.\nYou should have the following subfolders in your MatLab folder:\n\nEP_Toolkit\nFieldtrip-[version number]",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#matlabScripts",
    "href": "eegERP.html#matlabScripts",
    "title": "EEG and ERP Processing and Analysis",
    "section": "",
    "text": "We have scripts for each task that can prepare files for the HAPPE Pipeline and/or manage the files outputted from HAPPE. These actions can be done manually as well, but the MATLAB scripts make the process more efficient. The scripts will also generate a “log” of all of the files processed through HAPPE to facilitate tracking of EEG data processing. The sections below detail the code used to perform these actions as well as the instructions for using the current scripts.\nNote: Before using the scripts/code detailed below, ensure that all filepaths used are in your MATLAB path collection. These may include:\n\nThe location where the automatic scripts are stored (for our lab, this is under /Data Processing/6. MATLAB EEG Pipeline)\nThe location where the HAPPE pre-processing script is stored\nThe location of the raw data (to be processed)\nThe location(s) of any intermediate files for processing (e.g., the updated .mff files that contain accuracy information in FishShark)\nThe location(s) for any files outputted by HAPPE and/or places you wish to use the script to move them to\n\nTo add a file path, click on Home/Set Path/Add Folder in MATLAB. You don’t need to specify the file name, just its containing folder.",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#oddball-processing-pipeline",
    "href": "eegERP.html#oddball-processing-pipeline",
    "title": "EEG and ERP Processing and Analysis",
    "section": "",
    "text": "1a - Files for HAPPE\n1b - Manual Processing\n2 - Processed Files\n3 - Files for PCA\n4 - EPT Averages\n5 - PCA\n6 - PCA Components\n\n\n\n\nThe following section describes the MATLAB scripts used to manage HAPPE output files for the Oddball task data. If you are running HAPPE manually (i.e., without integrating these scripts), skip to the Oddball HAPPE inputs section.\n\nOpen the eegProccessingOddball.m file (accessible via Git here) in MATLAB\nUpdate all thresholds and filepaths in script file (must be done BEFORE running the script)\n\nIn the second section of our script file, we set our “threshold” for the minimum number of trials that need to be retained after pre-processing for a subject’s data to be eligible for PCA. Additional thresholds can also be set for things like number of channels retained, but these are not currently in use.\n\n% Set quality threshold parameters\ntrialCutoff = 10;\n\nWe also set environment variables with all of the filepaths that are relevant for managing HAPPE output files and tracking processed data. The following paths should be checked and updated as necessary to reflect the organization of processing on your computer.\n\npassPath is the location you wish to have files that meet or exceed the above-defined thresholds to be saved\nallPath is the location you wish to have ALL files outputted from HAPPE saved to (regardless of whether threshold is met or not)\nfailPath is the location you wish to have files that do not meet the above-defined thresholds to be copied to\n\n.mff files that do not meet threshold will be copied here as an indication that they should be processed manually to see if they meet threshold afterward\n\nsummPath is the location you wish to save the file that lists all files processed through HAPPE in the current batch\n\nWe currently use this to save the “processing log” to a location that all team members/computers have access to so it is easier to determine which files require processing when EEG data are not stored on a shared server\n\nmanPath is the location you wish to save the file that lists all files that do not meet the above-defined thresholds\n\nWe currently use this to save the “to process manually” list to a location that all team members/computers have access to so it is easier to determine which files require manual processing because the copied .mff file would not be acessible via our shared server\n\n\n\n% Set paths for file sorting\npassPath = 'V:\\Processing-Repo\\Folder Structure\\3 - Files for PCA'; %location for .txt output files\nallPath = 'V:\\Processing-Repo\\Folder Structure\\2 - Processed Files'; %location for all processed files to end up\nfailPath = 'V:\\Processing-Repo\\Folder Structure\\1b - Manual Processing'; %location to copy unsuccessful .mff to for manual process\n\n% Set path for processing summary\nsummPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processed Data Logs';\n\n% Set path for summary of files to process manually \nmanPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processing To Do\\Cases to Process Manually\\Lists of Cases by Batch';\nRun the HAPPE Pipeline\n\nThis first section is designed to rely on user input. Click “Run” on the MATLAB editor window with the file open to begin the process.\nA message will appear in the console prompting you to enter the filepath to the location of the HAPPE pre-processing file you wish to run\nOnce the path is entered, MATLAB will run the file\nNote: the filepath inputted should NOT be in quotes\n\n% Set path to HAPPE pre-processing script\nhappeRun = input('Enter the full path to the HAPPE pre-processing file:\\n&gt; ','s') ;\n\n% Call and run HAPPE pre-processing script\nrun(happeRun);\nEnter HAPPE inputs\n\nSee the following section for HAPPE user inputs for Oddball\n\n\nAfter the HAPPE inputs are entered, no more user interaction is required for the script to do its job. The HAPPE pipeline will run, and the remaining MATLAB code in the script file will evaluate the files outputted by HAPPE and move them to the appropriate locations based on this evaluation. See Oddball Post-HAPPE Steps for a detailed explanation of the code used to accomplish this.\n\n\n\n\nIf you are not using the MATLAB scripts described above, you must first open the HAPPE pipeline V4 script in MATLAB and click “Run”\n\nThis will be stored in the location to which you mapped the HAPPE Git repo (e.g. Documents/GitHub/HAPPE/1. pre-process/HAPPE_v4.m)\n\nUser Inputs\n\nEnter the path to the folder containing the dataset.\nSelect raw\nLoad pre-existing set of input parameter:\n\nN if this is your first time running data through the pipeline.\nY if you have decided on a set of parameters. Enter the path to the folder containing the input parameters.\n\nLow density data: N\nData type: task\nPerforming event-related potential (ERP) analysis: Y\nEnter the task onset tags\n\nTarget: tgt+\nFrequent: frq+\ndone\n\nDo multiple onset tags belong to a single condition? N\nFile format: 5\nAcquisition layout type: 2\nNumber of channels: 128\nDo you have additional type fields besides “code”? N\nSelect channels of interest: all\nFrequency of electrical noise in Hz: 60\nAre there any additional frequencies, (e.g., harmonics) to reduce? N\nLine Noise reduction method: notch\n\nLow cutoff: 59\nhigh cutoff: 61\n\nResample: N\nFilter\n\nLow Pass Cutoff: 30\nHigh Pass Cutoff: .1\n\nChoose a filter: fir\nBad Channel Detection: Y\n\nafter wavelet thresholding\n\nECGone: N\nWavelet Thresholding\n\ndefault\nThreshold rule: hard\n\nMuscIL: N\nSegmentation: Y\n\nStarting parameter for stimulus: -200\nEnding parameter for stimulus: 1000\nTask offset: 2\n\nBaseline Correction: Y\n\nBaseline Correction start: -200\nBaseline Correction end: 0\n\nInterpolation: Y\nSegment Rejection: Y\n\nSegment Rejection Method: amplitude\n\nminimum segment rejection threshold: -150\nmaximum segment rejection threshold: 150\nsegment rejection based on all channels or ROI: all\n\n\nRe-referencing: Y\n\nDoes your data contain a flatline or all zero reference channel? N\nre-referencing method: average\n\nSave format: 1\nVisualizations: N\nParameter file save name: default\n\n\n\n\n\nThis section details the actions performed by the MATLAB scripts once HAPPE is completed. These actions will take place automatically upon completion of the HAPPE pipeline. No user inputs or actions are necessary at this stage.\n\nExclude files that don’t have any output data from the dataset (containing filename and quality reports from HAPPE pipeline) that will be used to assess file quality\n\nThis step is important because “empty” files don’t play nicely with the code used to evaluate files that have some data in them (even if the data do not meet threshold)\nThis code relies on HAPPE’s quality data that remains in the MATLAB environment after the pipeline has finished.\n\n% Create a list of files that received some kind of error message\nnoTags = any(strcmp(dataQC, 'NO_TAGS'), 2);\noneRej = any(strcmp(dataQC, 'REJ_ONE_SEG'), 2);\nallRej = any(strcmp(dataQC, 'ALL_SEG_REJ'), 2);\nerror = any(strcmp(dataQC, 'ERROR'), 2);\nloadFail = any(strcmp(dataQC, 'LOAD_FAIL'), 2);\n\n% Combine filenames with quality data (for some reason, they are not automatically connected by HAPPE)\ndataQCNew = [FileNames', dataQC];\n\n% Remove all files in the above lists (those receiving errors) from the quality data\ndataQCNew(noTags | allRej | error | loadFail | oneRej, :) = [];\n\n% Create list of variable names for quality data\ndataQCnamesNew = [\"File\", dataQCnames];\n\n% Save the data as a table for ease of use in subsequent steps\nqcTable = cell2table(dataQCNew, 'VariableNames', dataQCnamesNew);\nIdentify the files that meet (or don’t meet) the threshold\n% Create a list of files (i.e., rows in the table) that meet threshold\nthresholdTest = qcTable.(\"Number_tgt+_Segs_Post-Seg_Rej\") &gt;= trialCutoff & qcTable.(\"Number_frq+_Segs_Post-Seg_Rej\") &gt;= trialCutoff;\n\n% Add a variable to the quality data table that include whether or not the file meet threshold\nqcTable.Test = thresholdTest;\nAdd an identifying variable to be used for data joining down the line\n\nThis variable is generated using its expected location in the file name (i.e., how many text characters “in” it is)\n\n% Generate IDs based on File variable\nidWaveQC = extractBefore(qcTable.File, 8);\n\n% Append ID variable to quality data\nqcTable.idWave = idWaveQC;\nCreate a subsetted dataset conataining only the idWave, file name, and “test results” (i.e., whether a given file meets the specified cutoff threshold for inclusion) variables\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\n\nfprintf('Assessment Complete \\n');\nGenerate a list of files outputted by HAPPE\n% Generate path for HAPPE pre-processing output (using the HAPPE environment variable from user's input of location of raw data for processing)\ninputPath = strcat(srcDir, \"\\5 - processed\");\n\n% Read in list of files outputted from HAPPE \npreprocessingOutput = dir(inputPath);\n\n% Remove \"empty\" rows \npreprocessingOutput = preprocessingOutput(~ismember({preprocessingOutput.name}, {'.', '..'}));\n\n% Save data as a table for ease of later use\npreprocessingOutput = struct2table(preprocessingOutput);\n\n% Subset to file info\nfileInfo = preprocessingOutput(:, [\"name\", \"folder\"]);\nSelect only desired files to be moved/copied\n\nCurrently, we don’t do anything with the “Individual Trial” files outputted by HAPPE. These files are quite large and take a long time to move, so it is more efficient to just remove them from the list of filenames and not worry about moving them anywhere.\n\n% Subset to desired files (AveOverTrial)\nfileSubset = fileInfo(contains(fileInfo.name, \"AveOverTrials\"), :);\nAdd condition, ID, and threshold-related variables to the file data\n\nNOTE: The value for Condition variable (i.e., “Target” or “Frequent”) should match the name of the condition-specific folders you wish the files to save to within the processing repo.\n\n% Generate list of IDs based on file name variable\nidWaveFS = extractBefore(fileSubset.name, 8);\n\n% Add ID list to file data\nfileSubset.idWave = idWaveFS;\n\n% Generate list of files belonging to each condition based on file name variable\ntarget = contains(fileSubset.name, \"tgt+\");\nfrequent = contains(fileSubset.name, \"frq+\");\n\n% Create empty variable for condition\nfileSubset.cond = cell(size(fileSubset, 1), 1);\n\n% Fill in condition variable based on the lists generated above\nfileSubset.cond(target) = {'Target'};\nfileSubset.cond(frequent) = {'Frequent'};\nfileSubset.cond(~target & ~frequent) = {'All'};\n\n% Join threshold test information\nfileTest = join(fileSubset, testInfo);\nPrepare data table with information about files that met the threshold\n\nThe data generated here are preparing to copy the .txt files outputted by HAPPE into a folder containing all files that are suitable for PCA\n\n% Create a separate table for only files that meet threshold\nmovingInfo = fileTest(fileTest.Test, :);\n\n% Create empty columns for filepath variables\nmovingInfo.destination = cell(size(movingInfo, 1), 1);\nmovingInfo.origin = cell(size(movingInfo, 1), 1);\nmovingInfo.processedTo = cell(size(movingInfo, 1), 1);\nmovingInfo.processedFrom = cell(size(movingInfo, 1), 1);\n\n% Generate file paths based on condition \nmovingInfo.destination = strcat({passPath}, \"\\\", movingInfo.cond, \"\\\", movingInfo.name);\nmovingInfo.origin = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nmovingInfo.processedTo = strcat({allPath}, \"\\\", movingInfo.name);\nmovingInfo.processedFrom = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nPrepare data table with information about files that do NOT meet the threshold\n\nThe data generated here are preparing to copy .mff files from the location of the raw files into a folder indicating the need for manual processing\n\n% Create a separate table for only files that did not meet threshold\nfailFiles = fileTest(~fileTest.Test, [\"File\", \"folder\", \"name\"]);\n\n% Create empty columns for filepath variables\nfailFiles.destination = cell(size(failFiles, 1), 1);\nfailFiles.origin = cell(size(failFiles, 1), 1);\nfailFiles.processedTo = cell(size(failFiles, 1), 1);\nfailFiles.processedFrom = cell(size(failFiles, 1), 1);\n\n% Generate filepaths based on ID and task\nfailFiles.destination = strcat({failPath}, \"\\\", failFiles.File);\nfailFiles.origin = strcat({srcDir}, \"\\\", failFiles.File);\nfailFiles.processedFrom = strcat(failFiles.folder, \"\\\", failFiles.name);\nfailFiles.processedTo = strcat({allPath}, \"\\\", failFiles.name);\n\nGenerate environment variables that correspond to the column index of relevant variables for file sorting\n\nNote that the very last line of this code defines the varaible(s) to exclude from the HAPPE outputted files. This variable must be stripped from the data before saving them, because the presence of the extra variable makes the file incompatible with EP Toolkit’s PCA process.\n\n% Define column locations for each filepath variable\n\n% For files that meet threshold:\ntoCol = find(strcmp(movingInfo.Properties.VariableNames, \"destination\"));\nfromCol = find(strcmp(movingInfo.Properties.VariableNames, \"origin\"));\nprocColto = find(strcmp(movingInfo.Properties.VariableNames, \"processedTo\"));\nprocColfrom = find(strcmp(movingInfo.Properties.VariableNames, \"processedFrom\"));\n\n% For files that do not meet threshold\nrawCol = find(strcmp(failFiles.Properties.VariableNames, \"origin\"));\nmanCol = find(strcmp(failFiles.Properties.VariableNames, \"destination\"));\nfailProcColto = find(strcmp(failFiles.Properties.VariableNames, \"processedTo\"));\nfailProcColFrom = find(strcmp(failFiles.Properties.VariableNames, \"processedFrom\"));\n\n% Define variable to exclude\nextraVar = 'Time';\nUse a loop to process all files that met threshold\n\nFor each row in the “movingInfo” dataset, the loop will:\n\nIdentify the origin and destination paths\nRead in the HAPPE output file\nRemove the extra variable\nSave the “cleaned” data in the appropriate folder (without variable names, as required by EP Toolkit)\n\n\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, fromCol};\n    pathTo = movingInfo{row, toCol};\n    % Read in the data\n    rawTable = readtable(pathFrom);\n    % Remove extra column (Time)\n    cleanTable = rawTable{:, ~strcmp(rawTable.Properties.VariableNames, extraVar)};\n    % Save without headers\n    writematrix(cleanTable, pathTo, 'Delimiter', '\\t')\nend\n\nUse a loop to copy raw (.mff) files into a location that stores files requiring manual processing\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, rawCol};\n    pathTo = failFiles{row, manCol};\n    % Copy file \n    copyfile(pathFrom, pathTo)\nend\nUse a set of loops to copy all HAPPE output files into a folder intended to house all output (whether threshold is met or not)\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, procColfrom};\n    pathTo = movingInfo{row, procColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\n\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, failProcColFrom};\n    pathTo = failFiles{row, failProcColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\nGenerate a .txt file listing all processed .mff files\n\nThis file will contain a list of all raw files (e.g., 1111_22_oddball.mff) and save the list to the specified location (summPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past logs\nThe list of processed files is generated using an environment object (FileNames) that the HAPPE pipeline creates that lists all files inputted to the pipeline\n\n% Create a table from HAPPE FileNames cell array\nprocessedList = cell2table(FileNames(:));\n\n% Rename file variable from default\nprocessedList = renamevars(processedList, {'Var1'}, {'File'});\n\n% Save current date as a string variable \ntoday = string(date());\n\n% Save time as a string variable, replacing \":\" with \"_\" so that file can be written \ntime = strrep(datestr(now, 'HH:MM:SS:FFF'), ':', \"_\");\n\n% Generate file name to include current date and time \nlistFile = strcat(\"\\oddballProcessed_\", today, \"_\", time);\n\n% Generate full path including file name\nsummPathFull = strcat(summPath, listFile);\n\n% Write table to specified location\nwritetable(processedList, summPathFull);\nGenerate a .txt file listing all of the files that did not meet threshold to go through subsequent processing\n\nThis file will contain a list of all raw files (e.g., 1111_22_oddball.mff) that did not meet threshold and save the list to the specified location (manPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past lists\nThis list of file to process manually is generated using the environment object FileNames that HAPPE creates; any files listed in FileNames but not in the dataset of files that pass threshold make up the manualList\n\n% Compare movingInfo with FileNames to isolate files that don't meet threshold\nmanualList = cell2table(FileNames(~ismember(FileNames', movingInfo.File)));\nmanualList = renamevars(manualList, {'Var1'}, {'File'});\n\n% Generate file name to include current date\nmanFile = strcat(\"\\oddballManual_\", today, \"_\", time);\n\n% Generate full path including filename\nmanPathFull = strcat(manPath, manFile);\n\n% Write table to specified location\nwritetable(manualList, manPathFull);",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#fishshark-processing-pipeline",
    "href": "eegERP.html#fishshark-processing-pipeline",
    "title": "EEG and ERP Processing and Analysis",
    "section": "",
    "text": "0 - Raw\n1a - Files for HAPPE\n1b - Manual Processing\n2 - Processed Files\n3 - Files for PCA\n4 - EPT Averages\n5 - PCA\n6 - PCA Components\n\n\n\n\nThe following section describes the MATLAB scripts used to manage HAPPE input and output files for the FishShark task data. If you are running HAPPE manually (i.e., without integrating these scripts), skip to the FishShark HAPPE inputs section.\nThe first two steps described in this section are the same as those described for Oddball processing. Processing actions specific to FishSharks task begin in Step 3.\n\nOpen the eegProccessingFishshark.m file (accessible via Git here) in MATLAB\nUpdate all thresholds and filepaths in script file (must be done BEFORE running the script)\n\nIn the fifth section of our script file, we set our “threshold” for the minimum number of trials that need to be retained after pre-processing for a subject’s data to be eligible for PCA. Additional thresholds can also be set for things like number of channels retained, but these are not currently in use.\nNote that setting these parameters occurs later in the FishSharks script than the one for Oddball; this is because FishSharks files require additional processing before they are ready for HAPPE (see step 3 for details)\n\n% Set quality threshold parameters\ntrialCutoff = 10;\n\nWe also set environment variables with all of the filepaths that are relevant for managing HAPPE output files and tracking processed data. The following paths should be checked and updated as necessary to reflect the organization of processing on your computer.\n\npassPath is the location you wish to have files that meet or exceed the above-defined thresholds to be saved\nallPath is the location you wish to have ALL files outputted from HAPPE saved to (regardless of whether threshold is met or not)\nfailPath is the location you wish to have files that do not meet the above-defined thresholds to be copied to\n\n.mff files that do not meet threshold will be copied here as an indication that they should be processed manually to see if they meet threshold afterward\n\nsummPath is the location you wish to save the file that lists all files processed through HAPPE in the current batch\n\nWe currently use this to save the “processing log” to a location that all team members/computers have access to so it is easier to determine which files require processing when EEG data are not stored on a shared server\n\nmanPath is the location you wish to save the file that lists all files that do not meet the above-defined thresholds\n\nWe currently use this to save the “to process manually” list to a location that all team members/computers have access to so it is easier to determine which files require manual processing because the copied .mff file would not be acessible via our shared server\n\n\n\n% Set paths for file sorting\npassPath = 'V:\\Processing-Repo\\Folder Structure\\3 - Files for PCA'; %location for .txt output files\nallPath = 'V:\\Processing-Repo\\Folder Structure\\2 - Processed Files'; %location for all processed files to end up\nfailPath = 'V:\\Processing-Repo\\Folder Structure\\1b - Manual Processing'; %location to copy unsuccessful .mff to for manual process\n\n% Set path for processing summary\nsummPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processed Data Logs';\n\n% Set path for summary of files to process manually \nmanPath = 'Z:\\Shared Server\\Study Folder\\Data Processing\\6. MATLAB EEG Pipeline\\Processing To Do\\Cases to Process Manually\\Lists of Cases by Batch';\nUpdate raw (.mff) files’ condition tags with accuracy information\n\nUnlike the Passive Oddball task, FishSharks trials can be either correct or incorrect. Whether or not a trial was “responded to” correctly is relevant to the nature of the extracted ERP. Because the event tags in our .mff files do not inherently contain information about whether each trial was responded to correctly, we need to add it ourselves. This process can be done manually in NetStation, but it can also be automated using the MATLAB code detailed below.\nNOTE: This code requires eeglab. Before running the code, open eeglab in your MATLAB session by typing eeglab into the console. You can close it as soon as it opens, but this step ensures that eeglab is loaded into your current session and helps prevent the subsequent code from erroring out.\n\n\nSet the filepaths for raw and updated .mff files\n\nThis section is set to rely on user inputs. Click “Run” in the MATLAB editor to start the processing.\nA message will appear in the console prompting you to enter two filepaths: the first is the location of the raw .mff files, and the second is the location you would like the updated files to save in.\nNote: the filepaths inputted should NOT be in quotes\n\n% User input for location of raw files\npathRaw = input('Enter the full path to the folder containing the raw files:\\n&gt; ','s');\n\n% User input for destination of subsetted files\npathSub = input('Enter the full path to the folder in which to save the updated files:\\n&gt; ','s');\nAt this point, there will be no user input/actions necessary until all of the .mff files in the pathRaw directory have been updated and saved into the pathSub directory. The code that asks the user for the path to HAPPE will run when that process has finished. The following section will describe the code used to automate the process of updating .mff event tags to include accuracy information at the trial level. Move on to Step 4 (Run the HAPPE Pipeline) when the process has completed.\nGather and manage information from the directory housing the raw (.mff) files\n% Have MATLAB gather a list of raw files housed in specified location (pathRaw)\ndirInfo = struct2table(dir(pathRaw));\n\n% Remove blank rows\nnoName = strcmp(dirInfo.name, '.') | strcmp(dirInfo.name, '..');\ndirInfo(noName, :) = [];\nGenerate variables necessary for managing raw and updated files\n\nThis code will generate the full filepaths (including file name) necessary for reading in each .mff file and saving its updated counterpart\nIt will also generate an ID variable for joining purposes based on the expected location of the subject name in the name of each file\n\n% Generate ID variable\ndataFiles = dirInfo(:, \"name\");\n\n% Add ID variable to file data\ndataFiles.ID = extractBefore(dataFiles.name, 8);\n\n% Generate path to read raw data\nrawPaths = dataFiles;\nrawPaths.path = strcat({pathRaw}, \"/\", dirInfo.name);\nrawPaths = rawPaths(:, [\"ID\", \"path\"]);\n\n% Generate path to save updated versions of the data (containing accuracy info at trial level)\nsubPaths = dataFiles;\nsubPaths.path = strcat({pathSub}, \"/\", subPaths.ID, \"_sub_fishshark.mff\");\nsubPaths = subPaths(:, [\"ID\", \"path\"]);\n\n% Join filepath datatables\nmergePaths = join(rawPaths, subPaths, 'Keys', {'ID'})\nUse a loop to update the event tags in each .mff file to reflect accuracy of response\n\nFor every file included in the mergePaths dataset, MATLAB will perform the following actions:\n\nSet environment variables representing the path to read in the original .mff file and save its updated counterpart (updates with each iteration of the loop)\nRead in the .mff file\nExtract the “event” information from the .mff file\nEvaluate whether there is usable data in the file\n\nThis step prevents the code from erroring out if a subject did not make it past the practice trials\nIf the evaluation determines that there is NOT usable data in the present file, the loop will jump to the next file\n\nCreate a table containing response information (response vs no response and reaction time) and condition information (go vs no-go) at the trial level\nEvaluate each trial to determine whether the response was correct or incorrect\n\nFor No-Go trials, responses are CORRECT if there was no response (incorrect if subject did respond)\nFor Go trials, responses are CORRECT if there was a response AND subject’s reaction time was at least 200 ms (incorrect if subject did not respond OR if subject responded too quickly for response to be considered “valid”)\n\nUpdate the event tags in the .mff file to contain a “c” for correct trials and an “x” for incorrect trials\n\nThe c/x indicator will be appended to the front of the existing event tag (e.g., Go++ will become cGo++)\n\nExport an updated version of the .mff file with accuracy information to the specified location (pathSub)\n\nNOTE: When exporting EEG data from MATLAB, it is absolutely necessary for the data to be organized in order of event latency. If the rows are out of order, latency errors may be introduced.\n\nCorrect row order can be achieved in .mff filetypes by sorting according to either the latency column or the urevent column (as shown below)\n\n\nfor row = 1:height(mergePaths)\n    % Specify paths\n    rawFolder = mergePaths{row, \"path_rawPaths\"}\n    subFolder = mergePaths{row, \"path_subPaths\"}\n\n    % Read in EEG data\n    EEGraw = pop_mffimport(char(rawFolder), 'code')\n\n    % Create table from \"event\" field of raw data\n    EEGevent = struct2table(EEGraw.event)\n\n    % Check for the existence of usable rows\n    checkVars = strcmp(EEGevent.Properties.VariableNames, 'mffkey_cel')\n\n    % Skip files without necessary variables\n    if max(checkVars) == 0\n        continue\n    end\n\n    % Create table without practice/training trials\n    keepRows = strcmp(EEGevent.mffkey_cel, '4')\n    EEGsub  = EEGevent(keepRows, :)\n\n    % Check for the existence of usable rows\n    checkRows = max(keepRows)\n\n    % Skip files with no usable rows\n    if checkRows == 0\n        continue\n    end\n\n    % Get response info at trial level\n    EEGresp = table(EEGsub.mffkey_obs, EEGsub.mffkey_eval, EEGsub.mffkey_rtim)\n    EEGresp = rmmissing(EEGresp)\n    EEGresp = renamevars(EEGresp, [\"Var1\", \"Var2\", \"Var3\"], [\"Trial\", \"Eval\", \"RTime\"])\n\n    % Get condition info at trial level\n    EEGconds = table(EEGsub.mffkey_obs, EEGsub.type)\n    EEGconds = renamevars(EEGconds, [\"Var1\", \"Var2\"], [\"Trial\", \"Cond\"])\n    keepConds = strcmp(EEGconds.Cond, 'Go++') | strcmp(EEGconds.Cond, 'NG++')\n    EEGcond = EEGconds(keepConds, :)\n\n    % Merge datasets\n    EEGtrials = join(EEGcond, EEGresp)\n\n    EEGtrials.RTime = cellfun(@str2num, EEGtrials.RTime)\n    % Evaluate trials for correct-ness of response\n    correct = strcmp(EEGtrials.Cond, 'Go++') & strcmp(EEGtrials.Eval, '1') & EEGtrials.RTime &gt; 200 | strcmp(EEGtrials.Cond, 'NG++') & strcmp(EEGtrials.Eval, '0')\n    EEGtrials.Acc = correct\n\n\n    % Create new code tags including accuracy information\n    EEGtrials.newCode(EEGtrials.Acc & strcmp(EEGtrials.Cond, 'Go++')) = {'cGo++'}\n    EEGtrials.newCode(~EEGtrials.Acc & strcmp(EEGtrials.Cond, 'Go++')) = {'xGo++'}\n    EEGtrials.newCode(EEGtrials.Acc & strcmp(EEGtrials.Cond, 'NG++')) = {'cNG++'}\n    EEGtrials.newCode(~EEGtrials.Acc & strcmp(EEGtrials.Cond, 'NG++')) = {'xNG++'}\n\n    % Subset information for merge\n    EEGmerge = EEGtrials(:, {'Trial', 'Cond', 'newCode'})\n\n    % Prep key in original data\n    EEGevent.key = strcat(EEGevent.mffkey_obs, EEGevent.type)\n\n    % Prep key in merge data\n    EEGmerge.key = strcat(EEGmerge.Trial, EEGmerge.Cond)\n    EEGmerge = EEGmerge(:, {'key', 'newCode'})\n\n    % Merge new codes with event table\n    EEGnew = outerjoin(EEGevent, EEGmerge)\n\n    % Replace codes where new code is needed\n    EEGnew.code(~strcmp(EEGnew.newCode, '')) = EEGnew.newCode(~strcmp(EEGnew.newCode, ''))\n    EEGnew.type(~strcmp(EEGnew.newCode, '')) = EEGnew.newCode(~strcmp(EEGnew.newCode, ''))\n\n    % Arrange table in order of event (CRUCIAL for correct export)\n    EEGnew = sortrows(EEGnew, 'urevent')\n\n    % Convert table back to struct and restore original dimensions\n    EEGnew = table2struct(EEGnew(:, 1:29))\n    EEGnew = reshape(EEGnew, [1, height(EEGnew)])\n\n    % Replace event table(s) in original struct\n    EEGraw.event = EEGnew\n    EEGraw.urevent = table2struct(struct2table(EEGnew(:, 1:28)))\n\n    % Export updated file\n    pop_mffexport(EEGraw, char(subFolder))\nend \n\nRun the HAPPE Pipeline\n\nThis section is designed to rely on user input.\nWhen the prior steps have finsihed, a message will appear in the console prompting you to enter the filepath to the location of the HAPPE pre-processing file you wish to run\nOnce the path is entered, MATLAB will run the file\nNote: the filepath inputted should NOT be in quotes\n\n% Set path to HAPPE pre-processing script\nhappeRun = input('Enter the full path to the HAPPE pre-processing file:\\n&gt; ','s') ;\n\n% Call and run HAPPE pre-processing script\nrun(happeRun);\nEnter HAPPE inputs\n\nSee the following section for HAPPE inputs for the Fish/Shark task\n\n\nAfter the HAPPE inputs are entered, no more user interaction is required for the script to do its job. The HAPPE pipeline will run, and the remaining MATLAB code in the script file will evaluate the files outputted by HAPPE and move them to the appropriate locations based on this evaluation. See FishShark Post-HAPPE Steps for a detailed explanation of the code used to accomplish this.\n\n\n\n\nIf you are not using the MATLAB scripts described above, you must first open the HAPPE pipeline V4 script in MATLAB and click “Run”\n\nThis will be stored in the location to which you mapped the HAPPE Git repo (e.g. Documents/GitHub/HAPPE/1. pre-process/HAPPE_v4.m)\n\nUser Inputs\n\nEnter the path to the folder containing the dataset.\nSelect raw\nLoad pre-existing set of input parameter:\n\nN if this is your first time running data through the pipeline.\nY if you have decided on a set of parameters. Enter the path to the folder containing the input parameters.\n\nLow density data: N\nData type: task\nPerforming event-related potential (ERP) analysis: Y\nEnter the task onset tags\n\nCorrect Go: cGo++\nIncorrect Go: xGo++\nCorrect NoGo: cNG++\nIncorrect NoGo: xNG++\ndone\n\nDo multiple onset tags belong to a single condition? N\nFile format: 5\nAcquisition layout type: 2\nNumber of channels: 128\nDo you have additional type fields besides “code”? N\nSelect channels of interest: all\nFrequency of electrical noice in Hz: 60\nAre there any additional frequencies, (e.g., harmonics) to reduce? N\nLine Noise reduction method: notch\n\nLow cutoff: 59\nhigh cutoff: 61\n\nResample: N\nFilter\n\nLow Pass Cutoff: 30\nHigh Pass Cutoff: .1\n\nChoose a filter: fir\nBad Channel Detection: Y\n\nafter wavelet thresholding\n\nECGone: N\nWavelet Thresholding\n\ndefault\nThreshold rule: hard\n\nMuscIL: N\nSegmentation: Y\n\nStarting parameter for stimulus: -200\nEnding parameter for stimulus: 1000\nTask offset: 17\n\nBaseline Correction: Y\n\nBaseline Correction start: -200\nBaseline Correction end: 0\n\nInterpolation: Y\nSegment Rejection: Y\n\nSegment Rejection Method: amplitude\n\nminimum segment rejection threshold: -150\nmaximum segment rejection threshold: 150\nsegment rejection based on all channels or ROI: all\n\n\nRe-referencing: Y\n\nDoes your data contain a flatline or all zero reference channel? N\nre-referencing method: average\n\nSave format: 1\nVisualizations: N\nParameter file save name: default\n\n\n\n\n\nThis section details the actions performed by the MATLAB scripts once HAPPE is completed. These actions will take place automatically upon completion of the HAPPE pipeline. No user inputs or actions are necessary at this stage.\n\nExclude files that don’t have any output data from the dataset (containing filename and quality reports from HAPPE pipeline) that will be used to assess file quality\n\nThis step is important because “empty” files don’t play nicely with the code used to evaluate files that have some data in them (even if the data do not meet threshold)\nThis code relies on HAPPE’s quality data that remains in the MATLAB environment after the pipeline has finished.\n\n% Create a list of files that received some kind of error message\nnoTags = any(strcmp(dataQC, 'NO_TAGS'), 2);\nallRej = any(strcmp(dataQC, 'ALL_SEG_REJ'), 2);\noneRej = any(strcmp(dataQC, 'REJ_ONE_SEG'), 2);\nerror = any(strcmp(dataQC, 'ERROR'), 2);\nloadFail = any(strcmp(dataQC, 'LOAD_FAIL'), 2);\n\n% Combine filenames with quality data (for some reason, they are not automatically connected by HAPPE)\ndataQCNew = [FileNames', dataQC];\n\n% Remove all files in the above lists (those receiving errors) from the quality data\ndataQCNew(noTags | allRej | error | loadFail | oneRej, :) = [];\n\n% Create list of variable names for quality data\ndataQCnamesNew = [\"File\", dataQCnames];\n\n% Save the data as a table for ease of use in subsequent steps\nqcTable = cell2table(dataQCNew, 'VariableNames', dataQCnamesNew);\n\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\nIdentify the files that meet (or don’t meet) the threshold\n% Create a list of files (i.e., rows in the table) that meet threshold\nthresholdTest = qcTable.(\"Number_cNG++_Segs_Post-Seg_Rej\") &gt;= trialCutoff & qcTable.(\"Number_cGo++_Segs_Post-Seg_Rej\") &gt;= trialCutoff;\n\n% Add a variable to the quality data table that include whether or not the file meet threshold\nqcTable.Test = thresholdTest;\nAdd an identifying variable to be used for data joining down the line\n\nThis variable is generated using its expected location in the file name (i.e., how many text characters “in” it is)\n\n% Generate IDs based on File variable\nidWaveQC = extractBefore(qcTable.File, 8);\n\n% Append ID variable to quality data\nqcTable.idWave = idWaveQC;\nCreate a subsetted dataset conataining only the idWave, file name, and “test” results (i.e., whether a given file meets the specified cutoff threshold for inclusion)\n% Subset to ID and threshold information\ntestInfo = qcTable(:, [\"idWave\", \"File\", \"Test\"]);\n\nfprintf('Assessment Complete \\n');\nGenerate a list of files outputted by HAPPE\n% Generate path for HAPPE pre-processing output (using the HAPPE environment variable from user's input of location of raw data for processing)\ninputPath = strcat(srcDir, \"\\5 - processed\");\n\n% Read in list of files outputted from HAPPE \npreprocessingOutput = dir(inputPath);\n\n% Remove \"empty\" rows \npreprocessingOutput = preprocessingOutput(~ismember({preprocessingOutput.name}, {'.', '..'}));\n\n% Save data as a table for ease of later use\npreprocessingOutput = struct2table(preprocessingOutput);\n\n% Subset to file info\nfileInfo = preprocessingOutput(:, [\"name\", \"folder\"]);\nSelect only desired files to be moved/copied\n\nCurrently, we don’t do anything with the “Individual Trial” files outputted by HAPPE. These files are quite large and take a long time to move, so it is more efficient to just remove them from the list of filenames and not worry about moving them anywhere.\n\n% Subset to desired files (AveOverTrial)\nfileSubset = fileInfo(contains(fileInfo.name, \"AveOverTrials\"), :);\nAdd condition, ID, and threshold-related variables to the file data\n\nNOTE: The value for Condition variable (i.e., “cGo” or “cNoGo”) should match the name of the condition-specific folders you wish the files to save to within the processing repo.\n\n% Generate list of IDs based on file name variable\nidWaveFS = extractBefore(fileSubset.name, 8);\n\n% Add ID list to file data\nfileSubset.idWave = idWaveFS;\n\n% Generate list of files belonging to each condition based on file name variable\ncGo = contains(fileSubset.name, \"cGo++\");\ncNoGo = contains(fileSubset.name, \"cNG++\");\nxGo = contains(fileSubset.name, \"xGo++\");\nxNoGo = contains(fileSubset.name, \"xNG++\");\n\n% Create empty variable for condition\nfileSubset.cond = cell(size(fileSubset, 1), 1);\n\n% Fill in condition variable based on the lists generated above\nfileSubset.cond(cGo) = {'cGo'};\nfileSubset.cond(cNoGo) = {'cNoGo'};\nfileSubset.cond(xGo) = {'xGo'};\nfileSubset.cond(xNoGo) = {'xNoGo'};\nfileSubset.cond(~cGo & ~cNoGo & ~xGo & ~xNoGo) = {'All'};\n\n% Join threshold test information\nfileTest = join(fileSubset, testInfo);\nPrepare data table with information about files that met the threshold\n\nThe data generated here are preparing to copy the .txt files outputted by HAPPE into a folder containing all files that are suitable for PCA\n\n% Create a separate table for only files that meet threshold\nmovingInfo = fileTest(fileTest.Test, :);\n\n% Create empty columns for filepath variables\nmovingInfo.destination = cell(size(movingInfo, 1), 1);\nmovingInfo.origin = cell(size(movingInfo, 1), 1);\nmovingInfo.processedTo = cell(size(movingInfo, 1), 1);\nmovingInfo.processedFrom = cell(size(movingInfo, 1), 1);\n\n% Generate file paths based on condition \nmovingInfo.destination = strcat({passPath}, \"\\\", movingInfo.cond, \"\\\", movingInfo.name);\nmovingInfo.origin = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nmovingInfo.processedTo = strcat({allPath}, \"\\\", movingInfo.name);\nmovingInfo.processedFrom = strcat(movingInfo.folder, \"\\\", movingInfo.name);\nPrepare data table with information about files that do NOT meet the threshold\n\nThe data generated here are preparing to copy .mff files from the location of the raw files into a folder indicating the need for manual processing\n\n% Create a separate table for only files that did not meet threshold\nfailFiles = fileTest(~fileTest.Test, [\"File\", \"folder\", \"name\"]);\n\n% Create empty columns for filepath variables\nfailFiles.destination = cell(size(failFiles, 1), 1);\nfailFiles.origin = cell(size(failFiles, 1), 1);\nfailFiles.processedTo = cell(size(failFiles, 1), 1);\nfailFiles.processedFrom = cell(size(failFiles, 1), 1);\n\n% Generate filepaths based on ID and task\nfailFiles.destination = strcat({failPath}, \"\\\", failFiles.File);\nfailFiles.origin = strcat({srcDir}, \"\\\", failFiles.File);\nfailFiles.processedFrom = strcat(failFiles.folder, \"\\\", failFiles.name);\nfailFiles.processedTo = strcat({allPath}, \"\\\", failFiles.name);\n\nGenerate environment variables that correspond to the column index of relevant variables for file sorting\n\nNote that the very last line of this code defines the varaible(s) to exclude from the HAPPE outputted files. This variable must be stripped from the data before saving them, because the presence of the extra variable makes the file incompatible with EP Toolkit’s PCA process.\n\n% Define column locations for each filepath variable\n\n% For files that meet threshold:\ntoCol = find(strcmp(movingInfo.Properties.VariableNames, \"destination\"));\nfromCol = find(strcmp(movingInfo.Properties.VariableNames, \"origin\"));\nprocColto = find(strcmp(movingInfo.Properties.VariableNames, \"processedTo\"));\nprocColfrom = find(strcmp(movingInfo.Properties.VariableNames, \"processedFrom\"));\n\n% For files that do not meet threshold\nrawCol = find(strcmp(failFiles.Properties.VariableNames, \"origin\"));\nmanCol = find(strcmp(failFiles.Properties.VariableNames, \"destination\"));\nfailProcColto = find(strcmp(failFiles.Properties.VariableNames, \"processedTo\"));\nfailProcColFrom = find(strcmp(failFiles.Properties.VariableNames, \"processedFrom\"));\n\n% Define variable to exclude\nextraVar = 'Time';\nUse a loop to process all files that met threshold\n\nFor each row in the “movingInfo” dataset, the loop will:\n\nIdentify the origin and destination paths\nRead in the HAPPE output file\nRemove the extra variable\nSave the “cleaned” data in the appropriate folder (without variable names, as required by EP Toolkit)\n\n\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, fromCol};\n    pathTo = movingInfo{row, toCol};\n    % Read in the data\n    rawTable = readtable(pathFrom);\n    % Remove extra column (Time)\n    cleanTable = rawTable{:, ~strcmp(rawTable.Properties.VariableNames, extraVar)};\n    % Save without headers\n    writematrix(cleanTable, pathTo, 'Delimiter', '\\t')\nend\n\nUse a loop to copy raw (.mff) files into a location that stores files requiring manual processing\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, rawCol};\n    pathTo = failFiles{row, manCol};\n    % Copy file \n    copyfile(pathFrom, pathTo)\nend\nUse a set of loops to copy all HAPPE output files into a folder intended to house all output (whether threshold is met or not)\nfor row = 1:height(movingInfo)\n    % Specify path info\n    pathFrom = movingInfo{row, procColfrom};\n    pathTo = movingInfo{row, procColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\n\nfor row = 1:height(failFiles)\n    % Specify path info\n    pathFrom = failFiles{row, failProcColFrom};\n    pathTo = failFiles{row, failProcColto};\n    % Copy file\n    copyfile(pathFrom, pathTo);\nend\nGenerate a .txt file listing all processed .mff files\n\nThis file will contain a list of all raw files (e.g., 1111_22_fishshark.mff) and save the list to the specified location (summPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past logs\nThe list of processed files is generated using an environment variable that the HAPPE pipeline creates that lists all files inputted to the pipeline\n\n% Create a table from HAPPE FileNames cell array\nprocessedList = cell2table(FileNames(:));\n\n% Rename file variable from default\nprocessedList = renamevars(processedList, {'Var1'}, {'File'});\n\n% Save current date as a string variable \ntoday = string(date());\n\n% Save time as a string variable, replacing \":\" with \"_\" so that file can be written \ntime = strrep(datestr(now, 'HH:MM:SS:FFF'), ':', \"_\");\n\n% Generate file name to include current date and time \nlistFile = strcat(\"\\oddballProcessed_\", today, \"_\", time);\n\n% Generate full path including file name\nsummPathFull = strcat(summPath, listFile);\n\n% Write table to specified location\nwritetable(processedList, summPathFull);\nGenerate a .txt file listing all of the files that did not meet threshold to go through subsequent processing\n\nThis file will contain a list of all raw files (e.g., 1111_22_fishshark.mff) that did not meet threshold and save the list to the specified location (manPath)\nThe file will have the current date and time appended to the end so that it will be distinguishable from past lists\nThis list of file to process manually is generated using the environment object FileNames that HAPPE creates; any files listed in FileNames but not in the dataset of files that pass threshold make up the manualList\n\n% Compare movingInfo with FileNames to isolate files that don't meet threshold\nmanualList = cell2table(FileNames(~ismember(FileNames', movingInfo.File)));\nmanualList = renamevars(manualList, {'Var1'}, {'File'});\n\n% Generate file name to include current date\nmanFile = strcat(\"\\fishSharkManual_\", today, \"_\", time);\n\n% Generate full path including filename\nmanPathFull = strcat(manPath, manFile);\n\n% Write table to specified location\nwritetable(manualList, manPathFull);",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#average",
    "href": "eegERP.html#average",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.1 Reading Text Files into EP Toolkit",
    "text": "2.1 Reading Text Files into EP Toolkit\n\nOpen MATLAB with “Run as Administrator”\nOpen ERP PCA Toolkit in MATLAB\n\nType ep in command prompt\n\nClick Read to import files\nUse the following options\n\nFormat = text (.txt)\nType = average\nMont = Adult Hydrocel 128-channel 1.0\n\nSelect Single File Mode.\n\nSingle file mode will use the filename to assign the task condition and participant ID for each file. Thus, it is critical to use a standard naming convention to name the files.\n\nFor example, an oddball file could be named: 1001_36_oddball_processed_AveOverTrials_tgt+.txt\nA FishSharks file meanwhile could be named: 1004_36_oddball_processed_AveOverTrials_cGo++.txt\nNote: The filename examples above reflect the automated naming convention of the files outputted by the HAPPE pipeline. If another processing pipeline is used or if the naming conventions become unstandardized, it is crucial that standardization of filenames is implemented\n\n\nIn the Single File Mode menu use the Subject field to denote which characters in the filename name will determine the participant ID.\n\nFor the above example 1:7 would correspond to 1001_36 in a file called 1001_36_oddball_processed_AveOverTrials_tgt+.txt (outputted by HAPPE)\n\nNext, In the Single File Mode menu use the Cell field to denote which characters in the file name will determine the task condition.\n\nFor the below example 41:43 would correspond to frq or tgt in a file called 1001_36_oddball_processed_AveOverTrials_tgt+.txt\nFor FishSharks files, it might be 47:49 that correspond to cgo or cng.\n\n\nSelect Read. You will be asked to name the aggregate file created by importing and combining the single files. Use the convention “task_condition_age” to name the file (e.g., ob_tgt_54)\nA new window will open prompting you to select the files you wish to import. Navigate to the folder housing the data to be imported (e.g., 3 - Files for PCA\\Target) and use Ctrl + A to select all files in that folder. Click Open once all desired files are selected\nSelect the 2_9AverageNet128.ced file when prompted by another popup. This file corresponds to the electrode template associated with the participant age range and type of net represented in the data\n\nFor our study using a 128-channel net on children aged 3-7, the 2_9AverageNet128.ced template is most appropriate \n\nThe new file will have the participant ID and will combine the conditions for each participant.\n\nSubject Names: \nTask Conditions",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#update-file-with-experiment-information",
    "href": "eegERP.html#update-file-with-experiment-information",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.2 Update File with Experiment Information",
    "text": "2.2 Update File with Experiment Information\n\nGo to Main and click Edit\nClick on the file you imported\nIn Overview, add the following information:\n\nExperiment Name: Oddball, FishShark, or StopSignal\nReference Type: change to average reference\nPrestimulus period: change to 200\nNominal sampling rate: change to 1000\n\nClick Done\nGo to Main and click Save\n\nSave the combined file as an .ept file in the 4-EPT Averages folder using the following naming convention: “task_condition_age”. For example, if you were working on the target condition of oddball for all age groups, you would save the file as ob_tgt_all",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#generating-grand-average-waveforms",
    "href": "eegERP.html#generating-grand-average-waveforms",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.3 Generating Grand Average Waveforms",
    "text": "2.3 Generating Grand Average Waveforms\n\nIf the EPT average file (e.g., ob_tgt_all) is not already in the working environment, read it in using the steps below\n\nGo to Read\nFormat = EP (.ept)\nClick Read\nNavigate to the 4 - EPT Averages folder and select desired file(s)\nClick Open in the browser window to read the file(s)\nClick Main to return to main menu\n\nSelect Edit\nWhen the editor window opens, navigate to the Subjects pane\nSelect All from among the many options along the lefthand pane of the editor\n\nThis will select all of the subjects included in the file and assign them a weight of 1\n\nConfirm that all subjects have been selected (look for a checked box in the subject row) and that all weights have been set to 1\nClick Add\nA new “subject” should have now been added to the bottom of the subjects list\n\nThis subject is called gave and represents the grand average across all subjects\n\nClick Done to exit the editor window, then Main to return to the EP Toolkit home",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#tPCA",
    "href": "eegERP.html#tPCA",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.4 Temporal PCA",
    "text": "2.4 Temporal PCA\n\nGo to Main and click PCA\nInput the following:\n\nMode: temporal\nRotation: promax\nFactors: 0\nTitle: tPCA_experimentname (example: tPCA_ob_tgt_all)\n\nClick the appropriate file (e.g., ob_tgt_all)\nDetermine how many factors to retain using the scree plot (keep the number of factors where the blue line is above the red line)\nDetermine the percent variance accounted for by the number of factors retained by changing the “minimum % age accounted for criterion”. Record the number of factors retained and % variance accounted for by that number of factors.\nRe-run the temporal PCA using the above inputs, but change the number of factors to the number of factors retained from the above step\nReturn to Main and click Save. Save the tPCA file in the 5-PCA folder",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#spatial-pca",
    "href": "eegERP.html#spatial-pca",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.5 Spatial PCA",
    "text": "2.5 Spatial PCA\n\nGo to Main and click PCA\nChange the PCA type, using the following inputs:\n\nMode: spatial\nRotation: infomax\nFactors: 0\nTitle: sPCA_experimentname (e.g., sPCA_ob_tgt_all)\n\nClick the appropriate file (e.g., ob_tgt_all)\nDetermine how many factors to retain using the scree plot (keep the number of factors where the blue line is above the red line)\nDetermine the percent variance accounted for by the number of factors retained by changing the “minimum % age accounted for criterion”. Record the number of factors retained and % variance accounted for by that number of factors.\nRe-run the spatial PCA using the above inputs, but change the number of factors to the number of factors retained from the above step\nReturn to Main and click Save. Save the sPCA file in the 5-PCA folder",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#temporospatial-pca",
    "href": "eegERP.html#temporospatial-pca",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.6 Temporospatial PCA",
    "text": "2.6 Temporospatial PCA\n\nGo to Main and click PCA\nChange the PCA type, using the following inputs:\n\nMode: spatial\nRotation: infomax\nFactors: 0\nTitle: tsPCA_experimentname (e.g., tsPCA_ob_tgt_all)\n\nClick the tPCA file (created in the previous step)\nDetermine how many factors to retain using the scree plot (keep the number of factors where the blue line is above the red line)\nDetermine the percent variance accounted for by the number of factors retained by changing the “minimum % age accounted for criterion”. Record the number of factors retained and % variance accounted for by that number of factors.\nRe-run the spatial PCA using the above inputs, but change the number of factors to the number of factors retained from the above step\nReturn to Main and click Save. Save the tsPCA file in the 5-PCA folder.",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#extract",
    "href": "eegERP.html#extract",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.7 PCA Component Selection and Extraction",
    "text": "2.7 PCA Component Selection and Extraction\nHere, the goal is to select the PCA component that corresponds to the ERP component of interest, and the extraction that supports the intended interpretability of the component.\n\nGo to View to begin the process of selecting the PCA component that corresponds to the ERP of interest.\n\nIteratively select and view each temporospatial PCA component to identify the PCA component (“factor”) that corresponds to the ERP of interest (e.g., N2 or P3). Select the temporospatial PCA component that corresponds to the ERP of interest based on the timing, spatial location, morphology, and (as relevant) any condition- or age-related differences of the component based on prior work.\n\nGenerate tsPCA components. Go to Window and input the following:\n\nselect the tsPCA file\nselect among mean, maxPeak, or other options. (According to Joe Dien), when using amplitudes from PCA components, it does not matter which option you select—all the different methods result in comparable p-values when dealing with PCA components. So, select a method that makes sense for the story you want to tell. The methods will yield different results when dealing with the raw waveforms.\nselect AutoPCA or Window to select channels. If the peak amplitude is where you expect temporally and spatially, then use the autoPCA function, and if it is not, then window to where you expect it to be. This will allow you to report results that are more interpretable. As Joe Dien described, the way that PCA data are stored internally in the toolkit are as factor scores (i.e., component scores). When you extract amplitudes from a PCA component, you are extracting the factor scores multiplied by a constant (some scaling factor, representing the electrode where you extract it from). Thus, according to Joe Dien, the p-values should be the same regardless of whether you use AutoPCA, or extract from a single electrode or multiple electrodes (it would be good to verify this). What is changing is merely the scaling factor (i.e., the constant that is multiplied by all factor scores). When you select multiple electrodes, it is computing the PCA-estimated amplitude at each electrode and performing a simple average across those electrodes. The AutoPCA extracts the PCA-estimated amplitude at the peak channel and the peak timepoint. If the waveform is negative-going at the peak channel, and you are interested in the positive-going dipole, you would select the peak positive channel to identify the PCA-estimated amplitude of the positive-going waveform on that PCA component. Nevertheless, even though you are selecting the PCA-estimated amplitude for a given channel at a given electrode, there are now “virtual channels”; the estimates include the contributions of all channels and all timepoints to the extent that they load onto the PCA component of interest. Thus, even if you select to window a PCA component from only 1 channel at 1 timepoint, it is using ALL channels and timepoints in the estimation—this is not the case if windowing the raw ERP waveforms.\nSave the files generated from the AutoPCA in the 6 - PCA Components folder using the following naming convention: “erp_task_condition_age_component” (e.g.,erp_ob_tgt_all_P3).\n\nTo view all of the tsPCA components, click View and input the following\n\nselect the appropriate file (e.g., ob_tgt_all)\nselect gave\nselect none\nclick Waves\n\nIt is good practice to check to make sure that components are comparable across different age ranges\n\nYou can check this in one of two ways:\n\nVisually examine grand averages between age ranges\nApply the PCA from one age group and apply it to another age group and examine whether the results hold up using cross-validation in EPToolkit\n\n\nIt is generally useful to keep track of the extraction process for each PCA component. Our current procedure for doing so is keeping a PCA_ComponentInfo.txt file in the 6-PCA Components folder.\n\nRelevant information may include the timing used to window a component and the electrodes that load onto the component of interest at a given threshold",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#channels",
    "href": "eegERP.html#channels",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.8 Identifying Electrodes that Load Onto PCA Component",
    "text": "2.8 Identifying Electrodes that Load Onto PCA Component\n\nGo to Window\nSelect the PCA file of interest (e.g., tsPCA_ob_tgt_all)\nClick the Channels button (about halfway down the Window window)\nClick Factor\n\nFrom the dropdown, select the PCA file of interest (e.g., tsPCA_ob_tgt_all)\nEnter the threshold in the space below (e.g., 0.5)\n\nThis sets the minumum factor loading value for an electrode to be “included” in the component-related cluster\n\nDepending on whenther you are interested in positive or negative factor loadings, select the appropriate sign (+, -, or +/-)\nA popup window with PCA factors will appear. Select the component(s) you wish to identify spatially (e.g., TF01SF01)\n\nWhen prompted, give the electrode cluster a name\nThe channels that change color are those which load onto the selected component at or above the threshold value\nIf you plan to use this cluster in extracting a PCA component, you’ll need save the cluster\n\nClick Save in the Window window\nSave the cluster to 6-PCA Components",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#exporting-grand-average-data",
    "href": "eegERP.html#exporting-grand-average-data",
    "title": "EEG and ERP Processing and Analysis",
    "section": "2.9 Exporting Grand Average Data",
    "text": "2.9 Exporting Grand Average Data\nIf you are only interested in the grand average data and not individual subjects, these instructions will allow you to export a .txt file containing only the grand average data.\n\nFrom EP Toolkit home (Main screen), select Edit\nSelect the .ept averages file (e.g., ob_tgt_all) that contains a “subject” representing the grand average\n\nIf the file does NOT contain a grand average subject, follow the steps in the above section to generate it\n\nRename the file\n\nFor example, ob_tgt_all could be renamed to ob_tgt_gav\nRenaming the file will prompt EP Toolkit to ask whether you want to generate a new file with this new name, or overwrite the existing datafile once your changes are complete\n\nSelect Subjects from the options at the top of the editor window\nClick All from among the options on the lefthand side of the Subjects window\n\nThis will select all of the subjects\n\nScroll to the bottom of the list of subjects and deselect the subject labeled grand average\n\nEssentially, the goal here is to create a dataset that includes ONLY the grand average information, rather than each individual subject\n\nOnce everything EXCEPT for the grand average subject is selected, click Delete on the lefthand side of the editor window\n\nThis will remove the individual subject data from the dataset and leave the grand average information\n\nClick Done\nIf you renamed the datafile, EP Toolkit should generate a popup message asking whether you would like to rename your dataset OR generate a new dataset using the new name (leaving the original dataset untouched). From the options presented, click New to generate a new file and preserve the original\nThe editor window should close, returning you to the EP Toolkit pane that asks you to select a dataset to edit. From here, click Main to return to EP Toolkit “home”\nOnce in the main window, click Save\n\nSet the save format to Text (.txt)\nClick the grand average data (e.g., ob_tgt_gav) to save it\nA file explorer window should open, prompting you to select the appropriate save location and give your file a name",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#r-code-for-grand-average-waveform-plot",
    "href": "eegERP.html#r-code-for-grand-average-waveform-plot",
    "title": "EEG and ERP Processing and Analysis",
    "section": "4.1 R Code for Grand Average Waveform Plot",
    "text": "4.1 R Code for Grand Average Waveform Plot\n\nRead in the grand average waveform data exported from EP Toolkit.\n\nWe currently process the conditions within a given task separately, so each condition should have its own grand average file.\n\n\n\nCode\nobTgt &lt;- read.table(\"V:/SRS-ERP-Oddball/Hard/All/4 - EPT Averages/2024-11-05/gave/ob_tgt_gav.txt\")\nobFrq &lt;- read.table(\"V:/SRS-ERP-Oddball/Hard/All/4 - EPT Averages/2024-11-05/gave/ob_frq_gav.txt\")\n\n\nCreate a subset of data that only includes those electrodes that are part of the clusters identified in EP Toolkit.\n\nThe grand average data does not have row or column labels, but the columns represent the EEG net channels in numerical order (1-129). We can therefore use their column index values to select the desired electrodes; so, the list containing the channel numbers should include ONLY numbers. The code that selects these channels out of the full dataset will rely on numerical input.\n\n\n\nCode\n# Set electrode clusters\nobElectrodes &lt;- c(58, 59, 64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 81, 82, 83, 84, 89, 80, 91, 95, 96, 101)\n\n# Subset to desired electrodes\nobTgt_sub &lt;- obTgt[, obElectrodes]\nobFrq_sub &lt;- obFrq[, obElectrodes]\n\n\nCompute averages and create labels for conditions\n\nOnce the data have been subsetted down to include only the electrode channels of interest, all that remains is to compute the average amplitude across all of those channels\nAdding a condition label will allow us to combine the two condition-specific datasets into one that can be used for visualizations\n\nFor ease of plotting, name the conditions the way that you would like them to appear on the figure (i.e., “Target” instead of “tgt”)\n\n\n\n\nCode\n# Compute averages \nobTgt_sub$amplitude &lt;- rowMeans(obTgt_sub)\nobFrq_sub$amplitude &lt;- rowMeans(obFrq_sub)\n\n# Remove raw values and add condition labels\nobTgt_amps &lt;- obTgt_sub %&gt;% select(amplitude) %&gt;% mutate(condition = \"Target\")\nobFrq_amps &lt;- obFrq_sub %&gt;% select(amplitude) %&gt;% mutate(condition = \"Frequent\")\n\n\nAdd timing-related information to the data\n\nEP Toolkit exports ERP data without timestamps, but arranges it in order of timing\nWe can create a template with the appropriate timestamps and append this column to the amplitude data\n\n\n\nCode\n# Create template\nerpTemplate &lt;- data.frame(\n  time = -199:1000\n)\n\n# Merge template with amplitude data\nobTgtTimes &lt;- cbind(erpTemplate, obTgt_amps)\nobFrqTimes &lt;- cbind(erpTemplate, obFrq_amps)\n\n\nCombine all conditions into a single data object to be used for plotting\n\n\nCode\noddball &lt;- rbind(obTgtTimes, obFrqTimes) %&gt;% \n           select(time, condition, amplitude) %&gt;% \n           arrange(time)\n\n\nGenerate the waveform figures\n\n\nCode\nggplot(\n  data = oddball,\n  aes(\n    x = time,\n    y = amplitude,\n    group = condition,\n    color = condition\n  )\n) +\n  geom_line(linewidth = 1.5) +\n  scale_x_continuous(\n    name = \"Time Relative to Stimulus Onset (ms)\",\n    limits = c(-200, 1000),\n    breaks = seq(from = -200, to = 1000, by = 200)) +\n  scale_y_continuous(\n    name = \"Voltage (microvolts)\",\n    limits = c(-4, 10),\n    breaks = seq(from = -10, to = 15, by = 2)) +\n  scale_color_viridis_d()+\n  theme_classic(base_size = 18) +\n  theme(\n    legend.position = c(.7, .9),\n    legend.title = element_blank())",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#troubleshooting",
    "href": "eegERP.html#troubleshooting",
    "title": "EEG and ERP Processing and Analysis",
    "section": "5.1 Troubleshooting",
    "text": "5.1 Troubleshooting\n\nRunning out of space on EP Toolkit? You can navigate to your folder (maybe under Documents/MATLAB/EPwork) and delete everything except for EPprefs to refresh your workspace. NOTE: This will delete everything stored in EP Toolkit, so remember to back up files that you need to save.\nIf you get an error or a warning from Git when trying to commit and/or push ERP files up to the repo, you may need to initialize Git LFS. The instructions for initializing Git LFS can be found here.\n\nThe filetypes that commonly cause such errors in our current ERP processing pipeline are:\n\n.txt files\n.ept files\n.set files",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#to-do",
    "href": "eegERP.html#to-do",
    "title": "EEG and ERP Processing and Analysis",
    "section": "5.2 To-do",
    "text": "5.2 To-do\n\nBetter describe the missingness for files\n\nWe need a systematic way to identify new ways to process the missingness\nFind a way to best describe and report the ways of missingness\n\nGo through the maxmem edits on the clean_rawData question. We want a standardized value on the machines\nLook at the warning messages for the automatic script updates\n\nautomatic cleaning of files problems\n\nIntegrate ERPLAB with our existing EEGLab Functions including:\n\nAdding an event list:\n\nCurrently, some code for this is updated in the script on the lab drive\nDocumentation is here\n\nFigure out how to average epochs and export to the EP Toolkit\n\nEvaluate the semi-automated pipelines from:\n\nDebnath et al. (2020)\nDesjardins et al. (2021)\nFlo et al. (2022)\nGabar-Durnam et al. (2018)\nHaresign et al. (2021)\nKumaravel et al. (2022)",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#eeglab",
    "href": "eegERP.html#eeglab",
    "title": "EEG and ERP Processing and Analysis",
    "section": "5.3 EEGLAB",
    "text": "5.3 EEGLAB\n\n5.3.1 EEGLab Processing Steps\nhttps://eeglab.org/tutorials/\n\nFiltering\nAverage Referencing\nArtifact Rejection\n\nAutomated artifact rejection (save intermediate file)\nManual selection of bad channels\nManual selection of bad time periods (save intermediate file)\nRemoval of manually selected bad channels\nRemoval of manually selected bad time periods (save intermediate file)\nIndependent Component Analysis (ICA)\n\nRun ICA\nAutomated removal of bad ICA components\nRe-run ICA (save intermediate file)\nManual selection of bad ICA components (save intermediate file)\nRemoval of manually selected bad ICA components\n\n\nInterpolation of Bad Channels\nAverage Referencing\nSegmentation\nBaseline Correction (save final file)\n\n\n\n5.3.2 Install Plugins\nMFFMatlabIO plugin\n\n\n5.3.3 Import Data\nFile → Import Data → Using EEGLAB functions and plugins → Import Magstim/EGI .mff file\nEEG = pop_mffimport({'\\\\\\\\lc-rs-store24.hpc.uiowa.edu\\\\lss_itpetersen\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\0-Raw Data (mff)\\\\1613_90_oddball.mff'},{'code'},0,0);\n\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'overwrite','on','gui','off');\nSelect .mff file(s)\nEvent type field (may select multiple): code\n\n\n5.3.4 Filter Data\nFilter → Basic FIR Filter\n0.1 – 30 Hz\nsave as new name\nEEG = pop_eegfiltnew(EEG, 'locutoff',0.1,'hicutoff',30,'plotfreqz',1);\n\n\n5.3.5 Average Referencing\nTools → Re-reference the data → Compute average reference\nEEG = eeg_checkset( EEG );\nEEG = pop_reref( EEG, []);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'setname','1613_90_oddball_fil_ref','overwrite','on','gui','off'); \neeglab redraw;\n\n\n5.3.6 Artifact Rejection\n\n5.3.6.1 Automated Artifact Rejection\nTools → Reject data using Clean Rawdata and ASR\nEEG = eeg_checkset( EEG );\nEEG = pop_clean_rawdata(EEG, 'FlatlineCriterion',5,'ChannelCriterion',0.8,'LineNoiseCriterion',4,'Highpass','off','BurstCriterion',20,'WindowCriterion',0.25,'BurstRejection','on','Distance','Euclidian','WindowCriterionTolerances',[-Inf 7] );\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'gui','off'); \neeglab redraw;\nCurrently, we run into a bug that yields the following error:\nNot enough memory, This is not a bug (Error occurred in function asr_process() at line 132)\nAs a workaround, type the following code in MATLAB to edit the function:\nedit clean_artifacts\nThen, change the number in the following line to a larger number (e.g., 256) and click save:\n{'max_mem','MaxMem'}, 256, ...\n\n\n5.3.6.2 Selection of Bad Channels\nView data to identify bad channels to reject.\nEdit → Select data\nSpecify channels to reject\nSpecify “on -&gt; remove these”\nEEG = eeg_checkset( EEG );\nEEG = pop_select( EEG, 'nochannel',{'E44','E56','E57','E113'});\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'setname','1613_90_oddball_fil_ref_chn','overwrite','on','gui','off'); \neeglab redraw;\n\n\n5.3.6.3 Selection of Bad Time Periods\nPlot → Channel data (scroll)\nChange voltage scale to 50\nSettings → Time range to display\nChange to 10 seconds\nSettings → Number of channels to display\nChange to number of channels to view at one time (e.g., 64)\nTo erase a selected portion of the data, first drag the mouse (holding down the left mouse button) horizontally across the time region of interest to mark it for rejection. To deselect a portion of the data, simply click on the selected region.\nAfter marking some portions of the data for rejection, press REJECT and a new data set will be created with the rejected data omitted. A new dataset will be created with the marked regions removed. Your goal is to reject non-stereotypic artifacts. Do not reject blinks/saccades, because independent component analysis will remove those.\nClicking “Stack” stacks all electrodes on top of each other to more easily identify noisy data.\nClick “REJECT” to remove the bad time periods from the data file.\n\n\n5.3.6.4 Independent Component Analysis\nhttps://eeglab.org/tutorials/06_RejectArtifacts/RunICA.html (archived at https://perma.cc/AEU9-GB3B)\nhttps://socialsci.libretexts.org/Bookshelves/Psychology/Book%3A_Applied_Event-Related_Potential_Data_Analysis_(Luck)/14%3A_Appendix_3%3A_Example_Processing_Pipeline (archived at https://perma.cc/9QYQ-BNFE)\nThe component order returned by runica.m is in decreasing order of the EEG variance accounted for by each component.\nTools → Decompose data by ICA\nEEG = eeg_checkset( EEG );\nEEG = pop_runica(EEG, 'icatype', 'runica', 'extended',1,'interrupt','on');\n[ALLEEG EEG] = eeg_store(ALLEEG, EEG, CURRENTSET);\neeglab redraw;\nPlot → Component maps → 2D\nVisually identify independent components to remove\nTools → Inspect/label components by map\nExample ICA Artifact Components\n    \nExample ICA Brain Components   \nOverview of ICA Components \nToggle the “Accept” button to reject an independent component, press “OK” to specify it for rejection\nAutomated detection of artifactual ICA components:\nhttps://eeglab.org/tutorials/06_RejectArtifacts/RunICA.html#automated-detection-of-artifactual-ica-components (archived at https://perma.cc/5RQ7-9WBT)\nEEG = eeg_checkset( EEG );\nEEG = pop_iclabel(EEG, 'default');\n[ALLEEG EEG] = eeg_store(ALLEEG, EEG, CURRENTSET);\nEEG = eeg_checkset( EEG );\nEEG = pop_icflag(EEG, [NaN NaN;0.9 1;0.9 1;NaN NaN;NaN NaN;NaN NaN;NaN NaN]);\n[ALLEEG EEG] = eeg_store(ALLEEG, EEG, CURRENTSET);\neeglab redraw;\nThere are six categories of components: Brain, Muscle, Eye, Heart, Line Noise, Channel Noise, and Other\nOur goal is to keep the brain components and to remove everything else (i.e., artifacts).\nTools → Classify components using ICLabel → Label components\nTools → Classify components using ICLabel → Flag components as artifacts\nSubstracting rejected ICA components:\nhttps://eeglab.org/tutorials/06_RejectArtifacts/RunICA.html#subtracting-ica-components-from-data (archived at https://perma.cc/HVH4-Z4SA)\nTools → Remove components\nEEG = pop_subcomp( EEG, [1    2    6    7   10   13   21   24   26   31   32   33   36   43   44   51   54   55   59   61   67   68   74   83   90   91   93   99  103  112  113  116  118  121], 0);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 6,'gui','off'); \neeglab redraw;\n\n\n\n5.3.7 Interpolate Bad Channels\nhttps://sccn.ucsd.edu/pipermail/eeglablist/2016/011199.html (archived at https://perma.cc/97NH-8LAR)\nTo interpolate channels you would load up one file that has only the good channels, then load up a second file that has the full channel list, and then run the channel interpolation function from the eeglab gui.\nTools → Interpolate Electrodes → Use all channels (or specific channels?) of other dataset\nUsing all channels of other dataset:\nImportant Note: Interpolating files will re-reference the data. Average reference the data after interpolating channels.\nEEG = eeg_checkset( EEG );\nEEG = pop_interp(EEG, ALLEEG(3).chanlocs, 'spherical');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 4,'gui','off'); \neeglab redraw;\nUsing specific channels of other dataset:\nEEG = eeg_checkset( EEG );\nEEG = pop_interp(EEG, ALLEEG(3).chanlocs([44   56   57  113]), 'spherical');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'gui','off'); \neeglab redraw;\nRemoved channels:\nEEG = eeg_checkset( EEG );\nEEG = pop_interp(EEG, EEG.chaninfo.nodatchans([44  56  57  113]), 'spherical');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 3,'gui','off'); \nEEG = eeg_checkset( EEG );\neeglab redraw;\nData channels:\nEEG = eeg_checkset( EEG );\nEEG = pop_interp(EEG, [44  56  57  113], 'spherical');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'gui','off'); \neeglab redraw;\n\n\n5.3.8 Average Referencing\nTools → Re-reference the data → Compute average reference\nEEG = eeg_checkset( EEG );\nEEG = pop_reref( EEG, []);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'setname','1613_90_oddball_fil_ref','overwrite','on','gui','off'); \neeglab redraw;\n\n\n5.3.9 Segmentation\nTools → Extract Epochs\nEEG = eeg_checkset( EEG );\nEEG = pop_epoch( EEG, {  'frq+'  'tgt+'  }, [-0.2           1], 'newname', '1613_90_oddball_fil_ref epochs', 'epochinfo', 'yes');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'overwrite','on','gui','off'); \n\n\n5.3.10 Baseline Correction\nEEG = eeg_checkset( EEG );\nEEG = pop_rmbase( EEG, [-200 0] ,[]);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 1,'overwrite','on','gui','off'); \neeglab redraw;\n\n\n5.3.11 Automatic Script example\n% Starting EEG Lab \n[ALLEEG EEG CURRENTSET ALLCOM] = eeglab;\n\n%% Helpful documentation is located here\n% https://eeglab.org/tutorials/11_Scripting/Using_EEGLAB_history.html (archived at https://perma.cc/Y687-5GKE)\n% https://eeglab.org/tutorials/ConceptsGuide/Data_Structures.html (archived at https://perma.cc/5F39-5S32)\n\n%Loading in the Dataset\nEEG = pop_mffimport({'R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\0-Raw Data (mff)\\\\1613_90_oddball.mff'},{'code'},0,0);\nEEG.setname='raw';\nEEG = eeg_checkset( EEG );\n%Storing the current dataset\n[ALLEEG, EEG] = eeg_store( ALLEEG, EEG, 1);\n% refreshing the graphical interface\neeglab redraw;\n\n%Filter the data\nEEG = pop_eegfiltnew(ALLEEG(1), 'locutoff',0.1,'hicutoff',30,'plotfreqz',1);\n%Save the Filtered dataset \n%ALLEEG EEG CURRENTSET seems to just be a name for all of the current data\n%sets \n%pop_newset seems to save the dataset in both memory and in the toolkit\n[ALLEEG, EEG, CURRENTSET] = pop_newset(ALLEEG, EEG, 2,'setname','fil','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\1-Filtering (fil)\\\\tcid_wave.set','gui','off'); \neeglab redraw;\n\n% Average referencing\n%EEG = eeg_checkset( EEG );\nEEG = pop_reref( ALLEEG(2), []);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 3,'setname','Avg Ref','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\9-Average Reference (ref)\\\\tcid_wave_avg.set','gui','off');\neeglab redraw;\n\n% Rejecting the artifacts\n% testing bumping the \nEEG = pop_clean_rawdata(ALLEEG(3), 'FlatlineCriterion',5,'ChannelCriterion',0.8,'LineNoiseCriterion',4,'Highpass','off','BurstCriterion',30,'WindowCriterion',0.25,'BurstRejection','on','Distance','Euclidian','WindowCriterionTolerances',[-Inf 7] );\n%Saving cleaned dataset\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 4,'setname','clean data','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\10-Clean Artificats (clean)\\\\tcid_wave_clean.set','gui','off'); \neeglab redraw;\n\n% Placeholder to manually remove bad channels\n\n\n% ICA components\nEEG = pop_runica(ALLEEG(4), 'icatype', 'runica', 'extended', 1,'interrupt','on');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 5,'setname','ICA test','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\11-ICA\\\\tcid_wave_ICA.set','gui','off'); \neeglab redraw;\n\n%Manually reject ICA components\nEEG = pop_subcomp( EEG, [1    2    6    7   10   13   21   24   26   31   32   33   36   43   44   51   54   55   59   61   67   68   74   83   90   91   93   99  103  112  113  116  118  121], 0);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 6,'gui','off'); \neeglab redraw;\n\n% Place holder to remind to manually remove the channels that we would\n% like to reject\n% taking the EEG channel lock of the first (raw) dataset\nEEG = eeg_checkset( EEG );\nEEG = pop_interp(ALLEEG(5), ALLEEG(1).chanlocs, 'spherical');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 6,'setname','Interpolated','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\12-Interpolate\\\\tcid_wave_Interpolate.set','gui','off'); \neeglab redraw;\n\n% Segmenting the Data\nEEG = eeg_checkset( EEG );\nEEG = pop_epoch( ALLEEG(6), {  'frq+'  'tgt+'  }, [-0.2 1], 'newname', 'tcid_wave_segmented', 'epochinfo', 'yes');\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 7,'setname','Segmented','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\13-Segment\\\\tcid_wave_Segment.set','gui','off'); \neeglab redraw;\n\n% Baseline Correcting the Data\nEEG = eeg_checkset( EEG );\nEEG = pop_rmbase( ALLEEG(7), [-200 0] ,[]);\n[ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 8,'setname','Baseline Correct','savenew','R:\\\\Lab\\\\Studies\\\\School Readiness Study\\\\Data\\\\LV2\\\\ERP\\\\Oddball\\\\MATLAB\\\\14-Baseline-Correct\\\\tcid_wave_baseline-correct.set','gui','off'); \neeglab redraw;\n\n\n5.3.12 Automatic script that loops files\n\n5.3.12.1 Warning messages with script\nWarning messages appeared when using the automatic cleaning of data. We may have to send a dataset to someone so they can check on it. Some documentation is found here:\n\nhttps://sccn.ucsd.edu/pipermail/eeglablist/2021/016222.html (archived at https://perma.cc/9SDG-NGXD)\nhttps://sccn.ucsd.edu/wiki/EEGLAB_bug1971 (archived at https://perma.cc/H7PA-TPTZ)\n\n\n\n5.3.12.2 Filtering, average referencing, and automatically cleaning the data.\nThis script batch reads in files, filters them, average references, and automatically cleans them. After that, this script reads in the cleaned files for manually processing to remove bad time periods and bad channels\n%10/20/22 Working script that reads everything into matlab\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%Location of Raw oddball files\nrawOddballFiles = '\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Studies\\School Readiness Study\\Data\\LV2\\ERP\\Oddball\\MATLAB\\0-Raw\\';\n%Location of path to save cleaned files\ncleanAutoPath = '\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Studies\\School Readiness Study\\Data\\LV2\\ERP\\Oddball\\MATLAB\\10-Clean Artificats (clean)\\';\n\n%all oddball files in the directory to be processed\nfilesInDirectory = dir(fullfile(rawOddballFiles, '*.mff')); %Reading the files in the directory\n\n%Listing the number of subjects for the number of times to loop\nnumberOfSubject = height(filesInDirectory);\n\n%Names of all the files in the directory\nlistOfSubjects={filesInDirectory.name}; \n\n%splitting the filename up\nfilenameSplit = regexp(listOfSubjects, '_', 'split');  \nid = cellfun(@(x) x{1},filenameSplit,'un',0);  %getting the id's\nwave = cellfun(@(x) x{2},filenameSplit,'un',0); %getting the waves\n\n\n\n[ALLEEG EEG CURRENTSET ALLCOM] = eeglab;\n\nfor i=1:numberOfSubject\n\n    %%%% Importing Data\n    path = [rawOddballFiles,filesInDirectory(i).name];\n    tcid = char(strcat(id(i),'_', wave(i))); %combining the TCID and wave\n\n    EEG = pop_mffimport({path},{'code'},0,0);\n    % Saving the data in memory\n   [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'setname', tcid,'gui','off');\n\n    % Filtering the data\n    EEG = pop_eegfiltnew(ALLEEG(i), 'locutoff',0.1,'hicutoff',30,'plotfreqz',1);\n    [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'overwrite','on','gui','off');\n    \n    % Average Referencing\n    EEG = pop_reref( ALLEEG(i), []);\n    [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'overwrite','on','gui','off');\n\n    % They strongly advise importing channel locations before using this\n    % function.\n    %Cleaning the Data\n    EEG = pop_clean_rawdata(ALLEEG(i), 'FlatlineCriterion',5,'ChannelCriterion',0.8,'LineNoiseCriterion',4,'Highpass','off','BurstCriterion',30,'WindowCriterion',0.25,'BurstRejection','on','Distance','Euclidian','WindowCriterionTolerances',[-Inf 7]);\n    nameClean = [tcid,'_autoClean.set'];\n    savePathClean = [cleanAutoPath,nameClean];\n    [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'setname', nameClean, 'savenew', savePathClean, 'gui','off', 'overwrite', 'on');\n\n\nend\neeglab redraw:\n\n%% Reading in automatically cleaned datasets\n\n%%%%%%%%%%%%%%\n% Clear Data becasue to overwrite anything in memory\n%%%%%%%%%%%%%%\nALLEEG = [];\nEEG = [];\n\n% Adding the path for cleaned files\ncleanAutoPath = '\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Studies\\School Readiness Study\\Data\\LV2\\ERP\\Oddball\\MATLAB\\10-Clean Artificats (clean)\\';\n\n%Starting EEGLAB\n[ALLEEG EEG CURRENTSET ALLCOM] = eeglab;\n\n\n%Location of cleaned oddball data\nfilesInDirectoryClean = dir(fullfile(cleanAutoPath, '*.set')); %Reading the files in the directory\n\n\n%Listing the number of subjects for the number of times to loop\nnumberOfSubjectClean = height(filesInDirectoryClean);\n\n%Names of all the files in the directory\nlistOfSubjectsClean = {filesInDirectoryClean.name}; \n\n%splitting the filename up\nfilenameSplitClean = regexp(listOfSubjectsClean, '_', 'split');  \nid = cellfun(@(x) x{1},filenameSplitClean,'un',0);  %getting the id's\nwave = cellfun(@(x) x{2},filenameSplitClean,'un',0); %getting the waves\n\n\nfor i=1:numberOfSubjectClean\n\n    %%%% Importing Data\n    tcidClean = char(strcat(id(i),'_', wave(i), '_autoClean.set')); %combining the TCID and wave to name the file\n    EEG = pop_loadset('filename', tcidClean, 'filepath', cleanAutoPath); \n    [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'setname', tcidClean,'gui','off');\n\nend\neeglab redraw;\n\n\n\n5.3.13 Removing bad time periods and channels\n\nIn the now open EEGLAB interface, select a dataset. Go to Tools &gt; Inspect/reject data by eye\n\n\n\n\nvisually reject artifacts\n\n\n\nGo through the raw signal and manually reject bad time periods\n\n\n\n\nvisually reject artifacts\n\n\n\nSelect REJECT and overwrite the file in memory\n\nselect Overwrite it in memory (set=yes; unset=create a new dataset)\n\nVisually inspect the data and select any bad channels, and write them down\nNext, manually reject the channels by selecting Edit then Select Data\n\n\n\n\nvisually reject artifacts\n\n\n\nManually enter the channels to be removed in the Channel range field and select the checkbox under on-&gt;remove these and select Ok\nSave the file as tcid_wave_manualClean.set in the following drive path\n\n\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Studies\\School Readiness Study\\Data\\LV2\\ERP\\Oddball\\MATLAB\\11-Manually Cleaned\nThis screenshot represents removing channels 23 56 57 97 \n\nRepeat step for each cleaned dataset\n\n\n\n5.3.14 Running the ICA\nThis script runs the ICA. We’ll want to think about how to automatically reject the components here. Once this runs, see the ICA section to reject components. Tools → Inspect/label components by map is how to reject components.\n% Running the ICA\n\n%% Reading in the manually cleaned datasets\n\n%%%%%%%%%%%%%%\n% Clear Data becasue to overwrite anything in memory\n%%%%%%%%%%%%%%\nALLEEG = [];\nEEG = [];\n\n% Adding the path for cleaned files\nmanualCleanPath = '\\\\lc-rs-store24.hpc.uiowa.edu\\lss_itpetersen\\Lab\\Studies\\School Readiness Study\\Data\\LV2\\ERP\\Oddball\\MATLAB\\11-Manually Cleaned\\';\n\n%Starting EEGLAB\n[ALLEEG EEG CURRENTSET ALLCOM] = eeglab;\n\n\n%Location of cleaned oddball data\nfilesInDirectoryManualClean = dir(fullfile(manualCleanPath, '*.set')); %Reading the files in the directory\n\n\n%Listing the number of subjects for the number of times to loop\nnumberOfSubjectManualClean = height(filesInDirectoryManualClean);\n\n%Names of all the files in the directory\nlistOfSubjectsManualClean = {filesInDirectoryManualClean.name}; \n\n%splitting the filename up\nfilenameSplitManualClean = regexp(listOfSubjectsManualClean, '_', 'split');  \nid = cellfun(@(x) x{1},filenameSplitManualClean,'un',0);  %getting the id's\nwave = cellfun(@(x) x{2},filenameSplitManualClean,'un',0); %getting the waves\n\n\nfor i=1:numberOfSubjectManualClean\n\n    %%%% Importing Data\n    tcidClean = char(strcat(id(i),'_', wave(i), '_manualClean.set')); %combining the TCID and wave to name the file\n    EEG = pop_loadset('filename', tcidClean, 'filepath', manualCleanPath); \n    [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, i,'setname', tcidClean,'gui','off');\n\n    %%% Running the ICA\n    EEG = eeg_checkset( EEG );\n    EEG = pop_runica(EEG, 'icatype', 'runica', 'extended',1,'interrupt','on');\n    [ALLEEG EEG] = eeg_store(ALLEEG, EEG, CURRENTSET);\n    eeglab redraw;\n\nend\neeglab redraw;",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "eegERP.html#netstation",
    "href": "eegERP.html#netstation",
    "title": "EEG and ERP Processing and Analysis",
    "section": "5.4 NetStation",
    "text": "5.4 NetStation\n\n5.4.1 Processing Order\n\nFiltering (fil)\nSegmentation (seg)\nManually selected bad channels (seg)\nBad Channel Replacement (bcr)\nArtifact Detection (art)\nBad Channel Replacement (bcr)\nArtifact Detection (art)\nAveraging (ave)\nAverage Reference (ref)\nBaseline Correction (blc)\nGrand-Averaging\n2-step PCA: Temporal PCA then Spatial PCA\n\n\n\n5.4.2 Folder Structure\n\n\nRaw Data (mff)\n\n\nFiltering (fil)\n\n\nSegmentation (seg)\n\n\nManually Selected Bad Channels\n\n\nBad Channel Replacement_m (bcr)\n\n\nArtifact Detection (art)\n\n\nBad Channel Replacement (bcr)\n\n\nArtifact Detection (art)\n\n\nAveraging (ave)\n\nSkip if doing PCA on individual trial-level data\n\n\n\nAverage Reference (ref)\n\n\nBaseline Correction (blc)\n\n\nFiles for PCA – drop files that have fewer than 10 artifact-free (and correct?) trials per condition\n\n\nSimple Binary\n\n\nIndividual Trials\n\n\nEPT Averages\n\n\nPCA\n\n\nPCA Components\n\n\n\n\n\n5.4.3 Data Processing\n\nOpen NetStation Tools and select Automated Processing Script 1: Run on raw data files (mff)\n\nFiltering\nSegmentation\n\nOpen segmented (seg) files in NetStation Review and select bad channels manually\n\nTo mark channels as bad across all segments:\n\nPut Ø next to bad channels, these changes will automatically be saved when you close the file\n\nTo mark channels as bad in individual segments:\n\nWhen in topoplot mode, make sure to have the “Click to Change Channel Status” checked, then click on the channel.\n\n\nOpen NetStation Tools and Manual Script 2: Run on segmented files (seg)\n\nBad Channel Replacement (bcr)\nArtifact Detection (art)\nBad Channel Replacement (bcr)\nArtifact Detection (art)\nAveraging (ave)\nAverage Reference (ref)\nBaseline Correction (blc)\n\nIn NetStation Review, open each file after the second Bad Channel Replacement\n\nDetermine how many trials are marked as good in each condition\n\nThis is found in the Segment panel on the bottom left when you have your file open. If you open that view, you can use the diverging arrows (those may be hard to see depending on your screen size) in the upper right corner of the “Bad Channels” button in the Segment panel to open in a new window for more screen real estate. The categories will be listed on the left, with the number of good trials noted over the total number of trials in that category.\n\nIf the file has 10 or more good trials in each condition:\n\nMake note of the bad channels (that are marked as bad across all segments) for each subject on “EEG Behavior and Bad Channels Sheet”\nThen, proceed with the next step\n\nIf the file does not have 10 more good trials in each condition:\n\nGo back to Step 2 and proceed from there\nSelect additional bad channels manually so that they aren’t included in later processing steps, which might increase the number of trials kept\nIt can be helpful to take a look, in the file after the second Bad Channel Replacement file, at each trial that was marked as bad.\n\nIf a trial was clearly bad (many artifacts across many channels) and correctly marked as bad, leave it alone.\nIf a trial was marked as bad but appeared to have only a few bad channels (i.e., only a few bad channels led it to be marked as bad), then it can be helpful to mark those channels as bad for that individual segment (rather than marking the channel as bad across all segments—unless the channel is bad across many segments)\n\n\n\nIn NetStation Review, open each file after the Averaging step\n\nIf there are continue to be channels that look bad, go back to Step 2 and proceed from there\nIn Step 2, mark those channel(s) as bad across all segments\nRepeat process until there are no more bad channels after Script 2\n\nFor files that have at least 10 good trials in each condition, copy and paste (don’t cut/move!) their file from the “10-Baseline Correction (blc)” folder into the “11-Files for PCA” folder\nRun the temporo-spatial PCA on those files for data reduction\n\n\n\n5.4.4 Notes\nOptions for removing bad segments: - The combine files tool has an exclude bad trials checkbox. If you run this tool on only one file, it is considered a no-op, the intended operation is not done, but it does appear to remove the bad segments. You would have to do this one run per file, if there are multiple files in the input subpane, they will be combined. - In the Net Station File Export Tool, there is a checkbox option to “Export only good segments” when exporting to both the MAT format, and also when exporting to Simple Binary (RAW) format. - In Net Station 5.3, EGI introduced the workflow concept into Net Station Review. The first workflow is combine segment/categories. This workflow allows for shuffling segments around between categories. It is quite a flexible tool, but importantly it includes a “Remove Bad Segments” button. So, you could do a no-op as far as changing categories but write out a MFF with the bad segments excluded.\nOptions for data processing: - Data processing in Net Station → file export to simple binary → PCA in ERP PCA Toolkit - Data processing and PCA in ERP PCA Toolkit - Data processing in EEGLab → PCA in ERP PCA Toolkit - Data processing in MATLAB using scripts → PCA in ERP PCA Toolkit - Ryan Priefer’s approach: - Filter in EEGLab → data processing and PCA in ERP PCA Toolkit: segment, artifact detection/bad channel replacement, montage re-reference/baseline correct, average, then export to .txt file and PCA\n\n\n5.4.5 Waveform Tools\n\nFiltering\n\nHighpass: 0.1 Hz\nLowpass: 30.0 Hz\n\n\nSegmentation\n\nOddball\n\nFrequent:\n\nCode is frq+\n\nTarget:\n\nCode is tgt+\n\nOptions:\n\nSegment length before: 200ms (baseline length)\nSegment length after: 1000ms (post-stimulus length)\nOffset: 2ms\nCheck “Copy specs to Segment”\n\n\n \nFishsharks\n\nCorrect go:\n\nCode is Go++\nTrial Spec eval is 1\nTrial Spec rtim is greater than or equal to 200\n\n\n\n\nCorrect no-go:\n\nCode is NG++\nTrial Spec eval is 0\n\n\n\n\nIncorrect go:\n\nCode is Go++\nTrial Spec eval is 0 or Trial Spec rtim is less than 200\n\n\n\n\nIncorrect no-go:\n\nCode is NG++\nTrial Spec eval is 1\n\n\n\n\nAll go:\n\nCode is Go++\n\n\n\n\nAll no-go:\n\nCode is NG++\n\n\n\n\nOptions:\n\nSegment length before: 200ms\nSegment length after: 1000ms\nOffset: 17ms\n\n\nStop-signal\n\nGo - correct response\n\nCode is GoCR or NGGC\nTrial Spec rtim is greater than or equal to 200\n\n\n\n\nGo - incorrect response\n\nCode is GoIn or NGGI\n\n\n\n\nGo - omission error\n\nCode is GoOm\n\n\n\n\nNoGo - correct\n\nCode is NNCR\n\n\n\n\nNoGo - incorrect\n\nCode is NNCC or NNCI\n\n\n\n\nNoGo - incorrect: correct E\n\nCode is NNCC\n\n\n\n\nNoGo - incorrect: incorrect E\n\nCode is NNCI\n\n\n\n\nAll go:\n\nCode is CoGR or GoOm or GoIn or NGGC\n\n\n\n\nAll no-go:\n\nCode is NNCR or NNCC or NNCI\n\n\n\n\nResponse-locked correct go\n\nCode is rcor\nKey code cel# is 3\n\n\n\n\nResponse-locked incorrect stop\n\nCode is resp\nKey Code cel# is 4\n\n\n\n\nResponse-locked incorrect go\n\nCode is rinc\nKey Code cel# is 3\n\n\n\n\nOptions:\n\nSegment length before: 200ms\nSegment length after: 1000ms\nOffset: 16ms\n\nNote: for “response locked”, offset is 0ms\n\n\n\n\nManually prcoess data\nBad channel replacement\n\nPrior to running, select bad channels manually\n\n\nArtifact detection\n\nBad channels:\n\nMax - min &gt; 200.00 uv\nEntire segment\nMoving average: 80ms\n\nEye blink:\n\nMax - min &gt; 175.00 uv\nWindow size: 640ms\nMoving average: 80ms\n\nEye movement:\n\nMax - min &gt; 200.00 uv\nWindow size: 640ms\nMoving average: 80ms\n\nMark channel bad for all sefments if bad for greater than: 20% of segments\nMark segment bad if it:\n\nContains more than 20 bad channels\nContains an eyeblink (remove if doing ocular artifact removal)\nContains an eye movement (remove if doing ocular artifact removal)\n\nOverwrite all previous bad Segment information\nOverwrite all previous bad Channel information\n\n\nBad channel replacement\n\nSame as step 4\nSpecification name: Oddball 06. Bad Channel Replacement\n\nArtifact detection\n\nSame as step 5\n\nAveraging\n\nHandle source files: Separately\nHandle subjects: Separately\n\nTo create final merge file for one ERP for whole sample (i.e., grand average – for graphing purposes only, not analysis – select “Together” for each)\n\nDo not compute noise estimate\n\n\nMontage options\n\nAverage reference\n\n\nBaseline correction\n\nSample baseline with respect to: segment time = 0\nBaseline begins: 200ms (length of baseline) before sample and is 200ms (length of baseline) long\n\n\nCombine files (using the Averaging Tool)\n\nHandle source files: Together\nHandle subjects: Separately\nDo not compute noise estimate\n\n\nFile Export\n\nSimple binary\nCalibrate data\nExport calibration information\nExport history information\nExport reference channel\nExport only good segments (if not doing data processing in othe software)\n\n\nGrand averaging\n\nHandle source files: Together\nHandle subjects: together\nCopy events from first file\n\n\nFile export\n\nFormat: Tab-delimited text (.txt)\nExport only good segments\n\n\n\n\n\n5.4.6 EEG Behaviors and Bad Channels Form\n\nIndicate which channels were badd across all trials in the last “bad channel replacement” (bcr) step (Step 6)\n\nInclude tirals that were marked bad (across all trials) either manually or automatically\n\nIndicate which channels were manually marked bad on each trial for those that were marked bad on a trial by trial basis (step 3)\n\n\n\n5.4.7 Troubleshooting and Q&A\nQ: How do I mark a channel as bad for an individual trial/segment?\nA: To mark an individual trial bad when in topoplot mode, make sure to have the “Click to Change Channel Status” checked and then click on the channel. Click again to make it good. Hold down the Command key to make it good or bad for the whole experiment.\nQ: When trying to do some batch data processing in NetStation Tools, I received an error when trying to add a group of .mff files to the “Input Files” box. The error says “Blue files are open in review. In order to process these files, they need to be closed in Review fist” What can I do?\nA: If you run files through the FileValidator found in the Utilities (Application&gt;EGI&gt;Utilities) folder, this will fix the error. If you don’t have that, you can right-click on the files, choose “Show package contents” and deleted the lock.txt or aquiriring.xml file if found\nQ: When trying to import files to NetStation Tools from an external hard drive, they do not show up in the “Input Files” box but no error is given. How can I resolve this?\nA: Copy the files to a local directory (e.g., a folder on the Desktop) before importing to NetStation Tools.",
    "crumbs": [
      "About",
      "Electroencephalography/Event-Related Potentials"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Git, GitLab, and GitHub",
    "section": "",
    "text": "For code below, Open Git (or Powershell, Terminal, Command Prompt, etc.) in directory of repository and use the relevant code.",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#toBegin",
    "href": "git.html#toBegin",
    "title": "Git, GitLab, and GitHub",
    "section": "3.1 To begin",
    "text": "3.1 To begin\n\nInstall git (https://git-scm.com/downloads)\nInstall GitHub Desktop app (https://desktop.github.com)\nCreate a Personal Access Token (https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html, archived at https://perma.cc/6ZH8-JNZC; or https://research-git.uiowa.edu/-/profile/personal_access_tokens, archived at https://perma.cc/7JPU-WDBS) on the UI GitLab instance (https://research-git.uiowa.edu) to access UI GitLab repositories from GitHub Desktop.\n\nFor application name, put “GitHub”\nFor expiration date, leave it blank (or set it for as late an expiration date as possible)\nFor scopes, select everything\n\nWhen cloning a repository from UI’s GitLab instance using GitHub Desktop, GitHub Desktop will ask for your username and password. Enter your HawkID as your username (or your GitHub.com username if different) and your UI GitLab Personal Access Token as your password.",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#bestPractices",
    "href": "git.html#bestPractices",
    "title": "Git, GitLab, and GitHub",
    "section": "3.2 Best practices working with version control",
    "text": "3.2 Best practices working with version control\n\nCreate a new repository (repo) whenever you start a new project\nFollow the Petersen Lab template for how to structure your repo (folder structure, .gitignore file, etc.):\n\nhttps://research-git.uiowa.edu/PetersenLab/Template\n\nTo collaborate with others:\n\nNavigate to the repo on the UI GitLab website\nWhen in the repo, click “Members”\nAdd the Collaborator\n\nEach time you want to work on the files in the repo, follow this cycle:\n\nUsing the GitHub Desktop app, sync the repo files from the cloud to the repo on your local computer (i.e., fetch any repo updates to your local machine from the cloud)\n\nMake sure to do this before many code changes so you are working with the latest version of files\n\nDo your work on the repo: make any code/file/folder additions, changes, or deletions\nUsing the GitHub Desktop app, commit the changes\n\nCommit changes to the cloud early and often; when deciding what to commit and when, try to group “similar changes” into the same commit (“like goes with like”)\nUse a separate “commit” for each separable “functional unit” changed\n\nUsing the GitHub Desktop app, sync the repo files from your local computer to the repo on the cloud (i.e., push all of your committed changes to the cloud)\n\nNever leave file changes uncommitted or unsynced when you stop working on the repo for the day!",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#instructions-for-using-github-desktop-app-with-gitlab",
    "href": "git.html#instructions-for-using-github-desktop-app-with-gitlab",
    "title": "Git, GitLab, and GitHub",
    "section": "3.3 Instructions for using GitHub Desktop app with GitLab",
    "text": "3.3 Instructions for using GitHub Desktop app with GitLab\n\nhttps://itnext.io/how-to-use-github-desktop-with-gitlab-cd4d2de3d104 (archived at https://perma.cc/S59R-3YT7)\nhttps://community.reclaimhosting.com/t/using-github-desktop-with-gitlab/876 (archived at https://perma.cc/ZPT9-KEQL)\nhttps://stackoverflow.com/questions/22639815/does-github-for-windows-work-with-gitlab (archived at https://perma.cc/99RR-6KLE)\nhttps://github.com/desktop/desktop/issues/852#issuecomment-402546848 (archived at https://perma.cc/3GCU-GTPU)\nhttps://github.com/desktop/desktop/issues/3816#issuecomment-421060974 (archived at https://perma.cc/N8RS-BFWR)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-create-a-repo-on-local-computer",
    "href": "git.html#how-to-create-a-repo-on-local-computer",
    "title": "Git, GitLab, and GitHub",
    "section": "3.4 How to create a repo on local computer",
    "text": "3.4 How to create a repo on local computer\n\nCreate repository on UI GitLab website (https://research-git.uiowa.edu)\nOpen GitHub Desktop app\nClone repository\nAdd .gitignore file (from Template project: https://research-git.uiowa.edu/petersenlab/Template/blob/master/.gitignore) to the root of the cloned project folder\nFollow the Petersen Lab template for how to structure your repo (folder structure, .gitignore file, etc.): https://research-git.uiowa.edu/PetersenLab/Template\n\nNote that a folder will not be synced if there are no files in them (i.e., if the folder is empty)\n\nSync file changes using GitHub Desktop (“Fetch origin”, “Push origin”)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#preExistingRepoLabDrive",
    "href": "git.html#preExistingRepoLabDrive",
    "title": "Git, GitLab, and GitHub",
    "section": "3.5 How to add a pre-existing repo from the lab drive (LSS/RDSS/network share) to your computer",
    "text": "3.5 How to add a pre-existing repo from the lab drive (LSS/RDSS/network share) to your computer\nFor example, the SRS-DataProcessing repo lives on the lab drive.\n\nMake sure Dr. Petersen has given you collaborator access to the repo\nOpen GitHub Desktop app\nNavigate to the folder location of the repo on the lab drive\nDrag the .git folder within the repo to the GitHub Desktop app",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-clone-a-repo-from-gitlab-to-local",
    "href": "git.html#how-to-clone-a-repo-from-gitlab-to-local",
    "title": "Git, GitLab, and GitHub",
    "section": "3.6 How to clone a repo from GitLab to local",
    "text": "3.6 How to clone a repo from GitLab to local\n\nNavigate to relevant repo on GitLab (https://research-git.uiowa.edu)\nClick Clone in right-hand corner, select Clone with HTTPS\nOpen GitHub Desktop App and click File, Clone Repository\nClick the URL tab\nPaste in the URL\nNavigate to where you want to save it\n\nThe recommended location for your repos is to create a folder titled GitHub in your Documents folder, and to put repos in the GitHub folder (by default: PC: C:/Users/[USERNAME]/Documents/GitHub/; Mac: /Users/[user]/Documents/GitHub/) because various lab scripts try to read the lab functions from this location; it is NOT recommended to put git repos in a OneDrive folder because [git files tend not to play nice with syncing services](https://stackoverflow.com/questions/19305033/why-is-putting-git-repositories-inside-of-a-dropbox-folder-not-recommended; archived at https://perma.cc/UTX8-KVL9) (e.g., OneDrive, Dropbox)\n\nClick Clone\nEnter your GitLab username as your username and your Personal Access Token as your password",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-add-modify-or-delete-files-in-a-repo",
    "href": "git.html#how-to-add-modify-or-delete-files-in-a-repo",
    "title": "Git, GitLab, and GitHub",
    "section": "3.7 How to add, modify, or delete files in a repo",
    "text": "3.7 How to add, modify, or delete files in a repo\n\nOpen relevant repo in GitHub Desktop app\nPull any repo updates from the server to the local files (“Fetch origin”, “Pull origin”)\nMake necessary additions, modifications, and deletions to the files\nCreate commits for all changes in GitHub Desktop app (one commit per substantive change): Enter “Summary” and “Commit to master”\nAfter making all changes and commits, push local file changes to the server using GitHub Desktop (“Fetch origin”, “Push origin”)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-collaborate-with-others",
    "href": "git.html#how-to-collaborate-with-others",
    "title": "Git, GitLab, and GitHub",
    "section": "3.8 How to collaborate with others",
    "text": "3.8 How to collaborate with others\n\nNavigate to the repo on the UI GitLab website\nWhen in the repo, click “Settings”\nClick “Members”\nAdd the Collaborator",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "Git, GitLab, and GitHub",
    "section": "3.9 How to create a pull request",
    "text": "3.9 How to create a pull request\nTo make/propose changes to a repo that you do not have write permissions for, you need to create a pull request.\n\nTo create a pull request to a repo that you do not have write permissions for, first create a fork of the repo. To fork the repo, see here: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo#forking-a-repository (archived at https://perma.cc/33P6-Y4BR)\nAfter forking the repo, sync the repo with the latest version from the cloud so you know you are editing the latest version of the files. When asked by GitHub Desktop how you plan to use the fork, select “To contribute to the parent project”, as in the image below. \nMake any changes to the files in the repo that you’d like to incorporate into the repo.\nOpen a pull request with your changes: https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork (archived at https://perma.cc/KAP3-L5L7)\nThe owner of the repo will review your changes and decide whether to accept them or whether additional changes are needed.\n\nYou can also create a pull request if you have write permissions to the repo. This can be useful when collaborating on code with multiple people. To do this, first, create a new branch. Then, make your changes in the new branch. After making your changes, create a pull request to merge the changes in the new branch into the main branch.\nNote: If your pull request is showing up in your local repo but not in the original repo (for the repo owner), you might need to delete your forked repo and re-fork the original repo.",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-clone-a-repo-into-local-directory-with-a-different-folder-name-directory-must-be-empty",
    "href": "git.html#how-to-clone-a-repo-into-local-directory-with-a-different-folder-name-directory-must-be-empty",
    "title": "Git, GitLab, and GitHub",
    "section": "3.10 How to clone a repo into local directory with a different folder name (directory must be empty)",
    "text": "3.10 How to clone a repo into local directory with a different folder name (directory must be empty)\n\nGit Bash into directory\n\ngit clone https://research-git.uiowa.edu/petersenlab/srs/SRS-DataProcessing.git .\ngit remote set-url --add origin https://research-git.uiowa.edu/petersenlab/srs/SRS-DataProcessing.git\ngit remote -v",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-transfer-a-repo-to-a-new-locationgroupsubgroup",
    "href": "git.html#how-to-transfer-a-repo-to-a-new-locationgroupsubgroup",
    "title": "Git, GitLab, and GitHub",
    "section": "3.11 How to transfer a repo to a new location/group/subgroup",
    "text": "3.11 How to transfer a repo to a new location/group/subgroup\n\nCreate location/group/subgroup (e.g., PetersenLab/School Readiness Study)\nWhen in the repo, click “Settings”\nGo to “Advanced”, and click “Expand”\nGo to “Transfer project”, and select the location/group/subgroup you want to transfer the repo to under “Select a new namespace”\nIn the local repo, edit the repo URL in the .git/config file\nWhen asked for your password, enter your username (HawkID) and GitLab Personal Access Token",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#gitLfs",
    "href": "git.html#gitLfs",
    "title": "Git, GitLab, and GitHub",
    "section": "3.12 How to use large file storage (LFS)",
    "text": "3.12 How to use large file storage (LFS)\nIn GitLab, we use large file storage (LFS) to store large files. In GitHub, we do not use LSS to store large files, because the University does not have an Enterprise agreement with GitHub (thus, we would have to pay to use it). As a consequence, individual files hosted in a GitHub repo must be less than 50 MB in size. By contrast, individual files hosted in a GitLab repo may be larger than 50 MB.\nTo set up LFS in for a GitLab repo, follow these steps:\n\nMake sure the large files are not in the repo yet.\nIn GitHub Desktop, open the repo you want to use LFS for\nIn GitHub Desktop, select the Repository tab, then select Command Prompt or similar\nIn the command prompt, type (based on instructions from: https://docs.gitlab.com/ee/topics/git/lfs/; archived at https://perma.cc/6WMC-GTKN):\n\ngit lfs install          # initialize the Git LFS project\ngit lfs track \"*.Rdata\"  # select the file extensions that you want to treat as large files\n\nThis should have created a .gitattributes file in the repo. In GitHub Desktop, commit and push the .gitattributes file to the cloud version of the repo.\nCopy the large files into the repo.\nIn GitHub Desktop, commit and push the large file to the cloud version of the repo.\n(Or, if not using GitHub Desktop, can commit in the command prompt (https://docs.gitlab.com/ee/topics/git/lfs/)):\n\ngit add .                                            # add the large file to the project\ngit commit -am \"insert name of commit message here\"  # commit the file meta data\ngit push origin master                               # sync the git repo and large file to the GitLab server\n\nWhen asked for credentials, use your HawkID as your username and your personal access token as your password",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#how-to-create-a-new-repository-on-a-shared-network-drive-e.g.-lab-drive",
    "href": "git.html#how-to-create-a-new-repository-on-a-shared-network-drive-e.g.-lab-drive",
    "title": "Git, GitLab, and GitHub",
    "section": "3.13 How to create a new repository on a shared network drive (e.g., Lab Drive)",
    "text": "3.13 How to create a new repository on a shared network drive (e.g., Lab Drive)\n\nCreate a repo on GitHub or GitLab online\nOpen Git Bash on the desktop\nUsing Git Bash, set the current directory to the path where the new repo will be. For example, use the following command to clone to the School Readiness Study\n\ncd \"R:\\Lab\\Studies\\School Readiness Study\"\n\nNext, go to GitLab or GitHub online and obtain the HTTPS URL to the new repository that was created. Clone the repository with the HTTPS link using Git Bash with the following command\n\ngit clone (HTTPS Link)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#revertCommit",
    "href": "git.html#revertCommit",
    "title": "Git, GitLab, and GitHub",
    "section": "3.14 How to revert changes to a previous commit",
    "text": "3.14 How to revert changes to a previous commit\n\nOpen the Github Desktop and navigate to the repository you would like to revert changes for\nNext, click the history tab\nRight click the commit you would like to revert back to, and select revert changes\nPush the changes to the repo to complete the revision\n\n\n\n\nrevert changes",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#partialCommit",
    "href": "git.html#partialCommit",
    "title": "Git, GitLab, and GitHub",
    "section": "3.15 How to perform a partial commit",
    "text": "3.15 How to perform a partial commit\n\nIn GitHub Desktop, click the desired lines in the gutter\nCreate the commit, and leave the other changes for you to continue working on.\n\nhttps://github.blog/2015-01-14-partial-commits-in-github-for-windows (archived at https://perma.cc/5U3V-YWRF)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#copyRepo",
    "href": "git.html#copyRepo",
    "title": "Git, GitLab, and GitHub",
    "section": "3.16 How to copy a repo to a new repo",
    "text": "3.16 How to copy a repo to a new repo\nhttps://github.com/new/import",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#templateRepo",
    "href": "git.html#templateRepo",
    "title": "Git, GitLab, and GitHub",
    "section": "3.17 How to make a repository a template repository",
    "text": "3.17 How to make a repository a template repository\nhttps://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository (archived at https://perma.cc/PYW5-KAP5)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#repoFromTemplate",
    "href": "git.html#repoFromTemplate",
    "title": "Git, GitLab, and GitHub",
    "section": "3.18 How to create a respository from a template",
    "text": "3.18 How to create a respository from a template\nhttps://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template (archived at https://perma.cc/9E2C-MUCK)",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#authenticationFailed",
    "href": "git.html#authenticationFailed",
    "title": "Git, GitLab, and GitHub",
    "section": "4.1 Error: Authentication Failed",
    "text": "4.1 Error: Authentication Failed\n\n\n\nauthentication failed\n\n\nThis error can occur for a variety of reasons. One possibility is that your GitLab Personal Access Token (PAT) has expired. To fix this:\n\nCreate a Personal Access Token (https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html, archived at https://perma.cc/6ZH8-JNZC; or https://research-git.uiowa.edu/-/profile/personal_access_tokens, archived at https://perma.cc/7JPU-WDBS) on the UI GitLab instance (https://research-git.uiowa.edu) to access UI GitLab repositories from GitHub Desktop.\n\nFor application name, put “GitHub”\nFor expiration date, leave it blank (or set it for as late an expiration date as possible)\nFor scopes, select everything\n\nWhen cloning a repository from UI’s GitLab instance using GitHub Desktop, GitHub Desktop will ask for your username and password. Enter your HawkID as your username and your UI GitLab Personal Access Token as your password.\n\nIf you do not receive a prompt to enter your username and password, try removing the saved GitHubcredentials from Windows Credential Manager: https://github.com/desktop/desktop/issues/8860#issuecomment-2211812646",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#github-trying-to-connect-to-git-config-in-another-users-user-folder",
    "href": "git.html#github-trying-to-connect-to-git-config-in-another-users-user-folder",
    "title": "Git, GitLab, and GitHub",
    "section": "4.2 GitHub trying to connect to git config in another user’s User folder",
    "text": "4.2 GitHub trying to connect to git config in another user’s User folder\nTo resolve, ensure that the HOME environment variable is set to your user folder (assuming that it is not a shared computer)\nTo do so: - Type “Environment Variables” into the search bar of your Windows task bar - Click “Open” under the result of “Edit the system environment variables” - Enter an admin user/password when prompted - Click “Environment Variables” at the bottom right corner of the popup window that should open after entering admin credentials - Find the HOME environment variable and click it to select (highlighted blue) - Click “Edit” and update the path to match your Users folder",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#github-shows-all-files-as-being-changed-even-though-the-files-havent-changed",
    "href": "git.html#github-shows-all-files-as-being-changed-even-though-the-files-havent-changed",
    "title": "Git, GitLab, and GitHub",
    "section": "4.3 GitHub shows all files as being changed even though the files haven’t changed",
    "text": "4.3 GitHub shows all files as being changed even though the files haven’t changed\nWindows and Mac use different line endings (https://github.com/Microsoft/WSL/issues/184; archived at https://perma.cc/F8UX-YJP3):\ngit config --global core.autocrlf true",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#error-you-are-not-allowed-to-push-code-to-protected-branches-on-this-project",
    "href": "git.html#error-you-are-not-allowed-to-push-code-to-protected-branches-on-this-project",
    "title": "Git, GitLab, and GitHub",
    "section": "4.4 Error: You are not allowed to push code to protected branches on this project",
    "text": "4.4 Error: You are not allowed to push code to protected branches on this project\n\nMake sure the owner of the repo unprotects the branch (https://stackoverflow.com/questions/32246503/fix-gitlab-error-you-are-not-allowed-to-push-code-to-protected-branches-on-thi; archived at https://perma.cc/98AF-N6BY)\nOpen the repo, click “Settings”, “Repository”, “Protected Branches”\nChange “Allowed to merge” and “Allowed to push” to “Developers and Maintainers”, and click “Unprotect”",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#error-creating-commit",
    "href": "git.html#error-creating-commit",
    "title": "Git, GitLab, and GitHub",
    "section": "4.5 Error creating commit",
    "text": "4.5 Error creating commit\ngit add -A\ngit status\ngit commit -m \"Message\"` (where \"Message\" is the summary message of the commit)\ngit push\ngit status",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#undo-commit-but-retain-file-changes",
    "href": "git.html#undo-commit-but-retain-file-changes",
    "title": "Git, GitLab, and GitHub",
    "section": "4.6 Undo commit (but retain file changes)",
    "text": "4.6 Undo commit (but retain file changes)\ngit reset --soft HEAD^",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#error-sync-failed",
    "href": "git.html#error-sync-failed",
    "title": "Git, GitLab, and GitHub",
    "section": "4.7 Error: Sync failed",
    "text": "4.7 Error: Sync failed\ngit status\ngit push",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#error-sync-failed----syncing-would-overwrite-your-uncommitted-changes",
    "href": "git.html#error-sync-failed----syncing-would-overwrite-your-uncommitted-changes",
    "title": "Git, GitLab, and GitHub",
    "section": "4.8 Error: Sync failed -- Syncing would overwrite your uncommitted changes",
    "text": "4.8 Error: Sync failed -- Syncing would overwrite your uncommitted changes\nhttps://stackoverflow.com/questions/23084822/github-some-uncommited-changes-would-be-over-written-by-syncing (archived at https://perma.cc/AP5S-G38B):\n\ngit stash -u\n(perform manual sync in GitHub Desktop)\ngit stash pop",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#error-pipeline-has-failed-for-master",
    "href": "git.html#error-pipeline-has-failed-for-master",
    "title": "Git, GitLab, and GitHub",
    "section": "4.9 Error: Pipeline has failed for master",
    "text": "4.9 Error: Pipeline has failed for master\nhttps://docs.gitlab.com/ee/topics/autodevops/#at-the-project-level (archived at https://perma.cc/3MAQ-ZBFF)\n\nOpen the repo in GitLab\nGo to your project’s “Settings” &gt; “CI/CD” &gt; “Auto DevOps”\nDisable (uncheck) the “Default to Auto DevOps pipeline”\nClick “Save changes”",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#creating-repo-on-local-directory-if-directory-is-empty",
    "href": "git.html#creating-repo-on-local-directory-if-directory-is-empty",
    "title": "Git, GitLab, and GitHub",
    "section": "5.1 Creating Repo on Local Directory (if Directory is empty)",
    "text": "5.1 Creating Repo on Local Directory (if Directory is empty)\n\nCreate Repo on GitHub\nDon’t add README yet\nOpen Git Shell, navigate to directory, and type:\n\ngit init\ngit remote add origin https://github.com/DevPsyLab/petersenlab.git\ngit remote -v\n\nDrag and drop the folder with the repository into the GUI app\nAdd .gitignore file with .Rhistory",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#creating-repo-on-local-directory-if-directory-is-not-empty",
    "href": "git.html#creating-repo-on-local-directory-if-directory-is-not-empty",
    "title": "Git, GitLab, and GitHub",
    "section": "5.2 Creating Repo on Local Directory (if Directory is not empty)",
    "text": "5.2 Creating Repo on Local Directory (if Directory is not empty)\n\nCreate Repo on GitHub\nDon’t add README yet\nOpen Git Shell, navigate to directory, and type:\n\ngit init\ngit add .\ngit commit -m 'First commit'\ngit remote add origin https://research-git.uiowa.edu/itpetersen/PetersenLab.git\ngit remote -v\ngit push -u origin master\n\nDrag and drop the folder with the repository into the GUI app\nAdd .gitignore file with .Rhistory",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "git.html#creating-r-scripts-repo-on-lab-server",
    "href": "git.html#creating-r-scripts-repo-on-lab-server",
    "title": "Git, GitLab, and GitHub",
    "section": "5.3 Creating R Scripts Repo on Lab Server",
    "text": "5.3 Creating R Scripts Repo on Lab Server\n\nCreate R Scripts Repo\nOpen R Scripts folder and delete .git folder\nOpen Git Shell and type:\n\ngit init\ngit add .\ngit commit -m 'First commit'\ngit remote add origin https://research-git.uiowa.edu/PetersenLab/R-Scripts.git\ngit remote -v\ngit push -u origin master\n\nMove R Scripts folder to another location\nClone repo into folder:\n\nopen Git Shell\nnavigate to Z:\\TDS II\\Data\\R Scripts\\\n\ngit clone https://research-git.uiowa.edu/PetersenLab/R-Scripts.git .\nThe dot on the end of the git clone command means “the current directory”\nDrag and drop the folder with the repository into the GUI app\nAdd .gitignore file with .Rhistory",
    "crumbs": [
      "About",
      "Git, GitLab, and GitHub"
    ]
  },
  {
    "objectID": "hlm.html",
    "href": "hlm.html",
    "title": "Hierarchical Linear Modeling",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n\n\n\n\n\n\n\nCode\nlibrary(\"petersenlab\")\nlibrary(\"lme4\")\nlibrary(\"nlme\")\nlibrary(\"lmerTest\")\nlibrary(\"MASS\")\nlibrary(\"MCMCglmm\")\nlibrary(\"performance\")\nlibrary(\"ggplot2\")\n\n\n\n\n\n\n\nCode\nmydata &lt;- read.csv(\"https://osf.io/cqn3d/download\")\n\n\n\n\n\n\n\nCode\nset.seed(52242)\n\nmydata$outcome &lt;- rpois(nrow(mydata), 4)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#install-libraries",
    "href": "hlm.html#install-libraries",
    "title": "Hierarchical Linear Modeling",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#load-libraries",
    "href": "hlm.html#load-libraries",
    "title": "Hierarchical Linear Modeling",
    "section": "",
    "text": "Code\nlibrary(\"petersenlab\")\nlibrary(\"lme4\")\nlibrary(\"nlme\")\nlibrary(\"lmerTest\")\nlibrary(\"MASS\")\nlibrary(\"MCMCglmm\")\nlibrary(\"performance\")\nlibrary(\"ggplot2\")",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#import-data",
    "href": "hlm.html#import-data",
    "title": "Hierarchical Linear Modeling",
    "section": "",
    "text": "Code\nmydata &lt;- read.csv(\"https://osf.io/cqn3d/download\")",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#simulate-data",
    "href": "hlm.html#simulate-data",
    "title": "Hierarchical Linear Modeling",
    "section": "",
    "text": "Code\nset.seed(52242)\n\nmydata$outcome &lt;- rpois(nrow(mydata), 4)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#gcm",
    "href": "hlm.html#gcm",
    "title": "Hierarchical Linear Modeling",
    "section": "6.1 Growth Curve Models",
    "text": "6.1 Growth Curve Models\n\n6.1.1 Linear Growth Curve Model\n\n6.1.1.1 Plot Observed Growth Curves\n\n\nCode\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = ageYears,\n    y = math,\n    group = id)) +\n  geom_line() +\n  scale_x_continuous(\n    name = \"Age (Years)\") +\n  scale_y_continuous(\n    name = \"Math Score\")\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.2 lme4\n\n\nCode\nlinearMixedModel &lt;- lmer(\n  math ~ female + ageYearsCentered + female:ageYearsCentered + (1 + ageYearsCentered | id), # random intercepts and slopes; sex as a fixed-effect predictor of the intercepts and slopes\n  data = mydata,\n  REML = FALSE, #for ML\n  na.action = na.exclude,\n  control = lmerControl(optimizer = \"bobyqa\"))\n\nsummary(linearMixedModel)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: math ~ female + ageYearsCentered + female:ageYearsCentered +  \n    (1 + ageYearsCentered | id)\n   Data: mydata\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  15857.9   15903.5   -7920.9   15841.9      2213 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3750 -0.5174  0.0051  0.5239  2.6396 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr\n id       (Intercept)      62.5365  7.9080       \n          ageYearsCentered  0.6767  0.8226   0.08\n Residual                  32.1505  5.6701       \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                         Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)              30.51401    0.56142 752.48747  54.352   &lt;2e-16 ***\nfemale                   -0.61290    0.79482 736.39886  -0.771    0.441    \nageYearsCentered          4.26792    0.11253 610.09410  37.925   &lt;2e-16 ***\nfemale:ageYearsCentered  -0.02558    0.16092 598.89155  -0.159    0.874    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female agYrsC\nfemale      -0.706              \nageYrsCntrd -0.635  0.448       \nfml:gYrsCnt  0.444 -0.631 -0.699\n\n\n\n6.1.1.2.1 Protoypical Growth Curve\n\n\nCode\nnewData &lt;- expand.grid(\n  female = c(0, 1),\n  ageYears = c(\n    min(mydata$ageYears, na.rm = TRUE),\n    max(mydata$ageYears, na.rm = TRUE))\n)\n\nnewData$ageYearsCentered &lt;- newData$ageYears - min(newData$ageYears)\n\nnewData$sex &lt;- NA\nnewData$sex[which(newData$female == 0)] &lt;- \"male\"\nnewData$sex[which(newData$female == 1)] &lt;- \"female\"\nnewData$sex &lt;- as.factor(newData$sex)\n\nnewData$predictedValue &lt;- predict( # predict.merMod\n  linearMixedModel,\n  newdata = newData,\n  re.form = NA\n)\n\nggplot(\n  data = newData,\n  mapping = aes(x = ageYears, y = predictedValue, color = sex)) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.2.2 Individuals’ Growth Curves\n\n\nCode\nmydata$predictedValue &lt;- predict(\n  linearMixedModel,\n  newdata = mydata,\n  re.form = NULL\n)\n\nggplot(\n  data = mydata,\n  mapping = aes(x = ageYears, y = predictedValue, group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.2.3 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot(\n  data = mydata,\n  mapping = aes(x = ageYears, y = predictedValue, group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line() +\n  geom_line(\n    data = newData,\n    mapping = aes(x = ageYears, y = predictedValue, group = sex, color = sex),\n    linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n6.1.1.2.4 Extract Random Effects\n\n\nCode\nranef(linearMixedModel)\n\n\n$id\n          (Intercept) ageYearsCentered\n201       1.125313761      0.214025701\n303     -12.515508864     -0.661489657\n2702     12.257492354      0.430762981\n4303      2.727957524      0.285002116\n5002      1.943700949      0.170789619\n5005      4.045982598      0.120286722\n5701     12.299499720      0.346624148\n6102     10.023593786      0.863981478\n6801     10.850796246      0.310493680\n6802     11.821009201      0.402109281\n6803     13.002187545      0.661133879\n9603      4.893027085      0.449774173\n12401     2.267934250      0.215890170\n12801     3.118093119      0.121909799\n13702     9.965512365      0.174385818\n13801    14.860587450      0.466936045\n13803    12.244139122      0.342974140\n17201     8.389382622      0.541441962\n18601    -9.114431785     -0.325174564\n22104    -8.224888366     -0.162602823\n22901    -3.619019303     -0.120180576\n23602     8.688461076      0.339697533\n23701     3.179383329     -0.022949766\n24402     5.504641645      0.352394764\n26201     9.719849574      0.702414569\n28503    -8.430258804      0.354120841\n29201    12.005754759      0.186774027\n29202     8.417070934      0.781161659\n34401    13.282239330      0.463013476\n35401     6.780780604      0.582522293\n35402     5.591003405     -0.049981300\n36901    15.699222750      0.479509729\n36902    10.244502566      0.276867605\n37403    -1.843161415     -0.402326018\n38202    -3.239859256     -0.028890813\n38802    -5.250923211     -0.340856108\n40501     3.463053677      0.153135154\n40702     7.971281145      0.173100533\n42504     2.888138805      0.321177217\n42505     6.585722700     -0.155175248\n43103     4.433636752      0.238674652\n45201     8.183501056      0.541547032\n45903    15.716178377      0.502448721\n46201    11.011297054      0.881456660\n47201    14.720202046      0.427719979\n47402     0.880121442     -0.190353845\n47601     1.216021904     -0.017165043\n48502    -2.519553727      0.312149007\n49501     1.743541361      0.182682300\n53601    -2.357792771     -0.378382000\n55201     8.475516854      0.442472479\n55801     2.607026981      0.226992780\n56301    -5.108650931      0.279722834\n56302    -0.069098635      0.007631557\n57604    10.137559691      0.648424109\n58503    -9.038592483     -0.145716134\n62102     7.594264242      0.514296379\n62103   -11.659099523      0.062806196\n62202    12.282894687      0.512176358\n62703     0.465584787      0.534532255\n64002     0.758192010     -0.318985718\n65601    12.288113181      0.422395609\n65701    11.634539842      0.659937401\n66002     4.577549270      0.481784449\n70801    -1.720978172     -0.098222417\n73602     3.129767946      0.121536903\n74602     4.600091569      0.228352651\n75603    -8.954581426     -0.158125796\n76402     0.105445380     -0.276301811\n76403     4.265544047      0.284415843\n80102     3.858025733      0.287026976\n82901    -3.386576272     -0.285969402\n83001    -1.334354997     -0.271017391\n83101    -6.092041392     -0.315778466\n83102    -0.060917140     -0.148750133\n83602     4.330407333     -0.021683400\n83603    -0.359555292      0.065349949\n85402    -2.755742657     -0.098799494\n87501    -1.199237512     -0.175971326\n87901     3.270068645      0.148146826\n88902    -5.915202379     -0.317691655\n89303    -0.058364793     -0.053342839\n89803     6.548041328      0.642729376\n90002    10.903475521      0.450119296\n92303     1.585091240     -0.179298523\n93202    -3.666591112     -0.040016525\n93901    -3.564923756     -0.080818748\n94002     0.625658727      0.677971537\n96201    -2.014940963     -0.158988595\n96202    -4.068449608     -0.163941086\n98102     6.960679147      0.369901362\n101701    0.168658221     -0.517413498\n101702   -3.629285495      0.142066301\n103501   -5.892939203     -0.645193590\n108201   16.298148686      0.607038953\n113001    8.357403604      0.635293810\n113201    5.570042678      0.382727677\n114101   -8.066311147     -0.029666619\n114702    4.802081813      0.209318715\n115901    2.373140471      0.543510541\n116002    0.862849698     -0.047444533\n116004   11.272635078      0.530370201\n118402   11.180501318      0.376954789\n122301   -7.041208476     -0.087766848\n124201   -2.235992657     -0.057998101\n124402   -6.654073179     -0.360197908\n125002   -2.036353581     -0.270098667\n126101    3.297895109      0.262547616\n129502   -6.797821803     -0.419879120\n129601   -4.382931097     -0.160115222\n129602    0.466740158      0.017451223\n132301    9.786381148      0.576039297\n132402   -8.839831108     -0.350099054\n132403    0.438064947     -0.144665332\n136001   -5.458706565     -0.412198348\n138901    2.422614923     -0.241055794\n138902   -0.353560559     -0.051066863\n139902   10.080325125      0.725346721\n140004   -3.496297543      0.282378348\n140104   -8.512781578     -0.655296107\n140901   -0.115083608      0.716800588\n140902    0.707055960     -0.308476123\n141001    9.865281000      0.428920620\n141601    3.020939622     -0.023542778\n141603    0.627584701     -0.092923215\n143901    3.106422910      0.127377649\n144004    0.045069645     -0.112213249\n153203   16.087092196      0.356217483\n153204  -17.430077261     -0.616817016\n153801    0.564118516     -0.113333334\n155902   -8.936419914     -0.348618461\n156401   12.187261187      0.348144705\n156605   10.323654494      0.486995803\n158701    8.513660697      0.336830662\n158702   10.497163871      0.545777637\n159605  -14.147922125     -0.755640107\n159606  -11.066745066     -0.540698054\n159801   -9.058337641      0.180842500\n165501   -8.160174645     -0.377886697\n165602    4.162599565      0.224329239\n165701  -10.200591739     -0.555846787\n165801    7.416486078      0.158087240\n166602   -1.002994170      0.072206197\n166603  -11.911124448     -0.292497710\n167501   11.597707011      0.876649436\n167502   17.304114029      0.737624890\n169003   -0.779869964     -0.052336104\n170403   -3.714540532     -0.672213772\n170404   -5.285749530     -0.231791835\n171401   -9.836468983     -0.069591914\n172502    4.642796953     -0.407352298\n173001   -5.588181640     -1.754863314\n173201    0.578502007      0.031869950\n174001   10.240215969      0.382702555\n175405  -10.792035414     -0.348151265\n175801   -7.789717327     -0.412713911\n176003    8.715520376     -0.004390948\n178901   -8.060279252     -0.232927660\n179401   -5.537013298     -0.293072902\n179701   -2.918516341     -0.698092331\n180301    7.530934833      0.497157407\n181801   -1.508291042     -0.134899316\n182903   -0.244975730     -0.013902331\n185304    2.293821497     -0.050900857\n187803    2.406041176      0.434335498\n188002    5.547847871     -0.253696393\n188502    7.326775168      0.461852110\n189601    7.020803583      0.239798464\n189602    2.295523328      0.565604150\n190202    2.361864346      0.109192928\n190501   11.272910613      0.411262167\n191202    0.109786069      0.287007128\n193601   -6.460102071      0.726747858\n193803    5.864314159      0.349916488\n196002   -6.930064877     -0.316839993\n196201    3.816025703      0.426787984\n196202   -0.173952604     -0.230887906\n204602    9.043456136      0.353334576\n205802    8.940355783      0.987958401\n207102   10.906815310      0.632212233\n207202    3.946083434      0.614767925\n207301   -0.305794959     -0.219240732\n212001    5.175627775      0.307630681\n213101   -1.604988068     -0.157754955\n213702    1.853348987      0.136708618\n215304   -1.680247506      0.283103740\n216701    7.463945088      0.259872555\n217402    4.141544076      0.234240048\n219003    2.764968942      0.701685107\n220401    3.029593716      0.699986352\n221202    8.544703486      0.357270902\n223002   15.104320740      0.623124524\n224901   -4.469678843     -0.379986217\n226001   -8.061210006      0.098195084\n226502   -7.274549310     -0.100782710\n226704   -7.527709790     -0.321128624\n227002    2.461705710     -0.802262248\n227101  -10.912047462     -0.095120844\n227102    3.993605419      0.448123850\n227201   -3.041064530     -0.137936840\n231502   -7.742581030      0.041937232\n233301   -1.019109719     -0.378574893\n233901    9.003114610      0.382666793\n233902    5.981925269      0.186142904\n235503   -8.586220486     -0.338828100\n236002   -3.383562465     -0.415619283\n236201   -6.061759642      0.038488620\n236202  -10.602033124     -0.071094358\n236901    5.500138415      0.331178677\n237501    6.492126787      0.518736481\n238301   -1.069920262     -0.345037345\n239603   -4.794009822     -0.505654150\n239604   -3.138157293     -0.053065007\n240701   -2.633824808     -0.230222843\n240702   -5.845413982      0.313454961\n244201    0.821623905     -0.010520847\n244702    0.386389868      0.049039506\n245903   -1.389917735     -0.441635774\n248402   -9.640091283     -0.385119312\n250502    1.575879281      0.185416145\n252902   -8.705342925     -0.428989701\n253001   -3.472250915     -0.321775121\n253603   -3.952048624     -0.234004326\n253802    0.813068757     -0.321677569\n254901    1.557440949      0.242732961\n255201    4.499756962      0.033367834\n255901  -12.966621777     -0.095135814\n255902   -6.408643305     -0.249179468\n255903   -0.068506317      0.018661532\n258801   10.066745258      0.473779137\n260702   10.140891704      0.574527205\n261501   -6.274288059     -0.078233942\n261502   -5.275763532      0.442941285\n261503   -3.768648304     -0.388219736\n262601    3.713484558     -0.009543953\n262602   13.156712564      0.474759507\n263801   11.035679006      0.567834817\n265902    0.656276991     -0.696886948\n268301   -7.098532980     -0.305426775\n268303   -3.792822734     -0.106412940\n268901   -2.171914979     -0.078913196\n268902   -0.431021818     -0.352369652\n269001   -6.011169857     -0.488260650\n269101    1.858122812      0.001046154\n269102   -2.443573441      0.415726816\n269902    4.732520442      0.381227590\n271001   -2.477610234     -0.146752234\n272903    3.562984351     -0.084305656\n273501    6.570778698      0.281221305\n274401   -2.812921787     -0.263584631\n277002   -1.087288775     -0.304425636\n277003    2.438031125      0.050445057\n277502   -0.269337940     -0.041138191\n278103    4.245009083      0.708932977\n281001   -0.006211057     -0.564986398\n281901    0.663646457      0.008984918\n282602    2.931241086      0.373471582\n282603    9.565849274      0.545267076\n287702   11.892041859      0.660432677\n288501    5.787422166      0.216667586\n290901   13.929416909      0.821404683\n290902    1.730188742      0.132665289\n291101    6.259580343      0.294992854\n294902   12.621060269      0.244878842\n294903    4.072045749      0.087980535\n295901   15.894522897      0.084921874\n295902   14.439897307      0.297412167\n296401    5.254230369      0.531416207\n297901    0.856386616     -0.254469811\n297902   -0.317311625      0.087017537\n298301   -0.060917140     -0.148750133\n301101   12.475974495      0.832250226\n301401    2.948685577     -0.147761333\n302301    0.416783844      0.060061145\n302702    0.821664368     -0.089719049\n302703   -0.236039902      0.125432324\n304103   -4.456206190      0.013561574\n304605   -4.255831240     -0.130330842\n307301    3.310356602     -0.241802776\n307501   -6.283556136     -0.083599724\n309802    9.967256858      0.457526730\n309901    3.484441714     -0.116828550\n310101    1.290473853      0.216435818\n311301   -0.080565206     -0.101463857\n311701    1.194153804      0.038496161\n312102    4.729692013      0.239638219\n312201    6.695393683      0.210621990\n312901   -2.426393020     -0.576742792\n314501   12.287217891      0.539511886\n315001    4.880252353      0.291273041\n315002   13.783249241      0.739403986\n315602    9.531309188      0.466146167\n316301    7.479274636      0.567983219\n316501    3.430731315      0.177619705\n316502    4.093043825      0.093542139\n319002   -0.169959992     -0.006645019\n319801    2.316432546     -0.055008041\n320101   -0.590631906     -0.433742436\n320102    5.817675834      0.281357695\n322505    8.674753283      0.562890678\n324401   12.299517798      0.797447725\n324402    4.946384990      0.388171137\n324801    8.928434794      0.521462250\n325401   -1.069020836      0.078768504\n325402   13.458927332      0.509744340\n325904   -7.522413853     -0.525541971\n326901   -1.850392851     -0.213568608\n328702    5.099223572     -0.092063809\n328703    4.774712475      0.172466962\n330601   -2.762453096     -0.532733818\n331506   -6.722315938     -0.032499717\n334201    0.565220953     -0.702132818\n335001    7.357640498      0.275755052\n335002    6.382367575      0.176234888\n335501   -2.374164673      0.235733420\n336401    1.185753997     -0.636458965\n336902   -2.810051956     -0.539564109\n340702    5.691374442      0.431253177\n340801   -1.079168948     -0.042968743\n342301    8.287865554      0.418811940\n342302    2.963952990      0.322217400\n342402    8.445022113      0.305838605\n343302    0.814012955     -0.236369473\n344602    3.405934612      0.408305541\n346501    5.906782681      0.108977062\n347603   -0.806916394     -0.093040005\n348103    3.580506175      0.234122072\n349101    1.276366531      0.448666727\n352201    3.186328990      0.040560676\n353101   -1.947908706     -0.288351018\n353102   -2.380699827     -0.099749105\n354502    9.850394404      0.528654400\n354503    9.198698723      0.503643488\n354801   -3.600435817     -0.005165597\n356402   -5.925097334     -0.571168841\n357203    5.670392546      0.505695693\n358401    7.765470294      0.330832752\n358601    6.140216902      0.314740464\n360901    7.365502773      0.282066811\n360903    2.573338090      0.148418657\n362402   12.566228909      0.502017802\n363501   -4.845935868     -0.025504536\n364103    4.081607768      0.345151016\n367903   -7.797804095     -0.300318789\n369702   -3.537069987     -0.288148448\n369704   -5.367416735     -0.466579600\n370303  -12.143514791     -0.661565047\n371502   -1.070681969      0.405907424\n371503    9.158128639      0.370666653\n372901   -2.054821266     -0.007560218\n373401    2.089441579     -0.635811582\n373402    0.662371372      0.159297299\n373701   -0.288263307      0.003967021\n373702    5.644432823      0.322687131\n374001   -0.110747486     -0.287723126\n374601    0.735524428      0.466568380\n374603   13.910932960      0.652962737\n375303    2.546408677     -0.043788948\n379301   -4.467474981     -0.061563521\n379801    2.594953809      0.092579877\n379805    6.491025148      0.348084566\n380201    8.253721688      0.600937233\n381701   11.880686797      0.601939035\n382101   -3.172071996     -0.512316258\n389902    8.452419713      0.525815025\n389903    8.152486867      0.455231844\n389904    7.664125643      0.176022901\n392102    3.039081126      0.530694935\n392103    5.318129275      0.121889968\n392301    5.085592795      0.793953355\n392303   12.050125677      0.437434640\n392401    7.264713421      0.552898978\n392402   13.512432396      0.609820206\n396202  -13.398745317     -0.946938695\n397202    8.679382924      0.091041479\n399503   -0.705322629     -0.146010479\n400101    8.118709048      0.075614547\n403701   -0.287986671     -0.051668622\n404503    2.423199648      0.012482426\n405002    6.685913424      0.373602320\n407702   -9.576743777     -0.251857014\n408501   -3.236321500     -0.053226091\n411002   -0.617464682     -0.225721762\n413202   -6.634279519     -1.016926229\n413203   -2.213288415     -0.954349714\n413301   -2.882921861     -0.161289727\n419401   -6.373053672      0.189996879\n420001   -4.571501859     -0.058357128\n421001    3.075297678     -0.268336113\n422401    5.343843769      0.178984109\n424602    5.060375861      0.294690080\n425401    5.432593598      0.382583700\n425402    2.867481495      0.648454212\n425601    2.336594616      0.003970176\n427502   -7.140019026     -0.228993677\n427703   -1.422417256      0.034062131\n431002    1.334772249     -0.038506921\n433901   15.572315818      0.288654765\n433902   11.537290700      0.286677735\n435201    8.604062734      0.596907622\n436602    1.286319883      0.083146056\n437201    8.199462385      0.436321271\n439602   -1.165039552     -0.145739082\n442301    9.283673513      0.599666755\n442401    5.058711460      0.149529313\n442801   12.422994645      0.514664140\n443701    3.372151631      0.034408513\n443702    2.965683552      0.327947098\n443902    3.330639612      0.094353232\n444702    2.245534630      0.067226124\n448304   -2.523919127     -0.125969751\n449401    6.124366003      0.278531064\n449501    8.108084433      0.541782203\n449601    0.408119166     -0.532499979\n449602    6.202286590      0.358563313\n452601    5.409230887      0.243634746\n453601    5.693903038      0.213866602\n453602   -5.393656916      0.057688338\n453801    1.806629624      0.220752263\n453802   -2.973447072     -0.002836201\n453901    7.956689417      0.239268753\n453902    8.794889567      0.541355860\n456401    5.376041588      0.429696055\n457002    5.982731795      0.382596107\n458902   -6.884003869     -0.177102358\n459201    9.832842881      0.295319911\n461402   -5.223562473      0.143944877\n463102   -5.914604999     -0.038007959\n463702    6.741926582      0.487732984\n466201   11.510648679      0.483259001\n468301    9.663006601      0.603336592\n468302   -8.402799551      0.015900699\n468303    8.048645994      0.423556521\n471701    4.069906707      0.601552192\n471702    3.402939075      0.080496875\n472303    7.309057094      0.598630621\n474602    2.784221554      0.020671451\n474603    4.789628688     -0.036726535\n474604   -0.450573465      0.677573886\n475401   16.015447512      0.789573937\n476601    5.337282412      0.220270084\n478501   -0.685925717      0.332780434\n480103    9.355832661      0.208839722\n481502    7.147688434      0.171696892\n482002    9.281364359      0.374158855\n482602    8.743116208      0.661362831\n487101   -1.809375644     -0.304686432\n488001   11.916067532      0.506033601\n488201    6.919367836      0.797842059\n488202    6.038326722      0.291781413\n489902   -0.996170096     -0.085142207\n492501    6.359078522      0.793704749\n492502    4.793253305      0.217510522\n492503    0.006056732      0.009332454\n493003   -5.864683724      0.089100729\n493301    2.406904487     -0.194779821\n495202    5.167157506      0.281204854\n497302   -0.429859827      0.381261492\n497303    1.799132506      0.107613092\n497304    4.510515532      0.277992784\n497403    3.287678510      0.373230303\n499703   -0.080124222     -0.004679628\n502802    6.182310300      0.839743266\n503201   11.013937291      0.385017956\n504101   13.683756959      0.341837997\n506601   -5.937988210     -0.115904557\n506602   -1.847719318     -0.128445017\n509801   -1.217008558     -0.013977666\n509802    0.352609641      0.119905736\n510002    0.591354117      0.692573879\n510301    0.260699490     -0.357018778\n510401    1.629703525      0.313254488\n511901    1.470324831      0.126388948\n513404   -5.060626103      0.282216269\n513405   -3.196731473     -0.480975630\n515102    8.546242755      0.295816500\n516401    1.572021876      0.083987753\n518003    8.172798539      0.353413081\n519101  -18.415106357     -0.896889938\n519503   -1.371753014     -0.455152222\n521602   -3.960317289     -0.532474702\n522401   -3.695855272     -0.143703661\n523101   -4.384639987     -0.047785062\n523201   -4.113749884      0.146941763\n523202    0.153012710      0.031958709\n524701    1.313244150      0.003955181\n524702    4.278511592     -0.264326940\n525801   10.488039492      0.466797622\n527204   -4.047762593     -0.234372042\n531401    0.940685154      0.455729347\n531402    7.289578167      0.190362256\n531404    7.240013774      0.331596609\n531704   -1.650361063     -0.144601048\n532601    1.800864423     -0.097583415\n532802   -1.677542168     -0.824563729\n533002   -5.703553240      0.170866305\n533003    6.363244048      0.404819652\n534103   15.151287734      0.685514121\n537002    4.332209827     -0.447212584\n537302    1.937192727      0.519193957\n537304   14.199697268      0.744478103\n537602    3.141027671      0.439343803\n538102   -0.138505963      0.111396227\n538703    1.378764507      0.785718978\n538704    4.075748871      0.100023981\n542702   -1.409693646     -0.230369024\n542703   -6.687669938     -0.266286499\n542802   13.407691263      0.356698396\n543601   -2.350160514     -0.672815693\n543602   -9.889088825     -0.257651627\n545402   -8.688790180     -0.309988996\n546505  -10.734656936     -1.346391440\n546702  -13.593010749     -0.531453404\n547701    1.515062357      0.166183160\n548501    0.782912576     -0.319916183\n549801    2.108432062     -0.070117907\n550901  -12.207590324     -0.606792845\n551501   -9.483258820     -0.249028575\n552203    5.375345633      0.119472457\n553701   -8.159310369      0.044498342\n553702   -2.139811693     -0.069649375\n556101   -4.916585005     -0.608556695\n556801   -0.653108704      0.014812013\n558301   -6.651518096     -0.724535151\n559302   -4.998455197     -0.009225589\n560902   -5.080406456     -0.546036745\n561202    0.914299930     -0.105581452\n561402   -1.650460891     -0.401645200\n564001   -3.981210746     -0.997318240\n564002  -11.964061394     -0.375078260\n565601    7.228780094      0.587282473\n567002    4.506841997      0.120062683\n570601   -4.581916892     -0.394347202\n571201   -4.006401224     -0.588944245\n571801    2.377697356      0.299360222\n571802    6.439438876      0.347286433\n572402   -7.448997941     -0.192697636\n572801    3.456546909      0.949451425\n572802    1.284389677      0.013912093\n572803    2.021954122      0.124533751\n574003   -9.727922929     -0.477835144\n574102   -0.320307604      0.020758475\n574602    2.463196484      0.058340403\n574603    2.549465760      0.313163093\n580202   12.203823999      0.289767654\n581303   -6.584442932     -0.299183252\n581802   -6.327584741     -0.271856705\n586102    5.668892816      0.647542806\n587301   -4.796456987     -0.357614187\n587303   -3.866231378     -0.114613742\n591903    8.151266437      0.427276109\n594102    1.983288336      0.760855980\n600203   -7.951308547     -0.219342937\n602301   -0.089742545      0.088702303\n602302   -0.541409817     -0.028474326\n602304   -0.129231552      0.002468560\n604004   -8.578060685     -0.654845280\n604607   -5.359169800      0.124050372\n604902   -5.221372571     -0.217412244\n607201    1.664383330      0.036623129\n607601   -2.474126536     -0.483079001\n607602  -13.455962660     -0.348304721\n607802   -1.271410157     -0.087873134\n610101   -9.640993658     -0.633241497\n611802   -0.940294528      0.512491849\n613201    5.447005406      0.140823109\n613402   -8.043157025     -0.603994188\n616105    3.646540805      0.040434304\n616402   -2.003597206     -0.432065590\n617501  -16.753064578     -0.758201897\n621501    2.090728189     -0.090591099\n621701    8.017513094      0.061602739\n622001   -3.117404067     -0.380963728\n623801   -1.096552561     -0.213769511\n626201    7.057285064      0.090615207\n627702   -2.395033070     -0.162102298\n627703   -8.597452591     -0.055319927\n627802   -9.020002885     -0.368023234\n629404    3.783652406     -0.355473733\n629502    4.176765010      0.230099968\n631801   -7.316391315     -0.406402539\n632205  -12.132319077     -0.249051645\n632702   -5.990892500     -0.171403525\n632703   -3.234462913      0.043606143\n632704   -5.733829052     -0.409870166\n634401   -8.312459893     -0.465527636\n634503    0.303502674     -0.086974048\n635302   -1.498033432     -0.406668296\n636402  -15.901044465     -0.608122091\n636802   -4.385982190     -0.388351268\n637110   -8.842661707     -0.116288161\n638402   -1.728818916     -0.404144561\n640002  -11.992489942     -0.411548816\n640402   -8.845688784     -0.421731402\n642601  -13.796063897     -0.693958033\n642701   -5.756427948     -0.057758542\n642702   -6.808932201     -0.248756386\n642901    1.603616016     -0.101585257\n642902   -8.239333907     -0.046044308\n642903  -10.242537352     -0.208712577\n642904   -3.080639103     -0.178553328\n643402   -9.413446438     -0.854906422\n644203   -2.938326170     -0.327776407\n644901  -12.965869279     -0.836975804\n648601   -2.497626187     -0.111582921\n648602   -1.817813178     -0.165024349\n651601   -9.206421401     -0.306663180\n665803   -7.937622756     -0.400586179\n668403   -3.094074869     -0.203791032\n669301    4.288543841      0.203992881\n671102    1.650096523      0.192741072\n675701   -6.926768526     -0.314028073\n677201   -2.217690247      0.184501031\n677202   -3.902369697     -0.142733255\n678804   -0.983163384     -0.077600270\n681601    0.310723059      0.325327512\n682502   -3.046918186     -0.108704560\n682903    2.590643226     -0.096908763\n684201   16.280078439      0.460490211\n684203    9.192836006      0.772579781\n687602    3.841932936      0.036274111\n689101    1.106616352     -0.011870061\n690101   -3.325980888     -0.747269213\n693001   -1.091016676     -0.435950300\n696601   -5.322948686     -0.193431191\n700002  -12.025093976     -0.339720875\n700003  -15.132582305     -0.552851222\n707701   -1.166124231      0.072680470\n708401   15.893806763      0.599080922\n711602   -9.495807218     -0.228793930\n711603   -5.049071022     -0.049297748\n712303    0.462837788     -0.633538578\n714801   -4.255894456     -0.412541945\n714802    6.314626282     -0.095917002\n715601    0.853826544      0.537570571\n715803  -14.172000226     -0.603752918\n716601   12.221371403      0.630392854\n716602    7.679000983      0.171371224\n717002   -7.515821696     -0.402873460\n717003   -7.575151040     -0.504060469\n717901   -9.823411841     -0.664607092\n717902   -5.873793984     -0.323729629\n717903   -6.289457181     -0.088526651\n718602    1.131820917      0.511550310\n722401   -1.006228629     -0.483989774\n722803    4.130434882      0.093201533\n723501    0.307440660      0.125419848\n725801   -1.696320252      0.438759492\n725903  -17.079275454     -1.211660542\n737702   -4.388651435     -0.022318441\n738201   -2.323832074     -0.082662961\n739002   -8.937134009     -0.727537671\n739102   -5.479622388     -0.066710565\n739301   -5.602359628     -0.445973585\n739401    2.968561068      0.001870184\n739601    6.194291875      0.076760325\n742301    1.224247223      0.083866088\n743601   -1.021533187     -0.055151050\n743602   -5.660397705     -0.963156384\n743801   -4.592250489      0.003493467\n743802  -10.643702763     -0.507328981\n744102  -17.016481993     -0.373824109\n744103   -5.158549836     -0.284186961\n744703   -3.259785398     -0.092759390\n744704   -8.163758988      0.005928817\n745103    5.381951413      0.177010141\n745904    0.971919319      0.042952211\n748003   -0.722331879     -0.164563391\n748502    4.174223809      0.139650670\n749802    8.449572727      0.231523244\n749803   -1.198480879     -0.318137067\n750104  -11.024695311     -0.334826635\n750404    1.351435367     -0.527974811\n751001   -6.099653262     -0.167631898\n752003    0.697845101     -0.445791062\n752501   -1.991270419     -0.087024018\n760102   -1.034728367     -0.174004389\n763603   -8.387036288     -0.315786406\n764503   -2.673395685     -0.260101723\n765702    1.706178547     -0.085713644\n767901    2.219000694      0.054666413\n771002   -8.432172662     -0.310977053\n775002    7.417445319      0.358097656\n778902  -11.782707125     -0.612757333\n778903   -1.249249505     -0.704340836\n783001   -8.898763592     -0.470550225\n783002   -2.057987793     -0.263859943\n783301    4.724730332      0.462334273\n783502    1.208861841      0.170083010\n783602   -1.253856860     -0.045073631\n783801    1.319050503      0.284306191\n785601    2.710879360      0.164296175\n786402    5.178327920      0.242087618\n788302  -10.726817911     -0.427258426\n788303  -12.016494569     -0.009831985\n792103   -2.449710306      0.266315368\n792704   -3.273626706     -0.225207718\n793001    1.210479848      0.539971187\n794301  -10.932162723     -0.450055577\n794503   -7.218166115     -0.425840929\n795201   -4.249738333     -0.343959448\n795901    4.579265935      0.094186547\n799803   13.971047989      0.503724235\n800602    1.913478000     -0.046791426\n804701   -1.519177743      0.205194007\n804702   -0.462858324     -0.180163109\n809102   -5.218471720     -0.585442071\n809103   -8.899982617     -0.839757022\n809301   -6.655780200     -0.705423986\n810303    1.045058979      0.084625194\n811002   -7.206412024     -0.269444781\n812504    1.486334628      0.056843620\n814101   -0.979875492     -0.047576390\n817402   -3.257668016     -0.497166854\n817403   -5.087137975     -0.711031193\n817404   -5.750879086     -0.581238557\n822602    7.013043086      0.420185194\n825702   -9.690081735     -0.278014140\n825902   -8.608742948     -0.209946447\n825903   -3.395839756     -0.527164765\n826503    1.469347140     -0.189504850\n826504  -18.358242061     -0.978124604\n826904  -17.309569594     -0.814240170\n826905   -9.616835993     -0.241896347\n827101    3.170957010      0.133926977\n827302   -6.417483438     -0.062114513\n828302    2.747989097      0.196935471\n828604   -0.432451327     -0.218443012\n828902   -4.744880520     -0.496371189\n828903   -4.962825174     -0.391887535\n828905  -12.837447967     -0.515666464\n829401   -7.300494921     -0.451136076\n829403    2.278496761      0.226701780\n833601   -0.954648942     -0.046110514\n834301    0.185859752     -0.232313965\n835202    2.006143445     -0.110537831\n835703   -2.622647289      0.215727790\n837503   -7.211226372     -0.265336449\n837504   -8.857256944     -0.212978546\n837602  -13.024762525     -0.390023252\n838301   -8.233751839     -0.348036919\n838903    3.304616402      0.392749927\n841601    0.886759956      0.215410809\n841602    0.537895525      0.013521833\n846301   10.279980222      0.313542745\n847301   -4.210466823     -0.624023570\n847302    3.291690158     -0.002044838\n847701    8.235072091      0.400242080\n847901   -0.432079542      0.114558409\n847902   -6.633722012     -0.097355701\n848201    4.468851642      0.670151752\n849401    0.189416375     -0.149836441\n853202   -0.429397010      0.218005012\n856103   -3.586214102     -0.092203244\n857003   -6.720617097     -0.493204616\n858302   -4.204535539     -0.609410105\n861503   -3.966967748      0.293400309\n861803   -3.135359757      0.264874987\n862201    4.653017738      0.132082293\n863101  -12.297582748     -0.439929413\n863403   -6.477163797     -0.474134043\n864302    2.085691014      0.074949397\n866203   -9.169437523     -0.147097347\n866205   -3.492643017     -0.061050645\n866402   -2.782792441     -0.029554824\n867903   11.942822075      0.499397590\n869001   -7.702502070     -0.202251421\n869501    0.806896368      0.352096898\n869601  -10.485942895     -0.461798451\n870803    8.928434794      0.521462250\n878701  -17.568709144     -0.626957686\n879403   11.121309995      0.417858606\n879404    3.311343239      0.047788637\n885801    3.486722522      0.081372459\n885802  -16.039468436     -1.050718441\n886401   11.077378767      0.671794102\n891202   -1.785951488      0.135840571\n892601   -6.248449211     -0.100279844\n894201   -4.410948902     -0.294281539\n894802   -0.313453837     -0.115560234\n894803   -5.508871671     -0.307101576\n894804   -4.959265629     -0.273862791\n897001   -3.323415615     -0.358621058\n905003   -6.766784958     -0.578860310\n907001   -0.044475957     -0.120023417\n908102   -0.826610098      0.037276094\n910103   -9.997531168     -0.318016485\n911701    1.189104202      0.248190222\n914402    4.330045404      0.059557193\n918002    1.312890067     -0.031666983\n918201    2.010858048      0.084121892\n918202    2.960751078      0.071789615\n918301   -0.253494219      0.421761227\n922901    4.736230982      0.019599188\n925503   -2.952847689     -0.184075604\n925504   -8.155463950     -0.407774825\n925701   -1.560286833     -0.085956943\n926102  -11.384931242     -0.440643401\n928101   -2.561314916     -0.175971001\n929805    2.299878514      0.120028438\n931202    6.182482161      0.760747880\n931703   -5.921728745      0.253419327\n938001   -4.268374580     -0.150332673\n938002   -5.203853762     -0.359216554\n938205    1.452960485      0.154925383\n940103   -2.332263976     -0.136215098\n951602   -9.264136115     -0.794456772\n952201   -3.740749071     -0.123357170\n964101    3.547967803      0.131810129\n964102    7.585006320      0.091026089\n966001    9.257072985      0.418793561\n968001   -0.898120469     -0.728569335\n968002   -4.421070337      0.099825430\n968003   -8.385332673     -0.022239581\n968401    2.945108480      0.304805734\n968402    7.276622407      0.604626813\n976803   -9.628901937     -0.742544730\n982001   -4.695771491     -0.306490550\n983703    3.745534505      0.165550609\n983902    2.596009547      0.125200150\n983903   -3.667967858     -0.155126491\n984402   -1.217021320     -0.058975181\n984404   -0.789019406     -0.327117617\n985802   -3.353538083     -0.397301364\n985803    0.978668861     -0.062038203\n986203   -2.764543447     -0.702253665\n986505    0.235705952     -0.568761949\n986506   -5.064852532     -0.934175547\n987401   -6.339050700      0.154132170\n987701   -4.792430373     -0.496558484\n988701   -8.738498236     -0.368233827\n989201   -5.672395817     -0.500824137\n989503   -3.963618084     -0.191185511\n992201    3.713250289      0.037354172\n993803   -0.339041448     -0.033276656\n995101  -16.855431161     -0.793196728\n995102  -16.732009976     -0.816271483\n995304    1.141931270      0.039760624\n997002   -7.890837919     -0.120332271\n998001    2.904616596     -0.099911409\n999202   -4.209454375     -0.249228417\n999203   -3.445087389     -0.642747361\n1002601 -12.615996043     -0.681804312\n1003202  -4.900766767     -0.081557288\n1003203  -7.168505838     -0.337373360\n1003204  -7.477961633     -0.234615583\n1003601   4.142234535      0.087851091\n1003802   6.694641940      0.582885762\n1004802  -6.010511003     -0.127669923\n1004903  -5.696529302      0.321126050\n1005201  -5.061927500     -0.154374548\n1005203   7.744852438      0.359834771\n1007201   1.485163277     -0.137844389\n1007301 -12.207661253     -0.289172581\n1007302 -12.256018327     -0.141761798\n1007901  -1.587140380      0.168986153\n1013201  -7.449018493     -0.201116136\n1017504 -15.113587952     -0.217160076\n1018402  -8.195378280     -0.336645460\n1018404  -6.060598198     -0.384207768\n1019104  -3.898712489     -0.395718677\n1019105 -10.441463111     -0.495045209\n1023702   0.230071978     -0.062853597\n1030801   9.614192285      0.359470695\n1031001   7.309989035      0.957296774\n1031002   3.338190748      0.463618589\n1031302  -7.796511938     -0.032990651\n1033202 -13.370171847     -0.281812065\n1033601  -5.677419865     -0.172171992\n1034001  -6.334557499     -0.663053901\n1036001  -2.492905799      0.635495607\n1036003  -1.474746601     -0.133442885\n1037502  -2.665942104     -0.015951671\n1040101   4.276289331      0.440820891\n1042101  -2.553330890     -0.258232185\n1042201  -6.059952254     -0.189338564\n1044801   4.128417062      0.245711117\n1047801  -2.849801051     -0.185397056\n1050001  -2.717825315     -0.095089574\n1050701  -5.872749060      0.457585048\n1053201  -0.250978709     -0.100236571\n1053202  -6.760663767     -0.293314450\n1053802  -6.284836936      0.080095322\n1053804  -7.223993827     -0.248573995\n1056501  -3.896931891     -0.118419463\n1081101  -5.400851712     -0.293731045\n1081103  -4.680976218     -0.137297987\n1176403  -1.958564728     -0.236731201\n1176502  -4.966749948      0.109135738\n1179201   5.308169099     -0.088424313\n1179202   1.720621134      0.117183301\n1180202  -0.431475168     -0.508419596\n1181201 -11.704987647     -0.583519704\n1181902  -2.349190562     -0.066365192\n1181904  -6.320454193     -0.333828815\n1182604  -0.232940420     -0.239191587\n1185003 -13.222234389     -0.226450008\n1187001   3.002185759     -0.284717029\n1187303   6.481394612      0.381759227\n1189901   8.842299522      0.382244575\n1190501  -2.594795950     -0.242898294\n1191001   3.134902432      0.262260144\n1191901   5.116734779      0.307548594\n1194901  -2.642814420      0.740744619\n1197902  -7.270379750     -0.242315430\n1198101  -1.607639092     -0.129777591\n1198901  15.013179772      0.359981902\n1201701  -0.835329508     -0.169484890\n1201702  -1.378932676     -0.575885435\n1203201  -2.032124763     -0.578525839\n1205201  -4.566609451      0.130473364\n1207601   4.894676196      0.201302925\n1209201   0.014426741     -0.133074642\n1211502   8.396956305      0.333608159\n1211503   5.392613109      0.178489598\n1212303  -2.575279428     -0.048814312\n1213501  -5.356118638      0.908621520\n1217202  -1.881659801     -0.624995211\n1217204  -6.521048243     -0.094817905\n1217702  -3.406869123     -0.086193891\n1217704  -2.337403065     -0.153791714\n1219103  -7.263847297     -0.311537184\n1219105 -12.688070163     -0.592413772\n1219106 -13.168088281     -0.487925838\n1219108   2.752059171      0.271825037\n1221003  -0.763516936     -0.102460692\n1221702  -3.839403600     -0.221064459\n1224001   3.233736846     -0.091309850\n1225402  -3.826603058     -0.116909854\n1228103  -0.096753765     -0.003285703\n1230302 -10.049267721     -0.752222025\n1256601  -7.299006078     -0.291423458\n\nwith conditional variances for \"id\" \n\n\n\n\n\n6.1.1.3 nlme\n\n\nCode\nlinearMixedModel_nlme &lt;- lme(\n  math ~ female + ageYearsCentered + female:ageYearsCentered, # sex as a fixed-effect predictor of the intercepts and slopes\n  random = ~ 1 + ageYearsCentered|id, # random intercepts and slopes\n  data = mydata,\n  method = \"ML\",\n  na.action = na.exclude)\n\nsummary(linearMixedModel_nlme)\n\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: mydata \n       AIC     BIC    logLik\n  15857.85 15903.5 -7920.926\n\nRandom effects:\n Formula: ~1 + ageYearsCentered | id\n Structure: General positive-definite, Log-Cholesky parametrization\n                 StdDev    Corr  \n(Intercept)      7.9079987 (Intr)\nageYearsCentered 0.8225933 0.082 \nResidual         5.6701380       \n\nFixed effects:  math ~ female + ageYearsCentered + female:ageYearsCentered \n                            Value Std.Error   DF  t-value p-value\n(Intercept)             30.514011 0.5619217 1287 54.30296  0.0000\nfemale                  -0.612896 0.7955333  930 -0.77042  0.4412\nageYearsCentered         4.267923 0.1126360 1287 37.89130  0.0000\nfemale:ageYearsCentered -0.025585 0.1610671 1287 -0.15885  0.8738\n Correlation: \n                        (Intr) female agYrsC\nfemale                  -0.706              \nageYearsCentered        -0.635  0.448       \nfemale:ageYearsCentered  0.444 -0.631 -0.699\n\nStandardized Within-Group Residuals:\n         Min           Q1          Med           Q3          Max \n-3.375034869 -0.517409797  0.005105047  0.523910718  2.639557775 \n\nNumber of Observations: 2221\nNumber of Groups: 932 \n\n\n\n\n6.1.1.4 Intraclass Correlation Coefficent\n\n\nCode\nicc(linearMixedModel)\n\n\n\n  \n\n\n\nCode\nicc(linearMixedModel_nlme)\n\n\n\n  \n\n\n\n\n\n\n6.1.2 Growth Curve Model with Timepoint-Specific Errors\nAdapted from Usami & Murayama (2018):\n\n\nCode\ntimepointSpecificErrorsMixedModel &lt;- lmer(\n  math ~ female + ageYearsCentered + female:ageYearsCentered + (1 | id) + (1 | ageYearsCentered), # timepoint-specific errors: observations are cross-classified with person and timepoint; sex as a fixed-effect predictor of the intercepts and slopes\n  data = mydata,\n  REML = FALSE, #for ML\n  na.action = na.exclude,\n  control = lmerControl(optimizer = \"bobyqa\"))\n\nsummary(timepointSpecificErrorsMixedModel)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: math ~ female + ageYearsCentered + female:ageYearsCentered +  \n    (1 | id) + (1 | ageYearsCentered)\n   Data: mydata\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  15808.6   15848.5   -7897.3   15794.6      2214 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5409 -0.5191  0.0051  0.5093  2.7345 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n id               (Intercept) 76.219   8.730   \n ageYearsCentered (Intercept)  4.503   2.122   \n Residual                     30.842   5.554   \nNumber of obs: 2221, groups:  id, 932; ageYearsCentered, 94\n\nFixed effects:\n                          Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)               29.87438    0.75758  211.81838  39.434   &lt;2e-16 ***\nfemale                    -0.38123    0.82243 1960.43213  -0.464    0.643    \nageYearsCentered           4.26419    0.14813  132.95739  28.786   &lt;2e-16 ***\nfemale:ageYearsCentered   -0.08143    0.14596 1364.56012  -0.558    0.577    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female agYrsC\nfemale      -0.545              \nageYrsCntrd -0.758  0.321       \nfml:gYrsCnt  0.356 -0.652 -0.482\n\n\n\n\n6.1.3 Quadratic Growth Curve Model\nWhen using higher-order polynomials, we could specify contrast codes for time to reduce multicollinearity between the linear and quadratic growth factors: https://tdjorgensen.github.io/SEM-in-Ed-compendium/ch27.html#saturated-growth-model\n\n\nCode\nfactorLoadings &lt;- poly(\n  x = c(0,1,2,3), # times (can allow unequal spacing)\n  degree = 2)\n\nfactorLoadings\n\n\n              1    2\n[1,] -0.6708204  0.5\n[2,] -0.2236068 -0.5\n[3,]  0.2236068 -0.5\n[4,]  0.6708204  0.5\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 1.5 1.5\n\nattr(,\"coefs\")$norm2\n[1] 1 4 5 4\n\nattr(,\"degree\")\n[1] 1 2\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nCode\nlinearLoadings &lt;- factorLoadings[,1]\nquadraticLoadings &lt;- factorLoadings[,2]\n\nlinearLoadings\n\n\n[1] -0.6708204 -0.2236068  0.2236068  0.6708204\n\n\nCode\nquadraticLoadings\n\n\n[1]  0.5 -0.5 -0.5  0.5\n\n\n\n6.1.3.1 Fit Model\n\n\nCode\nquadraticGCM &lt;- lmer(\n  math ~ female + ageYearsCentered + ageYearsCenteredSquared + female:ageYearsCentered + female:ageYearsCenteredSquared + (1 + ageYearsCentered | id), # random intercepts and linear slopes; fixed quadratic slopes; sex as a fixed-effect predictor of the intercepts and slopes\n  data = mydata,\n  REML = FALSE, #for ML\n  na.action = na.exclude,\n  control = lmerControl(optimizer = \"bobyqa\"))\n\nsummary(quadraticGCM)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: math ~ female + ageYearsCentered + ageYearsCenteredSquared +  \n    female:ageYearsCentered + female:ageYearsCenteredSquared +  \n    (1 + ageYearsCentered | id)\n   Data: mydata\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  15666.9   15724.0   -7823.5   15646.9      2211 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3660 -0.4945  0.0048  0.5085  2.4377 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n id       (Intercept)      69.8860  8.3598        \n          ageYearsCentered  0.7099  0.8426   -0.05\n Residual                  27.1959  5.2150        \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                                 Estimate Std. Error         df t value\n(Intercept)                      23.43112    0.84914 1380.85816  27.594\nfemale                            0.61167    1.18734 1370.91791   0.515\nageYearsCentered                  8.79334    0.42555 1381.21301  20.664\nageYearsCenteredSquared          -0.57623    0.05255 1382.10819 -10.966\nfemale:ageYearsCentered          -0.62379    0.60301 1386.34429  -1.034\nfemale:ageYearsCenteredSquared    0.05700    0.07582 1393.17373   0.752\n                               Pr(&gt;|t|)    \n(Intercept)                      &lt;2e-16 ***\nfemale                            0.607    \nageYearsCentered                 &lt;2e-16 ***\nageYearsCenteredSquared          &lt;2e-16 ***\nfemale:ageYearsCentered           0.301    \nfemale:ageYearsCenteredSquared    0.452    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female agYrsC agYrCS fml:YC\nfemale      -0.715                            \nageYrsCntrd -0.836  0.598                     \nagYrsCntrdS  0.759 -0.543 -0.969              \nfml:gYrsCnt  0.590 -0.829 -0.706  0.683       \nfml:gYrsCnS -0.526  0.751  0.671 -0.693 -0.968\n\n\nThis is equivalent to:\n\n\nCode\nquadraticGCM &lt;- lmer(\n  math ~ female + ageYearsCentered + I(ageYearsCentered^2) + female:ageYearsCentered + female:I(ageYearsCentered^2) + (1 + ageYearsCentered | id), # random intercepts and slopes; sex as a fixed-effect predictor of the intercepts and slopes\n  data = mydata,\n  REML = FALSE, #for ML\n  na.action = na.exclude,\n  control = lmerControl(optimizer = \"bobyqa\"))\n\nsummary(quadraticGCM)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nmath ~ female + ageYearsCentered + I(ageYearsCentered^2) + female:ageYearsCentered +  \n    female:I(ageYearsCentered^2) + (1 + ageYearsCentered | id)\n   Data: mydata\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  15666.9   15724.0   -7823.5   15646.9      2211 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3660 -0.4945  0.0048  0.5085  2.4377 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n id       (Intercept)      69.8860  8.3598        \n          ageYearsCentered  0.7099  0.8426   -0.05\n Residual                  27.1959  5.2150        \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                               Estimate Std. Error         df t value Pr(&gt;|t|)\n(Intercept)                    23.43112    0.84914 1380.85816  27.594   &lt;2e-16\nfemale                          0.61167    1.18734 1370.91791   0.515    0.607\nageYearsCentered                8.79334    0.42555 1381.21301  20.664   &lt;2e-16\nI(ageYearsCentered^2)          -0.57623    0.05255 1382.10819 -10.966   &lt;2e-16\nfemale:ageYearsCentered        -0.62379    0.60301 1386.34429  -1.034    0.301\nfemale:I(ageYearsCentered^2)    0.05700    0.07582 1393.17373   0.752    0.452\n                                \n(Intercept)                  ***\nfemale                          \nageYearsCentered             ***\nI(ageYearsCentered^2)        ***\nfemale:ageYearsCentered         \nfemale:I(ageYearsCentered^2)    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female agYrsC I(YC^2 fml:YC\nfemale      -0.715                            \nageYrsCntrd -0.836  0.598                     \nI(gYrsCn^2)  0.759 -0.543 -0.969              \nfml:gYrsCnt  0.590 -0.829 -0.706  0.683       \nfml:I(YC^2) -0.526  0.751  0.671 -0.693 -0.968\n\n\n\n\n6.1.3.2 Protoypical Growth Curve\n\n\nCode\nnewData &lt;- expand.grid(\n  female = c(0, 1),\n  ageYears = seq(from = min(mydata$ageYears, na.rm = TRUE), to = max(mydata$ageYears, na.rm = TRUE), length.out = 10000))\n\nnewData$ageYearsCentered &lt;- newData$ageYears - min(newData$ageYears)\nnewData$ageYearsCenteredSquared &lt;- newData$ageYearsCentered ^ 2\n\nnewData$sex &lt;- NA\nnewData$sex[which(newData$female == 0)] &lt;- \"male\"\nnewData$sex[which(newData$female == 1)] &lt;- \"female\"\nnewData$sex &lt;- as.factor(newData$sex)\n\nnewData$predictedValue &lt;- predict( # predict.merMod\n  quadraticGCM,\n  newdata = newData,\n  re.form = NA\n)\n\nggplot(\n  data = newData,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    color = sex)) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.3.3 Individuals’ Growth Curves\n\n\nCode\nmydata$predictedValue &lt;- predict(\n  quadraticGCM,\n  newdata = mydata,\n  re.form = NULL\n)\n\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.3.4 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line() +\n  geom_line(\n    data = newData,\n    mapping = aes(\n      x = ageYears,\n      y = predictedValue,\n      group = sex,\n      color = sex),\n    linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n6.1.3.5 Extract Random Effects\n\n\nCode\nranef(quadraticGCM)\n\n\n$id\n         (Intercept) ageYearsCentered\n201       0.33928713     0.2048916235\n303     -14.23955413    -0.5404361816\n2702     13.67892361     0.6053891399\n4303      1.87926497     0.2622509407\n5002      1.61867520     0.3766872283\n5005      5.00342166     0.1103061842\n5701     12.64428032     0.1076555275\n6102      9.72171668     1.0058459560\n6801     11.48936045     0.1758389252\n6802     12.40495519     0.2481464174\n6803     13.12879251     0.5013588794\n9603      5.52979990     0.3824649037\n12401     2.66417687     0.1687337604\n12801     2.60493276     0.0651466650\n13702    10.61884268     0.0773190112\n13801    15.36440208     0.2021123643\n13803    12.80038202     0.1110485222\n17201     8.24256456     0.4483461999\n18601   -10.55394363    -0.2272976140\n22104    -9.58932495    -0.0696876543\n22901    -3.36909773    -0.1407002246\n23602     8.73644115     0.2184893264\n23701     2.90381073    -0.1385981630\n24402     5.15050534     0.2763241243\n26201    11.74497151     0.5679253986\n28503    -7.77001348     0.3606132618\n29201    13.25930100     0.2793576856\n29202     8.50001271     0.7611707086\n34401    14.78277587     0.6559402946\n35401     6.58855382     0.6251281418\n35402     5.98225593    -0.1549366084\n36901    16.78961779     0.2896274008\n36902    11.30118873     0.3128106197\n37403    -0.68078847    -0.5658858237\n38202    -2.11982646    -0.0736750904\n38802    -6.46708684    -0.3200778563\n40501     3.50588381     0.0461949896\n40702     9.29628413    -0.1320987047\n42504     2.10774440     0.3442994520\n42505     7.14104463    -0.2915234595\n43103     5.89656370     0.3722011257\n45201     8.88186398     0.3752300489\n45903    16.37615560     0.2261389848\n46201    11.02652193     0.9001553207\n47201    15.95922365     0.1532676692\n47402     1.19209448    -0.1725448956\n47601     2.57376058     0.0546115265\n48502    -2.34123733     0.3721239616\n49501     2.28710572     0.1112760597\n53601    -2.11028218    -0.4186090518\n55201     9.05810571     0.4267232755\n55801     2.29575981     0.3260040562\n56301    -6.10596053     0.4517713308\n56302    -0.35533626    -0.0486686745\n57604    11.11331459     0.5245743801\n58503   -10.14200241     0.1536534485\n62102     8.48717539     0.4801453109\n62103   -12.69760037     0.2296958801\n62202    12.35806263     0.3191224628\n62703     0.77452010     0.5856085375\n64002     1.71077294    -0.3926996243\n65601    12.87544176     0.2085534688\n65701    11.78439474     0.5456233249\n66002     5.40379198     0.4392888893\n70801    -2.65816258    -0.0735123899\n73602     3.16513985     0.0135861002\n74602     5.72595047     0.0721540233\n75603    -8.90324447    -0.1220918375\n76402    -0.45283569    -0.3607788119\n76403     3.78350556     0.2085306910\n80102     4.53440782     0.4488751663\n82901    -4.12356887    -0.2517007665\n83001    -1.10845340    -0.2951734131\n83101    -5.98742772    -0.1378607049\n83102    -0.58918147    -0.2280723208\n83602     4.57144315     0.0534239295\n83603    -0.67651981     0.0386622205\n85402    -3.99838945    -0.0738311289\n87501    -1.77198447    -0.1566500216\n87901     3.86408253     0.0422021094\n88902    -5.04398320    -0.2410085840\n89303    -0.79795914    -0.1172080258\n89803     6.03154519     0.6208458885\n90002    11.08122472     0.2999687163\n92303     1.29841825    -0.2871718825\n93202    -4.20231846    -0.0342960418\n93901    -4.60550922    -0.0757234632\n94002     1.24371789     0.8570523782\n96201    -0.60464792    -0.2426181602\n96202    -5.47276305    -0.1474929765\n98102     7.26657937     0.3154701221\n101701    0.45365908    -0.5552006182\n101702   -3.25255366     0.2917523724\n103501   -4.84603574    -0.5917057237\n108201   17.45558375     0.3412531936\n113001    8.25332705     0.6615512318\n113201    4.85533732     0.3113791635\n114101   -7.66963585     0.1296121083\n114702    4.14381150     0.1089984927\n115901    3.94046139     0.4658765984\n116002    2.99535490     0.1558641516\n116004   11.48655100     0.3703171298\n118402   11.72607831     0.2642909521\n122301   -8.29724634     0.0008642110\n124201   -2.96428313    -0.0736833441\n124402   -7.33985437    -0.2569093160\n125002   -2.35193557    -0.2905243916\n126101    3.39381990     0.2271836954\n129502   -8.07909145    -0.3541445982\n129601   -3.45689228    -0.2490279131\n129602   -0.17487722    -0.0040696364\n132301   11.33564334     0.4280558536\n132402  -10.00119746     0.0534339563\n132403    1.08313514    -0.3260818342\n136001   -4.56348688    -0.2920187214\n138901    3.67379091    -0.5247768981\n138902   -0.43264445    -0.1346112407\n139902   11.60885012     0.8369706922\n140004   -3.95730575     0.3364681884\n140104   -8.95752535    -0.3326061040\n140901    1.37525793     0.5661277449\n140902    1.89127979    -0.4967538409\n141001   10.73732856     0.2560276404\n141601    2.59621507    -0.1471777397\n141603    1.93924036    -0.2911545453\n143901    2.76970719     0.0313528509\n144004   -0.45047988    -0.1165955448\n153203   17.80535189    -0.0154451664\n153204  -19.12376080    -0.4658278021\n153801   -0.09822118    -0.1918032108\n155902  -10.30867362    -0.2633796096\n156401   13.35462944     0.1125928594\n156605   10.63398943     0.3345083845\n158701    8.43286617     0.2151088880\n158702   11.40471372     0.6628119215\n159605  -16.00216781    -0.6142057674\n159606  -12.91084109    -0.4281848871\n159801   -9.06779608     0.5471884412\n165501   -9.75467115    -0.2868447245\n165602    3.97431586     0.1993120365\n165701  -11.32830715    -0.3639936202\n165801    7.78933517     0.2228525316\n166602   -1.94451444     0.1210180941\n166603  -12.98299208    -0.1850781038\n167501   11.99830144     0.7956783043\n167502   18.54940620     0.4534675477\n169003   -1.39626192     0.1178405897\n170403   -3.82923583    -0.5637318758\n170404   -6.32722876    -0.1990648050\n171401   -8.41664049    -0.0931688103\n172502    6.17635892    -0.4867508822\n173001   -3.35884458    -1.7939872366\n173201   -0.18374350    -0.0076553207\n174001   11.79368621     0.1958748643\n175405  -10.66754941    -0.3439776675\n175801   -7.65149455    -0.1293192916\n176003    9.51946854    -0.2589646986\n178901   -8.22500414    -0.1929562528\n179401   -5.98631450    -0.2120648182\n179701   -1.61184407    -0.4857992943\n180301    8.85342990     0.5405658890\n181801   -2.27056713    -0.1273457793\n182903   -1.19774678    -0.0520202928\n185304    2.22820417    -0.1693015421\n187803    2.95400007     0.4399274619\n188002    7.08589693    -0.1499573703\n188502    6.92497904     0.3668663068\n189601    7.28249552     0.2668362092\n189602    2.43919392     0.8899205128\n190202    2.24971310     0.0249384121\n190501   11.93065226     0.2763993586\n191202    1.02500680     0.2783541628\n193601   -7.97134181     1.0147012007\n193803    5.98928938     0.3131209595\n196002   -8.12887851    -0.2865607195\n196201    4.16588513     0.3172776489\n196202    1.07025430    -0.1986024080\n204602    9.15967430     0.1975876524\n205802   10.40644860     1.0378733000\n207102   10.84106680     0.5250234955\n207202    3.61959117     0.6627738055\n207301   -0.73393600    -0.2527850522\n212001    4.93197921     0.3994909698\n213101   -1.81856624    -0.2389323526\n213702    1.32189349     0.2106014492\n215304   -2.00425763     0.6764159818\n216701    7.52632017     0.1176741134\n217402    4.76817416     0.1592840231\n219003    2.40104521     0.8011217910\n220401    4.04986692     0.7066816283\n221202    9.95961321     0.1862956300\n223002   15.62296224     0.3904701241\n224901   -2.54424404    -0.5258845548\n226001   -8.69464302     0.2264437414\n226502   -8.46369480     0.0200013455\n226704   -8.69923742     0.0178851382\n227002    3.40751257    -0.7407147380\n227101  -12.50235206     0.2301686901\n227102    3.73798793     0.4743761774\n227201   -3.79582683    -0.0005880306\n231502   -8.62515101     0.1650450629\n233301   -0.48626285    -0.3585112075\n233901    8.94551149     0.2475461882\n233902    5.75246207     0.0696455531\n235503  -10.20414467    -0.2601952534\n236002   -3.74913131    -0.4357038295\n236201   -6.81517160     0.1094731371\n236202  -11.45090006     0.0579170219\n236901    6.27553009     0.2885721138\n237501    6.39835784     0.6413707011\n238301   -0.73799816    -0.2792083242\n239603   -3.80712545    -0.4593309794\n239604   -2.59999184    -0.1912901478\n240701   -2.85211887    -0.2449069755\n240702   -5.31021101     0.3038686704\n244201    0.30721779    -0.0868152479\n244702    0.10740152    -0.0041185657\n245903   -1.81076187    -0.5437115633\n248402  -11.46092125    -0.2965954073\n250502    1.47731586     0.2913200845\n252902   -9.61944151    -0.4193768556\n253001   -4.44367189    -0.3569182966\n253603   -4.69324115    -0.2576984585\n253802    1.95901726    -0.3467847444\n254901    0.70301926     0.2606403040\n255201    4.75464840    -0.1268180442\n255901  -14.59019692     0.1674480301\n255902   -7.16976223    -0.1536697557\n255903   -0.95919708     0.0018794288\n258801   10.31451130     0.3093277365\n260702   10.10142087     0.4556405379\n261501   -4.68923194    -0.2195455879\n261502   -6.97848701     0.8876398938\n261503   -3.97508886    -0.4137871613\n262601    3.67963270     0.0549935126\n262602   13.56456463     0.2596099288\n263801   11.68484953     0.4401367906\n265902    2.27971944    -0.6898163117\n268301   -8.63399203    -0.2602065420\n268303   -5.07797169    -0.0925705583\n268901   -2.73911711    -0.1080836736\n268902   -0.02099607    -0.3175720823\n269001   -6.40852830    -0.3308146153\n269101    1.64397767     0.1089492888\n269102   -3.43560768     0.6082890616\n269902    4.22173462     0.3316347284\n271001   -2.12334601    -0.1303907024\n272903    4.38185278    -0.3130899367\n273501    7.80382862     0.1887815078\n274401   -3.71240836    -0.2934601340\n277002   -1.47109846    -0.2299156546\n277003    2.20731651     0.1441361273\n277502   -0.87534853    -0.0642997393\n278103    3.72179290     0.8281766983\n281001    1.13895265    -0.7879445725\n281901   -0.07844630    -0.0334560664\n282602    1.87317561     0.3683149463\n282603    9.88421279     0.4294529720\n287702   11.82750378     0.5330757534\n288501    6.99971145     0.3983911307\n290901   14.11834959     0.6984566312\n290902    1.64361999     0.0477031151\n291101    6.58907582     0.1904245325\n294902   13.52910485     0.0996728574\n294903    4.04906007    -0.0348286225\n295901   17.84224419    -0.3638195950\n295902   16.72697563    -0.0149393415\n296401    4.87136372     0.5574356775\n297901    0.84687052    -0.2670345370\n297902    0.41130645     0.1145698966\n298301   -0.58918147    -0.2280723208\n301101   13.33506909     0.8782945971\n301401    2.99577186    -0.0972184286\n302301   -0.05534750     0.0132257043\n302702    0.63106664    -0.0995252635\n302703    2.31583623     0.0357357292\n304103   -4.10462187     0.0070017789\n304605   -4.98883064    -0.0687863431\n307301    4.16898443    -0.1861782885\n307501   -5.83199987    -0.1330714743\n309802   10.00231268     0.3151889843\n309901    3.61252935    -0.2797896894\n310101    2.72671748     0.0507819512\n311301    0.06910156    -0.1987183686\n311701    1.18602175     0.0214387601\n312102    5.24212368     0.1154880962\n312201    6.39157472     0.0804486994\n312901   -1.36434489    -0.5908760141\n314501   12.27895743     0.3561983640\n315001    4.80871788     0.4410052527\n315002   14.26360252     0.7848261037\n315602    9.46741718     0.3196613108\n316301    8.00376575     0.4756457183\n316501    3.25134480     0.2483367009\n316502    4.02667362    -0.0248071619\n319002   -0.99029147    -0.0247661620\n319801    2.51260084    -0.1898536250\n320101    1.05737472    -0.4057983860\n320102    5.50177655     0.1859729344\n322505    8.99961806     0.4525228302\n324401   12.67218826     0.7162788309\n324402    4.68943247     0.5037748780\n324801    8.98393031     0.4061052517\n325401   -0.30588497     0.1504441398\n325402   14.13520777     0.3677402341\n325904   -7.65060980    -0.4413674652\n326901   -2.47507183    -0.1880051850\n328702    5.09932905    -0.2731988498\n328703    6.05950377     0.0370733815\n330601   -1.52431849    -0.5623768857\n331506   -4.28604519    -0.0850776291\n334201    1.01569419    -0.7862710803\n335001    7.51568701     0.2375349381\n335002    7.63335237     0.0201921781\n335501   -1.26543947     0.1299250669\n336401    2.60372823    -0.5942708036\n336902   -2.40018714    -0.5858921336\n340702    7.26735711     0.5187392959\n340801   -0.85913530    -0.1064656599\n342301    8.52393897     0.4812263180\n342302    2.64890519     0.4751974833\n342402    8.88899509     0.2523561606\n343302    0.50426922    -0.3098845309\n344602    5.26053138     0.3281176910\n346501    5.84872166    -0.0365265198\n347603   -1.61086388    -0.0874382322\n348103    4.27199199     0.1467680540\n349101    2.25256295     0.3478203511\n352201    2.67063297    -0.0496699420\n353101   -2.64498856    -0.3421006608\n353102   -2.39007492    -0.1751609503\n354502    9.82539513     0.4014066125\n354503   10.55906179     0.3653128802\n354801   -3.86694825    -0.0322843779\n356402   -6.57993704    -0.3743321608\n357203    5.17173032     0.5089711989\n358401    7.50343498     0.2105341747\n358601    6.12932311     0.1929350331\n360901    7.50888046     0.1385205953\n360903    2.52646621     0.0660842802\n362402   12.98417162     0.3360153678\n363501   -5.97513635     0.0185743037\n364103    5.11580408     0.2476807110\n367903   -8.77993459    -0.1638201369\n369702   -2.33109907    -0.2469774747\n369704   -5.46331820    -0.4170202642\n370303  -12.92784420    -0.6396665809\n371502   -1.16045066     0.5141694436\n371503    9.26459844     0.2505812764\n372901   -2.54543475    -0.0346830323\n373401    3.89252518    -0.6735275919\n373402   -0.34294446     0.1740321059\n373701   -0.92276427    -0.0469166652\n373702    6.71282977     0.2128305418\n374001   -0.34611020    -0.3259551873\n374601    2.10709785     0.4917309343\n374603   14.31415879     0.4547050055\n375303    2.83899510     0.1296090616\n379301   -4.30401478    -0.0671579708\n379801    2.28828124     0.0492821342\n379805    7.33896632     0.2160689278\n380201    9.52474854     0.5817236580\n381701   11.83879963     0.4424732335\n382101   -2.63505628    -0.5593227510\n389902    9.17160521     0.3483770138\n389903    8.56024293     0.4365974884\n389904    8.48746494     0.2000239925\n392102    2.41224586     0.7326398460\n392103    6.09716713    -0.0834210203\n392301    4.46366973     0.9695841788\n392303   13.80226641     0.4437150645\n392401    7.55631490     0.4644799069\n392402   14.38671948     0.4037568725\n396202  -13.95313822    -0.8174885512\n397202    9.59582195    -0.1519018029\n399503    0.83924439    -0.3049420417\n400101    8.72427727     0.1216743764\n403701   -1.14444791    -0.0906498799\n404503    2.78156285     0.0223124926\n405002    6.89082758     0.4366989831\n407702  -10.75175041    -0.0473625787\n408501   -2.67091920    -0.0999998250\n411002   -0.65115621    -0.2409778765\n413202   -6.71808986    -0.7269686751\n413203   -1.37482771    -0.8201798158\n413301   -2.66172721    -0.2282981434\n419401   -6.15375567     0.4087437635\n420001   -5.17111419     0.0480885676\n421001    3.73802259    -0.4128200152\n422401    5.61585902     0.0906333666\n424602    4.85017805     0.3994443371\n425401    5.52869399     0.3801297733\n425402    3.23083171     0.9752376791\n425601    1.81800422    -0.0501868314\n427502   -8.34808469    -0.1748916703\n427703   -2.58105454     0.0612231823\n431002    1.45336857    -0.1526561148\n433901   16.71068167     0.0756074877\n433902   12.88988712    -0.0369414380\n435201    8.58357796     0.5638797341\n436602    2.25595485     0.0940861963\n437201    8.49615386     0.3070198746\n439602   -1.55173568    -0.1503155023\n442301    9.12453283     0.5035755763\n442401    6.10349394     0.1701659310\n442801   13.08240565     0.3121792055\n443701    3.33969119     0.0872672738\n443702    2.28727869     0.2925396650\n443902    2.74162480     0.0168517841\n444702    1.65622289    -0.0144715436\n448304   -3.52885765    -0.1349501331\n449401    5.73738288     0.1736730684\n449501    8.12107401     0.7443148376\n449601    0.61300547    -0.5809085853\n449602    8.00518898     0.3289994215\n452601    5.03808314     0.1335260446\n453601    5.55206975     0.1961560885\n453602   -6.51650685     0.1383941342\n453801    1.35359431     0.5149975292\n453802   -4.18396178     0.0579802297\n453901    8.50615398     0.1764667928\n453902    8.63811379     0.4579029277\n456401    8.83208735     0.3337672169\n457002    7.12177153     0.2272419224\n458902   -7.07325468    -0.1154965398\n459201   10.50421027     0.0871649428\n461402   -3.07817377     0.1717537953\n463102   -6.58913972     0.0101886383\n463702    6.24407745     0.4497041152\n466201   12.16211963     0.2995786843\n468301    9.83429950     0.5760321094\n468302   -9.24562541     0.1207549542\n468303    8.40057932     0.2977980399\n471701    3.99751144     0.5852492074\n471702    5.68307142     0.2003194023\n472303    7.54576634     0.5374487798\n474602    3.00860908    -0.0302540522\n474603    5.44654790     0.1726615893\n474604   -0.19750268     0.7471078221\n475401   18.28362371     0.4722576397\n476601    6.50818657     0.0592427574\n478501   -0.81262081     0.3287675477\n480103   10.04768928    -0.0078424346\n481502    7.48838897     0.0832038225\n482002    9.82930749     0.1981375666\n482602    9.21466111     0.6885883280\n487101   -2.54324691    -0.3876331314\n488001   12.10166948     0.3378831335\n488201    8.48944588     0.7108716573\n488202    6.15751067     0.2422759315\n489902    0.26262580    -0.2417015557\n492501    8.41181224     0.6103569142\n492502    4.41861846     0.1288007841\n492503    0.37103063    -0.0736242417\n493003   -5.81067852     0.1425329042\n493301    1.98819041    -0.3485372227\n495202    4.76071878     0.2109290560\n497302   -1.57528046     0.6780668624\n497303    1.14467161     0.0736007731\n497304    4.86088239     0.1749832668\n497403    3.06966419     0.3200731485\n499703   -0.90951377    -0.0411132215\n502802    6.63064288     0.7634407959\n503201   11.56783003     0.4111780168\n504101   14.34197985     0.0795024380\n506601   -6.74281671    -0.0894814507\n506602   -2.67318818    -0.1562468845\n509801   -1.60447882    -0.0512030176\n509802    0.09920010     0.0712008496\n510002    0.06201616     1.0278906947\n510301    0.62037154    -0.5245783769\n510401    2.20274717     0.2776540669\n511901    1.48200004     0.0720984268\n513404   -6.48298148     0.5778192504\n513405   -3.94051946    -0.5795427854\n515102    9.91045021     0.1772474519\n516401    0.83141062     0.0331710983\n518003    9.10181796     0.3141137266\n519101  -20.77844394    -0.7269373977\n519503   -1.52544510    -0.2971374733\n521602   -4.38337078    -0.4462260781\n522401   -3.64488511    -0.1793189072\n523101   -3.54440585     0.0523048270\n523201   -4.48478872     0.1515196713\n523202    3.39194104    -0.1145805906\n524701    0.87560675    -0.0715633618\n524702    6.49374444    -0.4977759622\n525801   10.91496982     0.6047560428\n527204   -2.79154351    -0.1141600287\n531401    0.92461113     0.5652710374\n531402    7.67485702     0.1132916289\n531404    6.87434217     0.2124329097\n531704   -2.34571786    -0.1902737399\n532601    1.65637871    -0.1243266382\n532802   -0.80063303    -0.7536805507\n533002   -6.07135817     0.3285783699\n533003    6.21812835     0.3855830767\n534103   15.91636250     0.5329495196\n537002    4.81376988    -0.4972065699\n537302    1.76851777     0.5350614031\n537304   15.97162327     0.5329034206\n537602    3.39849941     0.3674605788\n538102    0.25725389     0.0441781126\n538703    1.16742099     0.8334546607\n538704    4.14195624    -0.0318725903\n542702   -1.84566047    -0.1432129734\n542703   -7.26063262    -0.1844095081\n542802   14.09495305     0.0927618870\n543601   -1.69130567    -0.5096972220\n543602  -11.37736612    -0.0269849015\n545402  -10.12673169    -0.2180968585\n546505   -9.43666581    -1.1977857411\n546702  -15.78959238    -0.3948813191\n547701    1.05290957     0.1235885944\n548501    0.98894469    -0.1428407727\n549801    2.26942850    -0.1915722608\n550901  -12.55194571    -0.5194917699\n551501   -9.37562448    -0.2152411148\n552203    5.14090557    -0.0003448253\n553701   -9.73871205     0.3822216460\n553702   -3.03880617    -0.0738900118\n556101   -3.77283846    -0.7718382911\n556801   -1.47570623    -0.0171300296\n558301   -7.14324649    -0.5487469069\n559302   -6.07905791     0.0400240615\n560902   -5.54229041    -0.4336642727\n561202    0.75392263    -0.2218294392\n561402   -1.37556357    -0.4682488108\n564001   -3.48967750    -0.8175299915\n564002  -11.94574926    -0.0241249448\n565601    9.76657834     0.3680539267\n567002    4.15433725     0.0133426000\n570601   -4.51569455    -0.2663299700\n571201   -4.79272205    -0.6800050860\n571801    1.55323351     0.3051389409\n571802    6.13115978     0.2549047480\n572402   -8.32726732    -0.0656636224\n572801    4.62416002     0.8819368502\n572802    1.02157458     0.0998848978\n572803    1.51500140     0.2176483977\n574003  -10.11412909    -0.3340443301\n574102   -0.39386523     0.1430315957\n574602    2.48365191    -0.0516316786\n574603    2.95382533     0.4634923445\n580202   12.87049963     0.0508731037\n581303   -6.47450073    -0.2375153112\n581802   -7.42943481     0.0559642772\n586102    7.78230466     0.4290705200\n587301   -4.24121968    -0.3397978594\n587303   -4.01174036    -0.0621354353\n591903    8.30649668     0.3023708916\n594102    3.56604927     0.6478537247\n600203   -7.28290558    -0.2818346289\n602301   -0.45221453     0.0560703208\n602302   -1.52539018    -0.0595140731\n602304   -1.07015524    -0.0090999711\n604004   -9.38738626    -0.2985034137\n604607   -5.69204863     0.1792029099\n604902   -6.18766241    -0.2029661455\n607201    1.10043463    -0.0231308950\n607601   -2.05803539    -0.6345825343\n607602  -13.04776462    -0.2216714504\n607802   -2.20348449    -0.1004745522\n610101  -11.11435821    -0.5902563029\n611802   -0.13978342     0.6886120263\n613201    5.46727108     0.1714387560\n613402   -9.37607949    -0.5751985987\n616105    4.30973310    -0.1559299180\n616402   -2.39820960    -0.4127444922\n617501  -18.51801403    -0.6305657123\n621501    2.11446055    -0.2288824668\n621701    8.61553140    -0.1668611774\n622001   -2.94776456    -0.3621177926\n623801   -1.76507243    -0.2776193139\n626201    8.59262078     0.0751752786\n627702   -3.04326268    -0.2012757363\n627703   -8.52970641    -0.0080475529\n627802  -10.87215247    -0.2908242274\n629404    4.32172379    -0.1955163986\n629502    3.57445932     0.1489229923\n631801   -6.37143804    -0.2736703029\n632205  -12.35923616    -0.1477299841\n632702   -7.37589491    -0.1007359765\n632703   -4.02889088     0.0691741771\n632704   -5.47214016    -0.3255188682\n634401   -9.57468902    -0.3632211523\n634503   -0.27668954    -0.1141029945\n635302   -1.72811133    -0.4343437623\n636402  -18.26222597    -0.4408466977\n636802   -5.31144334    -0.3662305763\n637110   -8.68730381    -0.0962076301\n638402   -1.40445264    -0.4295161898\n640002  -12.44703441    -0.3543034884\n640402  -10.21629561    -0.2899051227\n642601  -15.49411435    -0.6138021458\n642701   -6.49047806    -0.0143410464\n642702   -8.10270904    -0.1815307040\n642901    1.59567545    -0.2238702653\n642902   -7.81501060     0.0529624930\n642903   -9.88083787    -0.1054004356\n642904   -4.08142053    -0.1563599421\n643402   -9.52229064    -0.5194602602\n644203   -3.21966631    -0.2293817309\n644901  -14.48784208    -0.7265333920\n648601   -3.07058758    -0.1420968794\n648602   -2.76078318    -0.1936279089\n651601  -10.17740034    -0.1418839577\n665803   -8.96721530    -0.2627590709\n668403   -2.15993031    -0.3023291319\n669301    3.69190506     0.1245682511\n671102    1.18744864     0.2579028620\n675701   -8.00959962    -0.1001605468\n677201   -3.19136184     0.3751325462\n677202   -4.94268506    -0.1345561528\n678804   -1.50642477    -0.1276213379\n681601    2.74788969     0.2361923161\n682502   -3.91922522    -0.0844073621\n682903    2.32299554    -0.2222974062\n684201   17.43724634     0.2825095093\n684203   10.72930348     0.5940951580\n687602    4.12189718    -0.1142496014\n689101    1.26233806    -0.1398257266\n690101   -2.68719251    -0.5813889877\n693001   -1.27687419    -0.2952748946\n696601   -5.58706503    -0.2173202332\n700002  -13.62742629    -0.1059751275\n700003  -17.28590299    -0.3872682730\n707701   -1.98391960     0.0604997322\n708401   16.47802141     0.3498523756\n711602  -10.51885330    -0.1282419784\n711603   -6.09351989    -0.0111973218\n712303    0.94979708    -0.7358028699\n714801   -4.94840910    -0.2783046126\n714802    7.19269091    -0.3411300223\n715601    0.62464588     0.6207971092\n715803  -15.05238385    -0.5123990154\n716601   14.25432852     0.6234244176\n716602    8.26359495     0.3057598999\n717002   -6.38681035    -0.1404121943\n717003   -8.22659222    -0.5415894988\n717901  -11.10174951    -0.5570404242\n717902   -5.58157915    -0.0975620911\n717903   -7.20059512     0.0114687740\n718602    0.75795977     0.7884020853\n722401   -0.78048999    -0.2717673659\n722803    4.03436179     0.0515907769\n723501    0.05908726     0.1043310084\n725801   -1.57495157     0.4853286644\n725903  -18.19366967    -1.0139223353\n737702   -5.72621066     0.0224789597\n738201   -0.76015466    -0.2989000201\n739002   -9.12384930    -0.4346644392\n739102   -5.01927551    -0.1190896852\n739301   -5.29714642    -0.1568363283\n739401    3.80759256     0.2419631799\n739601    6.28351106    -0.0899081928\n742301    0.62244510     0.0222874064\n743601   -1.26328765    -0.1224183171\n743602   -5.90891613    -0.9651072304\n743801   -5.77236269     0.0644369029\n743802  -11.75100265    -0.4618711705\n744102  -19.15264271    -0.0325021945\n744103   -6.49068171    -0.2704218056\n744703   -4.55666266    -0.0596432928\n744704   -8.55411132     0.0132832961\n745103    5.44376720     0.1246754172\n745904    0.07062409     0.0021356217\n748003   -1.25277068    -0.2412792565\n748502    6.80312497    -0.1043084221\n749802    8.84739932     0.1671736319\n749803   -1.80138996    -0.3964384985\n750104  -12.15768075    -0.2460528287\n750404    1.62886176    -0.5777132796\n751001   -6.96767775    -0.1472712091\n752003    1.86283704    -0.6042768788\n752501   -2.10429583    -0.1481498762\n760102    0.09321882    -0.3280027184\n763603   -9.97729015    -0.2324196873\n764503   -3.17013340    -0.2162743116\n765702    1.22290117    -0.1709812700\n767901    3.34640405    -0.1181786125\n771002   -9.80823742     0.0363041548\n775002    7.26535421     0.2424549293\n778902  -13.35237266    -0.2360579361\n778903   -1.11963310    -0.7415481973\n783001   -8.85817947    -0.1732627355\n783002   -2.11893251    -0.1922760022\n783301    3.92048142     0.4376465813\n783502    1.29473302     0.1308790786\n783602   -1.45070043    -0.0918634336\n783801    1.52791455     0.2600695857\n785601    3.85586110     0.0292718871\n786402    4.89369530     0.1606372328\n788302  -11.75548157    -0.3673913569\n788303  -11.61220554     0.1842763871\n792103   -1.43374677     0.2662663316\n792704   -4.02916326    -0.1507749643\n793001    3.20944565     0.4346527613\n794301  -10.79587928    -0.4337240934\n794503   -8.01216100    -0.0399914068\n795201   -4.76959755    -0.2763319871\n795901    5.55386225    -0.0616046594\n799803   14.32960293     0.2686957534\n800602    1.49286189    -0.1036309975\n804701   -1.17293613     0.1062343948\n804702    0.28408422    -0.3717005663\n809102   -5.50774099    -0.6120425279\n809103   -9.73439954    -0.6528738724\n809301   -6.17695516    -0.8572911313\n810303    0.51773971     0.0366535860\n811002   -8.62663496    -0.2007538130\n812504    0.87502453     0.0211229274\n814101    0.42841358    -0.1408931451\n817402   -3.54804620    -0.3472196503\n817403   -3.86157131    -0.7364346273\n817404   -4.76425763    -0.5337793547\n822602    9.16403288     0.1432312623\n825702  -10.49440781    -0.1380894481\n825902   -8.38039051    -0.1678709474\n825903   -1.49536360    -0.5381358973\n826503    3.08088843    -0.1347601961\n826504  -20.54599464    -0.8000334949\n826904  -17.87676247    -0.7851056092\n826905   -8.60874422    -0.2217742873\n827101    2.66183899     0.0591032723\n827302   -6.60753296    -0.0763585846\n828302    2.50246833     0.2719766465\n828604   -0.59185682    -0.3378811838\n828902   -5.44556650    -0.4725825266\n828903   -5.69626291    -0.1585394960\n828905  -14.30914021    -0.2877583739\n829401   -8.46306547    -0.3536454753\n829403    1.40174772     0.2195352916\n833601    0.46354614    -0.2416298693\n834301    0.69847503     0.0904905814\n835202    1.90946408    -0.2377733911\n835703   -3.71007588     0.4533939673\n837503   -7.10529269    -0.2262551947\n837504  -10.72089835    -0.0453328518\n837602  -14.41077229    -0.2665595443\n838301   -7.37823973    -0.4641650815\n838903    4.40667481     0.2845479867\n841601    0.49388173     0.2429527312\n841602    0.96254441    -0.0743071125\n846301   12.13543443     0.3270844443\n847301   -4.55142777    -0.5435344937\n847302    3.26421860     0.0257019024\n847701    8.06561086     0.3037528239\n847901   -0.38112182     0.0809138027\n847902   -7.53187036    -0.0486669826\n848201    4.84470600     0.6073714319\n849401    1.07203108     0.0033751301\n853202   -1.09748063     0.3094748612\n856103   -2.73582943    -0.1088943209\n857003   -7.66990411    -0.4150471804\n858302   -2.74849531    -0.5608923719\n861503   -3.78982918     0.2475855901\n861803   -2.97073434     0.2254911831\n862201    4.25200741     0.0319034787\n863101  -14.25117497    -0.3157709628\n863403   -6.18115441    -0.4539991103\n864302    1.50055212     0.0370319980\n866203  -10.11371357     0.0505763752\n866205   -4.68628568    -0.0307954020\n866402   -3.92159378    -0.0073773810\n867903   12.87009522     0.2625227045\n869001   -7.85906076    -0.1745015929\n869501   -0.26741551     0.3929540575\n869601  -12.03373308    -0.3736088706\n870803    8.98393031     0.4061052517\n878701  -18.68847074    -0.4881939933\n879403   12.53343479     0.2662535242\n879404    3.41746666    -0.0726789109\n885801    3.35147352     0.2891352889\n885802  -17.18001472    -0.8362180462\n886401   11.94508113     0.7070208010\n891202   -1.25942591     0.0769473146\n892601   -7.45849183    -0.0347396025\n894201   -4.96233381    -0.1898835371\n894802   -0.29049460    -0.0498230854\n894803   -6.83109938    -0.3037833656\n894804   -4.71097248    -0.1486124197\n897001   -3.23101044    -0.0424965645\n905003   -7.50461778    -0.2942284464\n907001   -0.68120953    -0.1740712403\n908102    0.61948614    -0.0643721505\n910103  -11.12529206    -0.2414020856\n911701    2.35085586     0.0316994804\n914402    4.16586576    -0.0659339313\n918002    0.71214806    -0.1178692639\n918201    1.37972142     0.0358380536\n918202    3.37420294    -0.0390783123\n918301   -0.39304186     0.4941953836\n922901    4.71588552    -0.1297348488\n925503   -3.46311918     0.1073699380\n925504   -9.37905557    -0.3726216531\n925701   -2.73247889    -0.1138434926\n926102  -13.27001949    -0.2987839730\n928101   -3.60879315    -0.2033903084\n929805    2.06871029     0.0363303706\n931202    6.64695761     1.0144100185\n931703   -5.07544155     0.2020379438\n938001   -3.80555671     0.1232922861\n938002   -4.90566464    -0.3514179845\n938205    1.71865625     0.0819954684\n940103   -3.38287479    -0.1529178398\n951602   -7.89307032    -0.5458241385\n952201   -3.92147381    -0.1430649901\n964101    4.17160699     0.0114549612\n964102    8.61276552    -0.1219688044\n966001    9.12818819     0.2929178899\n968001   -0.73632140    -0.7271500579\n968002   -4.62976856     0.1007097375\n968003   -8.56007668     0.0784538132\n968401    3.21394765     0.1983660519\n968402    7.70923179     0.6755622251\n976803   -8.95144070    -0.5889793307\n982001   -5.65102312    -0.3193414526\n983703    4.26011136     0.0564918407\n983902    3.14070377    -0.0393428433\n983903   -3.72071923    -0.1340056061\n984402   -2.09602353    -0.0859395972\n984404   -0.01448188    -0.2800213610\n985802   -2.95261179    -0.2024834226\n985803    0.85416414    -0.1662302976\n986203   -2.27856318    -0.8161348888\n986505    0.46788756    -0.6209365248\n986506   -4.83382985    -1.0580898374\n987401   -6.23784409     0.2277177885\n987701   -3.15407859    -0.6870811850\n988701   -9.40290690    -0.3343254520\n989201   -5.50937820    -0.3491790429\n989503   -4.05311451    -0.2403967494\n992201    4.25662180    -0.1207129833\n993803   -0.69437498     0.1764164915\n995101  -17.81253610    -0.3418437764\n995102  -18.01823777    -0.6091920302\n995304    0.82101435     0.0169707917\n997002   -8.97125105    -0.0363371746\n998001    3.02315351    -0.2582214586\n999202   -3.61008928    -0.4116637140\n999203   -2.69887313    -0.4170097125\n1002601 -12.91569829    -0.2434075380\n1003202  -5.65382038    -0.0147718027\n1003203  -7.68965384    -0.3249356517\n1003204  -8.23259578    -0.1417066181\n1003601   3.68020040    -0.0278905168\n1003802   6.22604589     0.5487903889\n1004802  -5.92929958    -0.0561947397\n1004903  -5.10086883     0.4179924258\n1005201  -5.83846763    -0.1489903827\n1005203   7.84438293     0.2294645755\n1007201   1.30617941    -0.1305151137\n1007301 -12.46457362     0.0949988152\n1007302 -12.65064253    -0.0076174583\n1007901  -2.17175547     0.1870277949\n1013201  -7.94123128    -0.1702334943\n1017504 -16.11797209     0.2694257975\n1018402  -8.50816043    -0.2456271516\n1018404  -6.81942108    -0.2592715409\n1019104  -4.30967307    -0.3239387802\n1019105 -11.59876369    -0.2794359515\n1023702  -0.04874114     0.0588083963\n1030801   9.88491137     0.2300356579\n1031001   8.41926674     0.8884365398\n1031002   2.61726444     0.5687764738\n1031302  -7.49864845     0.0238281057\n1033202 -15.27241276    -0.1013013000\n1033601  -6.77615959    -0.1356527064\n1034001  -6.38457498    -0.3774152235\n1036001  -2.90674838     0.7579398885\n1036003  -2.49687709    -0.1787402824\n1037502  -1.21200719    -0.1915285497\n1040101   3.61694408     0.4188760000\n1042101  -2.47889816    -0.3250838342\n1042201  -7.07864202    -0.1667731918\n1044801   5.27999007     0.1352621731\n1047801  -2.78469529    -0.2438990700\n1050001  -3.00945189    -0.1172746551\n1050701  -5.81647206     0.6866632213\n1053201  -0.95441977    -0.1619816337\n1053202  -6.95363077    -0.3185416524\n1053802  -7.07885643     0.1638374049\n1053804  -7.76244443    -0.2296186628\n1056501  -5.04202222    -0.0791672561\n1081101  -6.75642144    -0.2620547492\n1081103  -4.96763880    -0.0609534768\n1176403  -2.20154088    -0.3199205933\n1176502  -5.18399523     0.1404597238\n1179201   6.26432726    -0.3416049617\n1179202   3.04900080    -0.0282322304\n1180202   1.57919458    -0.5766208445\n1181201 -12.69246491    -0.5442199223\n1181902  -2.90077365    -0.1014802659\n1181904  -5.61383445    -0.1279697556\n1182604   0.53424136    -0.3697945245\n1185003 -13.13911291    -0.0029829473\n1187001   3.32587038    -0.3349836126\n1187303   5.94996716     0.2982764506\n1189901   9.09614222     0.3593369162\n1190501  -3.11456753    -0.0952460874\n1191001   2.42903740     0.2403311666\n1191901   4.88269987     0.2124648289\n1194901  -3.08601463     1.0566024696\n1197902  -5.58073826    -0.4460158896\n1198101  -1.27644936    -0.2101451737\n1198901  16.65324847    -0.0262390833\n1201701  -1.67105619    -0.2423445259\n1201702   0.08009328    -0.5883490291\n1203201  -2.06266202    -0.4507707221\n1205201  -6.00924682     0.2217953590\n1207601   4.48135562     0.1113411087\n1209201  -0.62585073    -0.2031446927\n1211502   9.32789455     0.1114924859\n1211503   5.69457884     0.1078567048\n1212303  -3.26708662     0.0656369113\n1213501  -3.82772221     1.0464054797\n1217202  -1.94939839    -0.5953455294\n1217204  -7.84971939    -0.0171599235\n1217702  -2.77265776    -0.0310256094\n1217704  -2.68889407    -0.1430860568\n1219103  -8.14296645    -0.0699275642\n1219105 -14.48626571    -0.4039643861\n1219106 -15.39021023    -0.3394775206\n1219108   2.23229848     0.3042720787\n1221003  -0.11492997    -0.1629173604\n1221702  -5.09500316    -0.2257966798\n1224001   2.98624921    -0.2283459623\n1225402  -4.74670456    -0.1094826571\n1228103  -0.52257207    -0.0103495485\n1230302  -8.82049460    -0.6378126834\n1256601  -8.41003160    -0.1124932283\n\nwith conditional variances for \"id\" \n\n\n\n\n\n6.1.4 Spline Growth Curve Model\n\n6.1.4.1 Create Knot\n\n\nCode\nmydata$knot &lt;- NA\nmydata$knot[which(mydata$ageYears &lt;= 11)] &lt;- 0\nmydata$knot[which(mydata$ageYears &gt; 11)] &lt;- 1\n\n\n\n\n6.1.4.2 Fit Model\n\n\nCode\nsplineGCM &lt;- lmer(\n  math ~ female + ageYearsCentered + female:ageYearsCentered + knot + knot:ageYearsCentered + female:knot + female:knot:ageYearsCentered + (1 + ageYearsCentered | id), # random intercepts and linear slopes; fixed quadratic slopes; sex as a fixed-effect predictor of the intercepts and slopes\n  data = mydata,\n  REML = FALSE, #for ML\n  na.action = na.exclude,\n  control = lmerControl(optimizer = \"bobyqa\"))\n\nsummary(splineGCM)\n\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: math ~ female + ageYearsCentered + female:ageYearsCentered +  \n    knot + knot:ageYearsCentered + female:knot + female:knot:ageYearsCentered +  \n    (1 + ageYearsCentered | id)\n   Data: mydata\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n  15691.2   15759.6   -7833.6   15667.2      2209 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3041 -0.5021 -0.0075  0.5034  2.4850 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n id       (Intercept)      68.3568  8.2678        \n          ageYearsCentered  0.7018  0.8377   -0.03\n Residual                  27.7772  5.2704        \nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                              Estimate Std. Error        df t value Pr(&gt;|t|)\n(Intercept)                    25.9318     0.7222 1223.0139  35.909  &lt; 2e-16\nfemale                          0.3473     1.0060 1208.6807   0.345    0.730\nageYearsCentered                6.2092     0.2328 1374.3269  26.674  &lt; 2e-16\nknot                           12.6515     1.8238 1241.0685   6.937 6.43e-12\nfemale:ageYearsCentered        -0.3785     0.3240 1373.1043  -1.168    0.243\nageYearsCentered:knot          -3.4908     0.3746 1283.6771  -9.318  &lt; 2e-16\nfemale:knot                    -1.2105     2.6098 1234.2550  -0.464    0.643\nfemale:ageYearsCentered:knot    0.3530     0.5383 1279.7386   0.656    0.512\n                                \n(Intercept)                  ***\nfemale                          \nageYearsCentered             ***\nknot                         ***\nfemale:ageYearsCentered         \nageYearsCentered:knot        ***\nfemale:knot                     \nfemale:ageYearsCentered:knot    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female agYrsC knot   fml:YC agYrC: fml:kn\nfemale      -0.718                                          \nageYrsCntrd -0.786  0.564                                   \nknot        -0.267  0.192  0.266                            \nfml:gYrsCnt  0.565 -0.776 -0.718 -0.191                     \nagYrsCntrd:  0.470 -0.337 -0.568 -0.925  0.408              \nfemale:knot  0.187 -0.259 -0.186 -0.699  0.262  0.646       \nfml:gYrsCn: -0.327  0.455  0.395  0.644 -0.559 -0.696 -0.927\n\n\n\n\n6.1.4.3 Protoypical Growth Curve\n\n\nCode\nnewData &lt;- expand.grid(\n  female = c(0, 1),\n  ageYears = seq(from = min(mydata$ageYears, na.rm = TRUE), to = max(mydata$ageYears, na.rm = TRUE), length.out = 10000))\n\nnewData$ageYearsCentered &lt;- newData$ageYears - min(newData$ageYears)\n\nnewData$knot &lt;- NA\nnewData$knot[which(newData$ageYears &lt;= 11)] &lt;- 0\nnewData$knot[which(newData$ageYears &gt; 11)] &lt;- 1\n\nnewData$sex &lt;- NA\nnewData$sex[which(newData$female == 0)] &lt;- \"male\"\nnewData$sex[which(newData$female == 1)] &lt;- \"female\"\nnewData$sex &lt;- as.factor(newData$sex)\n\nnewData$predictedValue &lt;- predict( # predict.merMod\n  splineGCM,\n  newdata = newData,\n  re.form = NA\n)\n\nggplot(\n  data = newData,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    color = sex)) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.4.4 Individuals’ Growth Curves\n\n\nCode\nmydata$predictedValue &lt;- predict(\n  splineGCM,\n  newdata = mydata,\n  re.form = NULL\n)\n\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n6.1.4.5 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = ageYears,\n    y = predictedValue,\n    group = factor(id))) +\n  xlab(\"Age (Years)\") +\n  ylab(\"Math Score\") +\n  geom_line() +\n  geom_line(\n    data = newData,\n    mapping = aes(\n      x = ageYears,\n      y = predictedValue,\n      group = sex,\n      color = sex),\n    linewidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n6.1.4.6 Extract Random Effects\n\n\nCode\nranef(splineGCM)\n\n\n$id\n          (Intercept) ageYearsCentered\n201       0.652144960     0.2083234957\n303     -13.900382644    -0.5384100866\n2702     13.248702169     0.4885445614\n4303      2.123617918     0.2695865314\n5002      1.813504926     0.3311534903\n5005      4.914571158     0.1306294817\n5701     12.746328285     0.1492250422\n6102      9.811998768     0.9898072080\n6801     11.251157093     0.2057798471\n6802     12.123413767     0.2804666424\n6803     13.281644740     0.5360890015\n9603      5.389868582     0.3847449591\n12401     2.656010922     0.1765637817\n12801     2.656929651     0.0729177050\n13702    10.715721324     0.1073154154\n13801    15.450806205     0.2520724528\n13803    12.401308398     0.1001032177\n17201     7.861599067     0.4026244286\n18601   -10.177265186    -0.2439801021\n22104    -9.772847154    -0.1501052395\n22901    -3.034416924    -0.1081377380\n23602     8.698629194     0.2387282168\n23701     3.154766594    -0.1140889363\n24402     4.792965940     0.2287609053\n26201    10.809083835     0.6206665402\n28503    -8.129799847     0.3794778473\n29201    13.049426803     0.2092913485\n29202     8.242040820     0.7597709985\n34401    14.369766601     0.5254802987\n35401     6.833553015     0.6305626334\n35402     5.815713216    -0.1275532970\n36901    16.472051954     0.3285090538\n36902    11.202203620     0.2812051478\n37403    -0.583891740    -0.5036468086\n38202    -2.437779179    -0.0475118466\n38802    -6.196997784    -0.2974576620\n40501     3.268217007     0.0214128468\n40702     9.112695574    -0.0492314988\n42504     1.995724022     0.3717940459\n42505     6.863666740    -0.2638968855\n43103     5.821996552     0.3275967123\n45201     8.831273789     0.4316009873\n45903    15.771000534     0.2051521382\n46201    11.264243763     0.9108447155\n47201    15.668737876     0.1788208935\n47402     1.397962004    -0.1572621684\n47601     2.316161114     0.0508547425\n48502    -2.010293653     0.3936983795\n49501     2.160142219     0.1134996285\n53601    -1.775617993    -0.3861595134\n55201     9.157903534     0.4402902346\n55801     2.566584709     0.3197868128\n56301    -6.033375659     0.4309999924\n56302    -0.497820148    -0.0766013567\n57604    10.920240580     0.5373312559\n58503    -9.809490058     0.1142761543\n62102     8.723136893     0.5120606501\n62103   -12.441656968     0.2256406920\n62202    12.496248891     0.3601452142\n62703     1.085446072     0.6091981739\n64002     1.922686288    -0.3572227883\n65601    12.556337701     0.2046818628\n65701    12.034497734     0.5863928139\n66002     5.629706249     0.4789736360\n70801    -2.802991869    -0.0481438997\n73602     2.928465530    -0.0111033594\n74602     5.341015494     0.0845985843\n75603    -9.063079470    -0.1325943166\n76402    -0.382474144    -0.3180360760\n76403     4.039892562     0.2205538357\n80102     4.373011570     0.4045018855\n82901    -3.998049469    -0.2277049608\n83001    -0.771642029    -0.2655275958\n83101    -5.938951649    -0.1721878170\n83102    -0.335988597    -0.2163531218\n83602     4.768937922     0.0307470347\n83603    -0.704466068     0.0518760872\n85402    -4.069035989    -0.0549955050\n87501    -2.216913886    -0.1313351231\n87901     3.713742743     0.0534331179\n88902    -5.217618285    -0.2443179929\n89303    -0.549963171    -0.1108078629\n89803     6.315856211     0.6272311822\n90002    11.200283209     0.3496123205\n92303     1.591599351    -0.2536957823\n93202    -4.272559716    -0.0686060268\n93901    -4.258376290    -0.0789601908\n94002     1.347898206     0.8180194366\n96201    -0.812094574    -0.1820390184\n96202    -5.188134771    -0.1368756110\n98102     7.021903082     0.3228415319\n101701    0.611491352    -0.5395367529\n101702   -3.309701677     0.2651586781\n103501   -4.855153690    -0.5988670475\n108201   17.109929489     0.3560101418\n113001    8.103995908     0.6662753745\n113201    5.068036082     0.3308741944\n114101   -7.638376061     0.1190733697\n114702    3.620812603     0.0676329525\n115901    3.596298646     0.5221457642\n116002    2.607162423     0.0792587285\n116004   11.141442069     0.3485010978\n118402   11.657468146     0.3032229400\n122301   -8.237554758    -0.0444601711\n124201   -3.201342516    -0.1291495001\n124402   -7.411447467    -0.2726549361\n125002   -2.324071622    -0.2732205004\n126101    3.395328628     0.2530541153\n129502   -8.109352437    -0.3358308547\n129601   -3.694174646    -0.1748879841\n129602   -0.004890503    -0.0001257246\n132301   11.022416508     0.4585683295\n132402   -9.869915706    -0.0353655374\n132403    1.015949190    -0.2664092494\n136001   -4.570641198    -0.3018957317\n138901    3.448681327    -0.4417087283\n138902   -0.659219825    -0.1648237787\n139902   11.569741040     0.7989640943\n140004   -4.143993718     0.2820237083\n140104   -9.393058287    -0.4183388716\n140901    0.506211936     0.5776220274\n140902    2.008178668    -0.4332679931\n141001   10.529025701     0.2681977001\n141601    2.834946460    -0.1234921691\n141603    1.864431525    -0.2273344283\n143901    2.992285556     0.0424144798\n144004   -0.604860664    -0.1055563297\n153203   17.584083365     0.0812167576\n153204  -19.217608036    -0.5563571110\n153801    0.213455250    -0.1616756876\n155902   -9.815326467    -0.2648213903\n156401   13.106919035     0.1431165911\n156605   10.401893628     0.3255663688\n158701    8.376065438     0.2669487460\n158702   11.327900531     0.6213357486\n159605  -15.721145813    -0.6151441672\n159606  -12.670157227    -0.4217667273\n159801   -8.837673378     0.4405456251\n165501   -9.556634747    -0.2746392920\n165602    3.848039614     0.2136728781\n165701  -10.922472572    -0.3906266026\n165801    7.847561348     0.2132949362\n166602   -1.772260785     0.1403003024\n166603  -13.018202108    -0.2436342004\n167501   11.902952461     0.8271717586\n167502   18.389913126     0.5251316841\n169003   -1.184804415     0.0920900276\n170403   -3.640211918    -0.5911589285\n170404   -5.888642474    -0.1909256162\n171401   -8.774188849    -0.0289529377\n172502    6.007624508    -0.4856175096\n173001   -3.532202556    -1.8229571244\n173201    0.100180877     0.0044099543\n174001   11.496154194     0.2386557661\n175405  -10.944056962    -0.3543867229\n175801   -7.388428629    -0.2115091628\n176003    9.279680284    -0.2422432351\n178901   -7.900264290    -0.1734262827\n179401   -5.569316716    -0.2015112502\n179701   -1.573630049    -0.6057534998\n180301    8.830015417     0.5339910070\n181801   -2.130233232    -0.1011293569\n182903   -0.781088208    -0.0357528437\n185304    2.054010804    -0.1857004637\n187803    3.237960366     0.4697704541\n188002    6.682525613    -0.2336373424\n188502    7.163180465     0.3866629904\n189601    7.497162134     0.2684616832\n189602    2.323101362     0.7762704681\n190202    2.038508673    -0.0034264974\n190501   11.512792021     0.3307443223\n191202    0.868352422     0.3079964455\n193601   -7.779654363     0.9775400072\n193803    5.906687900     0.3316013332\n196002   -8.362696350    -0.3529805778\n196201    4.219276510     0.3626475506\n196202    1.021205365    -0.2030038501\n204602    8.872590054     0.1807214418\n205802   10.242216349     1.0490612027\n207102   10.993956855     0.5654727084\n207202    3.525816664     0.6562284526\n207301   -0.791545723    -0.2182437680\n212001    5.056782807     0.4001862747\n213101   -2.062636132    -0.2755041558\n213702    1.551355094     0.2113731687\n215304   -2.031118393     0.5523556292\n216701    7.374241783     0.1131518312\n217402    5.064895697     0.2008526677\n219003    2.354076655     0.7937105244\n220401    3.782415835     0.7125569436\n221202   10.070996738     0.2486922321\n223002   15.728425584     0.4306337911\n224901   -3.013015917    -0.4251288567\n226001   -8.599400863     0.1950905140\n226502   -8.479978528     0.0002054359\n226704   -8.653802583    -0.0481777029\n227002    3.020943909    -0.7776319668\n227101  -12.003922326     0.1818461513\n227102    3.594571099     0.4742016447\n227201   -3.471070487    -0.0158667855\n231502   -8.470660512     0.1306369113\n233301   -0.277743553    -0.3491408232\n233901    9.142608012     0.2853892339\n233902    5.977265622     0.1037109941\n235503   -9.748939377    -0.2559857776\n236002   -3.956374105    -0.4352266309\n236201   -6.860293369     0.0635328590\n236202  -11.405396328     0.0112172260\n236901    6.537680492     0.3228252642\n237501    6.634815535     0.6219354631\n238301   -0.561639744    -0.2775688589\n239603   -3.807444046    -0.4613305887\n239604   -2.624217708    -0.1392523867\n240701   -2.814072362    -0.2265948459\n240702   -5.394431688     0.3318887771\n244201    0.055103450    -0.1242383480\n244702    0.003743181    -0.0272862586\n245903   -1.499682835    -0.5018116428\n248402  -11.221149647    -0.3177062488\n250502    1.687272155     0.2908951509\n252902   -9.840652753    -0.4844864290\n253001   -4.097697558    -0.3324151023\n253603   -4.974154728    -0.3192530906\n253802    1.821490157    -0.3424047377\n254901    0.933188357     0.2815704653\n255201    4.481286992    -0.1425491653\n255901  -14.401543986     0.1349502187\n255902   -6.802601303    -0.1557736752\n255903   -0.693978827     0.0204272984\n258801   10.512221706     0.3394946638\n260702    9.877213987     0.4397797986\n261501   -5.686362487    -0.1323804043\n261502   -6.663219382     0.8028337630\n261503   -4.189480113    -0.4156013821\n262601    3.851042222     0.0482523003\n262602   13.080986760     0.2369840503\n263801   11.669895820     0.4596394426\n265902    1.986862239    -0.7016725848\n268301   -8.241647519    -0.2575504204\n268303   -4.742035217    -0.0927816751\n268901   -2.954530211    -0.1571329152\n268902    0.138025639    -0.3118058171\n269001   -6.304862424    -0.3485069887\n269101    1.861546568     0.0974767425\n269102   -3.453675766     0.5784219826\n269902    4.445471097     0.3364996881\n271001   -1.830838376    -0.1057897562\n272903    4.287569963    -0.2487570271\n273501    7.635717755     0.2268629055\n274401   -3.403635800    -0.2756986901\n277002   -1.243933170    -0.2347542123\n277003    2.418219955     0.1380275097\n277502   -0.502617518    -0.0413044058\n278103    3.626497373     0.8115046407\n281001    1.184524548    -0.7241158255\n281901    0.213884274    -0.0154378167\n282602    2.118326702     0.3866349623\n282603    9.790238821     0.4315623788\n287702   11.817901548     0.5838264952\n288501    7.066996520     0.3201208641\n290901   14.099909280     0.7470816302\n290902    1.235288575    -0.0096163502\n291101    6.558065297     0.2267584373\n294902   13.061928696     0.1184870528\n294903    3.701090470    -0.0737191217\n295901   17.536351167    -0.2569181644\n295902   16.283912304     0.0475563269\n296401    4.814174048     0.5666228506\n297901    0.604055604    -0.2645571474\n297902    0.300175805     0.1178062781\n298301   -0.335988597    -0.2163531218\n301101   13.409420437     0.8805909885\n301401    3.178349996    -0.1070821273\n302301   -0.330216266    -0.0378310093\n302702    0.414075786    -0.0966631214\n302703    1.625142915     0.0869742473\n304103   -4.258930346    -0.0007285439\n304605   -4.594662272    -0.0609633247\n307301    4.097671261    -0.2070800977\n307501   -5.731119892    -0.0976376594\n309802   10.210768919     0.3486302793\n309901    3.365091391    -0.2940665304\n310101    2.347385110     0.1150987239\n311301   -0.079230537    -0.2061300176\n311701    1.441502874     0.0295624543\n312102    5.010523715     0.1092726691\n312201    6.531756956     0.1254815215\n312901   -1.529426522    -0.5730621638\n314501   12.368906421     0.4043090114\n315001    4.978399175     0.4164754804\n315002   14.324568079     0.7762316498\n315602    9.622624095     0.3415007309\n316301    7.813598649     0.4696356705\n316501    3.505645228     0.2486061869\n316502    3.814762666    -0.0454311390\n319002   -0.863031650    -0.0236853419\n319801    2.329578014    -0.1994627117\n320101    1.013056213    -0.4303786086\n320102    5.745253336     0.2079915499\n322505    8.800142108     0.4403072786\n324401   12.472711183     0.7386113252\n324402    4.926974035     0.4931659164\n324801    9.307156689     0.4423498499\n325401   -0.444107885     0.1568379273\n325402   13.738092571     0.3904219374\n325904   -7.266729700    -0.4349631549\n326901   -2.911228814    -0.1638876977\n328702    5.234498346    -0.2462162633\n328703    5.850018397     0.0647886740\n330601   -1.705856099    -0.5528835774\n331506   -5.101381795    -0.0305780561\n334201    1.152108584    -0.7610134381\n335001    7.715419757     0.2579289603\n335002    7.418598668     0.0551640301\n335501   -1.741422585     0.1710639480\n336401    2.346023519    -0.6229368200\n336902   -2.094162645    -0.5544568146\n340702    7.202832056     0.4778494391\n340801   -0.956913652    -0.1071881399\n342301    8.704835045     0.4720677533\n342302    2.921861452     0.4501960702\n342402    8.627781993     0.2631668185\n343302    0.566607364    -0.2693856801\n344602    4.712798604     0.3726930900\n346501    6.052797770    -0.0095679508\n347603   -1.721309205    -0.0576561583\n348103    4.136958195     0.1614105922\n349101    2.141431032     0.3887287248\n352201    2.857189324    -0.0203346113\n353101   -2.328845896    -0.3179772141\n353102   -2.795949498    -0.2292623024\n354502    9.414280018     0.3633543593\n354503   10.601682996     0.4236930057\n354801   -3.770758732    -0.0110762971\n356402   -6.397953611    -0.4121958923\n357203    5.134882822     0.5372189493\n358401    7.645910799     0.2562036013\n358601    6.311240253     0.2082612873\n360901    7.178736718     0.1105831553\n360903    2.392125042     0.0484097036\n362402   12.865014267     0.3642492572\n363501   -6.173965225    -0.0470830092\n364103    4.855755535     0.2614831694\n367903   -8.369199168    -0.1698641392\n369702   -2.628643614    -0.2467037984\n369704   -5.098361243    -0.4024509929\n370303  -12.934662970    -0.6739988743\n371502   -0.755066027     0.5223393717\n371503    8.883492300     0.3017167234\n372901   -2.738637191    -0.0793893415\n373401    3.810876560    -0.6791280325\n373402   -0.193460539     0.1999688177\n373701   -1.253211531    -0.1030847253\n373702    6.505222654     0.2386146419\n374001   -0.915908406    -0.2920226226\n374601    1.830191801     0.5260605541\n374603   14.495786752     0.4950411151\n375303    2.828177631     0.0725274247\n379301   -4.385038019    -0.0701858289\n379801    2.486466689     0.0596081939\n379805    7.128114745     0.2256932305\n380201    9.226166693     0.5937558998\n381701   12.021485874     0.4805700574\n382101   -2.365127627    -0.5252701788\n389902    9.142609358     0.4063503563\n389903    8.432352420     0.4517620633\n389904    8.418959235     0.2085503909\n392102    2.722267970     0.7043730423\n392103    6.031839924    -0.0211781682\n392301    4.788346193     0.9509812755\n392303   13.478435609     0.4404240465\n392401    7.351647293     0.4479190882\n392402   14.087324049     0.4080264824\n396202  -13.520441735    -0.8212379808\n397202    9.272004411    -0.1428923809\n399503    0.447986224    -0.2244860671\n400101    8.744736642     0.1113099082\n403701   -0.816164297    -0.0648390973\n404503    3.056984122     0.0438148488\n405002    7.123329178     0.4305210057\n407702  -10.789988674    -0.0837651028\n408501   -2.448039418    -0.0591879094\n411002   -0.340595602    -0.2232830212\n413202   -6.775502515    -0.8305434935\n413203   -1.457538922    -0.8797885682\n413301   -2.789310746    -0.2361028975\n419401   -6.015112675     0.3577753578\n420001   -5.237150882     0.0338472565\n421001    3.485009863    -0.3831222603\n422401    5.829261703     0.1234267846\n424602    5.052307780     0.3861497325\n425401    5.407125908     0.3893021825\n425402    3.005234654     0.8540066856\n425601    1.831274021    -0.0098355064\n427502   -8.384260437    -0.2235018781\n427703   -2.361884960     0.0833204408\n431002    1.055200188    -0.1972411019\n433901   16.286112281     0.1145098583\n433902   12.745319758     0.0454684445\n435201    8.239836920     0.5983216893\n436602    2.134630967     0.1009702983\n437201    8.212366679     0.2867709480\n439602   -1.754577388    -0.1520604389\n442301    8.736255166     0.4601878774\n442401    5.991235801     0.1594879876\n442801   12.791242284     0.3111619801\n443701    3.563785496     0.0892544026\n443702    2.576755826     0.2975002715\n443902    2.938201966     0.0543486000\n444702    1.950221752     0.0049582962\n448304   -3.212330335    -0.1286352898\n449401    5.890953444     0.1894601932\n449501    7.960661939     0.6962985659\n449601    0.819391948    -0.5633475827\n449602    7.586149616     0.3704181436\n452601    5.259189284     0.1519824650\n453601    5.215943667     0.2028775023\n453602   -6.639855425     0.0715128917\n453801    1.401026207     0.4361648604\n453802   -3.932446234     0.0704679585\n453901    8.294177499     0.1939561831\n453902    8.905960641     0.4911907904\n456401    7.892697861     0.3904344132\n457002    7.202016686     0.2828656599\n458902   -6.697520088    -0.0984893092\n459201   10.202304152     0.0858383908\n461402   -3.800733252     0.2040626388\n463102   -6.647717083    -0.0319570534\n463702    6.152834543     0.4869901782\n466201   11.929377475     0.3045271551\n468301    9.663137348     0.5884585664\n468302   -9.269048154     0.0687506220\n468303    8.216306160     0.2924034204\n471701    3.763886469     0.5444009129\n471702    5.305866596     0.1619076001\n472303    7.547316581     0.5698673804\n474602    3.235586532    -0.0028905854\n474603    5.262948752     0.0676086593\n474604    0.082404546     0.7703311029\n475401   17.901219942     0.5505570370\n476601    6.135158039     0.0860347065\n478501   -0.702645690     0.3558601308\n480103    9.785520520    -0.0023276926\n481502    7.192875249     0.0988762983\n482002    9.548194719     0.1913739120\n482602    9.475046751     0.7083578232\n487101   -2.225263156    -0.3656467799\n488001   12.247447681     0.3844159426\n488201    8.102629732     0.7783455025\n488202    5.987223071     0.2584736803\n489902    0.230219542    -0.1772300862\n492501    7.337092394     0.6755953993\n492502    4.671513267     0.1494787322\n492503    0.221469013    -0.0781491851\n493003   -5.478602850     0.1654778966\n493301    2.140099569    -0.3255400185\n495202    5.031530542     0.2393239489\n497302   -1.356430489     0.6230662894\n497303    1.443114768     0.0975309585\n497304    4.673049533     0.1651969361\n497403    2.638405610     0.2507020738\n499703   -0.440142997    -0.0209190836\n502802    6.577898855     0.8092018798\n503201   11.634706733     0.4083105680\n504101   13.921881377     0.0752724468\n506601   -6.932435031    -0.1507198724\n506602   -2.347027061    -0.1472239760\n509801   -1.730501392    -0.0825522054\n509802   -0.090075249     0.0360765068\n510002    0.205538243     0.9542280980\n510301    0.702579414    -0.4746678459\n510401    2.056260506     0.2848057954\n511901    1.453417905     0.0973429495\n513404   -6.042612306     0.5389918814\n513405   -3.646392133    -0.5580801923\n515102    9.722847709     0.2156090752\n516401    1.009858860     0.0426850477\n518003    9.302402137     0.3445363462\n519101  -20.257400756    -0.7379099829\n519503   -1.473934126    -0.3374921854\n521602   -4.153337712    -0.4494605391\n522401   -3.716144124    -0.1852121468\n523101   -3.507052647     0.0332933281\n523201   -4.472553649     0.1641359594\n523202    2.411681739    -0.0408955489\n524701    0.539203070    -0.1217671971\n524702    6.034277613    -0.3930035680\n525801   10.778043519     0.5620550084\n527204   -2.905376377    -0.1388765587\n531401    1.302471905     0.5699696024\n531402    6.962850983     0.1553970849\n531404    7.091999028     0.2485304739\n531704   -2.649919937    -0.2444868454\n532601    1.053870793    -0.0917910589\n532802   -0.974004431    -0.7839375568\n533002   -5.629425526     0.3264527147\n533003    5.636689708     0.4174516474\n534103   15.603846087     0.5641142512\n537002    4.800717507    -0.4796646686\n537302    1.656259892     0.5396232512\n537304   15.874065737     0.6042442216\n537602    3.326412952     0.4031131857\n538102    0.010031153     0.0388542070\n538703    1.271166802     0.8545769790\n538704    3.857883727    -0.0578289975\n542702   -1.605350599    -0.1450513194\n542703   -6.925617752    -0.1813053332\n542802   13.660919966     0.0865350801\n543601   -1.863309800    -0.5720061609\n543602  -10.895250243    -0.0513775254\n545402   -9.748793425    -0.2337083265\n546505   -9.465377543    -1.2643208291\n546702  -15.468168978    -0.4245138303\n547701    0.821040529     0.0787577217\n548501    0.890506609    -0.1850190548\n549801    2.185796044    -0.1904365254\n550901  -12.110261822    -0.5083420287\n551501   -9.458482079    -0.2274652381\n552203    5.366109394     0.0341539327\n553701   -9.306520650     0.3298493036\n553702   -2.677196414    -0.0662259959\n556101   -4.098853766    -0.6904491092\n556801   -1.123708759    -0.0070401992\n558301   -6.897870784    -0.5899296536\n559302   -6.228731802    -0.0210775598\n560902   -5.230972822    -0.4504349249\n561202    0.410588880    -0.2639003793\n561402   -1.027279529    -0.4305782115\n564001   -3.798078131    -0.8718677315\n564002  -12.087399682    -0.1383687684\n565601    9.165482401     0.4884945169\n567002    4.384027365     0.0373265194\n570601   -4.275409590    -0.2861293813\n571201   -4.512941408    -0.6460295376\n571801    1.832773693     0.3246793184\n571802    6.373805424     0.2765514830\n572402   -8.414508702    -0.0889523017\n572801    4.028924897     0.9429226251\n572802    1.320631615     0.0940227606\n572803    1.752194492     0.2131732656\n574003   -9.707119190    -0.3413922317\n574102   -0.155152926     0.1391133187\n574602    2.248962575    -0.0761357717\n574603    3.120128963     0.4340670920\n580202   13.022387194     0.0997430302\n581303   -6.120196772    -0.2233415625\n581802   -7.260506569    -0.0189295916\n586102    6.347244598     0.4276988765\n587301   -4.270522304    -0.3136476132\n587303   -3.704164917    -0.0663545187\n591903    8.016551729     0.2768465055\n594102    2.844198101     0.7059391803\n600203   -7.411005699    -0.2369221509\n602301   -0.541881945     0.0293960087\n602302   -1.364506766    -0.0564812144\n602304   -0.888339281     0.0198525430\n604004   -9.177265588    -0.4007321746\n604607   -5.803129595     0.1464837396\n604902   -5.714551254    -0.1910331391\n607201    1.408024569    -0.0024783694\n607601   -2.224009500    -0.6246790844\n607602  -12.974686863    -0.2209720964\n607802   -1.904649681    -0.0730417460\n610101  -10.765536149    -0.5739410895\n611802   -0.170691262     0.6648769176\n613201    5.669249506     0.1745448840\n613402   -9.326881293    -0.5537510022\n616105    4.261380940    -0.0972857910\n616402   -2.214467326    -0.3964480731\n617501  -18.639149690    -0.7186458035\n621501    1.835991838    -0.2542207736\n621701    8.286044132    -0.1704955040\n622001   -2.620652788    -0.3420891548\n623801   -1.461860958    -0.2594092231\n626201    8.298678423     0.0709192473\n627702   -3.294336634    -0.2528605997\n627703   -8.356383592     0.0167912871\n627802  -10.719655881    -0.3128250204\n629404    4.271949399    -0.2825751636\n629502    3.842041546     0.1691263674\n631801   -6.443346414    -0.2987834921\n632205  -12.355631470    -0.1639754635\n632702   -6.968964556    -0.0983124216\n632703   -4.205911096     0.0107936240\n632704   -5.201119015    -0.3205166214\n634401   -9.496305337    -0.3525666006\n634503   -0.300560452    -0.0830998575\n635302   -2.385332305    -0.4039133039\n636402  -17.864067138    -0.4747542127\n636802   -5.469860683    -0.3400287141\n637110   -8.582821719    -0.0688801171\n638402   -1.096630852    -0.4008948853\n640002  -12.467112891    -0.3845939644\n640402  -10.226452438    -0.2850022943\n642601  -15.027543402    -0.6283418399\n642701   -6.568055056    -0.0605927646\n642702   -7.786731063    -0.1934245975\n642901    1.365816287    -0.2461050023\n642902   -7.727724366     0.0552052437\n642903   -9.783845607    -0.1041269653\n642904   -3.902721179    -0.1359082868\n643402   -9.784829399    -0.6673450281\n644203   -2.893498650    -0.2359423115\n644901  -14.409620242    -0.7198468219\n648601   -3.285283895    -0.1909946975\n648602   -2.440359357    -0.1675653778\n651601  -10.164457337    -0.1656169561\n665803   -8.588165123    -0.2683286666\n668403   -2.155576156    -0.2480496971\n669301    3.140409865     0.1135270055\n671102    1.522198735     0.2626847368\n675701   -7.666821307    -0.1358802968\n677201   -2.786868394     0.3564281052\n677202   -5.112519532    -0.1836048939\n678804   -1.686968395    -0.1655281856\n681601    2.093995227     0.3277762462\n682502   -3.631163368    -0.0870500663\n682903    2.533028033    -0.2022871751\n684201   17.090879898     0.3183255812\n684203   10.550500892     0.6529026987\n687602    3.837137069    -0.1324482408\n689101    1.320276660    -0.0990678200\n690101   -2.777357589    -0.6658898264\n693001   -1.185516769    -0.3308131312\n696601   -5.488481284    -0.1918743288\n700002  -13.546584929    -0.1359268562\n700003  -16.826077607    -0.4179645171\n707701   -1.629690464     0.0634283004\n708401   16.095714969     0.3510674641\n711602  -10.364466117    -0.1311523796\n711603   -6.298026288    -0.0774363265\n712303    1.115210195    -0.7046623183\n714801   -4.701190798    -0.2928539317\n714802    7.156025117    -0.2743301594\n715601    1.071023211     0.6346586556\n715803  -14.869698337    -0.5096580848\n716601   14.055407036     0.6226336053\n716602    8.101777373     0.2717597314\n717002   -6.514717529    -0.2164293965\n717003   -7.880996157    -0.5365497268\n717901  -11.300617102    -0.5463604768\n717902   -5.808792038    -0.1781694346\n717903   -7.038698377     0.0096350581\n718602    0.930533655     0.7131227273\n722401   -0.985346156    -0.3272963500\n722803    3.508472031     0.0884016598\n723501    0.121789963     0.0956484976\n725801   -1.347171220     0.5068905555\n725903  -17.715805194    -1.0400137961\n737702   -5.364316034     0.0132311916\n738201   -1.546327158    -0.2326255486\n739002   -9.015182191    -0.5451153768\n739102   -5.064695317    -0.0888557675\n739301   -5.479133915    -0.2615794135\n739401    3.548340320     0.1235943748\n739601    5.911842256    -0.1183481266\n742301    0.224482160    -0.0377872368\n743601   -1.501981264    -0.1611641223\n743602   -5.764737947    -0.9465123547\n743801   -5.281092320     0.0737646699\n743802  -11.837457672    -0.5188800428\n744102  -19.056782948    -0.0882474409\n744103   -6.113028565    -0.2690950378\n744703   -4.318480494    -0.0407585018\n744704   -8.446451812     0.0303253095\n745103    5.127391691     0.1637150344\n745904   -0.132089799    -0.0043144746\n748003   -0.888274710    -0.2183155180\n748502    6.179130229     0.0174522426\n749802    8.993189664     0.1935373561\n749803   -1.527664909    -0.3678291583\n750104  -12.204430161    -0.3060585495\n750404    1.798556671    -0.5623006390\n751001   -7.194623280    -0.2135921704\n752003    1.980723949    -0.5464093787\n752501   -2.291324472    -0.1768991458\n760102    0.002779244    -0.2678997226\n763603   -9.515067832    -0.2267766664\n764503   -3.529410857    -0.1965419102\n765702    1.477915925    -0.1405890196\n767901    3.194300097    -0.0668313731\n771002   -9.628492583    -0.0331301722\n775002    7.459231867     0.2618756402\n778902  -13.159806097    -0.3228931091\n778903   -1.398100657    -0.7319082250\n783001   -8.902087517    -0.2744852246\n783002   -1.867724426    -0.1984787514\n783301    4.229424280     0.4597352393\n783502    1.293732538     0.1286967835\n783602   -1.511633114    -0.1079115367\n783801    1.466023472     0.2565860748\n785601    3.812960011     0.0865059612\n786402    5.189301366     0.1904138171\n788302  -11.728646898    -0.4109713490\n788303  -11.703157030     0.1904551466\n792103   -1.717768617     0.2765264126\n792704   -4.263432014    -0.1608600887\n793001    2.631618708     0.5444128410\n794301  -10.885986014    -0.4341363188\n794503   -8.046908048    -0.1400713658\n795201   -4.778623885    -0.2761614132\n795901    5.312868665    -0.0366368565\n799803   14.440612041     0.3134571096\n800602    1.575169789    -0.0671218693\n804701   -1.182292795     0.1527017788\n804702    0.188818697    -0.3088758177\n809102   -5.605012786    -0.6008609290\n809103   -9.565248243    -0.6846855454\n809301   -6.198420982    -0.7978763861\n810303    0.377371540     0.0057558757\n811002   -8.319520622    -0.2138774760\n812504    0.993614255     0.0264062237\n814101    0.131310877    -0.1119582959\n817402   -3.379127920    -0.3726538867\n817403   -4.085829565    -0.7050319754\n817404   -4.755705815    -0.5346998879\n822602    7.917443103     0.1327733745\n825702   -9.970917659    -0.1367166687\n825902   -8.435519858    -0.1686452329\n825903   -1.699014338    -0.5378369475\n826503    2.977856523    -0.1537479884\n826504  -20.169375616    -0.8092409240\n826904  -17.714957998    -0.7663456674\n826905   -8.867298417    -0.2094385227\n827101    2.911580798     0.0743696646\n827302   -6.515098274    -0.0472490379\n828302    2.758256821     0.2710918047\n828604   -0.387730778    -0.3245729275\n828902   -5.870203967    -0.4487020126\n828903   -5.540272045    -0.2053766389\n828905  -13.835763582    -0.3159407142\n829401   -8.807481226    -0.3429406667\n829403    1.481529252     0.2490552553\n833601   -0.624297944    -0.2652069381\n834301   -0.220777081    -0.0289871697\n835202    1.659102580    -0.2632755479\n835703   -3.328187213     0.4162598998\n837503   -6.753085789    -0.2036923842\n837504  -10.706741673    -0.0458817913\n837602  -14.410977124    -0.3320380379\n838301   -7.498085384    -0.4155690654\n838903    4.324118577     0.3380756621\n841601    0.380851813     0.2425089126\n841602    0.841025013    -0.0739130759\n846301   12.045171447     0.3091647315\n847301   -4.297862641    -0.5546112145\n847302    3.482212270     0.0265452839\n847701    7.917419013     0.3513418683\n847901   -0.460907634     0.0713428669\n847902   -7.658645851    -0.1069721236\n848201    4.801092377     0.6521619294\n849401    0.832776648    -0.0621926478\n853202   -1.129530150     0.3017979514\n856103   -2.805522684    -0.0979480708\n857003   -8.202760994    -0.4038646396\n858302   -2.788553034    -0.5891048005\n861503   -3.786890937     0.2863354777\n861803   -2.975009858     0.2570636647\n862201    4.450524308     0.0632242098\n863101  -13.753601142    -0.3306235274\n863403   -5.858524796    -0.4329759096\n864302    1.683787838     0.0675621624\n866203   -9.608254207     0.0391321674\n866205   -4.337558707    -0.0313018735\n866402   -3.581884688    -0.0063703398\n867903   12.840335929     0.3233423898\n869001   -7.891305491    -0.1876723061\n869501    0.086653086     0.3941996230\n869601  -11.564066942    -0.3804197551\n870803    9.307156689     0.4423498499\n878701  -18.564325364    -0.5304544041\n879403   12.691982843     0.3210558438\n879404    3.242777255    -0.0845540177\n885801    3.311057771     0.2319786141\n885802  -16.768413564    -0.8683749095\n886401   11.907542234     0.7056690429\n891202   -1.081121395     0.1152864260\n892601   -7.622453080    -0.1032087784\n894201   -5.013783151    -0.2009735418\n894802   -0.053149059    -0.0547022339\n894803   -6.490546445    -0.2994334585\n894804   -4.621840870    -0.1764300223\n897001   -3.365401619    -0.1629674155\n905003   -7.445714333    -0.3640961880\n907001   -0.374151219    -0.1495011196\n908102    0.219617324    -0.0345188263\n910103  -11.167480720    -0.2984328178\n911701    2.073122697     0.1071727942\n914402    4.370790200    -0.0459269353\n918002    1.007393620    -0.0978816035\n918201    1.660847815     0.0600406049\n918202    3.268499663    -0.0317340583\n918301    0.035351706     0.5101045390\n922901    4.361980088    -0.1613853887\n925503   -3.438776695     0.0453044433\n925504   -9.011543740    -0.3818310966\n925701   -2.371167896    -0.1043786247\n926102  -12.912275437    -0.3037111025\n928101   -3.267917120    -0.1876491634\n929805    1.834634487     0.0030545361\n931202    6.403164042     0.9217389556\n931703   -5.354493946     0.2360415478\n938001   -3.994223109     0.0204975093\n938002   -4.614140459    -0.3244595553\n938205    1.567384264     0.0714191192\n940103   -2.876967919    -0.1367363170\n951602   -8.075174085    -0.6479917899\n952201   -3.945860026    -0.1560927886\n964101    3.984661661     0.0219674976\n964102    8.360407930    -0.0953735583\n966001    9.174016794     0.3432238340\n968001   -0.629836663    -0.7223522955\n968002   -4.532019378     0.1264415938\n968003   -8.173387557     0.0922628561\n968401    3.224844494     0.2381162652\n968402    7.853238203     0.6730011095\n976803   -9.046486242    -0.6135737022\n982001   -5.313405819    -0.3165368330\n983703    4.116384781     0.0643450735\n983902    3.083564815     0.0115067859\n983903   -3.849424068    -0.1295673009\n984402   -1.843563677    -0.0852790816\n984404    0.142760805    -0.2835791644\n985802   -3.097528676    -0.2788940500\n985803    0.646538579    -0.1911647301\n986203   -2.560814112    -0.7966300263\n986505    0.638431884    -0.6053352476\n986506   -5.014895845    -1.0319359419\n987401   -6.344335597     0.2169455056\n987701   -3.506675824    -0.6063062528\n988701   -9.319877074    -0.3575980333\n989201   -5.305093824    -0.3802291161\n989503   -3.932088153    -0.2082724988\n992201    4.266447975    -0.0669857425\n993803   -0.530178376     0.1345058204\n995101  -17.804533585    -0.4776618045\n995102  -17.473883352    -0.6232233901\n995304    1.053291143     0.0243375727\n997002   -8.979013260    -0.0885973261\n998001    2.729012021    -0.2800570407\n999202   -3.670524953    -0.3511758409\n999203   -2.979903009    -0.5338115853\n1002601 -13.034835722    -0.4012982324\n1003202  -5.606812318    -0.0196977607\n1003203  -7.477668046    -0.3066633100\n1003204  -7.844700539    -0.1540920537\n1003601   3.930180280     0.0011913967\n1003802   6.540122419     0.5659007953\n1004802  -5.570807619    -0.0413438230\n1004903  -5.248322978     0.4111272596\n1005201  -5.984591162    -0.1987134049\n1005203   7.565038265     0.2057572959\n1007201   1.510667986    -0.1147921220\n1007301 -12.468870981    -0.0066654272\n1007302 -12.326574049     0.0022638649\n1007901  -2.264068422     0.1464422191\n1013201  -7.986005280    -0.2003928810\n1017504 -15.664769939     0.1460669934\n1018402  -8.067842105    -0.2363769816\n1018404  -6.508505979    -0.2698877112\n1019104  -4.099081306    -0.3246502915\n1019105 -11.252231849    -0.3048711496\n1023702   0.234904992     0.0414158079\n1030801   9.922354931     0.2550829939\n1031001   8.177431667     0.9571389929\n1031002   2.887020965     0.5672244473\n1031302  -7.580679673     0.0151221031\n1033202 -14.810454289    -0.1392893536\n1033601  -6.830059011    -0.1810000462\n1034001  -6.384315963    -0.4931655541\n1036001  -2.463853750     0.7652192225\n1036003  -2.232715330    -0.1682196086\n1037502  -1.522566381    -0.1243245413\n1040101   3.882500958     0.4223124057\n1042101  -2.389200357    -0.2881820739\n1042201  -7.337963729    -0.2381278056\n1044801   5.063737729     0.1565980098\n1047801  -2.760234293    -0.2101134212\n1050001  -2.834390560    -0.0925525719\n1050701  -5.695588916     0.6616487481\n1053201  -0.599389674    -0.1378580936\n1053202  -7.116932507    -0.3488730245\n1053802  -7.098840842     0.1161428377\n1053804  -7.518495890    -0.2127588435\n1056501  -4.662066297    -0.0688020154\n1081101  -6.509487554    -0.2433586628\n1081103  -4.535700136    -0.0489579883\n1176403  -2.478665580    -0.3619317394\n1176502  -5.361130357     0.1014948455\n1179201   6.169266543    -0.2715601330\n1179202   2.828003388     0.0468697930\n1180202   1.281663901    -0.5472646124\n1181201 -12.764929271    -0.5961826909\n1181902  -3.179238219    -0.1577544718\n1181904  -5.521812552    -0.1832687129\n1182604   0.677485736    -0.3209083737\n1185003 -12.971458846    -0.0415619496\n1187001   3.494246653    -0.3201390580\n1187303   6.157122681     0.3331643729\n1189901   9.250401396     0.3775423118\n1190501  -2.887202250    -0.1068928078\n1191001   2.654860996     0.2646823447\n1191901   4.496785221     0.1631820463\n1194901  -2.945928120     1.0021463443\n1197902  -6.624966665    -0.3709318601\n1198101  -1.383949304    -0.2114699808\n1198901  16.464380969     0.0561320208\n1201701  -1.448990580    -0.2336679925\n1201702  -0.096107319    -0.5865903882\n1203201  -1.895771421    -0.4878402424\n1205201  -5.525566536     0.2163459801\n1207601   4.719495051     0.1382756094\n1209201  -0.349522573    -0.1862900381\n1211502   9.264718431     0.1745673561\n1211503   5.881429621     0.1257080878\n1212303  -2.959730778     0.0557280182\n1213501  -4.357858111     1.0978055000\n1217202  -1.786493616    -0.5895225445\n1217204  -7.916052388    -0.0739003214\n1217702  -2.632215582    -0.0357903367\n1217704  -2.184052557    -0.1210937439\n1219103  -7.783664844    -0.1154947959\n1219105 -14.003540434    -0.4179680812\n1219106 -14.952823473    -0.3505976607\n1219108   1.648006949     0.3247083801\n1221003   0.152689839    -0.1226131532\n1221702  -4.589571607    -0.2141049895\n1224001   3.179239571    -0.2058228146\n1225402  -4.958955736    -0.1677144514\n1228103  -0.253089392    -0.0056286513\n1230302  -9.114014727    -0.6678043203\n1256601  -7.998906403    -0.1309251668\n\nwith conditional variances for \"id\"",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#lmer",
    "href": "hlm.html#lmer",
    "title": "Hierarchical Linear Modeling",
    "section": "7.1 lmer",
    "text": "7.1 lmer\n\n\nCode\ngeneralizedLinearMixedModel &lt;- glmer(\n  outcome ~ female + ageYearsCentered + (ageYearsCentered | id),\n  family = poisson(link = \"log\"),\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(generalizedLinearMixedModel)\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: outcome ~ female + ageYearsCentered + (ageYearsCentered | id)\n   Data: mydata\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   9329.7    9363.9   -4658.8    9317.7      2215 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.0331 -0.5546 -0.0205  0.5350  5.0222 \n\nRandom effects:\n Groups Name             Variance  Std.Dev. Corr \n id     (Intercept)      0.0058309 0.07636       \n        ageYearsCentered 0.0002845 0.01687  -1.00\nNumber of obs: 2221, groups:  id, 932\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.343986   0.026879  50.001   &lt;2e-16 ***\nfemale           0.007439   0.021268   0.350   0.7265    \nageYearsCentered 0.010279   0.005804   1.771   0.0766 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) female\nfemale      -0.421       \nageYrsCntrd -0.831  0.036\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#mass",
    "href": "hlm.html#mass",
    "title": "Hierarchical Linear Modeling",
    "section": "7.2 MASS",
    "text": "7.2 MASS\n\n\nCode\nglmmPQLmodel &lt;- glmmPQL(\n  outcome ~ female + ageYearsCentered,\n  random = ~ 1 + ageYearsCentered|id,\n  family = poisson(link = \"log\"),\n  data = mydata)\n\nsummary(glmmPQLmodel)\n\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: mydata \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 + ageYearsCentered | id\n Structure: General positive-definite, Log-Cholesky parametrization\n                 StdDev       Corr  \n(Intercept)      5.533290e-05 (Intr)\nageYearsCentered 9.320922e-08 0     \nResidual         1.014184e+00       \n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  outcome ~ female + ageYearsCentered \n                     Value   Std.Error   DF  t-value p-value\n(Intercept)      1.3453017 0.027100741 1288 49.64077  0.0000\nfemale           0.0074130 0.021543030  930  0.34410  0.7308\nageYearsCentered 0.0100806 0.005850753 1288  1.72296  0.0851\n Correlation: \n                 (Intr) female\nfemale           -0.423       \nageYearsCentered -0.829  0.036\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.00970268 -0.55110605 -0.02158036  0.53179575  4.98914420 \n\nNumber of Observations: 2221\nNumber of Groups: 932",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#mcmcglmm",
    "href": "hlm.html#mcmcglmm",
    "title": "Hierarchical Linear Modeling",
    "section": "7.3 MCMCglmm",
    "text": "7.3 MCMCglmm\n\n\nCode\nMCMCglmmModel &lt;- MCMCglmm(\n  outcome ~ female + ageYearsCentered,\n  random = ~ us(ageYearsCentered):id,\n  family = \"poisson\",\n  data = na.omit(mydata[,c(\"id\",\"outcome\",\"female\",\"ageYearsCentered\")]))\n\n\n\n                       MCMC iteration = 0\n\n Acceptance ratio for liability set 1 = 0.000410\n\n                       MCMC iteration = 1000\n\n Acceptance ratio for liability set 1 = 0.439819\n\n                       MCMC iteration = 2000\n\n Acceptance ratio for liability set 1 = 0.439977\n\n                       MCMC iteration = 3000\n\n Acceptance ratio for liability set 1 = 0.444608\n\n                       MCMC iteration = 4000\n\n Acceptance ratio for liability set 1 = 0.496749\n\n                       MCMC iteration = 5000\n\n Acceptance ratio for liability set 1 = 0.490778\n\n                       MCMC iteration = 6000\n\n Acceptance ratio for liability set 1 = 0.512362\n\n                       MCMC iteration = 7000\n\n Acceptance ratio for liability set 1 = 0.428231\n\n                       MCMC iteration = 8000\n\n Acceptance ratio for liability set 1 = 0.412336\n\n                       MCMC iteration = 9000\n\n Acceptance ratio for liability set 1 = 0.471726\n\n                       MCMC iteration = 10000\n\n Acceptance ratio for liability set 1 = 0.428491\n\n                       MCMC iteration = 11000\n\n Acceptance ratio for liability set 1 = 0.400078\n\n                       MCMC iteration = 12000\n\n Acceptance ratio for liability set 1 = 0.346435\n\n                       MCMC iteration = 13000\n\n Acceptance ratio for liability set 1 = 0.276170\n\n\nCode\nsummary(MCMCglmmModel)\n\n\n\n Iterations = 3001:12991\n Thinning interval  = 10\n Sample size  = 1000 \n\n DIC: 9323.524 \n\n G-structure:  ~us(ageYearsCentered):id\n\n                                     post.mean  l-95% CI  u-95% CI eff.samp\nageYearsCentered:ageYearsCentered.id 6.754e-06 1.087e-08 4.053e-05    7.788\n\n R-structure:  ~units\n\n      post.mean l-95% CI u-95% CI eff.samp\nunits   0.00877 0.001634  0.01728    7.797\n\n Location effects: outcome ~ female + ageYearsCentered \n\n                 post.mean  l-95% CI  u-95% CI eff.samp  pMCMC    \n(Intercept)       1.338707  1.281096  1.387614    55.58 &lt;0.001 ***\nfemale            0.007688 -0.031193  0.052271    58.65  0.706    \nageYearsCentered  0.010561  0.000813  0.021420    57.32  0.046 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#resources",
    "href": "hlm.html#resources",
    "title": "Hierarchical Linear Modeling",
    "section": "11.1 Resources",
    "text": "11.1 Resources\nPinheiro and Bates (2000) book (p. 174, section 4.3.1)\nhttps://stats.stackexchange.com/questions/77891/checking-assumptions-lmer-lme-mixed-models-in-r (archived at https://perma.cc/J5GC-PCUT)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#qq-plots",
    "href": "hlm.html#qq-plots",
    "title": "Hierarchical Linear Modeling",
    "section": "11.2 QQ Plots",
    "text": "11.2 QQ Plots\nMake QQ plots for each level of the random effects. Vary the level from 0, 1, to 2 so that you can check the between- and within-subject residuals.\n\n\nCode\nqqnorm(linearMixedModel_nlme,\n       ~ ranef(., level = 1))",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#pp-plots",
    "href": "hlm.html#pp-plots",
    "title": "Hierarchical Linear Modeling",
    "section": "11.3 PP Plots",
    "text": "11.3 PP Plots\n\n\nCode\nppPlot(linearMixedModel)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#qq-plot-of-residuals",
    "href": "hlm.html#qq-plot-of-residuals",
    "title": "Hierarchical Linear Modeling",
    "section": "11.4 QQ Plot of residuals",
    "text": "11.4 QQ Plot of residuals\n\n\nCode\nqqnorm(resid(linearMixedModel))\nqqline(resid(linearMixedModel))",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#plot-residuals",
    "href": "hlm.html#plot-residuals",
    "title": "Hierarchical Linear Modeling",
    "section": "11.5 Plot residuals",
    "text": "11.5 Plot residuals\n\n\nCode\nplot(linearMixedModel)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#plot-residuals-by-group-in-the-example-below-level-2-represents-the-individual",
    "href": "hlm.html#plot-residuals-by-group-in-the-example-below-level-2-represents-the-individual",
    "title": "Hierarchical Linear Modeling",
    "section": "11.6 Plot residuals by group (in the example below, level 2 represents the individual)",
    "text": "11.6 Plot residuals by group (in the example below, level 2 represents the individual)\n\n\nCode\nplot(linearMixedModel,\n     as.factor(id) ~ resid(.),\n     abline = 0,\n     xlab = \"Residuals\")",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#plot-residuals-by-levels-of-a-predictor",
    "href": "hlm.html#plot-residuals-by-levels-of-a-predictor",
    "title": "Hierarchical Linear Modeling",
    "section": "11.7 Plot residuals by levels of a predictor",
    "text": "11.7 Plot residuals by levels of a predictor\n\n\nCode\nplot(linearMixedModel_nlme,\n     resid(., type = \"p\") ~ fitted(.) | female) #type = \"p\" specifies standardized residuals",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#can-model-heteroscedasticity-of-the-within-group-error-with-the-weights-argument",
    "href": "hlm.html#can-model-heteroscedasticity-of-the-within-group-error-with-the-weights-argument",
    "title": "Hierarchical Linear Modeling",
    "section": "11.8 Can model heteroscedasticity of the within-group error with the weights argument",
    "text": "11.8 Can model heteroscedasticity of the within-group error with the weights argument\n\n\nCode\nlinearMixedModel_nlmeVarStructure &lt;- lme(\n  math ~ female + ageYearsCentered,\n  random = ~ 1 + ageYearsCentered|id,\n  weights = varIdent(form = ~ 1 | female),\n  method = \"ML\",\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(linearMixedModel_nlmeVarStructure)\n\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: mydata \n       AIC      BIC    logLik\n  15857.83 15903.48 -7920.915\n\nRandom effects:\n Formula: ~1 + ageYearsCentered | id\n Structure: General positive-definite, Log-Cholesky parametrization\n                 StdDev    Corr  \n(Intercept)      7.9177716 (Intr)\nageYearsCentered 0.8278343 0.076 \nResidual         5.6410162       \n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | female \n Parameter estimates:\n       1        0 \n1.000000 1.009161 \nFixed effects:  math ~ female + ageYearsCentered \n                     Value Std.Error   DF  t-value p-value\n(Intercept)      30.554856 0.5040373 1288 60.62022  0.0000\nfemale           -0.692653 0.6172485  930 -1.12216  0.2621\nageYearsCentered  4.255258 0.0805531 1288 52.82553  0.0000\n Correlation: \n                 (Intr) female\nfemale           -0.614       \nageYearsCentered -0.507  0.014\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.37982974 -0.51663213  0.00445497  0.52228733  2.63205084 \n\nNumber of Observations: 2221\nNumber of Groups: 932",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#plot-observed-and-fitted-values",
    "href": "hlm.html#plot-observed-and-fitted-values",
    "title": "Hierarchical Linear Modeling",
    "section": "11.9 Plot observed and fitted values",
    "text": "11.9 Plot observed and fitted values\n\n\nCode\nplot(linearMixedModel,\n     math ~ fitted(.))",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#plot-qq-plot-of-residuals-by-levels-of-a-predictor",
    "href": "hlm.html#plot-qq-plot-of-residuals-by-levels-of-a-predictor",
    "title": "Hierarchical Linear Modeling",
    "section": "11.10 Plot QQ plot of residuals by levels of a predictor",
    "text": "11.10 Plot QQ plot of residuals by levels of a predictor\n\n\nCode\nqqnorm(linearMixedModel_nlme, ~ resid(.) | female)\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(linearMixedModel_nlme, ~ resid(.))",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#qq-plot-of-random-effects",
    "href": "hlm.html#qq-plot-of-random-effects",
    "title": "Hierarchical Linear Modeling",
    "section": "11.11 QQ plot of random effects",
    "text": "11.11 QQ plot of random effects\nMake QQ plots for each level of the random effects. Vary the level from 0, 1, to 2 so that you can check the between- and within-subject residuals.\n\n\nCode\nqqnorm(linearMixedModel_nlme,\n       ~ ranef(., level = 0))\n\n\nError in `effects[[1L]]`:\n! subscript out of bounds\n\n\nCode\nqqnorm(linearMixedModel_nlme,\n       ~ ranef(., level = 1))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(linearMixedModel_nlme,\n       ~ ranef(., level = 2))\n\n\nError:\n! object '.y' not found",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#qq-plot-of-random-effects-by-levels-of-a-predictor",
    "href": "hlm.html#qq-plot-of-random-effects-by-levels-of-a-predictor",
    "title": "Hierarchical Linear Modeling",
    "section": "11.12 QQ plot of random effects by levels of a predictor",
    "text": "11.12 QQ plot of random effects by levels of a predictor\n\n\nCode\nqqnorm(linearMixedModel_nlme, \n       ~ ranef(., level = 1) | female)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#pairs-plot",
    "href": "hlm.html#pairs-plot",
    "title": "Hierarchical Linear Modeling",
    "section": "11.13 Pairs plot",
    "text": "11.13 Pairs plot\n\n\nCode\npairs(linearMixedModel_nlme)\n\n\n\n\n\n\n\n\n\nCode\npairs(linearMixedModel_nlme,\n      ~ ranef(., level = 1) | female)",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#variance-functions-for-modeling-heteroscedasticity",
    "href": "hlm.html#variance-functions-for-modeling-heteroscedasticity",
    "title": "Hierarchical Linear Modeling",
    "section": "11.14 Variance functions for modeling heteroscedasticity",
    "text": "11.14 Variance functions for modeling heteroscedasticity\n\nvarFixed: fixed variance\nvarIdent: different variances per stratum\nvarPower: power of covariate\nvarExp: exponential of covariate\nvarConstPower: constant plus power of covariate\nvarComb: combination of variance functions",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "hlm.html#correlation-structures-for-modeling-dependence",
    "href": "hlm.html#correlation-structures-for-modeling-dependence",
    "title": "Hierarchical Linear Modeling",
    "section": "11.15 Correlation structures for modeling dependence",
    "text": "11.15 Correlation structures for modeling dependence\n\ncorCompSymm: compound symmetry\ncorSymm: general\ncorAR1: autoregressive of order 1\ncorCAR1: continuous-time AR(1)\ncorARMA: autoregressive-moving average\ncorExp: exponential\ncorGaus: Gaussian\ncorLin: linear\ncorRatio: rational quadratic\ncorSpher: spherical",
    "crumbs": [
      "About",
      "Hierarchical Linear Modeling"
    ]
  },
  {
    "objectID": "sem.html",
    "href": "sem.html",
    "title": "Structural Equation Modeling",
    "section": "",
    "text": "For a resource on using lavaan in R for structural equation modeling, see the following e-book: https://tdjorgensen.github.io/SEM-in-Ed-compendium\n\n\n\n\nCode\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n\n\n\n\n\n\n\nCode\nlibrary(\"lavaan\")\nlibrary(\"semTools\")\nlibrary(\"semPlot\")\nlibrary(\"lavaanPlot\")\nlibrary(\"lavaangui\")\nlibrary(\"lcsm\")\nlibrary(\"MBESS\")\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\nCode\noptions(scipen = 999)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#install-libraries",
    "href": "sem.html#install-libraries",
    "title": "Structural Equation Modeling",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#load-libraries",
    "href": "sem.html#load-libraries",
    "title": "Structural Equation Modeling",
    "section": "",
    "text": "Code\nlibrary(\"lavaan\")\nlibrary(\"semTools\")\nlibrary(\"semPlot\")\nlibrary(\"lavaanPlot\")\nlibrary(\"lavaangui\")\nlibrary(\"lcsm\")\nlibrary(\"MBESS\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#options",
    "href": "sem.html#options",
    "title": "Structural Equation Modeling",
    "section": "",
    "text": "Code\noptions(scipen = 999)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#linearLGCM",
    "href": "sem.html#linearLGCM",
    "title": "Structural Equation Modeling",
    "section": "7.1 Linear Growth Curve Model",
    "text": "7.1 Linear Growth Curve Model\n\n7.1.1 Model Syntax\n\n7.1.1.1 Abbreviated\n\n\nCode\nlgcm1_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n'\n\n\n\n\n7.1.1.2 Full\n\n\nCode\nlgcm2_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n  \n  # Constrain observed intercepts to zero\n  t1 ~ 0\n  t2 ~ 0\n  t3 ~ 0\n  t4 ~ 0\n  \n  # Estimate mean of intercept and slope\n  intercept ~ 1\n  slope ~ 1\n'\n\n\n\n\n\n7.1.2 Fit the Model\n\n7.1.2.1 Abbreviated\n\n\nCode\nlgcm1_fit &lt;- growth(\n  lgcm1_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  int.ov.free = FALSE,\n  int.lv.free = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n7.1.2.2 Full\n\n\nCode\nlgcm2_fit &lt;- sem(\n  lgcm2_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n\n7.1.3 Summary Output\n\n7.1.3.1 Abbreviated\n\n\nCode\nsummary(\n  lgcm1_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 32 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                40.774      40.982\n  Degrees of freedom                                27          27\n  P-value (Chi-square)                           0.043       0.041\n  Scaling correction factor                                  0.995\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994       0.994\n  Tucker-Lewis Index (TLI)                       0.993       0.993\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.992\n  Robust Tucker-Lewis Index (TLI)                            0.992\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5782.507   -5782.507\n  Scaling correction factor                                  0.991\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11641.014   11641.014\n  Bayesian (BIC)                             11792.690   11792.690\n  Sample-size adjusted Bayesian (SABIC)      11672.114   11672.114\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.036       0.036\n  90 Percent confidence interval - lower         0.006       0.007\n  90 Percent confidence interval - upper         0.057       0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.854       0.849\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.019\n  90 Percent confidence interval - upper                     0.059\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.793\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030       0.030\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.386    0.875\n    t2                1.000                               1.386    0.660\n    t3                1.000                               1.386    0.507\n    t4                1.000                               1.386    0.411\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.769    0.366\n    t3                2.000                               1.539    0.562\n    t4                3.000                               2.308    0.685\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.608    0.059   10.275    0.000    0.439    0.453\n    x2                0.604    0.062    9.776    0.000    0.436    0.423\n  slope ~                                                               \n    x1                0.262    0.029    8.968    0.000    0.341    0.352\n    x2                0.522    0.032   16.302    0.000    0.678    0.658\n  t1 ~                                                                  \n    c1                0.143    0.045    3.198    0.001    0.143    0.089\n  t2 ~                                                                  \n    c2                0.289    0.047    6.215    0.000    0.289    0.131\n  t3 ~                                                                  \n    c3                0.328    0.047    7.011    0.000    0.328    0.112\n  t4 ~                                                                  \n    c4                0.330    0.057    5.814    0.000    0.330    0.090\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .slope             0.075    0.040    1.890    0.059    0.152    0.152\n  x1 ~~                                                                 \n    x2                0.141    0.050    2.798    0.005    0.141    0.140\n    c1               -0.039    0.051   -0.762    0.446   -0.039   -0.038\n    c2                0.023    0.048    0.493    0.622    0.023    0.024\n    c3                0.027    0.050    0.544    0.586    0.027    0.028\n    c4               -0.023    0.045   -0.519    0.604   -0.023   -0.024\n  x2 ~~                                                                 \n    c1               -0.018    0.050   -0.358    0.721   -0.018   -0.019\n    c2               -0.003    0.044   -0.075    0.940   -0.003   -0.004\n    c3                0.155    0.048    3.239    0.001    0.155    0.170\n    c4               -0.104    0.043   -2.421    0.015   -0.104   -0.116\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.793    0.073    0.080    0.086\n    c3               -0.030    0.050   -0.585    0.559   -0.030   -0.032\n    c4                0.127    0.048    2.668    0.008    0.127    0.140\n  c2 ~~                                                                 \n    c3                0.003    0.041    0.078    0.938    0.003    0.004\n    c4                0.031    0.044    0.715    0.475    0.031    0.036\n  c3 ~~                                                                 \n    c4                0.034    0.044    0.767    0.443    0.034    0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .intercept         0.580    0.061    9.501    0.000    0.419    0.419\n   .slope             0.958    0.030   32.177    0.000    1.244    1.244\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.580    0.091    6.386    0.000    0.580    0.231\n   .t2                0.596    0.056   10.627    0.000    0.596    0.135\n   .t3                0.481    0.051    9.434    0.000    0.481    0.064\n   .t4                0.535    0.094    5.709    0.000    0.535    0.047\n   .intercept         1.079    0.108    9.996    0.000    0.562    0.562\n   .slope             0.224    0.027    8.373    0.000    0.378    0.378\n    x1                1.064    0.068   15.614    0.000    1.064    1.000\n    x2                0.943    0.065   14.401    0.000    0.943    1.000\n    c1                0.972    0.064   15.306    0.000    0.972    1.000\n    c2                0.900    0.063   14.372    0.000    0.900    1.000\n    c3                0.876    0.067   13.041    0.000    0.876    1.000\n    c4                0.852    0.057   15.005    0.000    0.852    1.000\n\nR-Square:\n                   Estimate\n    t1                0.769\n    t2                0.865\n    t3                0.936\n    t4                0.953\n    intercept         0.438\n    slope             0.622\n\n\n\n\n7.1.3.2 Full\n\n\nCode\nsummary(\n  lgcm2_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 31 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                26.059      26.344\n  Degrees of freedom                                21          21\n  P-value (Chi-square)                           0.204       0.194\n  Scaling correction factor                                  0.989\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998       0.998\n  Tucker-Lewis Index (TLI)                       0.997       0.997\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.998\n  Robust Tucker-Lewis Index (TLI)                            0.997\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5775.149   -5775.149\n  Scaling correction factor                                  0.994\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11638.299   11638.299\n  Bayesian (BIC)                             11813.923   11813.923\n  Sample-size adjusted Bayesian (SABIC)      11674.308   11674.308\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.025       0.025\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.051       0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.938       0.933\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.024\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.051\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.940\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.014       0.014\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.386    0.875\n    t2                1.000                               1.386    0.660\n    t3                1.000                               1.386    0.507\n    t4                1.000                               1.386    0.412\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.768    0.366\n    t3                2.000                               1.536    0.562\n    t4                3.000                               2.304    0.684\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.608    0.059   10.275    0.000    0.439    0.451\n    x2                0.604    0.062    9.776    0.000    0.436    0.419\n  slope ~                                                               \n    x1                0.262    0.029    8.968    0.000    0.341    0.351\n    x2                0.522    0.032   16.301    0.000    0.679    0.653\n  t1 ~                                                                  \n    c1                0.143    0.045    3.198    0.001    0.143    0.089\n  t2 ~                                                                  \n    c2                0.289    0.047    6.215    0.000    0.289    0.131\n  t3 ~                                                                  \n    c3                0.328    0.047    7.011    0.000    0.328    0.112\n  t4 ~                                                                  \n    c4                0.330    0.057    5.814    0.000    0.330    0.091\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .slope             0.075    0.040    1.890    0.059    0.152    0.152\n  x1 ~~                                                                 \n    x2                0.153    0.049    3.129    0.002    0.153    0.155\n    c1               -0.038    0.050   -0.760    0.447   -0.038   -0.037\n    c2                0.026    0.048    0.547    0.585    0.026    0.027\n    c3                0.033    0.049    0.674    0.501    0.033    0.035\n    c4               -0.025    0.044   -0.560    0.575   -0.025   -0.026\n  x2 ~~                                                                 \n    c1               -0.019    0.050   -0.377    0.706   -0.019   -0.020\n    c2               -0.007    0.044   -0.167    0.867   -0.007   -0.008\n    c3                0.145    0.048    3.055    0.002    0.145    0.162\n    c4               -0.102    0.043   -2.371    0.018   -0.102   -0.115\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.789    0.074    0.080    0.085\n    c3               -0.030    0.050   -0.596    0.551   -0.030   -0.033\n    c4                0.128    0.048    2.669    0.008    0.128    0.140\n  c2 ~~                                                                 \n    c3                0.001    0.042    0.030    0.976    0.001    0.001\n    c4                0.032    0.044    0.729    0.466    0.032    0.036\n  c3 ~~                                                                 \n    c4                0.035    0.044    0.796    0.426    0.035    0.041\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .intercept         0.580    0.061    9.501    0.000    0.419    0.419\n   .slope             0.958    0.030   32.177    0.000    1.247    1.247\n    x1               -0.092    0.051   -1.793    0.073   -0.092   -0.090\n    x2                0.138    0.048    2.878    0.004    0.138    0.144\n    c1                0.008    0.049    0.158    0.874    0.008    0.008\n    c2                0.029    0.047    0.610    0.542    0.029    0.031\n    c3                0.068    0.047    1.449    0.147    0.068    0.072\n    c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.580    0.091    6.386    0.000    0.580    0.231\n   .t2                0.596    0.056   10.627    0.000    0.596    0.135\n   .t3                0.481    0.051    9.434    0.000    0.481    0.064\n   .t4                0.535    0.094    5.709    0.000    0.535    0.047\n   .intercept         1.079    0.108    9.996    0.000    0.562    0.562\n   .slope             0.224    0.027    8.373    0.000    0.379    0.379\n    x1                1.056    0.068   15.511    0.000    1.056    1.000\n    x2                0.924    0.065   14.153    0.000    0.924    1.000\n    c1                0.972    0.063   15.321    0.000    0.972    1.000\n    c2                0.899    0.062   14.432    0.000    0.899    1.000\n    c3                0.872    0.067   13.018    0.000    0.872    1.000\n    c4                0.851    0.057   15.001    0.000    0.851    1.000\n\nR-Square:\n                   Estimate\n    t1                0.769\n    t2                0.865\n    t3                0.936\n    t4                0.953\n    intercept         0.438\n    slope             0.621\n\n\n\n\n\n7.1.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  lgcm1_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              40.774               27.000                0.043 \n        chisq.scaled            df.scaled        pvalue.scaled \n              40.982               27.000                0.041 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               0.995             2345.885               30.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.036                0.994 \n                 tli                 srmr         rmsea.robust \n               0.993                0.030                0.040 \n          cfi.robust           tli.robust \n               0.992                0.992 \n\n\n\n\n7.1.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  lgcm1_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     x1     x2     c1     c2     c3     c4\nt1  0.000                                                               \nt2  0.010  0.000                                                        \nt3 -0.013 -0.001  0.000                                                 \nt4  0.012  0.002  0.002  0.000                                          \nx1  0.007  0.004  0.002  0.010  0.000                                   \nx2 -0.006  0.005  0.002 -0.002  0.015  0.000                            \nc1  0.006  0.018  0.001  0.056  0.001 -0.001  0.000                     \nc2  0.006 -0.005 -0.007 -0.005  0.003 -0.004  0.000  0.000              \nc3  0.046  0.018  0.030  0.030  0.007 -0.008 -0.001 -0.002  0.000       \nc4  0.038  0.027  0.001  0.006 -0.002  0.002  0.000  0.001  0.002  0.000\n\n$mean\n    t1     t2     t3     t4     x1     x2     c1     c2     c3     c4 \n 0.009  0.064  0.036  0.055 -0.090  0.144  0.008  0.031  0.072 -0.020 \n\n\n\n\n7.1.6 Modification Indices\n\n\nCode\nmodificationindices(\n  lgcm1_fit,\n  sort. = TRUE)\n\n\n\n  \n\n\n\n\n\n7.1.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(lgcm1_fit)\n\n\nnamed numeric(0)\n\n\n\n\n7.1.8 Path Diagram\n\n\nCode\nsemPlot::semPaths(\n  lgcm1_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  lgcm1_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  lgcm1_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(lgcm1_fit)\n\n\n\n\n7.1.9 Plot Trajectories\n\n7.1.9.1 Protoypical Growth Curve\nCalculated from intercept and slope parameters:\n\n\nCode\nlgcm1_intercept &lt;- coef(lgcm1_fit)[\"intercept~1\"]\nlgcm1_slope &lt;- coef(lgcm1_fit)[\"slope~1\"]\n\nggplot() +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_x_continuous(\n    limits = c(0, 3),\n    labels = 1:4) +\n  scale_y_continuous(\n    limits = c(0, 5)) +\n  geom_abline(\n    mapping = aes(\n      slope = lgcm1_slope,\n      intercept = lgcm1_intercept))\n\n\n\n\n\n\n\n\n\nCalculated manually:\n\n\nCode\ntimepoints &lt;- 4\n\nnewData &lt;- expand.grid(\n  time = c(1, 4)\n)\n\nnewData$predictedValue &lt;- NA\nnewData$predictedValue[which(newData$time == 1)] &lt;- lgcm1_intercept\nnewData$predictedValue[which(newData$time == 4)] &lt;- lgcm1_intercept + (timepoints - 1)*lgcm1_slope\n\nggplot(\n  data = newData,\n  mapping = aes(x = time, y = predictedValue)) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(0, 5)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.1.9.2 Individuals’ Growth Curves\nCalculated from intercept and slope parameters:\n\n\nCode\nnewData &lt;- as.data.frame(predict(lgcm1_fit))\nnewData$id &lt;- row.names(newData)\n\nggplot(\n  data = newData) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_x_continuous(\n    limits = c(0, 3),\n    labels = 1:4) +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_abline(\n    mapping = aes(\n      slope = slope,\n      intercept = intercept))\n\n\n\n\n\n\n\n\n\nCalculated manually:\n\n\nCode\nnewData$t1 &lt;- newData$intercept\nnewData$t4 &lt;- newData$intercept + (timepoints - 1)*newData$slope\n\nnewData2 &lt;- pivot_longer(\n  newData,\n  cols = c(t1, t4)) %&gt;% \n  select(-intercept, -slope)\n\nnewData2$time &lt;- NA\nnewData2$time[which(newData2$name == \"t1\")] &lt;- 1\nnewData2$time[which(newData2$name == \"t4\")] &lt;- 4\n\nggplot(\n  data = newData2,\n  mapping = aes(x = time, y = value, group = factor(id))) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.1.9.3 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nnewData &lt;- as.data.frame(predict(lgcm1_fit))\nnewData$id &lt;- row.names(newData)\n\nggplot(\n  data = newData) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_x_continuous(\n    limits = c(0, 3),\n    labels = 1:4) +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_abline(\n    mapping = aes(\n      slope = slope,\n      intercept = intercept)) +\n  geom_abline(\n    mapping = aes(\n      slope = lgcm1_slope,\n      intercept = lgcm1_intercept),\n    color = \"blue\",\n    linewidth = 2)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#latentBasisLGCM",
    "href": "sem.html#latentBasisLGCM",
    "title": "Structural Equation Modeling",
    "section": "7.2 Latent Basis Growth Curve Model",
    "text": "7.2 Latent Basis Growth Curve Model\n\n7.2.1 Model Syntax\n\n7.2.1.1 Abbreviated\n\n\nCode\nlbgcm1_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + a*t2 + b*t3 + 3*t4 # freely estimate the loadings for t2 and t3\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n'\n\n\n\n\n7.2.1.2 Full\n\n\nCode\nlbgcm2_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + a*t2 + b*t3 + 3*t4 # freely estimate the loadings for t2 and t3\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n  \n  # Constrain observed intercepts to zero\n  t1 ~ 0\n  t2 ~ 0\n  t3 ~ 0\n  t4 ~ 0\n  \n  # Estimate mean of intercept and slope\n  intercept ~ 1\n  slope ~ 1\n'\n\n\n\n\n\n7.2.2 Fit the Model\n\n7.2.2.1 Abbreviated\n\n\nCode\nlbgcm1_fit &lt;- growth(\n  lbgcm1_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  int.ov.free = FALSE,\n  int.lv.free = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n7.2.2.2 Full\n\n\nCode\nlbgcm2_fit &lt;- sem(\n  lbgcm2_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n\n7.2.3 Summary Output\n\n7.2.3.1 Abbreviated\n\n\nCode\nsummary(\n  lbgcm1_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 30 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                37.229      37.400\n  Degrees of freedom                                25          25\n  P-value (Chi-square)                           0.055       0.053\n  Scaling correction factor                                  0.995\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995       0.995\n  Tucker-Lewis Index (TLI)                       0.994       0.994\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.993\n  Robust Tucker-Lewis Index (TLI)                            0.992\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5780.735   -5780.735\n  Scaling correction factor                                  0.991\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11641.470   11641.470\n  Bayesian (BIC)                             11801.128   11801.128\n  Sample-size adjusted Bayesian (SABIC)      11674.206   11674.206\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.035       0.035\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.057       0.057\n  P-value H_0: RMSEA &lt;= 0.050                    0.855       0.851\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.018\n  90 Percent confidence interval - upper                     0.059\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.793\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.381    0.875\n    t2                1.000                               1.381    0.650\n    t3                1.000                               1.381    0.508\n    t4                1.000                               1.381    0.409\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2         (a)    1.044    0.039   26.867    0.000    0.811    0.382\n    t3         (b)    1.960    0.037   53.301    0.000    1.523    0.561\n    t4                3.000                               2.330    0.690\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.605    0.059   10.192    0.000    0.438    0.452\n    x2                0.598    0.062    9.619    0.000    0.433    0.420\n  slope ~                                                               \n    x1                0.265    0.030    8.969    0.000    0.341    0.351\n    x2                0.526    0.032   16.223    0.000    0.677    0.657\n  t1 ~                                                                  \n    c1                0.145    0.045    3.226    0.001    0.145    0.090\n  t2 ~                                                                  \n    c2                0.287    0.047    6.157    0.000    0.287    0.128\n  t3 ~                                                                  \n    c3                0.337    0.047    7.111    0.000    0.337    0.116\n  t4 ~                                                                  \n    c4                0.333    0.057    5.836    0.000    0.333    0.091\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .slope             0.072    0.040    1.790    0.074    0.144    0.144\n  x1 ~~                                                                 \n    x2                0.141    0.050    2.798    0.005    0.141    0.140\n    c1               -0.039    0.051   -0.762    0.446   -0.039   -0.038\n    c2                0.023    0.048    0.493    0.622    0.023    0.024\n    c3                0.027    0.050    0.544    0.586    0.027    0.028\n    c4               -0.023    0.045   -0.519    0.604   -0.023   -0.024\n  x2 ~~                                                                 \n    c1               -0.018    0.050   -0.358    0.721   -0.018   -0.019\n    c2               -0.003    0.044   -0.075    0.940   -0.003   -0.004\n    c3                0.155    0.048    3.239    0.001    0.155    0.170\n    c4               -0.104    0.043   -2.421    0.015   -0.104   -0.116\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.793    0.073    0.080    0.086\n    c3               -0.030    0.050   -0.585    0.559   -0.030   -0.032\n    c4                0.127    0.048    2.668    0.008    0.127    0.140\n  c2 ~~                                                                 \n    c3                0.003    0.041    0.078    0.938    0.003    0.004\n    c4                0.031    0.044    0.715    0.475    0.031    0.036\n  c3 ~~                                                                 \n    c4                0.034    0.044    0.767    0.443    0.034    0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .intercept         0.568    0.063    8.966    0.000    0.411    0.411\n   .slope             0.966    0.030   31.916    0.000    1.244    1.244\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.574    0.092    6.211    0.000    0.574    0.230\n   .t2                0.595    0.055   10.741    0.000    0.595    0.132\n   .t3                0.487    0.051    9.602    0.000    0.487    0.066\n   .t4                0.518    0.097    5.338    0.000    0.518    0.045\n   .intercept         1.079    0.109    9.932    0.000    0.566    0.566\n   .slope             0.229    0.027    8.388    0.000    0.379    0.379\n    x1                1.064    0.068   15.614    0.000    1.064    1.000\n    x2                0.943    0.065   14.401    0.000    0.943    1.000\n    c1                0.972    0.064   15.306    0.000    0.972    1.000\n    c2                0.900    0.063   14.372    0.000    0.900    1.000\n    c3                0.876    0.067   13.041    0.000    0.876    1.000\n    c4                0.852    0.057   15.005    0.000    0.852    1.000\n\nR-Square:\n                   Estimate\n    t1                0.770\n    t2                0.868\n    t3                0.934\n    t4                0.955\n    intercept         0.434\n    slope             0.621\n\n\n\n\n7.2.3.2 Full\n\n\nCode\nsummary(\n  lbgcm2_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        46\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                22.514      22.759\n  Degrees of freedom                                19          19\n  P-value (Chi-square)                           0.259       0.248\n  Scaling correction factor                                  0.989\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998       0.998\n  Tucker-Lewis Index (TLI)                       0.998       0.998\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.999\n  Robust Tucker-Lewis Index (TLI)                            0.998\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5773.377   -5773.377\n  Scaling correction factor                                  0.994\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11638.754   11638.754\n  Bayesian (BIC)                             11822.361   11822.361\n  Sample-size adjusted Bayesian (SABIC)      11676.400   11676.400\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.022       0.022\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.051       0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.944       0.939\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.021\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.051\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.945\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.013       0.013\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.381    0.875\n    t2                1.000                               1.381    0.651\n    t3                1.000                               1.381    0.509\n    t4                1.000                               1.381    0.409\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2         (a)    1.044    0.039   26.867    0.000    0.809    0.381\n    t3         (b)    1.960    0.037   53.301    0.000    1.520    0.560\n    t4                3.000                               2.326    0.689\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.605    0.059   10.192    0.000    0.438    0.450\n    x2                0.598    0.062    9.619    0.000    0.433    0.416\n  slope ~                                                               \n    x1                0.265    0.030    8.969    0.000    0.341    0.351\n    x2                0.526    0.032   16.223    0.000    0.678    0.652\n  t1 ~                                                                  \n    c1                0.145    0.045    3.226    0.001    0.145    0.090\n  t2 ~                                                                  \n    c2                0.287    0.047    6.157    0.000    0.287    0.128\n  t3 ~                                                                  \n    c3                0.337    0.047    7.111    0.000    0.337    0.116\n  t4 ~                                                                  \n    c4                0.333    0.057    5.836    0.000    0.333    0.091\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .slope             0.072    0.040    1.790    0.074    0.144    0.144\n  x1 ~~                                                                 \n    x2                0.153    0.049    3.129    0.002    0.153    0.155\n    c1               -0.038    0.050   -0.760    0.447   -0.038   -0.037\n    c2                0.026    0.048    0.547    0.585    0.026    0.027\n    c3                0.033    0.049    0.674    0.501    0.033    0.035\n    c4               -0.025    0.044   -0.560    0.575   -0.025   -0.026\n  x2 ~~                                                                 \n    c1               -0.019    0.050   -0.377    0.706   -0.019   -0.020\n    c2               -0.007    0.044   -0.167    0.867   -0.007   -0.008\n    c3                0.145    0.048    3.055    0.002    0.145    0.162\n    c4               -0.102    0.043   -2.371    0.018   -0.102   -0.115\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.789    0.074    0.080    0.085\n    c3               -0.030    0.050   -0.596    0.551   -0.030   -0.033\n    c4                0.128    0.048    2.669    0.008    0.128    0.140\n  c2 ~~                                                                 \n    c3                0.001    0.042    0.030    0.976    0.001    0.001\n    c4                0.032    0.044    0.729    0.466    0.032    0.036\n  c3 ~~                                                                 \n    c4                0.035    0.044    0.796    0.426    0.035    0.041\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .intercept         0.568    0.063    8.966    0.000    0.411    0.411\n   .slope             0.966    0.030   31.916    0.000    1.246    1.246\n    x1               -0.092    0.051   -1.793    0.073   -0.092   -0.090\n    x2                0.138    0.048    2.878    0.004    0.138    0.144\n    c1                0.008    0.049    0.158    0.874    0.008    0.008\n    c2                0.029    0.047    0.610    0.542    0.029    0.031\n    c3                0.068    0.047    1.449    0.147    0.068    0.072\n    c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.574    0.092    6.211    0.000    0.574    0.230\n   .t2                0.595    0.055   10.741    0.000    0.595    0.132\n   .t3                0.487    0.051    9.602    0.000    0.487    0.066\n   .t4                0.518    0.097    5.338    0.000    0.518    0.046\n   .intercept         1.079    0.109    9.932    0.000    0.566    0.566\n   .slope             0.229    0.027    8.388    0.000    0.381    0.381\n    x1                1.056    0.068   15.511    0.000    1.056    1.000\n    x2                0.924    0.065   14.153    0.000    0.924    1.000\n    c1                0.972    0.063   15.321    0.000    0.972    1.000\n    c2                0.899    0.062   14.432    0.000    0.899    1.000\n    c3                0.872    0.067   13.018    0.000    0.872    1.000\n    c4                0.851    0.057   15.001    0.000    0.851    1.000\n\nR-Square:\n                   Estimate\n    t1                0.770\n    t2                0.868\n    t3                0.934\n    t4                0.954\n    intercept         0.434\n    slope             0.619\n\n\n\n\n\n7.2.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  lbgcm1_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              37.229               25.000                0.055 \n        chisq.scaled            df.scaled        pvalue.scaled \n              37.400               25.000                0.053 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               0.995             2345.885               30.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.035                0.995 \n                 tli                 srmr         rmsea.robust \n               0.994                0.029                0.040 \n          cfi.robust           tli.robust \n               0.993                0.992 \n\n\n\n\n7.2.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  lbgcm1_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     x1     x2     c1     c2     c3     c4\nt1  0.000                                                               \nt2  0.012  0.000                                                        \nt3 -0.011 -0.003  0.000                                                 \nt4  0.016 -0.003  0.003  0.000                                          \nx1  0.008  0.003  0.003  0.009  0.000                                   \nx2 -0.003  0.001  0.004 -0.003  0.015  0.000                            \nc1  0.004  0.018  0.001  0.056  0.001 -0.001  0.000                     \nc2  0.006 -0.003 -0.007 -0.005  0.003 -0.004  0.000  0.000              \nc3  0.046  0.018  0.026  0.030  0.007 -0.008 -0.001 -0.002  0.000       \nc4  0.038  0.027  0.001  0.005 -0.002  0.002  0.000  0.001  0.002  0.000\n\n$mean\n    t1     t2     t3     t4     x1     x2     c1     c2     c3     c4 \n 0.017  0.046  0.048  0.051 -0.090  0.144  0.008  0.031  0.072 -0.020 \n\n\n\n\n7.2.6 Modification Indices\n\n\nCode\nmodificationindices(\n  lbgcm1_fit,\n  sort. = TRUE)\n\n\n\n  \n\n\n\n\n\n7.2.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(lbgcm1_fit)\n\n\nnamed numeric(0)\n\n\n\n\n7.2.8 Path Diagram\n\n\nCode\nsemPaths(\n  lbgcm1_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  lbgcm1_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  lbgcm1_fit,\n  stand = TRUE,\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(lbgcm1_fit)\n\n\n\n\n7.2.9 Plot Trajectories\n\n7.2.9.1 Protoypical Growth Curve\n\n\nCode\nlbgcm1_intercept &lt;- coef(lbgcm1_fit)[\"intercept~1\"]\nlbgcm1_slope &lt;- coef(lbgcm1_fit)[\"slope~1\"]\nlbgcm1_slopeloadingt2 &lt;- coef(lbgcm1_fit)[\"a\"]\nlbgcm1_slopeloadingt3 &lt;- coef(lbgcm1_fit)[\"b\"]\n\ntimepoints &lt;- 4\n\nnewData &lt;- data.frame(\n  time = 1:4,\n  slopeloading = c(0, lbgcm1_slopeloadingt2, lbgcm1_slopeloadingt3, 3)\n)\n\nnewData$predictedValue &lt;- NA\nnewData$predictedValue &lt;- lbgcm1_intercept + lbgcm1_slope * newData$slopeloading\n\nggplot(\n  data = newData,\n  mapping = aes(x = time, y = predictedValue)) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(0, 5)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.2.9.2 Individuals’ Growth Curves\n\n\nCode\nperson_factors &lt;- as.data.frame(predict(lbgcm1_fit))\nperson_factors$id &lt;- rownames(person_factors)\n\nslope_loadings &lt;- c(0, lbgcm1_slopeloadingt2, lbgcm1_slopeloadingt3, 3)\n\n# Compute model-implied values for each person at each time point\nindividual_trajectories &lt;- person_factors %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    t1 = intercept + slope * slope_loadings[1],\n    t2 = intercept + slope * slope_loadings[2],\n    t3 = intercept + slope * slope_loadings[3],\n    t4 = intercept + slope * slope_loadings[4]\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(id, t1, t2, t3, t4) %&gt;%\n  pivot_longer(\n    cols = t1:t4,\n    names_to = \"timepoint\",\n    values_to = \"value\") %&gt;%\n  mutate(\n    time = as.integer(substr(timepoint, 2, 2)) # extract number from \"t1\", \"t2\", etc.\n  )\n\nggplot(\n  data = individual_trajectories,\n  mapping = aes(x = time, y = value, group = factor(id))) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.2.9.3 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot() +\n  geom_line( # individuals' model-implied trajectories\n    data = individual_trajectories,\n    aes(\n      x = time,\n      y = value,\n      group = id),\n  ) +\n  geom_line( # prototypical trajectory\n    data = newData,\n    aes(\n      x = time,\n      y = predictedValue),\n    color = \"blue\",\n    linewidth = 2\n  )",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#quadraticLGCM",
    "href": "sem.html#quadraticLGCM",
    "title": "Structural Equation Modeling",
    "section": "7.3 Quadratic Growth Curve Model",
    "text": "7.3 Quadratic Growth Curve Model\nWhen using higher-order polynomials, we could specify contrast codes for time to reduce multicollinearity between the linear and quadratic growth factors: https://tdjorgensen.github.io/SEM-in-Ed-compendium/ch27.html#saturated-growth-model\n\n\nCode\nfactorLoadings &lt;- poly(\n  x = c(0,1,2,3), # times (can allow unequal spacing)\n  degree = 2)\n\nfactorLoadings\n\n\n              1    2\n[1,] -0.6708204  0.5\n[2,] -0.2236068 -0.5\n[3,]  0.2236068 -0.5\n[4,]  0.6708204  0.5\nattr(,\"coefs\")\nattr(,\"coefs\")$alpha\n[1] 1.5 1.5\n\nattr(,\"coefs\")$norm2\n[1] 1 4 5 4\n\nattr(,\"degree\")\n[1] 1 2\nattr(,\"class\")\n[1] \"poly\"   \"matrix\"\n\n\nCode\nlinearLoadings &lt;- factorLoadings[,1]\nquadraticLoadings &lt;- factorLoadings[,2]\n\nlinearLoadings\n\n\n[1] -0.6708204 -0.2236068  0.2236068  0.6708204\n\n\nCode\nquadraticLoadings\n\n\n[1]  0.5 -0.5 -0.5  0.5\n\n\n\n7.3.1 Model Syntax\n\n7.3.1.1 Abbreviated\n\n\nCode\nquadraticGCM1_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  linear =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n  quadratic =~ 0*t1 + 1*t2 + 4*t3 + 9*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  linear ~ x1 + x2\n  quadratic ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n'\n\n\n\n\n7.3.1.2 Full\n\n\nCode\nquadraticGCM2_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  linear =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n  quadratic =~ 0*t1 + 1*t2 + 4*t3 + 9*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  linear ~ x1 + x2\n  quadratic ~ x1 + x2\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n  \n  # Constrain observed intercepts to zero\n  t1 ~ 0\n  t2 ~ 0\n  t3 ~ 0\n  t4 ~ 0\n  \n  # Estimate mean of intercept and slope\n  intercept ~ 1\n  linear ~ 1\n  quadratic ~ 1\n'\n\n\n\n\n\n7.3.2 Fit the Model\n\n7.3.2.1 Abbreviated\n\n\nCode\nquadraticGCM1_fit &lt;- growth(\n  quadraticGCM1_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  int.ov.free = FALSE,\n  int.lv.free = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n7.3.2.2 Full\n\n\nCode\nquadraticGCM2_fit &lt;- sem(\n  quadraticGCM2_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n\n7.3.3 Summary Output\n\n7.3.3.1 Abbreviated\n\n\nCode\nsummary(\n  quadraticGCM1_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 56 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                35.756      35.604\n  Degrees of freedom                                21          21\n  P-value (Chi-square)                           0.023       0.024\n  Scaling correction factor                                  1.004\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.994       0.994\n  Tucker-Lewis Index (TLI)                       0.991       0.991\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.992\n  Robust Tucker-Lewis Index (TLI)                            0.989\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5779.998   -5779.998\n  Scaling correction factor                                  0.987\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11647.996   11647.996\n  Bayesian (BIC)                             11823.621   11823.621\n  Sample-size adjusted Bayesian (SABIC)      11684.006   11684.006\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.042       0.042\n  90 Percent confidence interval - lower         0.016       0.015\n  90 Percent confidence interval - upper         0.065       0.065\n  P-value H_0: RMSEA &lt;= 0.050                    0.692       0.698\n  P-value H_0: RMSEA &gt;= 0.080                    0.002       0.002\n                                                                  \n  Robust RMSEA                                               0.047\n  90 Percent confidence interval - lower                     0.026\n  90 Percent confidence interval - upper                     0.067\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.584\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.002\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030       0.030\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.509    0.956\n    t2                1.000                               1.509    0.715\n    t3                1.000                               1.509    0.553\n    t4                1.000                               1.509    0.448\n  linear =~                                                             \n    t1                0.000                               0.000    0.000\n    t2                1.000                               1.054    0.499\n    t3                2.000                               2.108    0.773\n    t4                3.000                               3.163    0.938\n  quadratic =~                                                          \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.164    0.078\n    t3                4.000                               0.655    0.240\n    t4                9.000                               1.474    0.437\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.615    0.063    9.801    0.000    0.407    0.420\n    x2                0.590    0.066    8.878    0.000    0.391    0.380\n  linear ~                                                              \n    x1                0.236    0.063    3.737    0.000    0.224    0.231\n    x2                0.557    0.073    7.595    0.000    0.528    0.513\n  quadratic ~                                                           \n    x1                0.009    0.020    0.456    0.649    0.055    0.056\n    x2               -0.012    0.021   -0.549    0.583   -0.070   -0.068\n  t1 ~                                                                  \n    c1                0.130    0.045    2.864    0.004    0.130    0.081\n  t2 ~                                                                  \n    c2                0.289    0.047    6.207    0.000    0.289    0.130\n  t3 ~                                                                  \n    c3                0.330    0.047    7.030    0.000    0.330    0.113\n  t4 ~                                                                  \n    c4                0.326    0.057    5.726    0.000    0.326    0.089\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .linear           -0.359    0.216   -1.658    0.097   -0.351   -0.351\n   .quadratic         0.108    0.053    2.025    0.043    0.552    0.552\n .linear ~~                                                             \n   .quadratic        -0.119    0.056   -2.121    0.034   -0.857   -0.857\n  x1 ~~                                                                 \n    x2                0.141    0.050    2.798    0.005    0.141    0.140\n    c1               -0.039    0.051   -0.762    0.446   -0.039   -0.038\n    c2                0.023    0.048    0.493    0.622    0.023    0.024\n    c3                0.027    0.050    0.544    0.586    0.027    0.028\n    c4               -0.023    0.045   -0.519    0.604   -0.023   -0.024\n  x2 ~~                                                                 \n    c1               -0.018    0.050   -0.358    0.721   -0.018   -0.019\n    c2               -0.003    0.044   -0.075    0.940   -0.003   -0.004\n    c3                0.155    0.048    3.239    0.001    0.155    0.170\n    c4               -0.104    0.043   -2.421    0.015   -0.104   -0.116\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.793    0.073    0.080    0.086\n    c3               -0.030    0.050   -0.585    0.559   -0.030   -0.032\n    c4                0.127    0.048    2.668    0.008    0.127    0.140\n  c2 ~~                                                                 \n    c3                0.003    0.041    0.078    0.938    0.003    0.004\n    c4                0.031    0.044    0.715    0.475    0.031    0.036\n  c3 ~~                                                                 \n    c4                0.034    0.044    0.767    0.443    0.034    0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .intercept         0.575    0.065    8.793    0.000    0.381    0.381\n   .linear            0.944    0.066   14.331    0.000    0.895    0.895\n   .quadratic         0.005    0.020    0.276    0.783    0.033    0.033\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.208    0.187    1.111    0.267    0.208    0.084\n   .t2                0.640    0.079    8.106    0.000    0.640    0.144\n   .t3                0.404    0.085    4.747    0.000    0.404    0.054\n   .t4                0.623    0.307    2.027    0.043    0.623    0.055\n   .intercept         1.445    0.208    6.939    0.000    0.635    0.635\n   .linear            0.723    0.229    3.160    0.002    0.650    0.650\n   .quadratic         0.027    0.020    1.353    0.176    0.993    0.993\n    x1                1.064    0.068   15.614    0.000    1.064    1.000\n    x2                0.943    0.065   14.401    0.000    0.943    1.000\n    c1                0.972    0.064   15.306    0.000    0.972    1.000\n    c2                0.900    0.063   14.372    0.000    0.900    1.000\n    c3                0.876    0.067   13.041    0.000    0.876    1.000\n    c4                0.852    0.057   15.005    0.000    0.852    1.000\n\nR-Square:\n                   Estimate\n    t1                0.916\n    t2                0.856\n    t3                0.946\n    t4                0.945\n    intercept         0.365\n    linear            0.350\n    quadratic         0.007\n\n\n\n\n7.3.3.2 Full\n\n\nCode\nsummary(\n  quadraticGCM2_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 56 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        50\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                21.040      21.042\n  Degrees of freedom                                15          15\n  P-value (Chi-square)                           0.136       0.136\n  Scaling correction factor                                  1.000\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.997       0.997\n  Tucker-Lewis Index (TLI)                       0.995       0.995\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.998\n  Robust Tucker-Lewis Index (TLI)                            0.995\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5772.640   -5772.640\n  Scaling correction factor                                  0.990\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11645.281   11645.281\n  Bayesian (BIC)                             11844.854   11844.854\n  Sample-size adjusted Bayesian (SABIC)      11686.201   11686.201\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.032       0.032\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.061       0.061\n  P-value H_0: RMSEA &lt;= 0.050                    0.828       0.828\n  P-value H_0: RMSEA &gt;= 0.080                    0.002       0.002\n                                                                  \n  Robust RMSEA                                               0.031\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.061\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.834\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.002\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.014       0.014\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.509    0.956\n    t2                1.000                               1.509    0.715\n    t3                1.000                               1.509    0.554\n    t4                1.000                               1.509    0.448\n  linear =~                                                             \n    t1                0.000                               0.000    0.000\n    t2                1.000                               1.053    0.499\n    t3                2.000                               2.106    0.773\n    t4                3.000                               3.158    0.938\n  quadratic =~                                                          \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.164    0.078\n    t3                4.000                               0.655    0.241\n    t4                9.000                               1.474    0.438\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.615    0.063    9.801    0.000    0.407    0.419\n    x2                0.590    0.066    8.878    0.000    0.391    0.376\n  linear ~                                                              \n    x1                0.236    0.063    3.737    0.000    0.224    0.230\n    x2                0.557    0.073    7.595    0.000    0.529    0.508\n  quadratic ~                                                           \n    x1                0.009    0.020    0.456    0.649    0.055    0.056\n    x2               -0.012    0.021   -0.549    0.583   -0.070   -0.068\n  t1 ~                                                                  \n    c1                0.130    0.045    2.864    0.004    0.130    0.081\n  t2 ~                                                                  \n    c2                0.289    0.047    6.207    0.000    0.289    0.130\n  t3 ~                                                                  \n    c3                0.330    0.047    7.030    0.000    0.330    0.113\n  t4 ~                                                                  \n    c4                0.326    0.057    5.726    0.000    0.326    0.089\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .linear           -0.359    0.216   -1.658    0.097   -0.351   -0.351\n   .quadratic         0.108    0.053    2.025    0.043    0.552    0.552\n .linear ~~                                                             \n   .quadratic        -0.119    0.056   -2.121    0.034   -0.857   -0.857\n  x1 ~~                                                                 \n    x2                0.153    0.049    3.129    0.002    0.153    0.155\n    c1               -0.038    0.050   -0.760    0.447   -0.038   -0.037\n    c2                0.026    0.048    0.547    0.585    0.026    0.027\n    c3                0.033    0.049    0.674    0.501    0.033    0.035\n    c4               -0.025    0.044   -0.560    0.575   -0.025   -0.026\n  x2 ~~                                                                 \n    c1               -0.019    0.050   -0.377    0.706   -0.019   -0.020\n    c2               -0.007    0.044   -0.167    0.867   -0.007   -0.008\n    c3                0.145    0.048    3.055    0.002    0.145    0.162\n    c4               -0.102    0.043   -2.371    0.018   -0.102   -0.115\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.789    0.074    0.080    0.085\n    c3               -0.030    0.050   -0.596    0.551   -0.030   -0.033\n    c4                0.128    0.048    2.669    0.008    0.128    0.140\n  c2 ~~                                                                 \n    c3                0.001    0.042    0.030    0.976    0.001    0.001\n    c4                0.032    0.044    0.729    0.466    0.032    0.036\n  c3 ~~                                                                 \n    c4                0.035    0.044    0.796    0.426    0.035    0.041\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .intercept         0.575    0.065    8.793    0.000    0.381    0.381\n   .linear            0.944    0.066   14.331    0.000    0.896    0.896\n   .quadratic         0.005    0.020    0.276    0.783    0.033    0.033\n    x1               -0.092    0.051   -1.793    0.073   -0.092   -0.090\n    x2                0.138    0.048    2.878    0.004    0.138    0.144\n    c1                0.008    0.049    0.158    0.874    0.008    0.008\n    c2                0.029    0.047    0.610    0.542    0.029    0.031\n    c3                0.068    0.047    1.449    0.147    0.068    0.072\n    c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.208    0.187    1.111    0.267    0.208    0.084\n   .t2                0.640    0.079    8.106    0.000    0.640    0.144\n   .t3                0.404    0.085    4.747    0.000    0.404    0.055\n   .t4                0.623    0.307    2.027    0.043    0.623    0.055\n   .intercept         1.445    0.208    6.939    0.000    0.635    0.635\n   .linear            0.723    0.229    3.160    0.002    0.652    0.652\n   .quadratic         0.027    0.020    1.353    0.176    0.993    0.993\n    x1                1.056    0.068   15.511    0.000    1.056    1.000\n    x2                0.924    0.065   14.153    0.000    0.924    1.000\n    c1                0.972    0.063   15.321    0.000    0.972    1.000\n    c2                0.899    0.062   14.432    0.000    0.899    1.000\n    c3                0.872    0.067   13.018    0.000    0.872    1.000\n    c4                0.851    0.057   15.001    0.000    0.851    1.000\n\nR-Square:\n                   Estimate\n    t1                0.916\n    t2                0.856\n    t3                0.945\n    t4                0.945\n    intercept         0.365\n    linear            0.348\n    quadratic         0.007\n\n\n\n\n\n7.3.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  quadraticGCM1_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              35.756               21.000                0.023 \n        chisq.scaled            df.scaled        pvalue.scaled \n              35.604               21.000                0.024 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.004             2345.885               30.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.042                0.994 \n                 tli                 srmr         rmsea.robust \n               0.991                0.030                0.047 \n          cfi.robust           tli.robust \n               0.992                0.989 \n\n\n\n\n7.3.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  quadraticGCM1_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     x1     x2     c1     c2     c3     c4\nt1  0.000                                                               \nt2  0.002  0.000                                                        \nt3  0.002  0.001  0.000                                                 \nt4  0.007  0.003  0.000  0.000                                          \nx1  0.002  0.011  0.004  0.008  0.000                                   \nx2  0.001  0.004 -0.003  0.001  0.015  0.000                            \nc1  0.014  0.018  0.001  0.056  0.001 -0.001  0.000                     \nc2  0.006 -0.004 -0.007 -0.005  0.003 -0.004  0.000  0.000              \nc3  0.047  0.018  0.028  0.031  0.007 -0.008 -0.001 -0.002  0.000       \nc4  0.038  0.027  0.002  0.006 -0.002  0.002  0.000  0.001  0.002  0.000\n\n$mean\n    t1     t2     t3     t4     x1     x2     c1     c2     c3     c4 \n 0.012  0.070  0.040  0.054 -0.090  0.144  0.008  0.031  0.072 -0.020 \n\n\n\n\n7.3.6 Modification Indices\n\n\nCode\nmodificationindices(\n  quadraticGCM1_fit,\n  sort. = TRUE)\n\n\n\n  \n\n\n\n\n\n7.3.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(quadraticGCM1_fit)\n\n\nnamed numeric(0)\n\n\n\n\n7.3.8 Path Diagram\n\n\nCode\nsemPlot::semPaths(\n  quadraticGCM1_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  quadraticGCM1_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  quadraticGCM1_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(quadraticGCM1_fit)\n\n\n\n\n7.3.9 Plot Trajectories\n\n7.3.9.1 Protoypical Growth Curve\nCalculated from intercept and slope parameters:\n\n\nCode\nquadraticGCM1_intercept &lt;- coef(quadraticGCM1_fit)[\"intercept~1\"]\nquadraticGCM1_linear &lt;- coef(quadraticGCM1_fit)[\"linear~1\"]\nquadraticGCM1_quadratic &lt;- coef(quadraticGCM1_fit)[\"quadratic~1\"]\n\ntimepoints &lt;- 4\n\nnewData &lt;- data.frame(\n  time = 1:4,\n  linearLoading = c(0, 1, 2, 3),\n  quadraticLoading = c(0, 1, 4, 9)\n)\n\nnewData$predictedValue &lt;- NA\nnewData$predictedValue &lt;- quadraticGCM1_intercept + (quadraticGCM1_linear * newData$linearLoading) + (quadraticGCM1_quadratic * newData$quadraticLoading)\n\nggplot(\n  data = newData,\n  mapping = aes(\n    x = time,\n    y = predictedValue)) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(0, 5)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.3.9.2 Individuals’ Growth Curves\n\n\nCode\nperson_factors &lt;- as.data.frame(predict(quadraticGCM1_fit))\nperson_factors$id &lt;- rownames(person_factors)\n\nlinear_loadings &lt;- c(0, 1, 2, 3)\nquadratic_loadings &lt;- c(0, 1, 4, 9)\n\n# Compute model-implied values for each person at each time point\nindividual_trajectories &lt;- person_factors %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    t1 = intercept + (linear * linear_loadings[1]) + (quadratic * quadratic_loadings[1]),\n    t2 = intercept + (linear * linear_loadings[2]) + (quadratic * quadratic_loadings[2]),\n    t3 = intercept + (linear * linear_loadings[3]) + (quadratic * quadratic_loadings[3]),\n    t4 = intercept + (linear * linear_loadings[4]) + (quadratic * quadratic_loadings[4])\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(id, t1, t2, t3, t4) %&gt;%\n  pivot_longer(\n    cols = t1:t4,\n    names_to = \"timepoint\",\n    values_to = \"value\") %&gt;%\n  mutate(\n    time = as.integer(substr(timepoint, 2, 2)) # extract number from \"t1\", \"t2\", etc.\n  )\n\nggplot(\n  data = individual_trajectories,\n  mapping = aes(\n    x = time,\n    y = value,\n    group = factor(id))) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.3.9.3 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot() +\n  geom_line( # individuals' model-implied trajectories\n    data = individual_trajectories,\n    aes(\n      x = time,\n      y = value,\n      group = id),\n  ) +\n  geom_line( # prototypical trajectory\n    data = newData,\n    aes(\n      x = time,\n      y = predictedValue),\n    color = \"blue\",\n    linewidth = 2\n  )",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#splineLGCM",
    "href": "sem.html#splineLGCM",
    "title": "Structural Equation Modeling",
    "section": "7.4 Spline (Piecewise) Growth Curve Model",
    "text": "7.4 Spline (Piecewise) Growth Curve Model\n\n7.4.1 Model Syntax\n\n7.4.1.1 Abbreviated\n\n\nCode\nsplineGCM1_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n  knot =~ 0*t1 + 0*t2 + 1*t3 + 1*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  knot ~ x1 + x2\n  \n  # Spline has no variance\n  knot ~~ 0*knot\n\n  # Spline does not covary with intercept and slope\n  knot ~~ 0*intercept\n  knot ~~ 0*slope\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n'\n\n\n\n\n7.4.1.2 Full\n\n\nCode\nsplineGCM2_syntax &lt;- '\n  # Intercept and slope\n  intercept =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n  knot =~ 0*t1 + 0*t2 + 1*t3 + 1*t4\n\n  # Regression paths\n  intercept ~ x1 + x2\n  slope ~ x1 + x2\n  knot ~ x1 + x2\n  \n  # Spline has no variance\n  knot ~~ 0*knot\n  \n  # Spline does not covary with intercept and slope\n  knot ~~ 0*intercept\n  knot ~~ 0*slope\n  \n  # Time-varying covariates\n  t1 ~ c1\n  t2 ~ c2\n  t3 ~ c3\n  t4 ~ c4\n  \n  # Constrain observed intercepts to zero\n  t1 ~ 0\n  t2 ~ 0\n  t3 ~ 0\n  t4 ~ 0\n  \n  # Estimate mean of intercept and slope\n  intercept ~ 1\n  slope ~ 1\n  knot ~ 1\n'\n\n\n\n\n\n7.4.2 Fit the Model\n\n7.4.2.1 Abbreviated\n\n\nCode\nsplineGCM1_fit &lt;- growth(\n  splineGCM1_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  int.ov.free = FALSE,\n  int.lv.free = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n7.4.2.2 Full\n\n\nCode\nsplineGCM2_fit &lt;- sem(\n  splineGCM2_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n\n7.4.3 Summary Output\n\n7.4.3.1 Abbreviated\n\n\nCode\nsummary(\n  splineGCM1_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        41\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                36.091      36.117\n  Degrees of freedom                                24          24\n  P-value (Chi-square)                           0.054       0.053\n  Scaling correction factor                                  0.999\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.995       0.995\n  Tucker-Lewis Index (TLI)                       0.993       0.994\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.993\n  Robust Tucker-Lewis Index (TLI)                            0.991\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5780.166   -5780.166\n  Scaling correction factor                                  0.989\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11642.331   11642.331\n  Bayesian (BIC)                             11805.981   11805.981\n  Sample-size adjusted Bayesian (SABIC)      11675.885   11675.885\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.035       0.036\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.058       0.058\n  P-value H_0: RMSEA &lt;= 0.050                    0.841       0.840\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.041\n  90 Percent confidence interval - lower                     0.019\n  90 Percent confidence interval - upper                     0.060\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.768\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.383    0.874\n    t2                1.000                               1.383    0.655\n    t3                1.000                               1.383    0.508\n    t4                1.000                               1.383    0.410\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.787    0.373\n    t3                2.000                               1.573    0.578\n    t4                3.000                               2.360    0.699\n  knot =~                                                               \n    t1                0.000                               0.000    0.000\n    t2                0.000                               0.000    0.000\n    t3                1.000                               0.059    0.022\n    t4                1.000                               0.059    0.017\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.604    0.059   10.254    0.000    0.437    0.451\n    x2                0.601    0.061    9.824    0.000    0.435    0.422\n  slope ~                                                               \n    x1                0.280    0.042    6.705    0.000    0.356    0.367\n    x2                0.535    0.043   12.349    0.000    0.680    0.660\n  knot ~                                                                \n    x1               -0.044    0.077   -0.572    0.568   -0.746   -0.770\n    x2               -0.033    0.088   -0.372    0.710   -0.555   -0.539\n  t1 ~                                                                  \n    c1                0.143    0.045    3.194    0.001    0.143    0.089\n  t2 ~                                                                  \n    c2                0.287    0.047    6.129    0.000    0.287    0.129\n  t3 ~                                                                  \n    c3                0.336    0.047    7.074    0.000    0.336    0.115\n  t4 ~                                                                  \n    c4                0.333    0.057    5.871    0.000    0.333    0.091\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .knot              0.000                                 NaN      NaN\n .slope ~~                                                              \n   .knot              0.000                                 NaN      NaN\n .intercept ~~                                                          \n   .slope             0.075    0.039    1.892    0.059    0.152    0.152\n  x1 ~~                                                                 \n    x2                0.141    0.050    2.798    0.005    0.141    0.140\n    c1               -0.039    0.051   -0.762    0.446   -0.039   -0.038\n    c2                0.023    0.048    0.493    0.622    0.023    0.024\n    c3                0.027    0.050    0.544    0.586    0.027    0.028\n    c4               -0.023    0.045   -0.519    0.604   -0.023   -0.024\n  x2 ~~                                                                 \n    c1               -0.018    0.050   -0.358    0.721   -0.018   -0.019\n    c2               -0.003    0.044   -0.075    0.940   -0.003   -0.004\n    c3                0.155    0.048    3.239    0.001    0.155    0.170\n    c4               -0.104    0.043   -2.421    0.015   -0.104   -0.116\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.793    0.073    0.080    0.086\n    c3               -0.030    0.050   -0.585    0.559   -0.030   -0.032\n    c4                0.127    0.048    2.668    0.008    0.127    0.140\n  c2 ~~                                                                 \n    c3                0.003    0.041    0.078    0.938    0.003    0.004\n    c4                0.031    0.044    0.715    0.475    0.031    0.036\n  c3 ~~                                                                 \n    c4                0.034    0.044    0.767    0.443    0.034    0.039\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .intercept         0.565    0.061    9.265    0.000    0.409    0.409\n   .slope             1.025    0.042   24.178    0.000    1.303    1.303\n   .knot             -0.167    0.081   -2.058    0.040   -2.839   -2.839\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .knot              0.000                               0.000    0.000\n   .t1                0.581    0.091    6.403    0.000    0.581    0.232\n   .t2                0.591    0.055   10.763    0.000    0.591    0.133\n   .t3                0.475    0.051    9.390    0.000    0.475    0.064\n   .t4                0.538    0.093    5.768    0.000    0.538    0.047\n   .intercept         1.080    0.108   10.010    0.000    0.565    0.565\n   .slope             0.224    0.027    8.392    0.000    0.361    0.361\n    x1                1.064    0.068   15.614    0.000    1.064    1.000\n    x2                0.943    0.065   14.401    0.000    0.943    1.000\n    c1                0.972    0.064   15.306    0.000    0.972    1.000\n    c2                0.900    0.063   14.372    0.000    0.900    1.000\n    c3                0.876    0.067   13.041    0.000    0.876    1.000\n    c4                0.852    0.057   15.005    0.000    0.852    1.000\n\nR-Square:\n                   Estimate\n    knot              1.000\n    t1                0.768\n    t2                0.867\n    t3                0.936\n    t4                0.953\n    intercept         0.435\n    slope             0.639\n\n\n\n\n7.4.3.2 Full\n\n\nCode\nsummary(\n  splineGCM2_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 39 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        47\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                21.375      21.504\n  Degrees of freedom                                18          18\n  P-value (Chi-square)                           0.261       0.255\n  Scaling correction factor                                  0.994\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              2345.885    2414.540\n  Degrees of freedom                                30          30\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.972\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.999       0.999\n  Tucker-Lewis Index (TLI)                       0.998       0.998\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.999\n  Robust Tucker-Lewis Index (TLI)                            0.998\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5772.808   -5772.808\n  Scaling correction factor                                  0.992\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5762.120   -5762.120\n  Scaling correction factor                                  0.993\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11639.615   11639.615\n  Bayesian (BIC)                             11827.214   11827.214\n  Sample-size adjusted Bayesian (SABIC)      11678.080   11678.080\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.022       0.022\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.052       0.052\n  P-value H_0: RMSEA &lt;= 0.050                    0.938       0.935\n  P-value H_0: RMSEA &gt;= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.022\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.052\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.938\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.013       0.013\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept =~                                                          \n    t1                1.000                               1.382    0.874\n    t2                1.000                               1.382    0.655\n    t3                1.000                               1.382    0.508\n    t4                1.000                               1.382    0.410\n  slope =~                                                              \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.785    0.372\n    t3                2.000                               1.570    0.577\n    t4                3.000                               2.355    0.699\n  knot =~                                                               \n    t1                0.000                               0.000    0.000\n    t2                0.000                               0.000    0.000\n    t3                1.000                               0.059    0.022\n    t4                1.000                               0.059    0.017\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept ~                                                           \n    x1                0.604    0.059   10.254    0.000    0.437    0.449\n    x2                0.601    0.061    9.824    0.000    0.435    0.418\n  slope ~                                                               \n    x1                0.280    0.042    6.705    0.000    0.356    0.366\n    x2                0.535    0.043   12.349    0.000    0.681    0.655\n  knot ~                                                                \n    x1               -0.044    0.077   -0.572    0.568   -0.746   -0.767\n    x2               -0.033    0.088   -0.372    0.710   -0.555   -0.534\n  t1 ~                                                                  \n    c1                0.143    0.045    3.194    0.001    0.143    0.089\n  t2 ~                                                                  \n    c2                0.287    0.047    6.129    0.000    0.287    0.129\n  t3 ~                                                                  \n    c3                0.336    0.047    7.074    0.000    0.336    0.115\n  t4 ~                                                                  \n    c4                0.333    0.057    5.871    0.000    0.333    0.091\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .intercept ~~                                                          \n   .knot              0.000                                 NaN      NaN\n .slope ~~                                                              \n   .knot              0.000                                 NaN      NaN\n .intercept ~~                                                          \n   .slope             0.075    0.039    1.892    0.059    0.152    0.152\n  x1 ~~                                                                 \n    x2                0.153    0.049    3.129    0.002    0.153    0.155\n    c1               -0.038    0.050   -0.760    0.447   -0.038   -0.037\n    c2                0.026    0.048    0.547    0.585    0.026    0.027\n    c3                0.033    0.049    0.674    0.501    0.033    0.035\n    c4               -0.025    0.044   -0.560    0.575   -0.025   -0.026\n  x2 ~~                                                                 \n    c1               -0.019    0.050   -0.377    0.706   -0.019   -0.020\n    c2               -0.007    0.044   -0.167    0.867   -0.007   -0.008\n    c3                0.145    0.048    3.055    0.002    0.145    0.162\n    c4               -0.102    0.043   -2.371    0.018   -0.102   -0.115\n  c1 ~~                                                                 \n    c2                0.080    0.045    1.789    0.074    0.080    0.085\n    c3               -0.030    0.050   -0.596    0.551   -0.030   -0.033\n    c4                0.128    0.048    2.669    0.008    0.128    0.140\n  c2 ~~                                                                 \n    c3                0.001    0.042    0.030    0.976    0.001    0.001\n    c4                0.032    0.044    0.729    0.466    0.032    0.036\n  c3 ~~                                                                 \n    c4                0.035    0.044    0.796    0.426    0.035    0.041\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .intercept         0.565    0.061    9.265    0.000    0.409    0.409\n   .slope             1.025    0.042   24.178    0.000    1.305    1.305\n   .knot             -0.167    0.081   -2.058    0.040   -2.839   -2.839\n    x1               -0.092    0.051   -1.793    0.073   -0.092   -0.090\n    x2                0.138    0.048    2.878    0.004    0.138    0.144\n    c1                0.008    0.049    0.158    0.874    0.008    0.008\n    c2                0.029    0.047    0.610    0.542    0.029    0.031\n    c3                0.068    0.047    1.449    0.147    0.068    0.072\n    c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .knot              0.000                               0.000    0.000\n   .t1                0.581    0.091    6.403    0.000    0.581    0.232\n   .t2                0.591    0.055   10.763    0.000    0.591    0.133\n   .t3                0.475    0.051    9.390    0.000    0.475    0.064\n   .t4                0.538    0.093    5.768    0.000    0.538    0.047\n   .intercept         1.080    0.108   10.010    0.000    0.565    0.565\n   .slope             0.224    0.027    8.392    0.000    0.363    0.363\n    x1                1.056    0.068   15.511    0.000    1.056    1.000\n    x2                0.924    0.065   14.153    0.000    0.924    1.000\n    c1                0.972    0.063   15.321    0.000    0.972    1.000\n    c2                0.899    0.062   14.432    0.000    0.899    1.000\n    c3                0.872    0.067   13.018    0.000    0.872    1.000\n    c4                0.851    0.057   15.001    0.000    0.851    1.000\n\nR-Square:\n                   Estimate\n    knot              1.000\n    t1                0.768\n    t2                0.867\n    t3                0.936\n    t4                0.953\n    intercept         0.435\n    slope             0.637\n\n\n\n\n\n7.4.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  splineGCM1_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              36.091               24.000                0.054 \n        chisq.scaled            df.scaled        pvalue.scaled \n              36.117               24.000                0.053 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               0.999             2345.885               30.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.035                0.995 \n                 tli                 srmr         rmsea.robust \n               0.993                0.029                0.041 \n          cfi.robust           tli.robust \n               0.993                0.991 \n\n\n\n\n7.4.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  splineGCM1_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     x1     x2     c1     c2     c3     c4\nt1  0.000                                                               \nt2  0.010  0.000                                                        \nt3 -0.011 -0.002  0.000                                                 \nt4  0.014  0.000  0.002  0.000                                          \nx1  0.009 -0.001  0.005  0.009  0.000                                   \nx2 -0.004  0.002  0.003 -0.003  0.015  0.000                            \nc1  0.005  0.018  0.001  0.056  0.001 -0.001  0.000                     \nc2  0.006 -0.004 -0.007 -0.005  0.003 -0.004  0.000  0.000              \nc3  0.046  0.018  0.027  0.030  0.007 -0.008 -0.001 -0.002  0.000       \nc4  0.038  0.027  0.001  0.005 -0.002  0.002  0.000  0.001  0.002  0.000\n\n$mean\n    t1     t2     t3     t4     x1     x2     c1     c2     c3     c4 \n 0.019  0.039  0.053  0.049 -0.090  0.144  0.008  0.031  0.072 -0.020 \n\n\n\n\n7.4.6 Modification Indices\n\n\nCode\nmodificationindices(\n  splineGCM1_fit,\n  sort. = TRUE)\n\n\n\n  \n\n\n\n\n\n7.4.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(splineGCM1_fit)\n\n\nnamed numeric(0)\n\n\n\n\n7.4.8 Path Diagram\n\n\nCode\nsemPlot::semPaths(\n  splineGCM1_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  splineGCM1_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  splineGCM1_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(splineGCM1_fit)\n\n\n\n\n7.4.9 Plot Trajectories\n\n7.4.9.1 Protoypical Growth Curve\nCalculated from intercept and slope parameters:\n\n\nCode\nsplineGCM1_intercept &lt;- coef(splineGCM1_fit)[\"intercept~1\"]\nsplineGCM1_slope &lt;- coef(splineGCM1_fit)[\"slope~1\"]\nsplineGCM1_knot &lt;- coef(splineGCM1_fit)[\"knot~1\"]\n\ntimepoints &lt;- 4\n\nnewData &lt;- data.frame(\n  time = 1:4,\n  linearLoading = c(0, 1, 2, 3),\n  knotLoading = c(0, 0, 1, 1)\n)\n\nnewData$predictedValue &lt;- NA\nnewData$predictedValue &lt;- splineGCM1_intercept + (splineGCM1_slope * newData$linearLoading) + (splineGCM1_knot * newData$knotLoading)\n\nggplot(\n  data = newData,\n  mapping = aes(\n    x = time,\n    y = predictedValue)) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(0, 5)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.4.9.2 Individuals’ Growth Curves\n\n\nCode\nperson_factors &lt;- as.data.frame(predict(splineGCM1_fit))\nperson_factors$id &lt;- rownames(person_factors)\n\nslope_loadings &lt;- c(0, 1, 2, 3)\nknot_loadings &lt;- c(0, 0, 1, 1)\n\n# Compute model-implied values for each person at each time point\nindividual_trajectories &lt;- person_factors %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    t1 = intercept + (slope * slope_loadings[1]) + (knot * knot_loadings[1]),\n    t2 = intercept + (slope * slope_loadings[2]) + (knot * knot_loadings[2]),\n    t3 = intercept + (slope * slope_loadings[3]) + (knot * knot_loadings[3]),\n    t4 = intercept + (slope * slope_loadings[4]) + (knot * knot_loadings[4])\n  ) %&gt;%\n  ungroup() %&gt;%\n  select(id, t1, t2, t3, t4) %&gt;%\n  pivot_longer(\n    cols = t1:t4,\n    names_to = \"timepoint\",\n    values_to = \"value\") %&gt;%\n  mutate(\n    time = as.integer(substr(timepoint, 2, 2)) # extract number from \"t1\", \"t2\", etc.\n  )\n\nggplot(\n  data = individual_trajectories,\n  mapping = aes(\n    x = time,\n    y = value,\n    group = factor(id))) +\n  xlab(\"Timepoint\") +\n  ylab(\"Score\") +\n  scale_y_continuous(\n    limits = c(-10, 20)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n7.4.9.3 Individuals’ Trajectories Overlaid with Prototypical Trajectory\n\n\nCode\nggplot() +\n  geom_line( # individuals' model-implied trajectories\n    data = individual_trajectories,\n    aes(\n      x = time,\n      y = value,\n      group = id),\n  ) +\n  geom_line( # prototypical trajectory\n    data = newData,\n    aes(\n      x = time,\n      y = predictedValue),\n    color = \"blue\",\n    linewidth = 2\n  )",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#saturatedGCM",
    "href": "sem.html#saturatedGCM",
    "title": "Structural Equation Modeling",
    "section": "7.5 Saturated Growth Curve Model",
    "text": "7.5 Saturated Growth Curve Model\nhttps://tdjorgensen.github.io/SEM-in-Ed-compendium/ch27.html#saturated-growth-model",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-4",
    "href": "sem.html#model-syntax-4",
    "title": "Structural Equation Modeling",
    "section": "8.1 Model Syntax",
    "text": "8.1 Model Syntax\n\n\nCode\nbivariateLCSM_syntax &lt;- specify_bi_lcsm(\n  timepoints = 3,\n  var_x = \"x\",\n  model_x = list(\n    alpha_constant = TRUE, # alpha = intercept (constant change factor)\n    beta = TRUE, # beta = proportional change factor (latent true score predicting its change score)\n    phi = TRUE), # phi = autoregression of change scores\n  var_y = \"y\",\n  model_y = list(\n    alpha_constant = TRUE, # alpha = intercept (constant change factor)\n    beta = TRUE, # beta = proportional change factor (latent true score predicting its change score)\n    phi = TRUE), # phi = autoregression of change scores\n  coupling = list(\n    delta_lag_xy = TRUE,\n    delta_lag_yx = TRUE),\n  change_letter_x = \"g\",\n  change_letter_y = \"j\")\n\ncat(bivariateLCSM_syntax)\n\n\n# # # # # # # # # # # # # # # # # # # # #\n# Specify parameters for construct x ----\n# # # # # # # # # # # # # # # # # # # # #\n# Specify latent true scores \nlx1 =~ 1 * x1 \nlx2 =~ 1 * x2 \nlx3 =~ 1 * x3 \n# Specify mean of latent true scores \nlx1 ~ gamma_lx1 * 1 \nlx2 ~ 0 * 1 \nlx3 ~ 0 * 1 \n# Specify variance of latent true scores \nlx1 ~~ sigma2_lx1 * lx1 \nlx2 ~~ 0 * lx2 \nlx3 ~~ 0 * lx3 \n# Specify intercept of obseved scores \nx1 ~ 0 * 1 \nx2 ~ 0 * 1 \nx3 ~ 0 * 1 \n# Specify variance of observed scores \nx1 ~~ sigma2_ux * x1 \nx2 ~~ sigma2_ux * x2 \nx3 ~~ sigma2_ux * x3 \n# Specify autoregressions of latent variables \nlx2 ~ 1 * lx1 \nlx3 ~ 1 * lx2 \n# Specify latent change scores \ndx2 =~ 1 * lx2 \ndx3 =~ 1 * lx3 \n# Specify latent change scores means \ndx2 ~ 0 * 1 \ndx3 ~ 0 * 1 \n# Specify latent change scores variances \ndx2 ~~ 0 * dx2 \ndx3 ~~ 0 * dx3 \n# Specify constant change factor \ng2 =~ 1 * dx2 + 1 * dx3 \n# Specify constant change factor mean \ng2 ~ alpha_g2 * 1 \n# Specify constant change factor variance \ng2 ~~ sigma2_g2 * g2 \n# Specify constant change factor covariance with the initial true score \ng2 ~~ sigma_g2lx1 * lx1\n# Specify proportional change component \ndx2 ~ beta_x * lx1 \ndx3 ~ beta_x * lx2 \n# Specify autoregression of change score \ndx3 ~ phi_x * dx2 \n# # # # # # # # # # # # # # # # # # # # #\n# Specify parameters for construct y ----\n# # # # # # # # # # # # # # # # # # # # #\n# Specify latent true scores \nly1 =~ 1 * y1 \nly2 =~ 1 * y2 \nly3 =~ 1 * y3 \n# Specify mean of latent true scores \nly1 ~ gamma_ly1 * 1 \nly2 ~ 0 * 1 \nly3 ~ 0 * 1 \n# Specify variance of latent true scores \nly1 ~~ sigma2_ly1 * ly1 \nly2 ~~ 0 * ly2 \nly3 ~~ 0 * ly3 \n# Specify intercept of obseved scores \ny1 ~ 0 * 1 \ny2 ~ 0 * 1 \ny3 ~ 0 * 1 \n# Specify variance of observed scores \ny1 ~~ sigma2_uy * y1 \ny2 ~~ sigma2_uy * y2 \ny3 ~~ sigma2_uy * y3 \n# Specify autoregressions of latent variables \nly2 ~ 1 * ly1 \nly3 ~ 1 * ly2 \n# Specify latent change scores \ndy2 =~ 1 * ly2 \ndy3 =~ 1 * ly3 \n# Specify latent change scores means \ndy2 ~ 0 * 1 \ndy3 ~ 0 * 1 \n# Specify latent change scores variances \ndy2 ~~ 0 * dy2 \ndy3 ~~ 0 * dy3 \n# Specify constant change factor \nj2 =~ 1 * dy2 + 1 * dy3 \n# Specify constant change factor mean \nj2 ~ alpha_j2 * 1 \n# Specify constant change factor variance \nj2 ~~ sigma2_j2 * j2 \n# Specify constant change factor covariance with the initial true score \nj2 ~~ sigma_j2ly1 * ly1\n# Specify proportional change component \ndy2 ~ beta_y * ly1 \ndy3 ~ beta_y * ly2 \n# Specify autoregression of change score \ndy3 ~ phi_y * dy2 \n# Specify residual covariances \nx1 ~~ sigma_su * y1 \nx2 ~~ sigma_su * y2 \nx3 ~~ sigma_su * y3 \n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n# Specify covariances betweeen specified change components (alpha) and intercepts (initial latent true scores lx1 and ly1) ----\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n# Specify covariance of intercepts \nlx1 ~~ sigma_ly1lx1 * ly1 \n# Specify covariance of constant change and intercept between constructs \nly1 ~~ sigma_g2ly1 * g2 \n# Specify covariance of constant change and intercept between constructs \nlx1 ~~ sigma_j2lx1 * j2 \n# Specify covariance of constant change factors between constructs \ng2 ~~ sigma_j2g2 * j2 \n# # # # # # # # # # # # # # # # # # # # # # # # # # #\n# Specify between-construct coupling parameters ----\n# # # # # # # # # # # # # # # # # # # # # # # # # # #\n# Change score x (t) is determined by true score y (t-1)  \ndx2 ~ delta_lag_xy * ly1 \ndx3 ~ delta_lag_xy * ly2 \n# Change score y (t) is determined by true score x (t-1)  \ndy2 ~ delta_lag_yx * lx1 \ndy3 ~ delta_lag_yx * lx2",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-4",
    "href": "sem.html#fit-the-model-4",
    "title": "Structural Equation Modeling",
    "section": "8.2 Fit the Model",
    "text": "8.2 Fit the Model\n\n\nCode\nbivariateLCSM_fit &lt;- fit_bi_lcsm(\n  data = data_bi_lcsm,\n  var_x = names(data_bi_lcsm)[2:4],\n  var_y = names(data_bi_lcsm)[12:14],\n  model_x = list(\n    alpha_constant = TRUE, # alpha = intercept (constant change factor)\n    beta = TRUE, # beta = proportional change factor (latent true score predicting its change score)\n    phi = TRUE), # phi = autoregression of change scores\n  model_y = list(\n    alpha_constant = TRUE, # alpha = intercept (constant change factor)\n    beta = TRUE, # beta = proportional change factor (latent true score predicting its change score)\n    phi = TRUE), # phi = autoregression of change scores\n  coupling = list(\n    delta_lag_xy = TRUE,\n    xi_lag_yx = TRUE),\n  fixed.x = FALSE\n  )",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-4",
    "href": "sem.html#summary-output-4",
    "title": "Structural Equation Modeling",
    "section": "8.3 Summary Output",
    "text": "8.3 Summary Output\n\n\nCode\nsummary(\n  bivariateLCSM_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 151 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        32\n  Number of equality constraints                     9\n\n  Number of observations                           500\n  Number of missing patterns                        23\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 6.870       4.777\n  Degrees of freedom                                 4           4\n  P-value (Chi-square)                           0.143       0.311\n  Scaling correction factor                                  1.438\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1435.712    1483.655\n  Degrees of freedom                                15          15\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.968\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998       0.999\n  Tucker-Lewis Index (TLI)                       0.992       0.998\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.999\n  Robust Tucker-Lewis Index (TLI)                            0.997\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -2973.817   -2973.817\n  Scaling correction factor                                  0.639\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -2970.382   -2970.382\n  Scaling correction factor                                  0.971\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                5993.634    5993.634\n  Bayesian (BIC)                              6090.570    6090.570\n  Sample-size adjusted Bayesian (SABIC)       6017.567    6017.567\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038       0.020\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.085       0.065\n  P-value H_0: RMSEA &lt;= 0.050                    0.599       0.835\n  P-value H_0: RMSEA &gt;= 0.080                    0.073       0.009\n                                                                  \n  Robust RMSEA                                               0.024\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.094\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.637\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.114\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  lx1 =~                                                                \n    x1                1.000                               0.719    0.867\n  lx2 =~                                                                \n    x2                1.000                               1.069    0.933\n  lx3 =~                                                                \n    x3                1.000                               1.559    0.967\n  dx2 =~                                                                \n    lx2               1.000                               0.600    0.600\n  dx3 =~                                                                \n    lx3               1.000                               0.374    0.374\n  g2 =~                                                                 \n    dx2               1.000                               1.027    1.027\n    dx3               1.000                               1.128    1.128\n  ly1 =~                                                                \n    y1                1.000                               0.485    0.755\n  ly2 =~                                                                \n    y2                1.000                               0.506    0.769\n  ly3 =~                                                                \n    y3                1.000                               0.756    0.874\n  dy2 =~                                                                \n    ly2               1.000                               0.619    0.619\n  dy3 =~                                                                \n    ly3               1.000                               0.510    0.510\n  j2 =~                                                                 \n    dy2               1.000                               1.037    1.037\n    dy3               1.000                               0.844    0.844\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  lx2 ~                                                                 \n    lx1               1.000                               0.673    0.673\n  lx3 ~                                                                 \n    lx2               1.000                               0.685    0.685\n  dx2 ~                                                                 \n    lx1     (bt_x)   -0.115    0.026   -4.469    0.000   -0.129   -0.129\n  dx3 ~                                                                 \n    lx2     (bt_x)   -0.115    0.026   -4.469    0.000   -0.211   -0.211\n    dx2     (ph_x)    0.025    0.041    0.613    0.540    0.027    0.027\n  ly2 ~                                                                 \n    ly1               1.000                               0.958    0.958\n  ly3 ~                                                                 \n    ly2               1.000                               0.669    0.669\n  dy2 ~                                                                 \n    ly1     (bt_y)   -0.406    0.070   -5.778    0.000   -0.628   -0.628\n  dy3 ~                                                                 \n    ly2     (bt_y)   -0.406    0.069   -5.900    0.000   -0.533   -0.533\n    dy2     (ph_y)    0.501    0.115    4.371    0.000    0.407    0.407\n  dx2 ~                                                                 \n    ly1     (dl__)    0.055    0.109    0.506    0.613    0.042    0.042\n  dx3 ~                                                                 \n    ly2     (dl__)    0.055    0.109    0.506    0.613    0.048    0.048\n  dy3 ~                                                                 \n    dx2     (x_l_)    0.269    0.080    3.370    0.001    0.448    0.448\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  lx1 ~~                                                                \n    g2 (sgm_g2lx1)    0.156    0.030    5.149    0.000    0.328    0.328\n  ly1 ~~                                                                \n    j2 (sgm_j2ly1)    0.057    0.011    4.989    0.000    0.361    0.361\n .x1 ~~                                                                 \n   .y1      (sgm_)    0.011    0.009    1.275    0.202    0.011    0.063\n .x2 ~~                                                                 \n   .y2      (sgm_)    0.011    0.009    1.275    0.202    0.011    0.063\n .x3 ~~                                                                 \n   .y3      (sgm_)    0.011    0.009    1.275    0.202    0.011    0.063\n  lx1 ~~                                                                \n    l1      (s_11)    0.196    0.026    7.639    0.000    0.562    0.562\n  g2 ~~                                                                 \n    l1 (sgm_g2ly1)    0.075    0.029    2.569    0.010    0.235    0.235\n  lx1 ~~                                                                \n    j2 (sgm_j2lx1)    0.128    0.017    7.455    0.000    0.549    0.549\n  g2 ~~                                                                 \n    j2      (s_22)    0.040    0.019    2.132    0.033    0.187    0.187\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    lx1  (gmm_lx1)   21.079    0.038  559.652    0.000   29.307   29.307\n   .lx2               0.000                               0.000    0.000\n   .lx3               0.000                               0.000    0.000\n   .x1                0.000                               0.000    0.000\n   .x2                0.000                               0.000    0.000\n   .x3                0.000                               0.000    0.000\n   .dx2               0.000                               0.000    0.000\n   .dx3               0.000                               0.000    0.000\n    g2   (alph_g2)    0.273    0.008   35.188    0.000    0.415    0.415\n    ly1  (gmm_ly1)    5.027    0.030  167.731    0.000   10.373   10.373\n   .ly2               0.000                               0.000    0.000\n   .ly3               0.000                               0.000    0.000\n   .y1                0.000                               0.000    0.000\n   .y2                0.000                               0.000    0.000\n   .y3                0.000                               0.000    0.000\n   .dy2               0.000                               0.000    0.000\n   .dy3               0.000                               0.000    0.000\n    j2   (alph_j2)    0.848    0.376    2.256    0.024    2.609    2.609\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    lx1 (sgm2_lx1)    0.517    0.042   12.414    0.000    1.000    1.000\n   .lx2               0.000                               0.000    0.000\n   .lx3               0.000                               0.000    0.000\n   .x1    (sgm2_x)    0.171    0.011   15.607    0.000    0.171    0.248\n   .x2    (sgm2_x)    0.171    0.011   15.607    0.000    0.171    0.130\n   .x3    (sgm2_x)    0.171    0.011   15.607    0.000    0.171    0.066\n   .dx2               0.000                               0.000    0.000\n   .dx3               0.000                               0.000    0.000\n    g2   (sgm2_g2)    0.434    0.038   11.298    0.000    1.000    1.000\n    ly1 (sgm2_ly1)    0.235    0.028    8.532    0.000    1.000    1.000\n   .ly2               0.000                               0.000    0.000\n   .ly3               0.000                               0.000    0.000\n   .y1    (sgm2_y)    0.177    0.012   14.979    0.000    0.177    0.429\n   .y2    (sgm2_y)    0.177    0.012   14.979    0.000    0.177    0.408\n   .y3    (sgm2_y)    0.177    0.012   14.979    0.000    0.177    0.236\n   .dy2               0.000                               0.000    0.000\n   .dy3               0.000                               0.000    0.000\n    j2   (sgm2_j2)    0.106    0.016    6.759    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    lx2               1.000\n    lx3               1.000\n    x1                0.752\n    x2                0.870\n    x3                0.934\n    dx2               1.000\n    dx3               1.000\n    ly2               1.000\n    ly3               1.000\n    y1                0.571\n    y2                0.592\n    y3                0.764\n    dy2               1.000\n    dy3               1.000",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-4",
    "href": "sem.html#estimates-of-model-fit-4",
    "title": "Structural Equation Modeling",
    "section": "8.4 Estimates of Model Fit",
    "text": "8.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  bivariateLCSM_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n               6.870                4.000                0.143 \n        chisq.scaled            df.scaled        pvalue.scaled \n               4.777                4.000                0.311 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.438             1435.712               15.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.038                0.998 \n                 tli                 srmr         rmsea.robust \n               0.992                0.031                0.024 \n          cfi.robust           tli.robust \n               0.999                0.997",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-4",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-4",
    "title": "Structural Equation Modeling",
    "section": "8.5 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "8.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  bivariateLCSM_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       x1     x2     x3     y1     y2     y3\nx1  0.000                                   \nx2 -0.002  0.000                            \nx3 -0.002  0.001  0.000                     \ny1  0.031 -0.017  0.018  0.000              \ny2 -0.013 -0.035 -0.004 -0.001  0.000       \ny3  0.013  0.000  0.006  0.010 -0.006  0.000\n\n$mean\n    x1     x2     x3     y1     y2     y3 \n-0.001  0.001  0.000  0.000 -0.004  0.001",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-4",
    "href": "sem.html#modification-indices-4",
    "title": "Structural Equation Modeling",
    "section": "8.6 Modification Indices",
    "text": "8.6 Modification Indices\n\n\nCode\nmodificationindices(\n  bivariateLCSM_fit,\n  sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-4",
    "href": "sem.html#path-diagram-4",
    "title": "Structural Equation Modeling",
    "section": "8.7 Path Diagram",
    "text": "8.7 Path Diagram\n\n\nCode\nsemPaths(\n  bivariateLCSM_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nplot_lcsm(\n  lavaan_object = bivariateLCSM_fit,\n  lcsm = \"bivariate\",\n  lavaan_syntax = bivariateLCSM_syntax,\n  edge.label.cex = .9,\n  lcsm_colours = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n#lavaanPlot::lavaanPlot( # throws error\n#  bivariateLCSM_fit,\n#  coefs = TRUE,\n#  #covs = TRUE,\n#  stand = TRUE)\n\nlavaanPlot::lavaanPlot2(\n  bivariateLCSM_fit,\n  stand = TRUE,\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(bivariateLCSM_fit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#plot-trajectories-4",
    "href": "sem.html#plot-trajectories-4",
    "title": "Structural Equation Modeling",
    "section": "8.8 Plot Trajectories",
    "text": "8.8 Plot Trajectories\n\n\nCode\nplot_trajectories(\n  data_bi_lcsm,\n  id_var = \"id\",\n  var_list = c(\n    \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \n    \"x6\", \"x7\", \"x8\", \"x9\", \"x10\"),\n  xlab = \"Time\",\n  ylab = \"X Score\",\n  connect_missing = FALSE)\n\n\n\n\n\n\n\n\n\nCode\n  plot_trajectories(\n  data_bi_lcsm,\n  id_var = \"id\",\n  var_list = c(\n    \"y1\", \"y2\", \"y3\", \"y4\", \"y5\",\n    \"y6\", \"y7\", \"y8\", \"y9\", \"y10\"),\n  xlab = \"Time\",\n  ylab = \"Y Score\",\n  connect_missing = FALSE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-5",
    "href": "sem.html#model-syntax-5",
    "title": "Structural Equation Modeling",
    "section": "9.1 Model Syntax",
    "text": "9.1 Model Syntax\n\n\nCode\nclpm_syntax &lt;- '\n  # Autoregressive Paths\n  t4 ~ t3\n  t3 ~ t2\n  t2 ~ t1\n  \n  c4 ~ c3\n  c3 ~ c2\n  c2 ~ c1\n  \n  # Concurrent Covariances\n  t1 ~~ c1\n  t2 ~~ c2\n  t3 ~~ c3\n  t4 ~~ c4\n  \n  # Cross-Lagged Paths\n  t4 ~ c3\n  t3 ~ c2\n  t2 ~ c1\n  \n  c4 ~ t3\n  c3 ~ t2\n  c2 ~ t1\n'",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-5",
    "href": "sem.html#fit-the-model-5",
    "title": "Structural Equation Modeling",
    "section": "9.2 Fit the Model",
    "text": "9.2 Fit the Model\n\n\nCode\nclpm_fit &lt;- sem(\n  clpm_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  std.lv = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-5",
    "href": "sem.html#summary-output-5",
    "title": "Structural Equation Modeling",
    "section": "9.3 Summary Output",
    "text": "9.3 Summary Output\n\n\nCode\nsummary(\n  clpm_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 25 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        32\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                55.624      54.099\n  Degrees of freedom                                12          12\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.028\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1933.670    1953.262\n  Degrees of freedom                                28          28\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.990\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977       0.978\n  Tucker-Lewis Index (TLI)                       0.947       0.949\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.977\n  Robust Tucker-Lewis Index (TLI)                            0.947\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4885.800   -4885.800\n  Scaling correction factor                                  1.001\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4857.988   -4857.988\n  Scaling correction factor                                  1.008\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9835.601    9835.601\n  Bayesian (BIC)                              9963.328    9963.328\n  Sample-size adjusted Bayesian (SABIC)       9861.790    9861.790\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.095       0.094\n  90 Percent confidence interval - lower         0.071       0.069\n  90 Percent confidence interval - upper         0.121       0.119\n  P-value H_0: RMSEA &lt;= 0.050                    0.002       0.002\n  P-value H_0: RMSEA &gt;= 0.080                    0.856       0.832\n                                                                  \n  Robust RMSEA                                               0.095\n  90 Percent confidence interval - lower                     0.070\n  90 Percent confidence interval - upper                     0.121\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.002\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.849\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  t4 ~                                                                  \n    t3                1.183    0.023   51.960    0.000    1.183    0.954\n  t3 ~                                                                  \n    t2                1.135    0.031   36.564    0.000    1.135    0.885\n  t2 ~                                                                  \n    t1                1.040    0.047   21.910    0.000    1.040    0.773\n  c4 ~                                                                  \n    c3                0.063    0.051    1.227    0.220    0.063    0.063\n  c3 ~                                                                  \n    c2               -0.015    0.046   -0.319    0.750   -0.015   -0.015\n  c2 ~                                                                  \n    c1                0.081    0.046    1.761    0.078    0.081    0.084\n  t4 ~                                                                  \n    c3               -0.323    0.065   -4.935    0.000   -0.323   -0.089\n  t3 ~                                                                  \n    c2               -0.336    0.069   -4.838    0.000   -0.336   -0.117\n  t2 ~                                                                  \n    c1               -0.114    0.065   -1.749    0.080   -0.114   -0.053\n  c4 ~                                                                  \n    t3               -0.030    0.018   -1.717    0.086   -0.030   -0.089\n  c3 ~                                                                  \n    t2                0.053    0.022    2.377    0.017    0.053    0.121\n  c2 ~                                                                  \n    t1                0.009    0.029    0.328    0.743    0.009    0.016\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  t1 ~~                                                                 \n    c1                0.114    0.081    1.408    0.159    0.114    0.073\n .t2 ~~                                                                 \n   .c2                0.244    0.064    3.798    0.000    0.244    0.191\n .t3 ~~                                                                 \n   .c3                0.376    0.072    5.224    0.000    0.376    0.310\n .t4 ~~                                                                 \n   .c4                0.268    0.055    4.913    0.000    0.268    0.246\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t4                0.593    0.083    7.110    0.000    0.593    0.176\n   .t3                0.704    0.086    8.145    0.000    0.704    0.259\n   .t2                1.056    0.071   14.794    0.000    1.056    0.497\n   .c4                0.056    0.067    0.835    0.404    0.056    0.061\n   .c3               -0.021    0.060   -0.351    0.725   -0.021   -0.023\n   .c2                0.023    0.049    0.465    0.642    0.023    0.024\n    t1                0.595    0.079    7.531    0.000    0.595    0.377\n    c1                0.008    0.049    0.158    0.874    0.008    0.008\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t4                1.416    0.092   15.404    0.000    1.416    0.124\n   .t3                1.703    0.125   13.598    0.000    1.703    0.230\n   .t2                1.825    0.137   13.275    0.000    1.825    0.405\n   .c4                0.844    0.056   15.195    0.000    0.844    0.991\n   .c3                0.859    0.065   13.252    0.000    0.859    0.986\n   .c2                0.892    0.061   14.608    0.000    0.892    0.992\n    t1                2.494    0.185   13.450    0.000    2.494    1.000\n    c1                0.972    0.063   15.321    0.000    0.972    1.000\n\nR-Square:\n                   Estimate\n    t4                0.876\n    t3                0.770\n    t2                0.595\n    c4                0.009\n    c3                0.014\n    c2                0.008",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-5",
    "href": "sem.html#estimates-of-model-fit-5",
    "title": "Structural Equation Modeling",
    "section": "9.4 Estimates of Model Fit",
    "text": "9.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  clpm_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              55.624               12.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n              54.099               12.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.028             1933.670               28.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.095                0.977 \n                 tli                 srmr         rmsea.robust \n               0.947                0.029                0.095 \n          cfi.robust           tli.robust \n               0.977                0.947",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-5",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-5",
    "title": "Structural Equation Modeling",
    "section": "9.5 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "9.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  clpm_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t4     t3     t2     c4     c3     c2     t1     c1\nt4  0.000                                                 \nt3  0.000  0.000                                          \nt2  0.044  0.000  0.000                                   \nc4  0.000  0.000  0.030  0.000                            \nc3  0.000  0.000  0.000  0.000  0.000                     \nc2  0.005  0.000  0.000  0.036  0.000  0.000              \nt1  0.068  0.038  0.000  0.052  0.024  0.000  0.000       \nc1  0.048 -0.022  0.000  0.140 -0.032  0.000  0.000  0.000\n\n$mean\nt4 t3 t2 c4 c3 c2 t1 c1 \n 0  0  0  0  0  0  0  0",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-5",
    "href": "sem.html#modification-indices-5",
    "title": "Structural Equation Modeling",
    "section": "9.6 Modification Indices",
    "text": "9.6 Modification Indices\n\n\nCode\nmodificationindices(\n  clpm_fit,\n  sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-5",
    "href": "sem.html#path-diagram-5",
    "title": "Structural Equation Modeling",
    "section": "9.7 Path Diagram",
    "text": "9.7 Path Diagram\n\n\nCode\nsemPaths(\n  clpm_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  clpm_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  clpm_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(clpm_fit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-6",
    "href": "sem.html#model-syntax-6",
    "title": "Structural Equation Modeling",
    "section": "10.1 Model Syntax",
    "text": "10.1 Model Syntax\n\n10.1.1 Abbreviated\nAdapted from Mulder & Hamaker (2021): https://doi.org/10.1080/10705511.2020.1784738\nhttps://jeroendmulder.github.io/RI-CLPM/lavaan.html (archived at https://perma.cc/2K6A-WUJQ)\n\n\nCode\nriclpm1_syntax &lt;- '\n  # Random Intercepts\n  t =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  c =~ 1*c1 + 1*c2 + 1*c3 + 1*c4\n  \n  # Create Within-Person Centered Variables\n  wt1 =~ 1*t1\n  wt2 =~ 1*t2\n  wt3 =~ 1*t3\n  wt4 =~ 1*t4\n  \n  wc1 =~ 1*c1\n  wc2 =~ 1*c2\n  wc3 =~ 1*c3\n  wc4 =~ 1*c4\n  \n  # Autoregressive Paths\n  wt4 ~ wt3\n  wt3 ~ wt2\n  wt2 ~ wt1\n  \n  wc4 ~ wc3\n  wc3 ~ wc2\n  wc2 ~ wc1\n  \n  # Concurrent Covariances\n  wt1 ~~ wc1\n  wt2 ~~ wc2\n  wt3 ~~ wc3\n  wt4 ~~ wc4\n  \n  # Cross-Lagged Paths\n  wt4 ~ wc3\n  wt3 ~ wc2\n  wt2 ~ wc1\n  \n  wc4 ~ wt3\n  wc3 ~ wt2\n  wc2 ~ wt1\n  \n  # Variance and Covariance of Random Intercepts\n  t ~~ t\n  c ~~ c\n  t ~~ c\n  \n  # Variances of Within-Person Centered Variables\n  wt1 ~~ wt1\n  wt2 ~~ wt2\n  wt3 ~~ wt3\n  wt4 ~~ wt4\n  \n  wc1 ~~ wc1\n  wc2 ~~ wc2\n  wc3 ~~ wc3\n  wc4 ~~ wc4\n'\n\n\n\n\n10.1.2 Full\nAdapted from Mund & Nestler (2017): https://osf.io/a4dhk\n\n\nCode\nriclpm2_syntax &lt;- '\n  # Random Intercepts\n  t =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  c =~ 1*c1 + 1*c2 + 1*c3 + 1*c4\n  \n  # Create Within-Person Centered Variables\n  wt1 =~ 1*t1\n  wt2 =~ 1*t2\n  wt3 =~ 1*t3\n  wt4 =~ 1*t4\n  \n  wc1 =~ 1*c1\n  wc2 =~ 1*c2\n  wc3 =~ 1*c3\n  wc4 =~ 1*c4\n  \n  # Autoregressive Paths\n  wt4 ~ wt3\n  wt3 ~ wt2\n  wt2 ~ wt1\n  \n  wc4 ~ wc3\n  wc3 ~ wc2\n  wc2 ~ wc1\n  \n  # Concurrent Covariances\n  wt1 ~~ wc1\n  wt2 ~~ wc2\n  wt3 ~~ wc3\n  wt4 ~~ wc4\n  \n  # Cross-Lagged Paths\n  wt4 ~ wc3\n  wt3 ~ wc2\n  wt2 ~ wc1\n  \n  wc4 ~ wt3\n  wc3 ~ wt2\n  wc2 ~ wt1\n  \n  # Variance and Covariance of Random Intercepts\n  t ~~ t\n  c ~~ c\n  t ~~ c\n  \n  # Variances of Within-Person Centered Variables\n  wt1 ~~ wt1\n  wt2 ~~ wt2\n  wt3 ~~ wt3\n  wt4 ~~ wt4\n  \n  wc1 ~~ wc1\n  wc2 ~~ wc2\n  wc3 ~~ wc3\n  wc4 ~~ wc4\n  \n  # Fix Error Variances of Observed Variables to Zero\n  t1 ~~ 0*t1\n  t2 ~~ 0*t2\n  t3 ~~ 0*t3\n  t4 ~~ 0*t4\n  \n  c1 ~~ 0*c1\n  c2 ~~ 0*c2\n  c3 ~~ 0*c3\n  c4 ~~ 0*c4\n  \n  # Fix the Covariances Between the Random Intercepts and the Latents at T1 to Zero\n  wt1 ~~ 0*t\n  wt1 ~~ 0*c\n  \n  wc1 ~~ 0*t\n  wc1 ~~ 0*c\n  \n  # Estimate Observed Intercepts\n  t1 ~ 1\n  t2 ~ 1\n  t3 ~ 1\n  t4 ~ 1\n  \n  c1 ~ 1\n  c2 ~ 1\n  c3 ~ 1\n  c4 ~ 1\n  \n  # Fix the Means of the Latents to Zero\n  wt1 ~ 0*1\n  wt2 ~ 0*1\n  wt3 ~ 0*1\n  wt4 ~ 0*1\n  \n  wc1 ~ 0*1\n  wc2 ~ 0*1\n  wc3 ~ 0*1\n  wc4 ~ 0*1\n  \n  t ~ 0*1\n  c ~ 0*1\n'",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-6",
    "href": "sem.html#fit-the-model-6",
    "title": "Structural Equation Modeling",
    "section": "10.2 Fit the Model",
    "text": "10.2 Fit the Model\n\n10.2.1 Abbreviated\n\n\nCode\nriclpm1_fit &lt;- lavaan(\n  riclpm1_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  meanstructure = TRUE,\n  int.ov.free = TRUE,\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)\n\n\n\n\n10.2.2 Full\n\n\nCode\nriclpm2_fit &lt;- sem(\n  riclpm2_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-6",
    "href": "sem.html#summary-output-6",
    "title": "Structural Equation Modeling",
    "section": "10.3 Summary Output",
    "text": "10.3 Summary Output\n\n10.3.1 Abbreviated\n\n\nCode\nsummary(\n  riclpm1_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                39.156      38.001\n  Degrees of freedom                                 9           9\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.030\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1933.670    1953.262\n  Degrees of freedom                                28          28\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.990\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.984       0.985\n  Tucker-Lewis Index (TLI)                       0.951       0.953\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.984\n  Robust Tucker-Lewis Index (TLI)                            0.951\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4877.566   -4877.566\n  Scaling correction factor                                  1.003\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4857.988   -4857.988\n  Scaling correction factor                                  1.008\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9825.132    9825.132\n  Bayesian (BIC)                              9964.833    9964.833\n  Sample-size adjusted Bayesian (SABIC)       9853.776    9853.776\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092       0.090\n  90 Percent confidence interval - lower         0.063       0.062\n  90 Percent confidence interval - upper         0.122       0.120\n  P-value H_0: RMSEA &lt;= 0.050                    0.009       0.011\n  P-value H_0: RMSEA &gt;= 0.080                    0.766       0.737\n                                                                  \n  Robust RMSEA                                               0.091\n  90 Percent confidence interval - lower                     0.062\n  90 Percent confidence interval - upper                     0.122\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.011\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.756\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023       0.023\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  t =~                                                                  \n    t1                1.000                                  NA       NA\n    t2                1.000                                  NA       NA\n    t3                1.000                                  NA       NA\n    t4                1.000                                  NA       NA\n  c =~                                                                  \n    c1                1.000                               0.206    0.209\n    c2                1.000                               0.206    0.217\n    c3                1.000                               0.206    0.220\n    c4                1.000                               0.206    0.224\n  wt1 =~                                                                \n    t1                1.000                               2.318    1.462\n  wt2 =~                                                                \n    t2                1.000                               2.695    1.284\n  wt3 =~                                                                \n    t3                1.000                               3.219    1.175\n  wt4 =~                                                                \n    t4                1.000                               3.786    1.118\n  wc1 =~                                                                \n    c1                1.000                               0.965    0.978\n  wc2 =~                                                                \n    c2                1.000                               0.927    0.976\n  wc3 =~                                                                \n    c3                1.000                               0.915    0.976\n  wc4 =~                                                                \n    c4                1.000                               0.894    0.975\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  wt4 ~                                                                 \n    wt3               1.126    0.029   38.930    0.000    0.957    0.957\n  wt3 ~                                                                 \n    wt2               1.093    0.025   43.596    0.000    0.915    0.915\n  wt2 ~                                                                 \n    wt1               1.005    0.025   40.746    0.000    0.864    0.864\n  wc4 ~                                                                 \n    wc3               0.004    0.061    0.063    0.950    0.004    0.004\n  wc3 ~                                                                 \n    wc2              -0.047    0.056   -0.839    0.401   -0.048   -0.048\n  wc2 ~                                                                 \n    wc1               0.042    0.051    0.816    0.415    0.043    0.043\n  wt4 ~                                                                 \n    wc3              -0.291    0.076   -3.838    0.000   -0.070   -0.070\n  wt3 ~                                                                 \n    wc2              -0.326    0.074   -4.413    0.000   -0.094   -0.094\n  wt2 ~                                                                 \n    wc1              -0.113    0.068   -1.661    0.097   -0.041   -0.041\n  wc4 ~                                                                 \n    wt3              -0.021    0.020   -1.044    0.296   -0.075   -0.075\n  wc3 ~                                                                 \n    wt2               0.020    0.023    0.881    0.378    0.059    0.059\n  wc2 ~                                                                 \n    wt1              -0.010    0.024   -0.420    0.675   -0.025   -0.025\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  wt1 ~~                                                                \n    wc1              -0.007    0.136   -0.054    0.957   -0.003   -0.003\n .wt2 ~~                                                                \n   .wc2               0.242    0.064    3.775    0.000    0.193    0.193\n .wt3 ~~                                                                \n   .wc3               0.385    0.074    5.191    0.000    0.322    0.322\n .wt4 ~~                                                                \n   .wc4               0.238    0.058    4.071    0.000    0.219    0.219\n  t ~~                                                                  \n    c                 0.082    0.107    0.761    0.447    0.235    0.235\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.595    0.079    7.531    0.000    0.595    0.375\n   .t2                1.673    0.106   15.763    0.000    1.673    0.797\n   .t3                2.593    0.136   19.058    0.000    2.593    0.947\n   .t4                3.639    0.169   21.572    0.000    3.639    1.074\n   .c1                0.008    0.049    0.158    0.874    0.008    0.008\n   .c2                0.029    0.047    0.610    0.542    0.029    0.030\n   .c3                0.068    0.047    1.449    0.147    0.068    0.072\n   .c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    t                -2.861    1.350   -2.119    0.034       NA       NA\n    c                 0.042    0.026    1.637    0.102    1.000    1.000\n    wt1               5.373    1.412    3.804    0.000    1.000    1.000\n   .wt2               1.829    0.138   13.209    0.000    0.252    0.252\n   .wt3               1.719    0.127   13.564    0.000    0.166    0.166\n   .wt4               1.484    0.097   15.331    0.000    0.103    0.103\n    wc1               0.931    0.067   13.826    0.000    1.000    1.000\n   .wc2               0.856    0.065   13.256    0.000    0.997    0.997\n   .wc3               0.832    0.070   11.817    0.000    0.995    0.995\n   .wc4               0.795    0.061   13.110    0.000    0.994    0.994\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .c1                0.000                               0.000    0.000\n   .c2                0.000                               0.000    0.000\n   .c3                0.000                               0.000    0.000\n   .c4                0.000                               0.000    0.000\n\nR-Square:\n                   Estimate\n    wt2               0.748\n    wt3               0.834\n    wt4               0.897\n    wc2               0.003\n    wc3               0.005\n    wc4               0.006\n    t1                1.000\n    t2                1.000\n    t3                1.000\n    t4                1.000\n    c1                1.000\n    c2                1.000\n    c3                1.000\n    c4                1.000\n\n\n\n\n10.3.2 Full\n\n\nCode\nsummary(\n  riclpm2_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 63 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                39.156      38.001\n  Degrees of freedom                                 9           9\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.030\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1933.670    1953.262\n  Degrees of freedom                                28          28\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.990\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.984       0.985\n  Tucker-Lewis Index (TLI)                       0.951       0.953\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.984\n  Robust Tucker-Lewis Index (TLI)                            0.951\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4877.566   -4877.566\n  Scaling correction factor                                  1.003\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4857.988   -4857.988\n  Scaling correction factor                                  1.008\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9825.132    9825.132\n  Bayesian (BIC)                              9964.833    9964.833\n  Sample-size adjusted Bayesian (SABIC)       9853.776    9853.776\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092       0.090\n  90 Percent confidence interval - lower         0.063       0.062\n  90 Percent confidence interval - upper         0.122       0.120\n  P-value H_0: RMSEA &lt;= 0.050                    0.009       0.011\n  P-value H_0: RMSEA &gt;= 0.080                    0.766       0.737\n                                                                  \n  Robust RMSEA                                               0.091\n  90 Percent confidence interval - lower                     0.062\n  90 Percent confidence interval - upper                     0.122\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.011\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.756\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.023       0.023\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  t =~                                                                  \n    t1                1.000                                  NA       NA\n    t2                1.000                                  NA       NA\n    t3                1.000                                  NA       NA\n    t4                1.000                                  NA       NA\n  c =~                                                                  \n    c1                1.000                               0.206    0.209\n    c2                1.000                               0.206    0.217\n    c3                1.000                               0.206    0.220\n    c4                1.000                               0.206    0.224\n  wt1 =~                                                                \n    t1                1.000                               2.318    1.462\n  wt2 =~                                                                \n    t2                1.000                               2.695    1.284\n  wt3 =~                                                                \n    t3                1.000                               3.219    1.175\n  wt4 =~                                                                \n    t4                1.000                               3.786    1.118\n  wc1 =~                                                                \n    c1                1.000                               0.965    0.978\n  wc2 =~                                                                \n    c2                1.000                               0.927    0.976\n  wc3 =~                                                                \n    c3                1.000                               0.915    0.976\n  wc4 =~                                                                \n    c4                1.000                               0.894    0.975\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  wt4 ~                                                                 \n    wt3               1.126    0.029   38.930    0.000    0.957    0.957\n  wt3 ~                                                                 \n    wt2               1.093    0.025   43.596    0.000    0.915    0.915\n  wt2 ~                                                                 \n    wt1               1.005    0.025   40.746    0.000    0.864    0.864\n  wc4 ~                                                                 \n    wc3               0.004    0.061    0.063    0.950    0.004    0.004\n  wc3 ~                                                                 \n    wc2              -0.047    0.056   -0.839    0.401   -0.048   -0.048\n  wc2 ~                                                                 \n    wc1               0.042    0.051    0.816    0.415    0.043    0.043\n  wt4 ~                                                                 \n    wc3              -0.291    0.076   -3.838    0.000   -0.070   -0.070\n  wt3 ~                                                                 \n    wc2              -0.326    0.074   -4.413    0.000   -0.094   -0.094\n  wt2 ~                                                                 \n    wc1              -0.113    0.068   -1.661    0.097   -0.041   -0.041\n  wc4 ~                                                                 \n    wt3              -0.021    0.020   -1.044    0.296   -0.075   -0.075\n  wc3 ~                                                                 \n    wt2               0.020    0.023    0.881    0.378    0.059    0.059\n  wc2 ~                                                                 \n    wt1              -0.010    0.024   -0.420    0.675   -0.025   -0.025\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  wt1 ~~                                                                \n    wc1              -0.007    0.136   -0.054    0.957   -0.003   -0.003\n .wt2 ~~                                                                \n   .wc2               0.242    0.064    3.775    0.000    0.193    0.193\n .wt3 ~~                                                                \n   .wc3               0.385    0.074    5.191    0.000    0.322    0.322\n .wt4 ~~                                                                \n   .wc4               0.238    0.058    4.071    0.000    0.219    0.219\n  t ~~                                                                  \n    c                 0.082    0.107    0.761    0.447    0.235    0.235\n    wt1               0.000                               0.000    0.000\n  c ~~                                                                  \n    wt1               0.000                               0.000    0.000\n  t ~~                                                                  \n    wc1               0.000                               0.000    0.000\n  c ~~                                                                  \n    wc1               0.000                               0.000    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.595    0.079    7.531    0.000    0.595    0.375\n   .t2                1.673    0.106   15.763    0.000    1.673    0.797\n   .t3                2.593    0.136   19.058    0.000    2.593    0.947\n   .t4                3.639    0.169   21.572    0.000    3.639    1.074\n   .c1                0.008    0.049    0.158    0.874    0.008    0.008\n   .c2                0.029    0.047    0.610    0.542    0.029    0.030\n   .c3                0.068    0.047    1.449    0.147    0.068    0.072\n   .c4               -0.018    0.046   -0.390    0.696   -0.018   -0.020\n    wt1               0.000                               0.000    0.000\n   .wt2               0.000                               0.000    0.000\n   .wt3               0.000                               0.000    0.000\n   .wt4               0.000                               0.000    0.000\n    wc1               0.000                               0.000    0.000\n   .wc2               0.000                               0.000    0.000\n   .wc3               0.000                               0.000    0.000\n   .wc4               0.000                               0.000    0.000\n    t                 0.000                                  NA       NA\n    c                 0.000                               0.000    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    t                -2.861    1.350   -2.119    0.034       NA       NA\n    c                 0.042    0.026    1.637    0.102    1.000    1.000\n    wt1               5.373    1.412    3.804    0.000    1.000    1.000\n   .wt2               1.829    0.138   13.209    0.000    0.252    0.252\n   .wt3               1.719    0.127   13.564    0.000    0.166    0.166\n   .wt4               1.484    0.097   15.331    0.000    0.103    0.103\n    wc1               0.931    0.067   13.826    0.000    1.000    1.000\n   .wc2               0.856    0.065   13.256    0.000    0.997    0.997\n   .wc3               0.832    0.070   11.817    0.000    0.995    0.995\n   .wc4               0.795    0.061   13.110    0.000    0.994    0.994\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .c1                0.000                               0.000    0.000\n   .c2                0.000                               0.000    0.000\n   .c3                0.000                               0.000    0.000\n   .c4                0.000                               0.000    0.000\n\nR-Square:\n                   Estimate\n    wt2               0.748\n    wt3               0.834\n    wt4               0.897\n    wc2               0.003\n    wc3               0.005\n    wc4               0.006\n    t1                1.000\n    t2                1.000\n    t3                1.000\n    t4                1.000\n    c1                1.000\n    c2                1.000\n    c3                1.000\n    c4                1.000",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-6",
    "href": "sem.html#estimates-of-model-fit-6",
    "title": "Structural Equation Modeling",
    "section": "10.4 Estimates of Model Fit",
    "text": "10.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  riclpm1_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              39.156                9.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n              38.001                9.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.030             1933.670               28.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.092                0.984 \n                 tli                 srmr         rmsea.robust \n               0.951                0.023                0.091 \n          cfi.robust           tli.robust \n               0.984                0.951",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-6",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-6",
    "title": "Structural Equation Modeling",
    "section": "10.5 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "10.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  riclpm1_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     c1     c2     c3     c4\nt1  0.000                                                 \nt2  0.007  0.000                                          \nt3  0.012 -0.004  0.000                                   \nt4  0.005  0.023  0.000  0.000                            \nc1  0.025  0.018 -0.009  0.062  0.000                     \nc2  0.003  0.000  0.000  0.005 -0.001  0.000              \nc3 -0.013  0.008  0.008  0.009 -0.074 -0.005  0.000       \nc4  0.026  0.003 -0.020 -0.013  0.090 -0.014  0.001  0.000\n\n$mean\nt1 t2 t3 t4 c1 c2 c3 c4 \n 0  0  0  0  0  0  0  0",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-6",
    "href": "sem.html#modification-indices-6",
    "title": "Structural Equation Modeling",
    "section": "10.6 Modification Indices",
    "text": "10.6 Modification Indices\n\n\nCode\nmodificationindices(\n  riclpm1_fit,\n  sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#internal-consistency-reliability-4",
    "href": "sem.html#internal-consistency-reliability-4",
    "title": "Structural Equation Modeling",
    "section": "10.7 Internal Consistency Reliability",
    "text": "10.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(riclpm1_fit)\n\n\n     t      c \n-0.544  0.166",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-6",
    "href": "sem.html#path-diagram-6",
    "title": "Structural Equation Modeling",
    "section": "10.8 Path Diagram",
    "text": "10.8 Path Diagram\n\n\nCode\nsemPaths(\n  riclpm1_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  riclpm1_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  riclpm1_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(riclpm1_fit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-7",
    "href": "sem.html#model-syntax-7",
    "title": "Structural Equation Modeling",
    "section": "11.1 Model Syntax",
    "text": "11.1 Model Syntax\nAdapted from Mund & Nestler (2017): https://osf.io/a4dhk\n\n\nCode\nlcmsr_syntax &lt;- '\n  # Define intercept and growth factors\n  intercept.t =~ 1*t1 + 1*t2 + 1*t3 + 1*t4\n  slope.t =~ 0*t1 + 1*t2 + 2*t3 + 3*t4\n  \n  intercept.c =~ 1*c1 + 1*c2 + 1*c3 + 1*c4\n  slope.c =~ 0*c1 + 1*c2 + 2*c3 + 3*c4\n  \n  # Define phantom latent variables\n  e.t1 =~ 1*t1\n  e.t2 =~ 1*t2\n  e.t3 =~ 1*t3\n  e.t4 =~ 1*t4\n  \n  e.c1 =~ 1*c1\n  e.c2 =~ 1*c2\n  e.c3 =~ 1*c3\n  e.c4 =~ 1*c4\n  \n  # Autoregressive paths\n  e.t2 ~ a1*e.t1\n  e.t3 ~ a1*e.t2\n  e.t4 ~ a1*e.t3\n  \n  e.c2 ~ a2*e.c1\n  e.c3 ~ a2*e.c2\n  e.c4 ~ a2*e.c3\n  \n  # Cross-lagged paths\n  e.c2 ~ c1*e.t1\n  e.c3 ~ c1*e.t2\n  e.c4 ~ c1*e.t3\n  \n  e.t2 ~ c2*e.c1\n  e.t3 ~ c2*e.c2\n  e.t4 ~ c2*e.c3\n  \n  # Some further constraints on the variance structure\n  # 1. Set error variances of the observed variables to zero\n  t1 ~~ 0*t1\n  t2 ~~ 0*t2\n  t3 ~~ 0*t3\n  t4 ~~ 0*t4\n  \n  c1 ~~ 0*c1\n  c2 ~~ 0*c2\n  c3 ~~ 0*c3\n  c4 ~~ 0*c4\n  \n  # 2. Let lavaan estimate the variance of the latent variables (residuals)\n  e.t1 ~~ vart1*e.t1\n  e.t2 ~~ vart2*e.t2\n  e.t3 ~~ vart3*e.t3\n  e.t4 ~~ vart4*e.t4\n  \n  e.c1 ~~ varc1*e.c1\n  e.c2 ~~ varc2*e.c2\n  e.c3 ~~ varc3*e.c3\n  e.c4 ~~ varc4*e.c4\n  \n  # 3. We also want estimates of the intercept factor variances, the slope\n  #    variances, and the covariances\n  intercept.t ~~ varintercept.t*intercept.t\n  intercept.c ~~ varintercept.c*intercept.c\n  slope.t ~~ varslope.t*slope.t\n  slope.c ~~ varslope.c*slope.c\n  \n  intercept.t ~~ covintercept*intercept.c\n  slope.t ~~ covslope*slope.c\n  \n  intercept.t ~~ covintercept.tslope.t*slope.t\n  intercept.t ~~ covintercept.tslope.c*slope.c\n  intercept.c ~~ covintercept.cslope.t*slope.t\n  intercept.c ~~ covintercept.cslope.c*slope.c\n  \n  # 4. We have to define that the covariance between the intercepts and\n  #    the slopes and the latents of the first time point are zero\n  e.t1 ~~ 0*intercept.t\n  e.c1 ~~ 0*intercept.t\n  e.t1 ~~ 0*slope.t\n  e.c1 ~~ 0*slope.t\n  e.t1 ~~ 0*intercept.c\n  e.c1 ~~ 0*intercept.c\n  e.t1 ~~ 0*slope.c\n  e.c1 ~~ 0*slope.c\n  \n  # 5. Finally, we estimate the covariance between the latents of x and y\n  #    of the first time point, the second time-point and so on. Note that\n  #    for the second to fourth time point the correlation is constrained to\n  #    the same value\n  e.t1 ~~ cov1*e.c1\n  e.t2 ~~ e1*e.c2\n  e.t3 ~~ e1*e.c3\n  e.t4 ~~ e1*e.c4\n  \n  # The model also contains a mean structure and we have to define some\n  # constraints for this part of the model. The assumption is that we\n  # only want estimates of the mean of the intercept factors. All other means\n  # are defined to be zero:\n  t1 ~ 0*1\n  t2 ~ 0*1\n  t3 ~ 0*1\n  t4 ~ 0*1\n  \n  c1 ~ 0*1\n  c2 ~ 0*1\n  c3 ~ 0*1\n  c4 ~ 0*1\n  \n  e.t1 ~ 0*1\n  e.t2 ~ 0*1\n  e.t3 ~ 0*1\n  e.t4 ~ 0*1\n  \n  e.c1 ~ 0*1\n  e.c2 ~ 0*1\n  e.c3 ~ 0*1\n  e.c4 ~ 0*1\n  \n  intercept.t ~ 1\n  intercept.c ~ 1\n  slope.t ~ 1\n  slope.c ~ 1\n'",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-7",
    "href": "sem.html#fit-the-model-7",
    "title": "Structural Equation Modeling",
    "section": "11.2 Fit the Model",
    "text": "11.2 Fit the Model\n\n\nCode\nlcmsr_fit &lt;- sem(\n  lcmsr_syntax,\n  data = Demo.growth,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  fixed.x = FALSE,\n  em.h1.iter.max = 100000)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-7",
    "href": "sem.html#summary-output-7",
    "title": "Structural Equation Modeling",
    "section": "11.3 Summary Output",
    "text": "11.3 Summary Output\n\n\nCode\nsummary(\n  lcmsr_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 68 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n  Number of equality constraints                    10\n\n  Number of observations                           400\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                34.165      34.572\n  Degrees of freedom                                16          16\n  P-value (Chi-square)                           0.005       0.005\n  Scaling correction factor                                  0.988\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              1933.670    1953.262\n  Degrees of freedom                                28          28\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  0.990\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.990       0.990\n  Tucker-Lewis Index (TLI)                       0.983       0.983\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.990\n  Robust Tucker-Lewis Index (TLI)                            0.983\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -4875.071   -4875.071\n  Scaling correction factor                                  0.751\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -4857.988   -4857.988\n  Scaling correction factor                                  1.008\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                                9806.141    9806.141\n  Bayesian (BIC)                              9917.902    9917.902\n  Sample-size adjusted Bayesian (SABIC)       9829.056    9829.056\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053       0.054\n  90 Percent confidence interval - lower         0.028       0.029\n  90 Percent confidence interval - upper         0.078       0.079\n  P-value H_0: RMSEA &lt;= 0.050                    0.380       0.365\n  P-value H_0: RMSEA &gt;= 0.080                    0.037       0.041\n                                                                  \n  Robust RMSEA                                               0.053\n  90 Percent confidence interval - lower                     0.028\n  90 Percent confidence interval - upper                     0.078\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.380\n  P-value H_0: Robust RMSEA &gt;= 0.080                         0.037\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.047       0.047\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  intercept.t =~                                                        \n    t1                1.000                               1.314    0.827\n    t2                1.000                               1.314    0.623\n    t3                1.000                               1.314    0.483\n    t4                1.000                               1.314    0.389\n  slope.t =~                                                            \n    t1                0.000                               0.000    0.000\n    t2                1.000                               0.740    0.351\n    t3                2.000                               1.480    0.544\n    t4                3.000                               2.220    0.657\n  intercept.c =~                                                        \n    c1                1.000                                  NA       NA\n    c2                1.000                                  NA       NA\n    c3                1.000                                  NA       NA\n    c4                1.000                                  NA       NA\n  slope.c =~                                                            \n    c1                0.000                                  NA       NA\n    c2                1.000                                  NA       NA\n    c3                2.000                                  NA       NA\n    c4                3.000                                  NA       NA\n  e.t1 =~                                                               \n    t1                1.000                               0.894    0.562\n  e.t2 =~                                                               \n    t2                1.000                               0.892    0.423\n  e.t3 =~                                                               \n    t3                1.000                               0.852    0.313\n  e.t4 =~                                                               \n    t4                1.000                               0.794    0.235\n  e.c1 =~                                                               \n    c1                1.000                               1.021    1.041\n  e.c2 =~                                                               \n    c2                1.000                               0.964    1.007\n  e.c3 =~                                                               \n    c3                1.000                               0.943    1.009\n  e.c4 =~                                                               \n    c4                1.000                               0.967    1.052\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  e.t2 ~                                                                \n    e.t1      (a1)    0.145    0.096    1.511    0.131    0.145    0.145\n  e.t3 ~                                                                \n    e.t2      (a1)    0.145    0.096    1.511    0.131    0.152    0.152\n  e.t4 ~                                                                \n    e.t3      (a1)    0.145    0.096    1.511    0.131    0.156    0.156\n  e.c2 ~                                                                \n    e.c1      (a2)    0.056    0.080    0.697    0.486    0.059    0.059\n  e.c3 ~                                                                \n    e.c2      (a2)    0.056    0.080    0.697    0.486    0.057    0.057\n  e.c4 ~                                                                \n    e.c3      (a2)    0.056    0.080    0.697    0.486    0.054    0.054\n  e.c2 ~                                                                \n    e.t1      (c1)    0.027    0.075    0.361    0.718    0.025    0.025\n  e.c3 ~                                                                \n    e.t2      (c1)    0.027    0.075    0.361    0.718    0.026    0.026\n  e.c4 ~                                                                \n    e.t3      (c1)    0.027    0.075    0.361    0.718    0.024    0.024\n  e.t2 ~                                                                \n    e.c1      (c2)    0.023    0.066    0.354    0.723    0.027    0.027\n  e.t3 ~                                                                \n    e.c2      (c2)    0.023    0.066    0.354    0.723    0.026    0.026\n  e.t4 ~                                                                \n    e.c3      (c2)    0.023    0.066    0.354    0.723    0.028    0.028\n\nCovariances:\n                           Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  intercept.t ~~                                                       \n                    (cvnt)   -0.117    0.137   -0.854    0.393   -0.313\n  slope.t ~~                                                           \n                    (cvsl)   -0.049    0.028   -1.743    0.081   -0.348\n  intercept.t ~~                                                       \n         (cvntrcpt.tslp.t)    0.688    0.091    7.525    0.000    0.707\n         (cvntrcpt.tslp.c)    0.068    0.056    1.224    0.221    0.274\n  slope.t ~~                                                           \n         (cvntrcpt.cslp.t)    0.095    0.055    1.728    0.084    0.454\n  intercept.c ~~                                                       \n         (cvntrcpt.cslp.c)    0.052    0.064    0.821    0.412    0.971\n  intercept.t ~~                                                       \n                              0.000                               0.000\n                              0.000                               0.000\n  slope.t ~~                                                           \n                              0.000                               0.000\n                              0.000                               0.000\n  intercept.c ~~                                                       \n                              0.000                               0.000\n                              0.000                               0.000\n  slope.c ~~                                                           \n                              0.000                               0.000\n                              0.000                               0.000\n  e.t1 ~~                                                              \n                    (cov1)    0.275    0.138    1.993    0.046    0.301\n .e.t2 ~~                                                              \n   .                  (e1)    0.317    0.054    5.859    0.000    0.373\n .e.t3 ~~                                                              \n   .                  (e1)    0.317    0.054    5.859    0.000    0.400\n .e.t4 ~~                                                              \n   .                  (e1)    0.317    0.054    5.859    0.000    0.419\n  Std.all\n         \n   -0.313\n         \n   -0.348\n         \n    0.707\n    0.274\n         \n    0.454\n         \n    0.971\n         \n    0.000\n    0.000\n         \n    0.000\n    0.000\n         \n    0.000\n    0.000\n         \n    0.000\n    0.000\n         \n    0.301\n         \n    0.373\n         \n    0.400\n         \n    0.419\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t1                0.000                               0.000    0.000\n   .t2                0.000                               0.000    0.000\n   .t3                0.000                               0.000    0.000\n   .t4                0.000                               0.000    0.000\n   .c1                0.000                               0.000    0.000\n   .c2                0.000                               0.000    0.000\n   .c3                0.000                               0.000    0.000\n   .c4                0.000                               0.000    0.000\n    e.t1              0.000                               0.000    0.000\n   .e.t2              0.000                               0.000    0.000\n   .e.t3              0.000                               0.000    0.000\n   .e.t4              0.000                               0.000    0.000\n    e.c1              0.000                               0.000    0.000\n   .e.c2              0.000                               0.000    0.000\n   .e.c3              0.000                               0.000    0.000\n   .e.c4              0.000                               0.000    0.000\n    intercept.t       0.612    0.077    7.962    0.000    0.465    0.465\n    intercept.c       0.027    0.040    0.661    0.509       NA       NA\n    slope.t           1.008    0.042   24.209    0.000    1.362    1.362\n    slope.c          -0.003    0.020   -0.166    0.868       NA       NA\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .t                 0.000                               0.000    0.000\n   .t                 0.000                               0.000    0.000\n   .t                 0.000                               0.000    0.000\n   .t                 0.000                               0.000    0.000\n   .c                 0.000                               0.000    0.000\n   .c                 0.000                               0.000    0.000\n   .c                 0.000                               0.000    0.000\n   .c                 0.000                               0.000    0.000\n    e       (vrt1)    0.799    0.193    4.137    0.000    1.000    1.000\n   .e       (vrt2)    0.777    0.083    9.416    0.000    0.976    0.976\n   .e       (vrt3)    0.706    0.101    6.979    0.000    0.973    0.973\n   .e       (vrt4)    0.613    0.121    5.054    0.000    0.972    0.972\n    e       (vrc1)    1.043    0.184    5.680    0.000    1.000    1.000\n   .e       (vrc2)    0.925    0.081   11.425    0.000    0.995    0.995\n   .e       (vrc3)    0.885    0.088   10.060    0.000    0.995    0.995\n   .e       (vrc4)    0.931    0.130    7.180    0.000    0.995    0.995\n    i (vrntrcpt.t)    1.727    0.224    7.708    0.000    1.000    1.000\n    i (vrntrcpt.c)   -0.080    0.166   -0.484    0.628       NA       NA\n    s    (vrslp.t)    0.548    0.065    8.464    0.000    1.000    1.000\n    s    (vrslp.c)   -0.036    0.038   -0.956    0.339       NA       NA\n\nR-Square:\n                   Estimate\n    t1                1.000\n    t2                1.000\n    t3                1.000\n    t4                1.000\n    c1                1.000\n    c2                1.000\n    c3                1.000\n    c4                1.000\n    e.t2              0.024\n    e.t3              0.027\n    e.t4              0.028\n    e.c2              0.005\n    e.c3              0.005\n    e.c4              0.005",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-7",
    "href": "sem.html#estimates-of-model-fit-7",
    "title": "Structural Equation Modeling",
    "section": "11.4 Estimates of Model Fit",
    "text": "11.4 Estimates of Model Fit\n\n\nCode\nfitMeasures(\n  lcmsr_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n\n               chisq                   df               pvalue \n              34.165               16.000                0.005 \n        chisq.scaled            df.scaled        pvalue.scaled \n              34.572               16.000                0.005 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               0.988             1933.670               28.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.053                0.990 \n                 tli                 srmr         rmsea.robust \n               0.983                0.047                0.053 \n          cfi.robust           tli.robust \n               0.990                0.983",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-7",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-7",
    "title": "Structural Equation Modeling",
    "section": "11.5 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "11.5 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(\n  lcmsr_fit,\n  type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n       t1     t2     t3     t4     c1     c2     c3     c4\nt1  0.000                                                 \nt2  0.013  0.000                                          \nt3 -0.005 -0.004  0.000                                   \nt4  0.001  0.001 -0.001  0.000                            \nc1 -0.028 -0.017 -0.061 -0.011  0.000                     \nc2  0.029 -0.026 -0.042 -0.026  0.046  0.000              \nc3  0.100  0.090  0.120  0.127 -0.065 -0.071  0.000       \nc4 -0.063 -0.061 -0.082 -0.070  0.055  0.007  0.014  0.000\n\n$mean\n    t1     t2     t3     t4     c1     c2     c3     c4 \n-0.011  0.025 -0.013  0.001 -0.019  0.006  0.051 -0.038",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-7",
    "href": "sem.html#modification-indices-7",
    "title": "Structural Equation Modeling",
    "section": "11.6 Modification Indices",
    "text": "11.6 Modification Indices\n\n\nCode\nmodificationindices(\n  lcmsr_fit,\n  sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#internal-consistency-reliability-5",
    "href": "sem.html#internal-consistency-reliability-5",
    "title": "Structural Equation Modeling",
    "section": "11.7 Internal Consistency Reliability",
    "text": "11.7 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(lcmsr_fit)\n\n\nintercept.t     slope.t intercept.c     slope.c \n      0.328       0.314      -0.315      -0.469",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-7",
    "href": "sem.html#path-diagram-7",
    "title": "Structural Equation Modeling",
    "section": "11.8 Path Diagram",
    "text": "11.8 Path Diagram\n\n\nCode\nsemPaths(\n  lcmsr_fit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\n#lavaanPlot::lavaanPlot( # throws error\n#  lcmsr_fit,\n#  coefs = TRUE,\n#  #covs = TRUE,\n#  stand = TRUE)\n\nlavaanPlot::lavaanPlot2(\n  lcmsr_fit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(lcmsr_fit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#overview-1",
    "href": "sem.html#overview-1",
    "title": "Structural Equation Modeling",
    "section": "12.1 Overview",
    "text": "12.1 Overview\nIt is important not to just consider mediation effects where there is a bivariate association between the predictor and the outcome. Sometimes inconsistent mediation occurs where the indirect effect has an opposite sign (i.e., positive or negative) from the direct or total effect. The idea is there there may be multiple mediating mechanisms, and that the predictor/cause may have both beneficial and harmful effects on the outcome, and that the mediating mechanisms can “cancel each other out” in terms of the total effect. For instance, the predictor may help via mechanism A, but may hurt via mechanism B. In this case, the total effect may be weak or small, even though there may be strong mediating effects.",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-8",
    "href": "sem.html#model-syntax-8",
    "title": "Structural Equation Modeling",
    "section": "12.2 Model Syntax",
    "text": "12.2 Model Syntax\n\n\nCode\nmediationModel &lt;- '\n  # direct effect (cPrime)\n  Y ~ direct*X\n  \n  # mediator\n  M ~ a*X\n  Y ~ b*M\n  \n  # indirect effect = a*b\n  indirect := a*b\n  \n  # total effect (c)\n  total := direct + indirect\n  totalAbs := abs(direct) + abs(indirect)\n  \n  # proportion mediated\n  Pm := abs(indirect) / totalAbs\n'",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-8",
    "href": "sem.html#fit-the-model-8",
    "title": "Structural Equation Modeling",
    "section": "12.3 Fit the Model",
    "text": "12.3 Fit the Model\nTo get a robust estimate of the indirect effect, we obtain bootstrapped estimates from 1,000 bootstrap draws. Typically, we would obtain bootstrapped estimates from 10,000 bootstrap draws, but this example uses only 1,000 bootstrap draws for a shorter runtime.\n\n\nCode\nmediationFit &lt;- sem(\n  mediationModel,\n  data = mydata,\n  se = \"bootstrap\",\n  bootstrap = 1000, # generally use 10,000 bootstrap draws; this example uses 1,000 for speed\n  parallel = \"multicore\", # parallelization for speed: use \"multicore\" for Mac/Linux; \"snow\" for PC\n  iseed = 52242, # for reproducibility\n  missing = \"ML\",\n  estimator = \"ML\",\n  # std.lv = TRUE, # for models with latent variables\n  fixed.x = FALSE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-8",
    "href": "sem.html#summary-output-8",
    "title": "Structural Equation Modeling",
    "section": "12.4 Summary Output",
    "text": "12.4 Summary Output\n\n\nCode\nsummary(\n  mediationFit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 4 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           100\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                                79.768\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -394.296\n  Loglikelihood unrestricted model (H1)       -394.296\n                                                      \n  Akaike (AIC)                                 806.592\n  Bayesian (BIC)                               830.039\n  Sample-size adjusted Bayesian (SABIC)        801.614\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050                NA\n  P-value H_0: Robust RMSEA &gt;= 0.080                NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Y ~                                                                   \n    X       (drct)   -0.045    0.107   -0.423    0.672   -0.045   -0.038\n  M ~                                                                   \n    X          (a)    0.568    0.090    6.328    0.000    0.568    0.549\n  Y ~                                                                   \n    M          (b)    0.714    0.118    6.075    0.000    0.714    0.616\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Y                 0.028    0.098    0.287    0.774    0.028    0.024\n   .M                -0.072    0.084   -0.857    0.392   -0.072   -0.073\n    X                -0.173    0.096   -1.811    0.070   -0.173   -0.181\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Y                 0.850    0.124    6.841    0.000    0.850    0.644\n   .M                 0.686    0.083    8.262    0.000    0.686    0.699\n    X                 0.916    0.129    7.095    0.000    0.916    1.000\n\nR-Square:\n                   Estimate\n    Y                 0.356\n    M                 0.301\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    indirect          0.406    0.094    4.323    0.000    0.406    0.338\n    total             0.361    0.108    3.354    0.001    0.361    0.300\n    totalAbs          0.451    0.130    3.476    0.001    0.451    0.376\n    Pm                0.900    0.104    8.680    0.000    0.900    0.900",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#parameter-estimates",
    "href": "sem.html#parameter-estimates",
    "title": "Structural Equation Modeling",
    "section": "12.5 Parameter Estimates",
    "text": "12.5 Parameter Estimates\n\n12.5.1 Bias-Corrected Bootstrap\nAdjusted bootstrap percentile (BCa) method, but with no correction for acceleration (only for bias):\n\n\nCode\nmediationFit_estimates_bca &lt;- parameterEstimates(\n  mediationFit,\n  boot.ci.type = \"bca.simple\",\n  standardized = TRUE)\n\nmediationFit_estimates &lt;- mediationFit_estimates_bca\n\nmediationFit_estimates_bca\n\n\n\n  \n\n\n\n\n\n12.5.2 Percentile Bootstrap\n\n\nCode\nmediationFit_estimates_perc &lt;- parameterEstimates(\n  mediationFit,\n  boot.ci.type = \"perc\",\n  standardized = TRUE)\n\nmediationFit_estimates_perc",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#indirect-effect",
    "href": "sem.html#indirect-effect",
    "title": "Structural Equation Modeling",
    "section": "12.6 Indirect Effect",
    "text": "12.6 Indirect Effect\n\n12.6.1 Parameter Estimate\nBias-Corrected Bootstrap:\n\n\nCode\nmediationFit_estimates_bca %&gt;% \n  filter(label == \"indirect\")\n\n\n\n  \n\n\n\nPercentile Bootstrap:\n\n\nCode\nmediationFit_estimates_perc %&gt;% \n  filter(label == \"indirect\")\n\n\n\n  \n\n\n\n\n\n12.6.2 Effect Size\n\n12.6.2.1 Standardized Estimate (\\(\\beta\\))\n\\[\n\\beta(ab) = ab \\cdot \\frac{SD_\\text{Y}}{SD_\\text{X}}\n\\]\n\n\nCode\nmediationFit_indirect &lt;- mediationFit_estimates %&gt;% \n  filter(label == \"indirect\") %&gt;% \n  select(std.all) %&gt;% \n  as.numeric\n\nmediationFit_indirect\n\n\n[1] 0.3380996\n\n\n\n\n12.6.2.2 Proportion Mediated (PM)\n\\[\nP_M = \\frac{|ab|}{|c|} = \\frac{|ab|}{|c'| + |ab|}\n\\]\nEffect size: Proportion mediated (PM); i.e., the proportion of the total effect that is mediated; calculated by the magnitude of the indirect effect divided by the magnitude of the total effect:\n\n\nCode\nmediationFit_total &lt;- mediationFit_estimates %&gt;% \n  filter(label == \"totalAbs\") %&gt;% \n  select(std.all) %&gt;% \n  as.numeric\n\nmediationFit_pm &lt;- abs(mediationFit_indirect) / abs(mediationFit_total)\nmediationFit_pm\n\n\n[1] 0.8998773\n\n\nIn this case, the direct effect and indirect effect have opposite signs (negative and positive, respectively). This is called inconsistent mediation, and can render the estimate of proportion mediated not a meaningful estimate of effect size (in this case, the estimate would exceed 1.0; Fairchild & McDaniel, 2017). To address this, we can use the absolute value of the direct and indirect effects (in computation of the total effect), as recommended by MacKinnon et al. (2007). When using the absolute value of the direct and indirect effects, the proportion mediated (PM) is 0.90, indicating that the mediator explained 90% of the total effect.\n\n\n12.6.2.3 Proportion of Variance in Y That is Explained by the Indirect Effect (R2mediated)\nFormulas from Lachowicz et al. (2018):\n\\[\n\\begin{aligned}\n  R^2_\\text{mediated} &= r^2_{\\text{MY}} - (R^2_{\\text{Y} \\cdot \\text{MX}} - r^2_{\\text{XY}}) \\\\\n  &= (\\beta^2_{\\text{YM} \\cdot \\text{X}} + \\beta_{\\text{YX} \\cdot \\text{M}} \\cdot \\beta_{\\text{MX}}) ^2 - [\\beta^2_{\\text{YX}} + \\beta^2_{\\text{YM} \\cdot \\text{X}}(1 - \\beta^2_{\\text{MX}}) - \\beta^2_{\\text{YX}}]\n\\end{aligned}\n\\]\n\n\nCode\nrXY &lt;- as.numeric(cor.test(\n  ~ X + Y,\n  data = mydata\n)$estimate)\n\nrMY &lt;- as.numeric(cor.test(\n  ~ M + Y,\n  data = mydata\n)$estimate)\n\nRsquaredYmx &lt;- summary(lm(\n  Y ~ M + X,\n  data = mydata))$r.squared\n\nRsquaredMed1 &lt;- (rMY^2) - (RsquaredYmx - (rXY^2))\nRsquaredMed1\n\n\n[1] 0.08930037\n\n\nCode\nbetaYMgivenX &lt;- mediationFit_estimates %&gt;% \n  filter(label == \"b\") %&gt;% \n  select(std.all) %&gt;% \n  as.numeric\n\nbetaYXgivenM &lt;- mediationFit_estimates %&gt;% \n  filter(label == \"direct\") %&gt;% \n  select(std.all) %&gt;% \n  as.numeric\n\nbetaMX &lt;- mediationFit_estimates %&gt;% \n  filter(label == \"a\") %&gt;% \n  select(std.all) %&gt;% \n  as.numeric\n\nbetaYX &lt;- as.numeric(cor.test(\n  ~ X + Y,\n  data = mydata\n)$estimate)\n\nRsquaredMed2 &lt;- ((betaYMgivenX + (betaYXgivenM * betaMX))^2) - ((betaYX^2) + (betaYMgivenX^2)*(1 - (betaMX^2)) - (betaYX^2))\nRsquaredMed2\n\n\n[1] 0.08930037\n\n\n\n\n12.6.2.4 The Proportion of Variance in Y That is Accounted for Jointly by M and X (upsilon; \\(v\\))\nFormulas from Lachowicz et al. (2018):\n\\[\n\\begin{aligned}\n  v &= (r_{\\text{YM}} - \\beta_{\\text{MX}} \\cdot \\beta^2_{\\text{YX} \\cdot \\text{M}}) ^ 2 - (R^2_{\\text{Y} \\cdot \\text{MX}} - r^2_{\\text{YX}})\\\\\n  &= \\beta^2_a \\cdot \\beta^2_b\n\\end{aligned}\n\\]\nwhere \\(a\\) is the \\(a\\) path (\\(\\beta^2_{\\text{MX}}\\)), and \\(b\\) is the \\(b\\) path (\\(\\beta^2_{\\text{YM} \\cdot \\text{X}}\\)).\nThe estimate corrects for spurious correlation induced by the ordering of variables.\n\n\nCode\nupsilon1 &lt;- ((rMY - (betaMX * (betaYXgivenM^2)))^2) - (RsquaredYmx - (rXY^2))\nupsilon1\n\n\n[1] 0.08837615\n\n\nCode\nupsilon2 &lt;- (betaYMgivenX^2) - (RsquaredYmx - (rXY^2))\nupsilon2\n\n\n[1] 0.1143113\n\n\nCode\nupsilon3 &lt;- mediationFit_indirect ^ 2\nupsilon3\n\n\n[1] 0.1143113\n\n\nCode\nupsilon(\n  x = mydata$X,\n  mediator = mydata$M,\n  dv = mydata$Y,\n  bootstrap = FALSE\n)\n\n\n\n  \n\n\n\n\n\n12.6.2.5 Ratio of the Indirect Effect Relative to Its Maximum Possible Value in the Data (\\(\\kappa^2\\))\n\\[\n\\kappa^2 = \\frac{ab}{\\text{MAX}(ab)}\n\\]\nKappa-squared (\\(\\kappa^2\\)) is the ratio of the indirect effect relative to its maximum possible value in the data given the observed variability of X, Y, and M and their intercorrelations in the data. This estimate is no longer recommended (Wen & Fan, 2015).\n\n\n12.6.2.6 Other Effect Sizes\n\n\nCode\nmediation(\n  x = mydata$X,\n  mediator = mydata$M,\n  dv = mydata$Y,\n  bootstrap = FALSE\n)\n\n\n$Y.on.X\n$Y.on.X$Regression.Table\n                Estimate Std. Error    t value   p(&gt;|t|) Low Conf Limit\nIntercept.Y_X -0.0234265  0.1124386 -0.2083494 0.8353886     -0.2465572\nc (Regressor)  0.3605967  0.1156225  3.1187424 0.0023850      0.1311477\n              Up Conf Limit\nIntercept.Y_X     0.1997041\nc (Regressor)     0.5900458\n\n$Y.on.X$Model.Fit\n       Residual standard error (RMSE) numerator df denomenator df F-Statistic\nValues                        1.10643            1             98    9.726554\n       p-value (F)        R^2    Adj R^2 Low Conf Limit Up Conf Limit\nValues    0.002385 0.09028929 0.08100653     0.01207068     0.2183844\n\n\n$M.on.X\n$M.on.X$Regression.Table\n                 Estimate Std. Error    t value          p(&gt;|t|) Low Conf Limit\nIntercept.M_X -0.07206805 0.08501733 -0.8476865 0.39867814281309     -0.2407822\na (Regressor)  0.56815370 0.08742477  6.4987723 0.00000000339244      0.3946621\n              Up Conf Limit\nIntercept.M_X    0.09664608\na (Regressor)    0.74164532\n\n$M.on.X$Model.Fit\n       Residual standard error (RMSE) numerator df denomenator df F-Statistic\nValues                      0.8365964            1             98    42.23404\n            p-value (F)       R^2   Adj R^2 Low Conf Limit Up Conf Limit\nValues 0.00000000339244 0.3011683 0.2940373      0.1546027     0.4498597\n\n\n$Y.on.X.and.M\n$Y.on.X.and.M$Regression.Table\n                       Estimate Std. Error    t value          p(&gt;|t|)\nIntercept.Y_XM       0.02804007 0.09547198  0.2936994 0.76961507960580\nc.prime (Regressor) -0.04514372 0.11701196 -0.3858043 0.70048641908141\nb (Mediator)         0.71413854 0.11302357  6.3184922 0.00000000802134\n                    Low Conf Limit Up Conf Limit\nIntercept.Y_XM          -0.1614454     0.2175255\nc.prime (Regressor)     -0.2773801     0.1870926\nb (Mediator)             0.4898180     0.9384590\n\n$Y.on.X.and.M$Model.Fit\n       Residual standard error (RMSE) numerator df denomenator df F-Statistic\nValues                      0.9360479            2             97    26.75653\n              p-value (F)       R^2   Adj R^2 Low Conf Limit Up Conf Limit\nValues 0.0000000005572898 0.3555377 0.3422498      0.1958319     0.4955139\n\n\n$Effect.Sizes\n                                              [,1]\nIndirect.Effect                         0.40574046\nIndirect.Effect.Partially.Standardized  0.35154486\nIndex.of.Mediation                      0.33809959\nR2_4.5                                  0.08930037\nR2_4.6                                  0.08781296\nR2_4.7                                  0.24698638\nRatio.of.Indirect.to.Total.Effect       1.12519172\nRatio.of.Indirect.to.Direct.Effect     -8.98774887\nSuccess.of.Surrogate.Endpoint           0.63468166\nResidual.Based_Gamma                    0.08153354\nResidual.Based.Standardized_gamma       0.08679941\nSOS                                     0.98904723",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-8",
    "href": "sem.html#estimates-of-model-fit-8",
    "title": "Structural Equation Modeling",
    "section": "12.7 Estimates of Model Fit",
    "text": "12.7 Estimates of Model Fit\nThe model is saturated because it has as many estimated parameters as there are data points (i.e., in terms of means, variances, and covariances), so it has zero degrees of freedom. Because the model is saturated, it has “perfect” fit.\n\n\nCode\nfitMeasures(\n  mediationFit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\"))\n\n\n          chisq              df          pvalue  baseline.chisq     baseline.df \n          0.000           0.000              NA          79.768           3.000 \nbaseline.pvalue           rmsea             cfi             tli            srmr \n          0.000           0.000           1.000           1.000           0.000",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-8",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-8",
    "title": "Structural Equation Modeling",
    "section": "12.8 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "12.8 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(mediationFit, type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n  Y M X\nY 0    \nM 0 0  \nX 0 0 0\n\n$mean\nY M X \n0 0 0",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-8",
    "href": "sem.html#modification-indices-8",
    "title": "Structural Equation Modeling",
    "section": "12.9 Modification Indices",
    "text": "12.9 Modification Indices\n\n\nCode\nmodificationindices(mediationFit, sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#internal-consistency-reliability-6",
    "href": "sem.html#internal-consistency-reliability-6",
    "title": "Structural Equation Modeling",
    "section": "12.10 Internal Consistency Reliability",
    "text": "12.10 Internal Consistency Reliability\n\n\nCode\ncompRelSEM(mediationFit)\n\n\nnamed numeric(0)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-8",
    "href": "sem.html#path-diagram-8",
    "title": "Structural Equation Modeling",
    "section": "12.11 Path Diagram",
    "text": "12.11 Path Diagram\n\n\nCode\nsemPaths(\n  mediationFit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  mediationFit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  mediationFit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(mediationFit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#preparing-the-predictors",
    "href": "sem.html#preparing-the-predictors",
    "title": "Structural Equation Modeling",
    "section": "13.1 Preparing the Predictors",
    "text": "13.1 Preparing the Predictors\nMake sure to mean-center or orthogonalize predictors before computing the interaction term.\n\n13.1.1 Mean Center Predictors\n\n\nCode\nstates$Illiteracy_centered &lt;- scale(states$Illiteracy, scale = FALSE)\nstates$Murder_centered &lt;- scale(states$Murder, scale = FALSE)\n\n\n\n\n13.1.2 Orthogonalized Predictors\nOrthogonalizing is residual centering.\n\n\nCode\nstates$interaction_notCentered &lt;- states$Illiteracy * states$Murder\n\nstates$Illiteracy_orthogonalized &lt;- resid(lm(\n  data = states,\n  interaction_notCentered ~ Illiteracy\n))\n\nstates$Murder_orthogonalized &lt;- resid(lm(\n  data = states,\n  interaction_notCentered ~ Murder\n))",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#compute-interaction-term",
    "href": "sem.html#compute-interaction-term",
    "title": "Structural Equation Modeling",
    "section": "13.2 Compute Interaction Term",
    "text": "13.2 Compute Interaction Term\n\n\nCode\nstates$interaction &lt;- states$Illiteracy_centered * states$Murder_centered # or: states$Illiteracy_orthogonalized * states$Murder_orthogonalized",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#model-syntax-9",
    "href": "sem.html#model-syntax-9",
    "title": "Structural Equation Modeling",
    "section": "13.3 Model Syntax",
    "text": "13.3 Model Syntax\n\n\nCode\nmoderationModel &lt;- '\n  Income_rescaled ~ Illiteracy_centered + Murder_centered + interaction + HS.Grad\n'",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#fit-the-model-9",
    "href": "sem.html#fit-the-model-9",
    "title": "Structural Equation Modeling",
    "section": "13.4 Fit the Model",
    "text": "13.4 Fit the Model\n\n\nCode\nmoderationFit &lt;- sem(\n  moderationModel,\n  data = states,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  std.lv = TRUE,\n  fixed.x = FALSE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#summary-output-9",
    "href": "sem.html#summary-output-9",
    "title": "Structural Equation Modeling",
    "section": "13.5 Summary Output",
    "text": "13.5 Summary Output\n\n\nCode\nsummary(\n  moderationFit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                            50\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                                33.312      29.838\n  Degrees of freedom                                 4           4\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.116\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                         1.000\n  Robust Tucker-Lewis Index (TLI)                            1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -570.333    -570.333\n  Loglikelihood unrestricted model (H1)       -570.333    -570.333\n                                                                  \n  Akaike (AIC)                                1180.666    1180.666\n  Bayesian (BIC)                              1218.907    1218.907\n  Sample-size adjusted Bayesian (SABIC)       1156.130    1156.130\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000          NA\n  90 Percent confidence interval - lower         0.000          NA\n  90 Percent confidence interval - upper         0.000          NA\n  P-value H_0: RMSEA &lt;= 0.050                       NA          NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA          NA\n                                                                  \n  Robust RMSEA                                               0.000\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050                            NA\n  P-value H_0: Robust RMSEA &gt;= 0.080                            NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nRegressions:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Income_rescaled ~                                                      \n    Illitrcy_cntrd     0.371    1.881    0.197    0.844    0.371    0.037\n    Murder_centerd     0.171    0.245    0.696    0.486    0.171    0.103\n    interaction       -0.970    0.254   -3.823    0.000   -0.970   -0.355\n    HS.Grad            0.408    0.149    2.727    0.006    0.408    0.536\n\nCovariances:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Illiteracy_centered ~~                                                      \n    Murder_centerd          1.550    0.315    4.924    0.000    1.550    0.703\n    interaction             0.733    0.323    2.271    0.023    0.733    0.546\n    HS.Grad                -3.171    0.711   -4.459    0.000   -3.171   -0.657\n  Murder_centered ~~                                                          \n    interaction             2.223    1.620    1.372    0.170    2.223    0.273\n    HS.Grad               -14.259    4.049   -3.522    0.000  -14.259   -0.488\n  interaction ~~                                                              \n    HS.Grad                -7.938    2.987   -2.657    0.008   -7.938   -0.446\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Income_rescald   24.215    7.858    3.081    0.002   24.215    3.981\n    Illitrcy_cntrd   -0.000    0.085   -0.000    1.000   -0.000   -0.000\n    Murder_centerd   -0.000    0.517   -0.000    1.000   -0.000   -0.000\n    interaction       1.550    0.315    4.924    0.000    1.550    0.696\n    HS.Grad          53.108    1.131   46.966    0.000   53.108    6.642\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Income_rescald   19.006    4.200    4.525    0.000   19.006    0.514\n    Illitrcy_cntrd    0.364    0.066    5.534    0.000    0.364    1.000\n    Murder_centerd   13.355    1.754    7.614    0.000   13.355    1.000\n    interaction       4.956    1.462    3.390    0.001    4.956    1.000\n    HS.Grad          63.933    9.944    6.429    0.000   63.933    1.000\n\nR-Square:\n                   Estimate\n    Income_rescald    0.486",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#estimates-of-model-fit-9",
    "href": "sem.html#estimates-of-model-fit-9",
    "title": "Structural Equation Modeling",
    "section": "13.6 Estimates of Model Fit",
    "text": "13.6 Estimates of Model Fit\nThe model is saturated because it has as many estimated parameters as there are data points (i.e., in terms of means, variances, and covariances), so it has zero degrees of freedom. Because the model is saturated, it has “perfect” fit.\n\n\nCode\nfitMeasures(\n  moderationFit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\"))\n\n\n          chisq              df          pvalue  baseline.chisq     baseline.df \n          0.000           0.000              NA          33.312           4.000 \nbaseline.pvalue           rmsea             cfi             tli            srmr \n          0.000           0.000           1.000           1.000           0.000",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-9",
    "href": "sem.html#residuals-of-observed-vs.-model-implied-correlation-matrix-9",
    "title": "Structural Equation Modeling",
    "section": "13.7 Residuals of Observed vs. Model-Implied Correlation Matrix",
    "text": "13.7 Residuals of Observed vs. Model-Implied Correlation Matrix\n\n\nCode\nresiduals(moderationFit, type = \"cor\")\n\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                    Incm_r Illtr_ Mrdr_c intrct HS.Grd\nIncome_rescaled          0                            \nIlliteracy_centered      0      0                     \nMurder_centered          0      0      0              \ninteraction              0      0      0      0       \nHS.Grad                  0      0      0      0      0\n\n$mean\n    Income_rescaled Illiteracy_centered     Murder_centered         interaction \n                  0                   0                   0                   0 \n            HS.Grad \n                  0",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#modification-indices-9",
    "href": "sem.html#modification-indices-9",
    "title": "Structural Equation Modeling",
    "section": "13.8 Modification Indices",
    "text": "13.8 Modification Indices\n\n\nCode\nmodificationindices(moderationFit, sort. = TRUE)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#path-diagram-9",
    "href": "sem.html#path-diagram-9",
    "title": "Structural Equation Modeling",
    "section": "13.9 Path Diagram",
    "text": "13.9 Path Diagram\n\n\nCode\nsemPaths(\n  moderationFit,\n  what = \"Std.all\",\n  layout = \"tree2\",\n  edge.label.cex = 1.5)\n\n\n\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot(\n  moderationFit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nCode\nlavaanPlot::lavaanPlot2(\n  moderationFit,\n  #stand = TRUE, # currently throws error; uncomment out when fixed: https://github.com/alishinski/lavaanPlot/issues/52\n  coef_labels = TRUE)\n\n\n\n\n\n\nTo generate an interactive/modifiable path diagram, you can use the following syntax:\n\n\nCode\nlavaangui::plot_lavaan(moderationFit)",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#moderationInteractionPlot",
    "href": "sem.html#moderationInteractionPlot",
    "title": "Structural Equation Modeling",
    "section": "13.10 Interaction Plot",
    "text": "13.10 Interaction Plot\n\n\nCode\n# Created Model-Implied Predicted Data Object\nmodelImpliedPredictedData &lt;- expand.grid(\n  Illiteracy_factor = c(\"Low\",\"Middle\",\"High\"),\n  Murder_factor = c(\"Low\",\"Middle\",\"High\"))\n\nIlliteracy_mean &lt;- mean(states$Illiteracy, na.rm = TRUE)\nIlliteracy_sd &lt;- sd(states$Illiteracy, na.rm = TRUE)\n\nMurder_mean &lt;- mean(states$Murder, na.rm = TRUE)\nMurder_sd &lt;- sd(states$Murder, na.rm = TRUE)\n\nIlliteracy_centered_mean &lt;- mean(states$Illiteracy_centered, na.rm = TRUE)\nIlliteracy_centered_sd &lt;- sd(states$Illiteracy_centered, na.rm = TRUE)\n\nMurder_centered_mean &lt;- mean(states$Murder_centered, na.rm = TRUE)\nMurder_centered_sd &lt;- sd(states$Murder_centered, na.rm = TRUE)\n\nmodelImpliedPredictedData &lt;- modelImpliedPredictedData %&gt;%\n  mutate(\n    Illiteracy = case_when(\n      Illiteracy_factor == \"Low\" ~ Illiteracy_mean - Illiteracy_sd,\n      Illiteracy_factor == \"Middle\" ~ Illiteracy_mean,\n      Illiteracy_factor == \"High\" ~ Illiteracy_mean + Illiteracy_sd\n    ),\n    Illiteracy_centered = case_when(\n      Illiteracy_factor == \"Low\" ~ Illiteracy_centered_mean - Illiteracy_centered_sd,\n      Illiteracy_factor == \"Middle\" ~ Illiteracy_centered_mean,\n      Illiteracy_factor == \"High\" ~ Illiteracy_centered_mean + Illiteracy_centered_sd\n    ),\n    Murder = case_when(\n      Murder_factor == \"Low\" ~ Murder_mean - Murder_sd,\n      Murder_factor == \"Middle\" ~ Murder_mean,\n      Murder_factor == \"High\" ~ Murder_mean + Murder_sd\n    ),\n    Murder_centered = case_when(\n      Murder_factor == \"Low\" ~ Murder_centered_mean - Murder_centered_sd,\n      Murder_factor == \"Middle\" ~ Murder_centered_mean,\n      Murder_factor == \"High\" ~ Murder_centered_mean + Murder_centered_sd\n    ),\n    interaction = Illiteracy_centered * Murder_centered,\n    HS.Grad = mean(states$HS.Grad, na.rm = TRUE), # mean for covariates\n    Income_rescaled = NA\n  )\n\nMurder_labels &lt;- factor(\n  modelImpliedPredictedData$Murder_factor,\n  levels = c(\"High\", \"Middle\", \"Low\"),\n  labels = c(\"High (+1 SD)\", \"Middle (mean)\", \"Low (−1 SD)\"))\n\nmodelImpliedPredictedData$Income_rescaled &lt;- lavPredictY(\n  moderationFit,\n  newdata = modelImpliedPredictedData,\n  ynames = \"Income_rescaled\"\n) %&gt;% \n  as.vector()\n\n# Verify Computation Manually\nmoderationFit_parameters &lt;- parameterEstimates(moderationFit)\n\nmoderationFit_parameters\n\n\n\n  \n\n\n\nCode\nintercept &lt;- moderationFit_parameters[which(moderationFit_parameters$lhs == \"Income_rescaled\" & moderationFit_parameters$op == \"~1\"), \"est\"]\nb_Illiteracy_centered &lt;- moderationFit_parameters[which(moderationFit_parameters$lhs == \"Income_rescaled\" & moderationFit_parameters$rhs == \"Illiteracy_centered\"), \"est\"]\nb_Murder_centered &lt;- moderationFit_parameters[which(moderationFit_parameters$lhs == \"Income_rescaled\" & moderationFit_parameters$rhs == \"Murder_centered\"), \"est\"]\nb_interaction &lt;- moderationFit_parameters[which(moderationFit_parameters$lhs == \"Income_rescaled\" & moderationFit_parameters$rhs == \"interaction\"), \"est\"]\nb_HS.Grad &lt;- moderationFit_parameters[which(moderationFit_parameters$lhs == \"Income_rescaled\" & moderationFit_parameters$rhs == \"HS.Grad\"), \"est\"]\n\nmodelImpliedPredictedData &lt;- modelImpliedPredictedData %&gt;%\n  mutate(\n    Income_rescaled_calculatedManually = intercept + (b_Illiteracy_centered * Illiteracy_centered) + (b_Murder_centered * Murder_centered) + (b_interaction * interaction) + (b_HS.Grad * HS.Grad))\n\n# Model-Implied Predicted Data\nmodelImpliedPredictedData\n\n\n\n  \n\n\n\nCode\n# Plot\nggplot(\n  data = modelImpliedPredictedData,\n  mapping = aes(\n    x = Illiteracy,\n    y = Income_rescaled,\n    color = Murder_labels\n  )\n) +\n  geom_line() +\n  labs(color = \"Murder\")",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#moderationRegionsOfSignificance",
    "href": "sem.html#moderationRegionsOfSignificance",
    "title": "Structural Equation Modeling",
    "section": "13.11 Simple Slopes and Regions of Significance",
    "text": "13.11 Simple Slopes and Regions of Significance\nhttps://gabriellajg.github.io/EPSY-579-R-Cookbook-for-SEM/week6_1-lavaan-lab-4-mediated-moderation-moderated-mediation.html#step-5-johnson-neyman-interval (archived at https://perma.cc/6XR6-ZPSL)\n\n\nCode\n# Find the min and max values of the moderator\nMurder_centered_min &lt;- min(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\nMurder_centered_max &lt;- max(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\n\nMurder_centered_cutoff1 &lt;- -1.5 # pick and titrate cutoff to help find the lower bound of the region of significance\nMurder_centered_cutoff2 &lt;- -1 # pick and titrate cutoff to help find the upper bound of the region of significance\n\nMurder_centered_sd &lt;- sd(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\n\nMurder_centered_low &lt;- mean(modelImpliedPredictedData$Murder_centered, na.rm = TRUE) - sd(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\nMurder_centered_mean &lt;- mean(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\nMurder_centered_high &lt;- mean(modelImpliedPredictedData$Murder_centered, na.rm = TRUE) + sd(modelImpliedPredictedData$Murder_centered, na.rm = TRUE)\n\n# Extend the moderation model to compute the simple slopes and conditional effects at specific values of the moderator\nmoderationModelSimpleSlopes &lt;- paste0('\n  # Regression\n  Income_rescaled ~ b1*Illiteracy_centered + b2*Murder_centered + b3*interaction + b4*HS.Grad\n  \n  # Simple Slopes\n  SS_min := b1 + b3 * ', Murder_centered_min, '\n  SS_cutoff1 := b1 + b3 * ', Murder_centered_cutoff1, '\n  SS_cutoff2 := b1 + b3 * ', Murder_centered_cutoff2, '\n  SS_low := b1 + b3 * ', Murder_centered_low, '\n  SS_mean := b1 + b3 * ', Murder_centered_mean, '\n  SS_high := b1 + b3 * ', Murder_centered_high, '\n  SS_max := b1 + b3 * ', Murder_centered_max, '\n')\n\n# Fit the Model\nset.seed(52242) # for reproducibility\n\nmoderationModelSimpleSlopes_fit &lt;- sem(\n  model = moderationModelSimpleSlopes, \n  data = states,\n  missing = \"ML\",\n  estimator = \"ML\",\n  se = \"bootstrap\",\n  bootstrap = 1000,\n  std.lv = TRUE,\n  fixed.x = FALSE)\n\nsummary(\n  moderationModelSimpleSlopes_fit,\n  #fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                            50\n  Number of missing patterns                         1\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Income_rescaled ~                                                      \n    Illtrcy_c (b1)     0.371    2.266    0.164    0.870    0.371    0.037\n    Mrdr_cntr (b2)     0.171    0.271    0.630    0.528    0.171    0.103\n    interactn (b3)    -0.970    0.307   -3.157    0.002   -0.970   -0.355\n    HS.Grad   (b4)     0.408    0.162    2.521    0.012    0.408    0.536\n\nCovariances:\n                         Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Illiteracy_centered ~~                                                      \n    Murder_centerd          1.550    0.312    4.963    0.000    1.550    0.703\n    interaction             0.733    0.320    2.290    0.022    0.733    0.546\n    HS.Grad                -3.171    0.700   -4.532    0.000   -3.171   -0.657\n  Murder_centered ~~                                                          \n    interaction             2.223    1.596    1.393    0.164    2.223    0.273\n    HS.Grad               -14.259    3.925   -3.633    0.000  -14.259   -0.488\n  interaction ~~                                                              \n    HS.Grad                -7.938    2.988   -2.656    0.008   -7.938   -0.446\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Income_rescald   24.215    8.505    2.847    0.004   24.215    3.981\n    Illitrcy_cntrd   -0.000    0.087   -0.000    1.000   -0.000   -0.000\n    Murder_centerd   -0.000    0.524   -0.000    1.000   -0.000   -0.000\n    interaction       1.550    0.312    4.969    0.000    1.550    0.696\n    HS.Grad          53.108    1.124   47.235    0.000   53.108    6.642\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .Income_rescald   19.006    3.928    4.838    0.000   19.006    0.514\n    Illitrcy_cntrd    0.364    0.065    5.586    0.000    0.364    1.000\n    Murder_centerd   13.355    1.780    7.501    0.000   13.355    1.000\n    interaction       4.956    1.441    3.439    0.001    4.956    1.000\n    HS.Grad          63.933    9.923    6.443    0.000   63.933    1.000\n\nR-Square:\n                   Estimate\n    Income_rescald    0.486\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    SS_min            3.953    2.924    1.352    0.176    3.953    1.348\n    SS_cutoff1        1.827    2.492    0.733    0.464    1.827    0.570\n    SS_cutoff2        1.342    2.409    0.557    0.578    1.342    0.392\n    SS_low            3.473    2.818    1.233    0.218    3.473    1.172\n    SS_mean           0.371    2.266    0.164    0.870    0.371    0.037\n    SS_high          -2.731    2.063   -1.324    0.185   -2.731   -1.099\n    SS_max           -3.211    2.071   -1.550    0.121   -3.211   -1.274\n\n\nCode\nmoderationModelSimpleSlopesFit_parameters &lt;- parameterEstimates(\n  moderationModelSimpleSlopes_fit,\n  level = 0.95,\n  boot.ci.type = \"bca.simple\")\n\nmoderationModelSimpleSlopesFit_parameters\n\n\n\n  \n\n\n\nA simple slope of the predictor on the outcome is considered significant at a given level of the moderator if the 95% confidence interval from the bootstrapped estimates of the simple slopes at that level of the moderator (i.e., [ci.lower,ci.upper]) does not include zero. In this particular model, the predictor (Illiteracy) is not significant at any of the levels of the moderator (Murder), because the 95% confidence intervals of all simple slopes include zero, in this case, likely due to a small sample size (\\(N = 50\\)) and the resulting low power.",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "sem.html#johnsonNeymanPlot",
    "href": "sem.html#johnsonNeymanPlot",
    "title": "Structural Equation Modeling",
    "section": "13.12 Johnson-Neyman Plot",
    "text": "13.12 Johnson-Neyman Plot\nAs I noted above, the predictor is not significant at any levels of the moderator. Nevertheless, I created a made up Johnson-Neyman plot by specifying the (fictitious) range of significance, for purposes of demonstration. The band around the line indicates the 95% confidence interval of the simple slope of the predictor on the outcome as a function of different levels of the moderator. In reality (unlike in this fictitious example), the regions of significance would only be regions where the 95% confidence interval of the simple slope does not include zero.\nThe standard error of the slope is the square root of the variance of the slope. The forumula for computing the standard error of the slope is based on the formula for computing the variance of a weighted sum.\nThe slope of the predictor on the outcome at different levels of the moderator is calculated as (Jaccard & Turisi, 2003):\n\\[\n\\text{slope}_\\text{predictor} = b_1 + b_3 \\cdot Z\n\\]\nThe standard error of the slope of the predictor on the outcome at different levels of the moderator is calculated as (h&lt;ttps://stats.stackexchange.com/a/55973/20338&gt;; archived at https://perma.cc/V255-853Z; Jaccard & Turisi, 2003):\n\\[\n\\begin{aligned}\nSE(\\text{slope}_\\text{predictor}) &= \\sqrt{Var(b_1) + Var(b_3) \\cdot Z^2 + 2 \\cdot Z \\cdot Cov(b1, b3)} \\\\\nSE(b_1 + b_3 \\cdot Z) &=\n\\end{aligned}\n\\]\nwhere:\n\n\\(b_1\\) is the slope of the predictor on the outcome\n\\(b_3\\) is the slope of the interaction term on the outcome\n\\(Z\\) is the moderator\n\nThe variance of a weighted sum is:\n\\[\n\\begin{aligned}\nVar(\\text{slope}_\\text{predictor}) &= Var(b_1) + Var(b_3) \\cdot Z^2 + 2 \\cdot Z \\cdot Cov(b1, b3) \\\\\nVar(b_1 + b_3 \\cdot Z) &=\n\\end{aligned}\n\\]\nThe standard error is the square root of the variance. The 95% confidence interval of the slope is \\(\\pm\\) 1.959964 (i.e., qnorm(.975)) standard errors of the slope estimate.\n\n\nCode\n# Create a data frame for plotting\nMurder_min &lt;- min(states$Murder, na.rm = TRUE)\nMurder_max &lt;- max(states$Murder, na.rm = TRUE)\n\nplot_data &lt;- data.frame(\n  Murder = seq(Murder_min, Murder_max, length.out = 10000)\n)\n\nplot_data$Murder_centered &lt;- scale(plot_data$Murder, scale = FALSE)\n\n# Calculate predicted slopes and confidence intervals\nb1 &lt;- moderationModelSimpleSlopesFit_parameters[which(moderationModelSimpleSlopesFit_parameters$label == \"b1\"), \"est\"]\nb3 &lt;- moderationModelSimpleSlopesFit_parameters[which(moderationModelSimpleSlopesFit_parameters$label == \"b3\"), \"est\"]\n\nb1_se &lt;- moderationModelSimpleSlopesFit_parameters[which(moderationModelSimpleSlopesFit_parameters$label == \"b1\"), \"se\"]\nb3_se &lt;- moderationModelSimpleSlopesFit_parameters[which(moderationModelSimpleSlopesFit_parameters$label == \"b3\"), \"se\"]\n\nvarianceCovarianceMatrix &lt;- vcov(moderationFit)\n\nb1_var &lt;- varianceCovarianceMatrix[\"Income_rescaled~Illiteracy_centered\",\"Income_rescaled~Illiteracy_centered\"]\nb3_var &lt;- varianceCovarianceMatrix[\"interaction~~interaction\",\"interaction~~interaction\"]\ncov_b1b3 &lt;- varianceCovarianceMatrix[\"Income_rescaled~Illiteracy_centered\",\"interaction~~interaction\"]\n\n#sqrt((b1_se^2) + ((b3_se^2) * plot_data$Murder_centered^2) + (2 * plot_data$Murder_centered * cov_b1b3))\n#sqrt((b1_var) + ((b3_var) * plot_data$Murder_centered^2) + (2 * plot_data$Murder_centered * cov_b1b3))\n\nplot_data$predicted_slopes &lt;- b1 + b3 * plot_data$Murder_centered\nplot_data$slope_se &lt;- sqrt((b1_var) + ((b3_var) * plot_data$Murder_centered^2) + (2 * plot_data$Murder_centered * cov_b1b3))\n\n# Calculated the 95% confidence interval around the simple slope\nplot_data$lower_ci &lt;- plot_data$predicted_slopes - qnorm(.975) * plot_data$slope_se\nplot_data$upper_ci &lt;- plot_data$predicted_slopes + qnorm(.975) * plot_data$slope_se\n\n# Specify the significant range (based on the regions identified in the simple slopes analysis, see \"Simple Slopes and Regions of Significance\" section above)\nplot_data$significant_slope &lt;- FALSE\nplot_data$significant_slope[which(plot_data$Murder_centered &lt; -4.2 | plot_data$Murder_centered &gt; 3.75)] &lt;-TRUE # specify significant range\n\n# Specify the significant region number (there are either 0, 1, or 2 significant regions; in such cases, there would be 1, 0 or 1 or 2, or 1 nonsignificant regions, respectively)--for instance, sig from 0-4, ns from 4-12, and sig from 12-16 would be 2 significant regions and 1 nonsignificant region\nplot_data$significantRegionNumber &lt;- NA\nplot_data$significantRegionNumber[which(plot_data$Murder_centered &lt; -4.2)] &lt;- 1 # specify significant range 1\nplot_data$significantRegionNumber[which(plot_data$Murder_centered &gt; 3.75)] &lt;- 2 # specify significant range 2\n\nmin(plot_data$Murder[which(plot_data$significant_slope == FALSE)])\n\n\n[1] 4.051215\n\n\nCode\nmax(plot_data$Murder[which(plot_data$significant_slope == FALSE)])\n\n\n[1] 11.99938\n\n\nCode\nggplot(plot_data, aes(x = Murder, y = predicted_slopes)) +\n  geom_ribbon(\n    data = plot_data %&gt;% filter(significant_slope == FALSE),\n    aes(ymin = lower_ci, ymax = upper_ci),\n    fill = \"#F8766D\",\n    alpha = 0.2) + \n  geom_ribbon(\n    data = plot_data %&gt;% filter(significantRegionNumber == 1),\n    aes(ymin = lower_ci, ymax = upper_ci),\n    fill = \"#00BFC4\",\n    alpha = 0.2) + \n  geom_ribbon(\n    data = plot_data %&gt;% filter(significantRegionNumber == 2),\n    aes(ymin = lower_ci, ymax = upper_ci),\n    fill = \"#00BFC4\",\n    alpha = 0.2) +\n  geom_line(\n    data = plot_data %&gt;% filter(significant_slope == FALSE),\n    aes(x = Murder, y = predicted_slopes),\n    color = \"#F8766D\",\n    linewidth = 2) +\n  geom_line(\n    data = plot_data %&gt;% filter(significantRegionNumber == 1),\n    aes(x = Murder, y = predicted_slopes),\n    color = \"#00BFC4\",\n    linewidth = 2) +\n  geom_line(\n    data = plot_data %&gt;% filter(significantRegionNumber == 2),\n    aes(x = Murder, y = predicted_slopes),\n    color = \"#00BFC4\",\n    linewidth = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = c(4.051215, 11.99938), linetype = 2, color = \"#00BFC4\") + # update based on numbers above\n  labs(\n    title = \"Johnson-Neyman Plot\",\n    subtitle = \"(blue = significant slope; pink = nonsignificant slope)\",\n    x = \"Moderator (Murder)\",\n    y = \"Simple Slope of Predictor (Illiteracy)\") +\n  theme_classic()",
    "crumbs": [
      "About",
      "Structural Equation Modeling"
    ]
  },
  {
    "objectID": "jsPsych.html",
    "href": "jsPsych.html",
    "title": "jsPsych",
    "section": "",
    "text": "1 Documentation\nhttps://www.jspsych.org\n\n\n2 Example Tasks\nhttps://github.com/isaactpetersen/EF-battery\n\n\n3 Troubleshooting\n\nPress F12 to Open Chrome Developer Tools\nSwitch to the Sources tab\nPress F8 when you want to pause execution\nThen, you can run commands in the console\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "jsPsych"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "1 Overview\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#pca\n\n\n2 Analysis example\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#pcaAnalysis\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "mplus.html",
    "href": "mplus.html",
    "title": "Mplus",
    "section": "",
    "text": "Mplus is software for structural equation modeling. A summary of the Mplus language syntax is here (archived at: https://perma.cc/962G-QUEG). The Mplus User’s Guide is located here (archived at: https://perma.cc/W39W-NRGH).",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#specifyVariables",
    "href": "mplus.html#specifyVariables",
    "title": "Mplus",
    "section": "6.1 Specify Variables",
    "text": "6.1 Specify Variables\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    CLUSTER = ID;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#categoricalVariables",
    "href": "mplus.html#categoricalVariables",
    "title": "Mplus",
    "section": "6.2 Categorical Variables",
    "text": "6.2 Categorical Variables\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    CATEGORICAL = x1 x2;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#countVariables",
    "href": "mplus.html#countVariables",
    "title": "Mplus",
    "section": "6.3 Count Variables",
    "text": "6.3 Count Variables\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    COUNT = x1 x2;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#idVariable",
    "href": "mplus.html#idVariable",
    "title": "Mplus",
    "section": "6.4 ID Variable",
    "text": "6.4 ID Variable\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    IDVARIABLE = ID;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#clusterVariable",
    "href": "mplus.html#clusterVariable",
    "title": "Mplus",
    "section": "6.5 Cluster Variable",
    "text": "6.5 Cluster Variable\nThere are multiple ways of accounting for nested data in structural equation modeling. One way to account for nested data is to use multilevel structural equation modeling. Another approach is to use a cluster variable to generate cluster-robust standard errors of parameters. To use a cluster variable, specify CLUSTER under the VARIABLE section, and specify TYPE = COMPLEX under the ANALYSIS section:\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    CLUSTER = ID;\n\nANALYSIS:\n    TYPE = COMPLEX;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#auxiliaryVariables",
    "href": "mplus.html#auxiliaryVariables",
    "title": "Mplus",
    "section": "6.6 Auxiliary Variables",
    "text": "6.6 Auxiliary Variables\nVARIABLE:\n    NAMES = ID age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID x1 x2 y1;\n    AUXILIARY = age;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#samplingWeight",
    "href": "mplus.html#samplingWeight",
    "title": "Mplus",
    "section": "6.7 Sampling Weight Variable",
    "text": "6.7 Sampling Weight Variable\nVARIABLE:\n    NAMES = ID wt age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    WEIGHT = wt;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#multilevelVariables",
    "href": "mplus.html#multilevelVariables",
    "title": "Mplus",
    "section": "6.8 Multilevel Variables",
    "text": "6.8 Multilevel Variables\nBetween- and within-cluster variables:\nVARIABLE:\n    NAMES = ID wt age x1 x2 x3 y1;\n    MISSING = .;\n    USEVARIABLES = ID age x1 x2 y1;\n    WITHIN = x1;\n    BETWEEN = x2;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#analysisTypes",
    "href": "mplus.html#analysisTypes",
    "title": "Mplus",
    "section": "7.1 Analysis Types",
    "text": "7.1 Analysis Types\n\nTYPE = COMPLEX\nTYPE = TWOLEVEL\nTYPE = EFA\nTYPE = MIXTURE",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#estimators",
    "href": "mplus.html#estimators",
    "title": "Mplus",
    "section": "7.2 Model Estimators",
    "text": "7.2 Model Estimators\nANALYSIS:\n    ESTIMATOR = MLR;\n\nMLR: for likert/continuous data\nWLSMV: for ordinal/categorical data\nBAYES",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#bootstrap",
    "href": "mplus.html#bootstrap",
    "title": "Mplus",
    "section": "7.3 Bootstrap Draws",
    "text": "7.3 Bootstrap Draws\nBOOTSTRAP = 2000; ! insert number of bootstrap draws",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#starts",
    "href": "mplus.html#starts",
    "title": "Mplus",
    "section": "7.4 Starts",
    "text": "7.4 Starts\nSTARTS = 20; ! insert number of initial stage starts and number of final stage optimizations",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#lowCovarianceCoverage",
    "href": "mplus.html#lowCovarianceCoverage",
    "title": "Mplus",
    "section": "7.5 Low Covariance Coverage",
    "text": "7.5 Low Covariance Coverage\nTo estimate a model with low covariance coverage, lower the COVERAGE value under the ANALYSIS section:\nANALYSIS:\n    COVERAGE = 0;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#oblique",
    "href": "mplus.html#oblique",
    "title": "Mplus",
    "section": "8.1 Oblique Rotation",
    "text": "8.1 Oblique Rotation\nANALYSIS:\n    TYPE = EFA 1 5; ! extract 1-5 factors\n    ROTATION = GEOMIN;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#orthogonal",
    "href": "mplus.html#orthogonal",
    "title": "Mplus",
    "section": "8.2 Orthogonal Rotation",
    "text": "8.2 Orthogonal Rotation\nANALYSIS:\n    TYPE = EFA 1 5; ! extract 1-5 factors\n    ROTATION = VARIMAX;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#latentVariables",
    "href": "mplus.html#latentVariables",
    "title": "Mplus",
    "section": "10.1 Define Latent Variables",
    "text": "10.1 Define Latent Variables\nMODEL:\n    latent1 BY x1 x2 x3;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#regressionPaths",
    "href": "mplus.html#regressionPaths",
    "title": "Mplus",
    "section": "10.2 Regression Paths",
    "text": "10.2 Regression Paths\nRegress outcome variable on predictor variable(s):\nMODEL:\n    y1 ON x1 x2;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#covariancePaths",
    "href": "mplus.html#covariancePaths",
    "title": "Mplus",
    "section": "10.3 Covariance Paths",
    "text": "10.3 Covariance Paths\nMODEL:\n    x1 WITH x2;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#indirectEffects",
    "href": "mplus.html#indirectEffects",
    "title": "Mplus",
    "section": "10.4 Indirect Effects",
    "text": "10.4 Indirect Effects\nANALYSIS:\n    TYPE = GENERAL;\n    ESTIMATOR = ML;\n    BOOTSTRAP = 1000;\n\nMODEL:\n    MODEL INDIRECT:\n        y IND x;\n\nOUTPUT:\n    STAND;\n    CINTERVAL (BOOTSTRAP); !percentile boostrap CI\n    CINTERVAL (BCBOOTSTRAP); !bias-corrected boostrap CI",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#means",
    "href": "mplus.html#means",
    "title": "Mplus",
    "section": "10.5 Means/Intercepts",
    "text": "10.5 Means/Intercepts\nFreely estimate:\nMODEL:\n    [x1];\nFix to zero:\nMODEL:\n    [x1@0];",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#variances",
    "href": "mplus.html#variances",
    "title": "Mplus",
    "section": "10.6 Variances",
    "text": "10.6 Variances\nFreely estimate:\nMODEL:\n    x1;\nFix to one:\nMODEL:\n    x1@1;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#parameterLabel",
    "href": "mplus.html#parameterLabel",
    "title": "Mplus",
    "section": "10.7 Parameter Label",
    "text": "10.7 Parameter Label\nTo specify a parameter label, provide the label in parentheses after the parameter:\nMODEL:\n    latent1 BY x1* x2 x3 (load1-3);\n    latent2 BY x4* x5 (load5) x6;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#multigroup",
    "href": "mplus.html#multigroup",
    "title": "Mplus",
    "section": "10.8 Multigroup Model",
    "text": "10.8 Multigroup Model\nVARIABLE:\n    NAMES = group x1 x2 x3 y1;\n    GROUPING = group (0=boys, 1=girls);\n    MISSING = .;\n    USEVARIABLES = group x1 x2 x3 y1;\n\nMODEL:\n    Model boys:\n        latent BY x1* x2 x3;\n        [latent@0];\n        latent@1;\n        y ~ latent;\n\n    Model girls:\n        latent BY x1* x2 x3;\n        [latent@0];\n        latent@1;\n        y ~ latent;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#measurementInvariance",
    "href": "mplus.html#measurementInvariance",
    "title": "Mplus",
    "section": "10.9 Multigroup Measurement Invariance",
    "text": "10.9 Multigroup Measurement Invariance\n\n10.9.1 Configural Invariance\nVARIABLE:\n    NAMES = group x1 x2 x3 y1;\n    GROUPING = group (0=boys, 1=girls);\n    MISSING = .;\n    USEVARIABLES = group x1 x2 x3;\n\nMODEL:\n    Model boys:\n        latent BY x1* x2 x3;\n        [latent@0];\n        latent@1;\n\n    Model girls:\n        latent BY x1* x2 x3;\n        [latent@0];\n        latent@1;\n\n\n10.9.2 Metric (Weak Factorial) Invariance\nVARIABLE:\n    NAMES = group x1 x2 x3 y1;\n    GROUPING = group (0=boys, 1=girls);\n    MISSING = .;\n    USEVARIABLES = group x1 x2 x3;\n\nMODEL:\n    Model boys:\n        latent BY x1* (load1); ! constrain factor loading across groups (same parameter label)\n        latent BY x2* (load2); ! constrain factor loading across groups (same parameter label)\n        latent BY x3* (load3); ! constrain factor loading across groups (same parameter label)\n\n        [latent@0];\n        latent@1;\n\n    Model girls:\n        latent BY x1* (load1); ! constrain factor loading across groups (same parameter label)\n        latent BY x2* (load2); ! constrain factor loading across groups (same parameter label)\n        latent BY x3* (load3); ! constrain factor loading across groups (same parameter label)\n\n        [latent@0];\n        latent@1;\n\n\n10.9.3 Scalar (Strong Factorial) Invariance\nVARIABLE:\n    NAMES = group x1 x2 x3 y1;\n    GROUPING = group (0=boys, 1=girls);\n    MISSING = .;\n    USEVARIABLES = group x1 x2 x3;\n\nMODEL:\n    Model boys:\n        latent BY x1* (load1); ! constrain factor loading across groups (same parameter label)\n        latent BY x2* (load2); ! constrain factor loading across groups (same parameter label)\n        latent BY x3* (load3); ! constrain factor loading across groups (same parameter label)\n\n        [x1] (int1); ! constrain intercept across groups (same parameter label)\n        [x2] (int2); ! constrain intercept across groups (same parameter label)\n        [x3] (int3); ! constrain intercept across groups (same parameter label)\n\n        [latent@0];\n        latent@1;\n\n    Model girls:\n        latent BY x1* (load1); ! constrain factor loading across groups (same parameter label)\n        latent BY x2* (load2); ! constrain factor loading across groups (same parameter label)\n        latent BY x3* (load3); ! constrain factor loading across groups (same parameter label)\n\n        [x1] (int1); ! constrain intercept across groups (same parameter label)\n        [x2] (int2); ! constrain intercept across groups (same parameter label)\n        [x3] (int3); ! constrain intercept across groups (same parameter label)\n\n        [latent@0];\n        latent@1;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#freeParameter",
    "href": "mplus.html#freeParameter",
    "title": "Mplus",
    "section": "12.1 Freeing a Parameter",
    "text": "12.1 Freeing a Parameter\nBy default, the first loading on a factor is fixed to zero. You can freely estimate the parameter by adding an asterisk:\nMODEL:\n    latent1 BY x1* x2 x3;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#constrainParameter",
    "href": "mplus.html#constrainParameter",
    "title": "Mplus",
    "section": "12.2 Constraing a Parameter",
    "text": "12.2 Constraing a Parameter\nMODEL:\n    latent1 BY x1@1 x2 x3;\n    [latent1@0];\n    latent1@1;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#equalParameters",
    "href": "mplus.html#equalParameters",
    "title": "Mplus",
    "section": "12.3 Setting Two Parameters to be Equal",
    "text": "12.3 Setting Two Parameters to be Equal\nTo set two parameters to be equal, provide the same parameter label for each parameter.",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#parameterBounds",
    "href": "mplus.html#parameterBounds",
    "title": "Mplus",
    "section": "12.4 Setting Lower and Upper Bounds on a Parameter",
    "text": "12.4 Setting Lower and Upper Bounds on a Parameter\nTo set lower and upper bounds on a parameter, you can assign the parameter a parameter label. Then, you can assign the constraint to the parameter (via the label) under the MODEL CONSTRAINT section. For example, to constrain a parameter between 0–1,\nMODEL:\n    latent1 BY x1* x2 x3 (load3);\n\nMODEL CONSTRAINT:\n    load3 &gt; 0; load3 &lt; 1;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#equallySpaced",
    "href": "mplus.html#equallySpaced",
    "title": "Mplus",
    "section": "17.1 Equally Spaced Time Points",
    "text": "17.1 Equally Spaced Time Points\nVARIABLE: \n    NAMES = id y0 y1 y2 y3 y4 x;\n    USEVARIABLES = y0 y1 y2 y3 y4 x;\n\nANALYSIS:\n    TYPE = RANDOM;\n\nMODEL:\n    i s q | y0 y1 y2 y3 y4 AT t0 t1 t2 t3 t4; ! intercept, linear slope, quadratic slopes\n    i s q ON x;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#tScores",
    "href": "mplus.html#tScores",
    "title": "Mplus",
    "section": "17.2 Unequally Spaced Time Points (Individually Varying Times of Observation)",
    "text": "17.2 Unequally Spaced Time Points (Individually Varying Times of Observation)\nUnequally spaced time points can be accounted for using the TSCORES option.\nhttps://stats.oarc.ucla.edu/mplus/faq/how-can-i-run-a-growth-model-in-wide-form-with-unequally-spaced-time-points-tscore/ (archived at https://perma.cc/Z7QY-TL6W)\nVARIABLE: \n    NAMES = id t0 t1 t2 t3 t4 y0 y1 y2 y3 y4 x;\n    USEVARIABLES = t0 t1 t2 t3 t4 y0 y1 y2 y3 y4 x;\n    TSCORES = t0 t1 t2 t3 t4;\n\nANALYSIS:\n    TYPE = RANDOM;\n\nMODEL:\n    i s q | y1@0 y2@1 y3@2 y4@3; ! intercept, linear slope, quadratic slope\n    i s q ON x;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#monteCarloSingleGroup",
    "href": "mplus.html#monteCarloSingleGroup",
    "title": "Mplus",
    "section": "20.1 Single Group Model",
    "text": "20.1 Single Group Model\nTITLE: Single-Group Monte Carlo Simulation with Ordinal Items and Common Factor;\n\nMONTECARLO:\n    NAMES = v1-v5; ! variable names\n    NOBSERVATIONS = 500; ! number of participants in each sample\n    NREPS = 100000; ! number of samples to create\n    SEED = 52242; ! random seed\n    GENERATE = v1-v5 (3 p); ! specify the scale of the DVs; number of thresholds; probit (p)\n    CATEGORICAL = v1-v5; ! specify the variables that are (ordered) categorical\n\nANALYSIS:\n    PROCESSORS = 4 1; ! number of processors; number of threads\n    ESTIMATOR = WLSMV;\n    PARAMETERIZATION = THETA;\n\nMODEL POPULATION: ! tell Mplus how to generate the population data; can use asterisks (*) and at symbols (@) interchangeably here, but they differ in the MODEL command (see below); I use the same symbol as in the MODEL command\n    dep BY v1-v5*.7; ! factor loadings\n    [dep@0]; ! set factor mean to 0\n    dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1); [v1$2*1.0] (v1t2); [v1$3*1.5] (v1t3); ! item thresholds for v1\n    [v2$1*0.5] (v2t1); [v2$2*1.0] (v2t2); [v2$3*1.5] (v2t3); ! item thresholds for v2\n    [v3$1*0.5] (v3t1); [v3$2*1.0] (v3t2); [v3$3*1.5] (v3t3); ! item thresholds for v3\n    [v4$1*0.0] (v4t1); [v4$2*0.5] (v4t2); [v4$3*1.0] (v4t3); ! item thresholds for v4\n    [v5$1*0.0] (v5t1); [v5$2*0.5] (v5t2); [v5$3*1.0] (v5t3); ! item thresholds for v5\n    v1-v5@1; ! item residual variances\n\nMODEL: ! tell Mplus to estimate our model; asterisks (*) are free estimates with a starting value; at symbols (@) are fixed estimates\n    dep BY v1-v5*.7; ! factor loadings\n    [dep@0]; ! set factor mean to 0\n    dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1); [v1$2*1.0] (v1t2); [v1$3*1.5] (v1t3); ! item thresholds for v1\n    [v2$1*0.5] (v2t1); [v2$2*1.0] (v2t2); [v2$3*1.5] (v2t3); ! item thresholds for v2\n    [v3$1*0.5] (v3t1); [v3$2*1.0] (v3t2); [v3$3*1.5] (v3t3); ! item thresholds for v3\n    [v4$1*0.0] (v4t1); [v4$2*0.5] (v4t2); [v4$3*1.0] (v4t3); ! item thresholds for v4\n    [v5$1*0.0] (v5t1); [v5$2*0.5] (v5t2); [v5$3*1.0] (v5t3); ! item thresholds for v5\n    v1-v5@1; ! item residual variances\n\nMODEL CONSTRAINT:\n    NEW (stdt stdt1 stdt2 stdt3 noninvt noninvt1 noninvt2 noninvt3 diff);\n    \n    stdt1 = (v1t1 + v2t1 + v3t1) / 3;\n    stdt2 = (v1t2 + v2t2 + v3t2) / 3;\n    stdt3 = (v1t3 + v2t3 + v3t3) / 3;\n    \n    noninvt1 = (v4t1 + v5t1) / 2;\n    noninvt2 = (v4t2 + v5t2) / 2;\n    noninvt3 = (v4t3 + v5t3) / 2;\n    \n    stdt = (stdt1 + stdt2 + stdt3) / 3;\n    noninvt = (noninvt1 + noninvt2 + noninvt3) / 3;\n    \n    diff = noninvt - stdt;\n\nOUTPUT:\n    TECH9;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "mplus.html#monteCarloMultigroup",
    "href": "mplus.html#monteCarloMultigroup",
    "title": "Mplus",
    "section": "20.2 Multi-Group Model",
    "text": "20.2 Multi-Group Model\nTITLE: Multi-Group Monte Carlo Simulation with Ordinal Items and Common Factor;\n\nMONTECARLO:\n    NAMES = v1-v5; ! variable names\n    NGROUPS = 2; ! number of groups\n    NOBSERVATIONS = 500 300; ! number of participants in each sample\n    NREPS = 100000; ! number of samples to create\n    SEED = 52242; ! random seed\n    GENERATE = v1-v5 (3 p); ! specify the scale of the DVs; number of thresholds; probit (p)\n    CATEGORICAL = v1-v5; ! specify the variables that are (ordered) categorical\n\nANALYSIS:\n    PROCESSORS = 4 1; ! number of processors; number of threads\n    ESTIMATOR = WLSMV;\n    PARAMETERIZATION = THETA;\n\nMODEL POPULATION: ! tell Mplus how to generate the population data; can use asterisks (*) and at symbols (@) interchangeably here, but they differ in the MODEL command (see below); I use the same symbol as in the MODEL command\n    dep BY v1-v5*.7; ! factor loadings\n    [dep@0]; ! set factor mean to 0\n    dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1g1); [v1$2*1.0] (v1t2g1); [v1$3*1.5] (v1t3g1); ! item thresholds for v1\n    [v2$1*0.5] (v2t1g1); [v2$2*1.0] (v2t2g1); [v2$3*1.5] (v2t3g1); ! item thresholds for v2\n    [v3$1*0.5] (v3t1g1); [v3$2*1.0] (v3t2g1); [v3$3*1.5] (v3t3g1); ! item thresholds for v3\n    [v4$1*0.5] (v4t1g1); [v4$2*1.0] (v4t2g1); [v4$3*1.5] (v4t3g1); ! item thresholds for v4\n    [v5$1*0.5] (v5t1g1); [v5$2*1.0] (v5t2g1); [v5$3*1.5] (v5t3g1); ! item thresholds for v5\n    v1-v5@1; ! item residual variances\n    \nMODEL POPULATION-g2: ! tell Mplus how to generate the population data for group 2; can use asterisks (*) and at symbols (@) interchangeably here, but they differ in the MODEL command (see below); I use the same symbol as in the MODEL command\n    !dep BY v1-v5*.7; ! factor loadings\n    ![dep@0]; ! set factor mean to 0\n    !dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1g2); [v1$2*1.0] (v1t2g2); [v1$3*1.5] (v1t3g2); ! item thresholds for v1\n    [v2$1*0.5] (v2t1g2); [v2$2*1.0] (v2t2g2); [v2$3*1.5] (v2t3g2); ! item thresholds for v2\n    [v3$1*0.5] (v3t1g2); [v3$2*1.0] (v3t2g2); [v3$3*1.5] (v3t3g2); ! item thresholds for v3\n    [v4$1*0.0] (v4t1g2); [v4$2*0.5] (v4t2g2); [v4$3*1.0] (v4t3g2); ! item thresholds for v4\n    [v5$1*0.0] (v5t1g2); [v5$2*0.5] (v5t2g2); [v5$3*1.0] (v5t3g2); ! item thresholds for v5\n    !v1-v5@1; ! item residual variances\n\nMODEL: ! tell Mplus to estimate our model; asterisks (*) are free estimates with a starting value; at symbols (@) are fixed estimates\n    dep BY v1-v5*.7; ! factor loadings\n    [dep@0]; ! set factor mean to 0\n    dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1g1); [v1$2*1.0] (v1t2g1); [v1$3*1.5] (v1t3g1); ! item thresholds for v1\n    [v2$1*0.5] (v2t1g1); [v2$2*1.0] (v2t2g1); [v2$3*1.5] (v2t3g1); ! item thresholds for v2\n    [v3$1*0.5] (v3t1g1); [v3$2*1.0] (v3t2g1); [v3$3*1.5] (v3t3g1); ! item thresholds for v3\n    [v4$1*0.5] (v4t1g1); [v4$2*1.0] (v4t2g1); [v4$3*1.5] (v4t3g1); ! item thresholds for v4\n    [v5$1*0.5] (v5t1g1); [v5$2*1.0] (v5t2g1); [v5$3*1.5] (v5t3g1); ! item thresholds for v5\n    v1-v5@1; ! item residual variances\n   \nMODEL g2: ! tell Mplus to estimate our model in group 2; asterisks (*) are free estimates with a starting value; at symbols (@) are fixed estimates\n    !dep BY v1-v5*.7; ! factor loadings\n    ![dep@0]; ! set factor mean to 0\n    !dep@1; ! set factor variance to 1 (standardize)\n    [v1$1*0.5] (v1t1g2); [v1$2*1.0] (v1t2g2); [v1$3*1.5] (v1t3g2); ! item thresholds for v1\n    [v2$1*0.5] (v2t1g2); [v2$2*1.0] (v2t2g2); [v2$3*1.5] (v2t3g2); ! item thresholds for v2\n    [v3$1*0.5] (v3t1g2); [v3$2*1.0] (v3t2g2); [v3$3*1.5] (v3t3g2); ! item thresholds for v3\n    [v4$1*0.0] (v4t1g2); [v4$2*0.5] (v4t2g2); [v4$3*1.0] (v4t3g2); ! item thresholds for v4\n    [v5$1*0.0] (v5t1g2); [v5$2*0.5] (v5t2g2); [v5$3*1.0] (v5t3g2); ! item thresholds for v5\n    !v1-v5@1; ! item residual variances\n\nMODEL CONSTRAINT:\n    NEW (stdt1g1 stdt2g1 stdt3g1 stdt1g2 stdt2g2 stdt3g2\n    nonit1g1 nonit2g1 nonit3g1 nonit1g2 nonit2g2 nonit3g2\n    stdtg1 stdtg2 nonitg1 nonitg2 diffwg diffbg);\n    \n    stdt1g1 = (v1t1g1 + v2t1g1 + v3t1g1) / 3;\n    stdt2g1 = (v1t2g1 + v2t2g1 + v3t2g1) / 3;\n    stdt3g1 = (v1t3g1 + v2t3g1 + v3t3g1) / 3;\n    \n    stdt1g2 = (v1t1g2 + v2t1g2 + v3t1g2) / 3;\n    stdt2g2 = (v1t2g2 + v2t2g2 + v3t2g2) / 3;\n    stdt3g2 = (v1t3g2 + v2t3g2 + v3t3g2) / 3;\n    \n    nonit1g1 = (v4t1g1 + v5t1g1) / 2;\n    nonit2g1 = (v4t2g1 + v5t2g1) / 2;\n    nonit3g1 = (v4t3g1 + v5t3g1) / 2;\n    \n    nonit1g2 = (v4t1g2 + v5t1g2) / 2;\n    nonit2g2 = (v4t2g2 + v5t2g2) / 2;\n    nonit3g2 = (v4t3g2 + v5t3g2) / 2;\n    \n    stdtg1 = (stdt1g1 + stdt2g1 + stdt3g1) / 3;\n    stdtg2 = (stdt1g2 + stdt2g2 + stdt3g2) / 3;\n    \n    nonitg1 = (nonit1g1 + nonit2g1 + nonit3g1) / 3;\n    nonitg2 = (nonit1g2 + nonit2g2 + nonit3g2) / 3;\n    \n    diffwg = nonitg2 - stdtg2; ! difference within group\n    \n    diffbg = nonitg2 - nonitg1; ! difference between groups\n    \nOUTPUT:\n    TECH9;",
    "crumbs": [
      "About",
      "Mplus"
    ]
  },
  {
    "objectID": "abcdData.html",
    "href": "abcdData.html",
    "title": "ABCD Data",
    "section": "",
    "text": "The Adolescent Brain Cognitive Development (ABCD) Study is the largest long-term study of brain development and child health in the United States. This study explores how childhood experiences (such as sports, videogames, social media, unhealthy sleep patterns, and smoking) interact with each other and with a child’s changing biology to affect brain development and social, behavioral, academic, health, and other outcomes. The ABCD study follows a cohort of over 10,000 children from pre-adolescence into adulthood.",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#dataUseCertificate",
    "href": "abcdData.html#dataUseCertificate",
    "title": "ABCD Data",
    "section": "3.1 Step 1: Data Use Certificate",
    "text": "3.1 Step 1: Data Use Certificate\nWork with Dr. Petersen to get added to the Data Use Certificate.",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#labProtocols",
    "href": "abcdData.html#labProtocols",
    "title": "ABCD Data",
    "section": "3.2 Step 2: Read (and Follow) the Lab’s Protocols for Handling ABCD Data",
    "text": "3.2 Step 2: Read (and Follow) the Lab’s Protocols for Handling ABCD Data\n\nRead and follow the lab’s Data Security Protocols outlined above for handling ABCD data.\nRead and follow the lab’s Standard Operating Procedures (SOP) for handling ABCD data. The SOP is located at the following location of the lab’s LSS share: \\Lab\\Studies\\ABCD Data\\Security Requirements for Accessing Data\\.",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#connectRRDS",
    "href": "abcdData.html#connectRRDS",
    "title": "ABCD Data",
    "section": "3.3 Step 3: Connect to the Research Remote Desktop Service (RRDS)",
    "text": "3.3 Step 3: Connect to the Research Remote Desktop Service (RRDS)\nTo access ABCD data, you need to access the Research Remote Desktop Service (RRDS).\n\nCreate an RRDS Account: https://its.uiowa.edu/services/research-remote-desktop-service\n\nInstructions for Windows: https://its.uiowa.edu/services/research-remote-desktop-service/connecting-rrds-using-windows-pc\nInstructions for Mac: https://its.uiowa.edu/services/research-remote-desktop-service/connecting-rrds-using-mac\n\nLog in to the RRDS using your HawkID and password\n\nIf you run into issues connecting to the RRDS, please reach out to PBS IT Support (PBS-help@uiowa.edu) or Research Computing Support (research-computing@uiowa.edu)",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#connectLSS",
    "href": "abcdData.html#connectLSS",
    "title": "ABCD Data",
    "section": "3.4 Step 4: Connect to the ABCD LSS Server",
    "text": "3.4 Step 4: Connect to the ABCD LSS Server\nPlease contact Dr. Petersen to receive the network path for the ABCD Large Scale Storage (LSS) server.\nMapping the Server on Windows: Windows Explorer -&gt; This PC -&gt; Computer Tab -&gt; Map Network Drive button: \\\\server path\nMapping the Server on MacOS: Finder -&gt; Go menu -&gt; Connect to Server: smb://iowa\\hawkid:*@server path",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#accessManagement",
    "href": "abcdData.html#accessManagement",
    "title": "ABCD Data",
    "section": "3.5 Access Management",
    "text": "3.5 Access Management\n[XXX]\nDr. Petersen can grant or revoke access to the share, or give other people roles if they are to manage the share using the Access Management tool: https://iam.uiowa.edu/access\n\nInstructions for granting or revoking access: https://its.uiowa.edu/support/article/118771\nDelegate management of share access: https://its.uiowa.edu/support/article/119191\n\nA few notes:\n\nWhen granting someone Read/Write or Read Only permissions and search for their name or ID, you may see both a HealthcareID and HawkID listed, select their HawkID.\nChanges take about an hour to propagate through Argon and will not be reflected in existing sessions.\nChanges are immediately reflected for SMB connections, however, users may have to remap/remount the volume or log out then back in for Windows/MacOS to be happy.",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#analysesProjects",
    "href": "abcdData.html#analysesProjects",
    "title": "ABCD Data",
    "section": "4.1 Analyses Projects",
    "text": "4.1 Analyses Projects\nAnalyses Projects &gt; PetersenLab contains user folders for anyone working on projects with the ABCD data.\n\nCreate a folder with your HawkID as the folder name\nCreate a subfolder for your specific project\nFollow Petersen Lab template for structuring your project folder\nFollow the version control instructions below",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#data",
    "href": "abcdData.html#data",
    "title": "ABCD Data",
    "section": "4.2 Data",
    "text": "4.2 Data\nThe folder Data &gt; Version 6.0 contains tabulated ABCD data.\nImaging data, wearables data, and concatenated data are available for download here\n\n[XXX] need instructions for accessing this\nNOTE: The file sizes for these data exceed the capacity of our LSS\n\n01-Raw contains the raw ABCD data files that should not be altered.\n02-Processed contains scored measures and other cleaned data for universal use.",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#fileTypes",
    "href": "abcdData.html#fileTypes",
    "title": "ABCD Data",
    "section": "4.3 File types",
    "text": "4.3 File types\n\nJSON files: data dictionary/codebook files\nParquet files: columnar structured data (efficient for storage, can be used with the arrow package in R)\nTSV files: tab-separated value data files",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "abcdData.html#availableMeasures",
    "href": "abcdData.html#availableMeasures",
    "title": "ABCD Data",
    "section": "4.4 Available Measures",
    "text": "4.4 Available Measures\n[XXX] Add measures currently downloaded",
    "crumbs": [
      "About",
      "ABCD Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis Guides for the Developmental Psychopathology Lab",
    "section": "",
    "text": "ABCD Data\nBayesian Analysis\nData Management\nDevelopmental Scaling\nDirected Acyclic Graphs\nElectroencephalography/Event-Related Potentials\nExploratory Data Analysis\nFactor Analysis\nFigures\nGit, GitLab, and GitHub\nHierarchical Linear Modeling\nHigh-Performance Computing\njamovi\nJATOS (Just Another Tool for Online Studies)\njsPsych\nItem Response Theory\nLAMP (Linux, Apache, MySQL, PHP)\nLongitudinal Data Analysis\nMarkdown\nMediation\nModeration/Interaction\nMplus\nMultiple Imputation\nOpen Science Framework\nPrincipal Component Analysis\nPython\nR\nRegression\nSPSS\nStatistics\nStructural Equation Modeling\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "multipleImputation.html",
    "href": "multipleImputation.html",
    "title": "Multiple Imputation",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n\n\n\n\n\n\n\nCode\nlibrary(\"tidyverse\")\nlibrary(\"psych\")\nlibrary(\"mice\")\nlibrary(\"micemd\")\nlibrary(\"miceadds\")\nlibrary(\"mitml\")\nlibrary(\"Amelia\")\nlibrary(\"jomo\")\nlibrary(\"parallelly\")\nlibrary(\"future\")\nlibrary(\"furrr\")\nlibrary(\"nlme\")\nlibrary(\"lme4\")\nlibrary(\"broom.mixed\")\nlibrary(\"MplusAutomation\")\n\n\n\n\n\n\n\nCode\ndata(Oxboys, package = \"nlme\")\n\nOxboys_addNA &lt;- data.frame(Oxboys)\nOxboys_addNA &lt;- Oxboys_addNA %&gt;% \n  rename(id = Subject)\n\nOxboys_addNA$id &lt;- as.integer(Oxboys_addNA$id)\nOxboys_addNA$Occasion &lt;- as.integer(Oxboys_addNA$Occasion)\n\nset.seed(52242)\nOxboys_addNA[sample(1:nrow(Oxboys_addNA), 25), \"height\"] &lt;- NA\n\ndataToImpute &lt;- Oxboys_addNA",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#install-libraries",
    "href": "multipleImputation.html#install-libraries",
    "title": "Multiple Imputation",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#load-libraries",
    "href": "multipleImputation.html#load-libraries",
    "title": "Multiple Imputation",
    "section": "",
    "text": "Code\nlibrary(\"tidyverse\")\nlibrary(\"psych\")\nlibrary(\"mice\")\nlibrary(\"micemd\")\nlibrary(\"miceadds\")\nlibrary(\"mitml\")\nlibrary(\"Amelia\")\nlibrary(\"jomo\")\nlibrary(\"parallelly\")\nlibrary(\"future\")\nlibrary(\"furrr\")\nlibrary(\"nlme\")\nlibrary(\"lme4\")\nlibrary(\"broom.mixed\")\nlibrary(\"MplusAutomation\")",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#data",
    "href": "multipleImputation.html#data",
    "title": "Multiple Imputation",
    "section": "",
    "text": "Code\ndata(Oxboys, package = \"nlme\")\n\nOxboys_addNA &lt;- data.frame(Oxboys)\nOxboys_addNA &lt;- Oxboys_addNA %&gt;% \n  rename(id = Subject)\n\nOxboys_addNA$id &lt;- as.integer(Oxboys_addNA$id)\nOxboys_addNA$Occasion &lt;- as.integer(Oxboys_addNA$Occasion)\n\nset.seed(52242)\nOxboys_addNA[sample(1:nrow(Oxboys_addNA), 25), \"height\"] &lt;- NA\n\ndataToImpute &lt;- Oxboys_addNA",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#for-mcarmar-missingness",
    "href": "multipleImputation.html#for-mcarmar-missingness",
    "title": "Multiple Imputation",
    "section": "3.1 For MCAR/MAR Missingness",
    "text": "3.1 For MCAR/MAR Missingness\n\nFull Information Maximum Likelihood (FIML)\nMultiple Imputation",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#for-mnar-missingness",
    "href": "multipleImputation.html#for-mnar-missingness",
    "title": "Multiple Imputation",
    "section": "3.2 For MNAR Missingness",
    "text": "3.2 For MNAR Missingness\nhttps://stefvanbuuren.name/fimd/sec-nonignorable.html (archived at https://perma.cc/N7WW-HDZF)\nhttps://cran.r-project.org/web/packages/missingHE/vignettes/Fitting_MNAR_models_in_missingHE.html (archived at https://perma.cc/9X25-5D8G)\n\nfind more data about the causes for the missingness\nsensitivity analyses (what-if analyses) to see how sensitive the results are under various scenarios\n\nhttps://stefvanbuuren.name/fimd/sec-MCAR.html (archived at https://perma.cc/9KM9-3NX3)\n\nselection models\n\nsimultaneously estimate the focal model and a missingness model, where the missingness model has the missing data indicator as a dependent variable, as predicted by the original dependent variable, the original predictors, and any additional covariates etc.\nif the missingness model is approximately correct, the focal model adjusts in way that removes nonresponse bias\nsimilar to a mediation process\n\nX → Y\nY → missingness\nX → missingness\n\nhttps://quantitudepod.org/s4e08-craig-enders/ (archived at https://perma.cc/FY9L-L3F7)\n\npattern-mixture models\n\nmissing data indicator is a predictor variable\nsimilar to a moderation process; subgroups of cases have different parameter estimates",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#specify-variables-to-impute",
    "href": "multipleImputation.html#specify-variables-to-impute",
    "title": "Multiple Imputation",
    "section": "6.1 Specify Variables to Impute",
    "text": "6.1 Specify Variables to Impute\n\n\nCode\nvarsToImpute &lt;- c(\"height\")\nY &lt;- varsToImpute",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#specify-number-of-imputations",
    "href": "multipleImputation.html#specify-number-of-imputations",
    "title": "Multiple Imputation",
    "section": "6.2 Specify Number of Imputations",
    "text": "6.2 Specify Number of Imputations\n\n\nCode\nnumImputations &lt;- 5 # generally use 100 imputations; this example uses 5 for speed",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#detect-cores",
    "href": "multipleImputation.html#detect-cores",
    "title": "Multiple Imputation",
    "section": "6.3 Detect Cores",
    "text": "6.3 Detect Cores\n\n\nCode\nnumCores &lt;- parallelly::availableCores() - 1",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#three-level-and-cross-classified-data",
    "href": "multipleImputation.html#three-level-and-cross-classified-data",
    "title": "Multiple Imputation",
    "section": "7.1 Three-Level and Cross-Classified Data",
    "text": "7.1 Three-Level and Cross-Classified Data\nhttps://simongrund1.github.io/posts/multiple-imputation-for-three-level-and-cross-classified-data/ (archived at https://perma.cc/N4PP-A3V6)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#methods",
    "href": "multipleImputation.html#methods",
    "title": "Multiple Imputation",
    "section": "7.2 Methods",
    "text": "7.2 Methods\n\n7.2.1 mice\n\n7.2.1.1 Types\n\n7.2.1.1.1 Continuous Outcomes\nhttps://stefvanbuuren.name/fimd/sec-multioutcome.html#methods (archived at https://perma.cc/8CDA-TS3K)\n\n\nCode\n?mice::mice.impute.2l.norm\n?mice::mice.impute.2l.pan\n?mice::mice.impute.2l.lmer\n?miceadds::mice.impute.2l.pmm\n?miceadds::mice.impute.2l.contextual.pmm\n?miceadds::mice.impute.2l.continuous\n?micemd::mice.impute.2l.2stage.norm\n?micemd::mice.impute.2l.2stage.pmm\n?micemd::mice.impute.2l.glm.norm\n?micemd::mice.impute.2l.jomo\n\n\n\n\n7.2.1.1.2 Binary Outcomes\nhttps://stefvanbuuren.name/fimd/sec-catoutcome.html#methods-1 (archived at https://perma.cc/5QHF-YRP6)\n\n\nCode\n?mice::mice.impute.2l.bin\n?miceadds::mice.impute.2l.binary\n?miceadds::mice.impute.2l.pmm\n?miceadds::mice.impute.2l.contextual.pmm\n?micemd::mice.impute.2l.2stage.bin\n?micemd::mice.impute.2l.glm.bin\n\n\n\n\n7.2.1.1.3 Ordinal Outcomes\nhttps://stefvanbuuren.name/fimd/sec-multioutcome.html#methods (archived at https://perma.cc/8CDA-TS3K)\n\n\nCode\n?miceadds::mice.impute.2l.pmm\n?miceadds::mice.impute.2l.contextual.pmm\n?micemd::mice.impute.2l.2stage.pmm\n\n\n\n\n7.2.1.1.4 Count Outcomes\nhttps://stefvanbuuren.name/fimd/sec-catoutcome.html#methods-1 (archived at https://perma.cc/5QHF-YRP6)\n\n\nCode\n?micemd::mice.impute.2l.2stage.pois\n?micemd::mice.impute.2l.glm.pois\n?countimp::mice.impute.2l.poisson\n?countimp::mice.impute.2l.nb2\n?countimp::mice.impute.2l.zihnb\n\n\n\n\n\n7.2.1.2 Specifying the Imputation Method\n\n\nCode\nmeth &lt;- make.method(dataToImpute)\nmeth[1:length(meth)] &lt;- \"\"\nmeth[Y] &lt;- \"2l.pmm\" # specify the imputation method here; this can differ by outcome variable\n\n\n\n\n7.2.1.3 Specifying the Predictor Matrix\nA predictor matrix is a matrix of values, where:\n\ncolumns with non-zero values are predictors of the variable specified in the given row\nthe diagonal of the predictor matrix should be zero because a variable cannot predict itself\n\nThe values are:\n\nNOT a predictor of the outcome: 0\ncluster variable: -2\nfixed effect of predictor: 1\nfixed effect and random effect of predictor: 2\ninclude cluster mean of predictor in addition to fixed effect of predictor: 3\ninclude cluster mean of predictor in addition to fixed effect and random effect of predictor: 4\n\n\n\nCode\npred &lt;- make.predictorMatrix(dataToImpute)\npred[1:nrow(pred), 1:ncol(pred)] &lt;- 0\npred[Y, \"id\"] &lt;- (-2) # cluster variable\npred[Y, \"Occasion\"] &lt;- 1 # fixed effect predictor\npred[Y, \"age\"] &lt;- 2 # random effect predictor\npred[Y, Y] &lt;- 1 # fixed effect predictor\n\ndiag(pred) &lt;- 0\npred\n\n\n         id age height Occasion\nid        0   0      0        0\nage       0   0      0        0\nheight   -2   2      0        1\nOccasion  0   0      0        0\n\n\n\n\n7.2.1.4 Syntax\n\n\nCode\nmi_mice &lt;- mice(\n  as.data.frame(dataToImpute),\n  method = meth,\n  predictorMatrix = pred,\n  m = numImputations,\n  maxit = 5, # generally use 100 maximum iterations; this example uses 5 for speed\n  seed = 52242)\n\n\n\n iter imp variable\n  1   1  height\n  1   2  height\n  1   3  height\n  1   4  height\n  1   5  height\n  2   1  height\n  2   2  height\n  2   3  height\n  2   4  height\n  2   5  height\n  3   1  height\n  3   2  height\n  3   3  height\n  3   4  height\n  3   5  height\n  4   1  height\n  4   2  height\n  4   3  height\n  4   4  height\n  4   5  height\n  5   1  height\n  5   2  height\n  5   3  height\n  5   4  height\n  5   5  height\n\n\n\n\n\n7.2.2 jomo\n\n\nCode\nlevel1Vars &lt;- c(\"height\")\nlevel2Vars &lt;- c(\"v3\",\"v4\")\nclusterVars &lt;- c(\"id\")\nfullyObservedCovariates &lt;- c(\"age\",\"Occasion\")\n\nset.seed(52242)\n\nmi_jomo &lt;- jomo(\n  Y = data.frame(dataToImpute[, level1Vars]),\n  #Y2 = data.frame(dataToImpute[, level2Vars]),\n  X = data.frame(dataToImpute[, fullyObservedCovariates]),\n  clus = data.frame(dataToImpute[, clusterVars]),\n  nimp = numImputations,\n  meth = \"random\"\n)\n\n\n\n\n7.2.3 Amelia\n\n! in the console output indicates that the current estimated complete data covariance matrix is not invertible\n* in the console output indicates that the likelihood has not monotonically increased in that step\n\n\n\nCode\nboundVars &lt;- c(\"height\")\nboundCols &lt;- which(names(dataToImpute) %in% boundVars)\nboundLower &lt;- 100\nboundUpper &lt;- 200\n\nvarBounds &lt;- cbind(boundCols, boundLower, boundUpper)\n\nset.seed(52242)\n\nmi_amelia &lt;- amelia(\n  dataToImpute,\n  m = numImputations,\n  ts = \"age\",\n  cs = \"id\",\n  polytime = 1,\n  intercs = TRUE,\n  #ords = ordinalVars,\n  #bounds = varBounds,\n  parallel = \"snow\",\n  #ncpus = numCores,\n  empri = .01*nrow(dataToImpute)) # ridge prior for numerical stability\n\n\n-- Imputation 1 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n 21\n\n-- Imputation 2 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18\n\n-- Imputation 3 --\n\n  1  2  3  4  5  6  7  8\n\n-- Imputation 4 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40\n 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80\n 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100\n 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n 121 122 123\n\n-- Imputation 5 --\n\n  1  2  3  4  5  6  7  8  9 10 11 12\n\n\n\n\n7.2.4 Mplus\n\n7.2.4.1 Save Mplus Data File\nSave R object as Mplus data file:\n\n\nCode\nprepareMplusData(dataToImpute, file.path(\"dataToImpute.dat\"))\n\n\n\n\n7.2.4.2 Mplus Syntax for Multilevel Imputation\nMplus syntax for multilevel imputation:\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n!!!!!  MPLUS SYNTAX LINES CANNOT EXCEED 90 CHARACTERS;\n!!!!!  VARIABLE NAMES AND PARAMETER LABELS CANNOT EXCEED 8 CHARACTERS EACH;\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\nTITLE: Model Title\n\nDATA: FILE = \"dataToImpute.dat\";\n\nVARIABLE:\n  NAMES = id age height Occasion;\n  MISSING = .;\n  USEVARIABLES ARE age height Occasion;\n  !CATEGORICAL ARE INSERT_NAMES_OF_CATEGORICAL_VARIABLES_HERE;\n  CLUSTER = id;\n\nANALYSIS:\n  TYPE = twolevel basic;\n  bseed = 52242;\n  PROCESSORS = 2;\n    \nDATA IMPUTATION:\n  IMPUTE = age(0-100) height Occasion; !put ' (c)' after categorical vars\n  NDATASETS = 100;\n  SAVE = imp*.dat\nPutting a range of values after a variable (e.g., 0-100) sets the lower and upper bounds of the imputed values. This would save a implist.dat file that can be used to run the model on the multiply imputed data, as shown here.",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#logged-events",
    "href": "multipleImputation.html#logged-events",
    "title": "Multiple Imputation",
    "section": "9.1 Logged Events",
    "text": "9.1 Logged Events\n\n\nCode\nmi_mice$loggedEvents\n\n\nNULL",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#trace-plots",
    "href": "multipleImputation.html#trace-plots",
    "title": "Multiple Imputation",
    "section": "9.2 Trace Plots",
    "text": "9.2 Trace Plots\nOn convergence, the streams of the trace plots should intermingle and be free of any trend (at the later iterations). Convergence is diagnosed when the variance between different sequences is no larger than the variance within each individual sequence.\n\nhttps://stefvanbuuren.name/fimd/sec-algoptions.html (archived at https://perma.cc/4S54-465R)\n\n\n\nCode\nplot(mi_mice, c(\"height\"))",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#density-plots",
    "href": "multipleImputation.html#density-plots",
    "title": "Multiple Imputation",
    "section": "9.3 Density Plots",
    "text": "9.3 Density Plots\n\n\nCode\ndensityplot(mi_mice)\n\n\n\n\n\n\n\n\n\nCode\ndensityplot(mi_mice, ~ height)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#strip-plots",
    "href": "multipleImputation.html#strip-plots",
    "title": "Multiple Imputation",
    "section": "9.4 Strip Plots",
    "text": "9.4 Strip Plots\n\n\nCode\nstripplot(mi_mice)\n\n\n\n\n\n\n\n\n\nCode\nstripplot(mi_mice, height ~ .imp)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#modifycreate-new-variables",
    "href": "multipleImputation.html#modifycreate-new-variables",
    "title": "Multiple Imputation",
    "section": "10.1 Modify/Create New Variables",
    "text": "10.1 Modify/Create New Variables\n\n10.1.1 mice\n\n\nCode\nmi_mice_long &lt;- complete(\n  mi_mice,\n  action = \"long\",\n  include = TRUE)\n\nmi_mice_long$newVar &lt;- mi_mice_long$age * mi_mice_long$height\n\n\n\n\n10.1.2 jomo\n\n\nCode\nmi_jomo &lt;- mi_jomo %&gt;% \n  rename(height = dataToImpute...level1Vars.)\n\nmi_jomo$newVar &lt;- mi_jomo$age * mi_jomo$height\n\n\n\n\n10.1.3 Amelia\n\n\nCode\nfor(i in 1:length(mi_amelia$imputations)){\n  mi_amelia$imputations[[i]]$newVar &lt;- mi_amelia$imputations[[i]]$age * mi_amelia$imputations[[i]]$height\n}",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#convert-to-mids-object",
    "href": "multipleImputation.html#convert-to-mids-object",
    "title": "Multiple Imputation",
    "section": "10.2 Convert to mids object",
    "text": "10.2 Convert to mids object\n\n10.2.1 mice\n\n\nCode\nmi_mice_mids &lt;- as.mids(mi_mice_long)\n\n\n\n\n10.2.2 jomo\n\n\nCode\nmi_jomo_mids &lt;- miceadds::jomo2mids(mi_jomo)\n\n\n\n\n10.2.3 Amelia\n\n\nCode\nmi_amelia_mids &lt;- miceadds::datlist2mids(mi_amelia$imputations)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#export-for-mplus",
    "href": "multipleImputation.html#export-for-mplus",
    "title": "Multiple Imputation",
    "section": "10.3 Export for Mplus",
    "text": "10.3 Export for Mplus\n\n\nCode\nmids2mplus(mi_mice_mids, path = file.path(\"InsertFilePathHere\", fsep = \"\"))\nmids2mplus(mi_jomo_mids, path = file.path(\"InsertFilePathHere\", fsep = \"\"))\nmids2mplus(mi_amelia_mids, path = file.path(\"InsertFilePathHere\", fsep = \"\"))",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#multiple-regression",
    "href": "multipleImputation.html#multiple-regression",
    "title": "Multiple Imputation",
    "section": "11.1 Multiple Regression",
    "text": "11.1 Multiple Regression\n\n\nCode\nfit_lm &lt;- with(\n  data = mi_mice,\n  expr = lm(height ~ age + Occasion))\n\n\n\n11.1.1 Pool Results Across Models\n\n\nCode\nfit_lm_pooled &lt;- mice::pool(fit_lm)\n\nfit_lm_pooled\n\n\nError:\n! arguments imply differing number of rows: 1, 0\n\n\nCode\nsummary(fit_lm_pooled)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "multipleImputation.html#mixed-model",
    "href": "multipleImputation.html#mixed-model",
    "title": "Multiple Imputation",
    "section": "11.2 Mixed Model",
    "text": "11.2 Mixed Model\n\n\nCode\nfit_lmer &lt;- with(\n  data = mi_mice,\n  expr = lme4::lmer(height ~ age + Occasion + (1|id)))\n\n\n\n11.2.1 Pool Results Across Models\n\n\nCode\nfit_lmer_pooled &lt;- mice::pool(fit_lmer)\n\nfit_lmer_pooled\n\n\nError:\n! arguments imply differing number of rows: 1, 0\n\n\nCode\nsummary(fit_lmer_pooled)",
    "crumbs": [
      "About",
      "Multiple Imputation"
    ]
  },
  {
    "objectID": "irt.html",
    "href": "irt.html",
    "title": "Item Response Theory",
    "section": "",
    "text": "https://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#parameter-logistic-model",
    "href": "irt.html#parameter-logistic-model",
    "title": "Item Response Theory",
    "section": "2.1 1-parameter logistic model",
    "text": "2.1 1-parameter logistic model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-onePL",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#parameter-logistic-model-1",
    "href": "irt.html#parameter-logistic-model-1",
    "title": "Item Response Theory",
    "section": "2.2 2-parameter logistic model",
    "text": "2.2 2-parameter logistic model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-twoPL",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#parameter-multi-dimensional-logistic-model",
    "href": "irt.html#parameter-multi-dimensional-logistic-model",
    "title": "Item Response Theory",
    "section": "2.3 2-parameter multi-dimensional logistic model",
    "text": "2.3 2-parameter multi-dimensional logistic model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-twoPLmultidimensional",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#parameter-logistic-model-2",
    "href": "irt.html#parameter-logistic-model-2",
    "title": "Item Response Theory",
    "section": "2.4 3-parameter logistic model",
    "text": "2.4 3-parameter logistic model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-threePL",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#parameter-logistic-model-3",
    "href": "irt.html#parameter-logistic-model-3",
    "title": "Item Response Theory",
    "section": "2.5 4-parameter logistic model",
    "text": "2.5 4-parameter logistic model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-fourPL",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "irt.html#graded-response-model",
    "href": "irt.html#graded-response-model",
    "title": "Item Response Theory",
    "section": "2.6 Graded response model",
    "text": "2.6 Graded response model\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/irt.html#irt-gradedResponseModel",
    "crumbs": [
      "About",
      "Item Response Theory"
    ]
  },
  {
    "objectID": "factorAnalysis.html",
    "href": "factorAnalysis.html",
    "title": "Factor Analysis",
    "section": "",
    "text": "1 Overview\nhttps://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html\n\n\n2 Confirmatory Factor Analysis (CFA)\nOverview: https://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#cfa\nCode examples: https://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#cfaExamples\n\n\n3 Exploratory Factor Analysis (EFA)\nOverview: https://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#efa\nCode examples: https://isaactpetersen.github.io/Principles-Psychological-Assessment/factor-analysis-PCA.html#efaExamples\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Factor Analysis"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "The Lab Wiki was generated with rmarkdown (archived at https://perma.cc/VYU5-J2BF). The codebase that was used to generate the Data Analysis Guides is located here.\n\n1 Contact\nIf you have questions about the lab and are not sure who to contact, please contact devpsy-lab@uiowa.edu.\n\n\n2 License\n\n\n\nCC BY 4.0\n\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nCC BY 4.0\n\n\n\n\n3 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.2 (2025-10-31)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.2    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.2       htmltools_0.5.9   otel_0.2.0        yaml_2.3.12      \n [9] rmarkdown_2.30    knitr_1.51        jsonlite_2.0.0    xfun_0.55        \n[13] digest_0.6.39     rlang_1.1.6       evaluate_1.0.5   \n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "dags.html",
    "href": "dags.html",
    "title": "Directed Acyclic Graphs (DAGs)",
    "section": "",
    "text": "When conceptualizing and designing a study, or when developing plans to test a research question, it is important to draw a directed acyclic graph (DAG). DAGs, like path diagrams, are causal diagrams. Causal diagrams depict the hypoothesized causal processes that link two or more variables. Path diagrams are typically used after analysis to describe and report the findings in analysis (when using path analysis, factor analysis, or structural equation modeling). By contrast, DAGs are particularly useful when designing a study or before analysis, because they can help specify which variables it is important to control for and—just as importantly—which variables it is important not to control for.\nWhen drawing a DAG for your study, draw all the variables that link the hypothesized cause to the hypothesized effect, including confounders, mediators, and colliders. In your study, it is important to control for confounders. Moreover, it is important not to control to control for mediators when you are interested in the total effect of the predictor on the outcome. In addition, it is important not to control for descendants of the outcome variable. When there is a collision, it is important not to control for the collider when examining the association between the two causes of the collider. The only time when one should control for a collider is when the collider is also a cause (i.e., confound) of both the predictor and outcome variable rather than a common effect of both.\nFor more information on DAGs, including ancestors, descendants, confounders, and colliders, see here: https://isaactpetersen.github.io/Fantasy-Football-Analytics-Textbook/causal-inference.html#sec-causalDiagrams.\nAfter determining what variables are confounders and what are important to control for, there are various ways one can control for variables, as described here: https://isaactpetersen.github.io/Fantasy-Football-Analytics-Textbook/causal-inference.html#sec-causalInferenceControlVariables.\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "Directed Acyclic Graphs"
    ]
  },
  {
    "objectID": "dataManagement.html",
    "href": "dataManagement.html",
    "title": "Data Management",
    "section": "",
    "text": "Importing data takes syntax of the following form for .csv files:\n\n\nCode\ndata &lt;- read.csv(\"filepath/filename.csv\")\n\n\nNote: it is important to use forward slashes (“/”) rather than backslashes (“\\”) when specifying filepaths in R.\nBelow, I import a .csv file and save it into an object called mydata (you could call this object whatever you want):\n\n\nCode\nmydata &lt;- read.csv(\"https://osf.io/s6wrm/download\")\n\n\nImporting data takes syntax of the following form for .RData files:\n\n\nCode\nload(\"filepath/filename.RData\")\n\n\n\n\n\n\nCode\ndataNames &lt;- paste(\"data\", 1:100, sep = \"\")\ndataFilenames &lt;- paste(dataNames, \".csv\", sep = \"\")\ndataFilepaths &lt;- paste(\"C:/users/username/\", dataFilenames, sep = \"\")\n\ndata_list &lt;- lapply(dataFilepaths, read.csv) # lapply(dataFilepaths, data.table::fread) is even faster\nnames(data_list) &lt;- basename(dataFilepaths)\n\n\nAlternatively, if you want to load all .csv files in a directory, you can identify the filenames programmatically:\n\n\nCode\ndataFilenames &lt;- list.files(\n  path = \"C:/users/username/\",\n  pattern = \"\\\\.csv$\")\n\ndataFilepaths &lt;- list.files(\n  path = \"C:/users/username/\",\n  pattern = \"\\\\.csv$\",\n  full.names = TRUE)\n\ndata_list &lt;- lapply(dataFilepaths, read.csv) # lapply(dataFilepaths, data.table::fread) is even faster\nnames(data_list) &lt;- basename(dataFilepaths)",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#importMultipleDataFiles",
    "href": "dataManagement.html#importMultipleDataFiles",
    "title": "Data Management",
    "section": "",
    "text": "Code\ndataNames &lt;- paste(\"data\", 1:100, sep = \"\")\ndataFilenames &lt;- paste(dataNames, \".csv\", sep = \"\")\ndataFilepaths &lt;- paste(\"C:/users/username/\", dataFilenames, sep = \"\")\n\ndata_list &lt;- lapply(dataFilepaths, read.csv) # lapply(dataFilepaths, data.table::fread) is even faster\nnames(data_list) &lt;- basename(dataFilepaths)\n\n\nAlternatively, if you want to load all .csv files in a directory, you can identify the filenames programmatically:\n\n\nCode\ndataFilenames &lt;- list.files(\n  path = \"C:/users/username/\",\n  pattern = \"\\\\.csv$\")\n\ndataFilepaths &lt;- list.files(\n  path = \"C:/users/username/\",\n  pattern = \"\\\\.csv$\",\n  full.names = TRUE)\n\ndata_list &lt;- lapply(dataFilepaths, read.csv) # lapply(dataFilepaths, data.table::fread) is even faster\nnames(data_list) &lt;- basename(dataFilepaths)",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-equal-to",
    "href": "dataManagement.html#is-equal-to",
    "title": "Data Management",
    "section": "9.1 Is Equal To: ==",
    "text": "9.1 Is Equal To: ==\n\n\nCode\nmydata$survived == 1\n\n\n   [1]  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n  [13]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [25] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n  [37] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n  [61]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n  [73] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n  [85]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n  [97] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [109]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [121]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n [133]  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [145]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [169] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [181] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n [193] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n [205]  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n [217] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [229]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [241]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [253] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n [265] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [277] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [289] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n [301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [313] FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n [337]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n [349] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n [361]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [373]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [397] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n [409] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [421] FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [433]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n [445]  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [469] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n [481] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [493] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n [505] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n [517] FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [529]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [541]  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [553]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n [577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [589]  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [601] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [625] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE\n [661] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [685]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [697]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [709] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n [721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [733] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n [745] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [769]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [781] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [793] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [805] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n [817]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n [829] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n [853] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [865] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [877] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [889] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [901] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [925] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [961] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [985] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n [997] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[1009] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[1033] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-not-equal-to",
    "href": "dataManagement.html#is-not-equal-to",
    "title": "Data Management",
    "section": "9.2 Is Not Equal To: !=",
    "text": "9.2 Is Not Equal To: !=\n\n\nCode\nmydata$survived != 1\n\n\n   [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n  [13] FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n  [25]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [37]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n  [49]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n  [73]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n  [85] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n  [97]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [109] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n [121] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n [133] FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [145] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n [169]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n [181]  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [193]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE\n [205] FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n [217]  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [241] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [253]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n [265]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n [277]  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n [289]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE\n [301] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [313]  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n [337] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n [349]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [361] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [373] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [385] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n [397]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n [409]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n [421]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [433] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [445] FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [469]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n [481]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [493]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE\n [505]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n [517]  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [529] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [541] FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [553] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [565]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n [577]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [589] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [601]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE\n [613]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [625]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [637]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [649] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [661]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [673] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [685] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [697] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n [709]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n [721]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n [745]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [757]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [769] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [781]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n [793]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [805]  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [817] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE\n [829]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [841]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n [853]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [865]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE\n [877]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [889]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [901]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [913]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [925]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [937]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [949]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [961]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [985]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n [997]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1009]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1021]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[1033]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n[1045]  TRUE  TRUE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#greater-than",
    "href": "dataManagement.html#greater-than",
    "title": "Data Management",
    "section": "9.3 Greater Than: >",
    "text": "9.3 Greater Than: &gt;\n\n\nCode\nmydata$parch &gt; 1\n\n\n   [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [85] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [97] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [109]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [277]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [337] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [397]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [433]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [493] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [541]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [577] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [589]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [745] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [829]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [877] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [913] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [925] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [949] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [961] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1009] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#less-than",
    "href": "dataManagement.html#less-than",
    "title": "Data Management",
    "section": "9.4 Less Than: <",
    "text": "9.4 Less Than: &lt;\n\n\nCode\nmydata$parch &lt; 1\n\n\n   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [13]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n  [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n  [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n  [49]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n  [73] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n  [85] FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n  [97]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [109] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n [121]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [133]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [169]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [193]  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [205]  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [217]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [241] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [253]  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [265]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [277] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [301] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [313] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [337]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE\n [349]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n [361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [385] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [397] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n [409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [433] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [445] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n [457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [469]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [481]  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [493]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [505]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [541] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [553]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [565]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [577]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [589] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [601]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [613]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [625] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [637]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [649]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [661]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [673] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [685]  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [697]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [709]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [721]  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [733] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE\n [745]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [757]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [769]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [781]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [793]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [805]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [817] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [829] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [841]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [853]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [865]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n [877]  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [889]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [901]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [925] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [937]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [949]  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [961]  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [973] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [985]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [997]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n[1009]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[1021]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1033]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1045]  TRUE  TRUE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#greater-than-or-equal-to",
    "href": "dataManagement.html#greater-than-or-equal-to",
    "title": "Data Management",
    "section": "9.5 Greater Than or Equal To: >=",
    "text": "9.5 Greater Than or Equal To: &gt;=\n\n\nCode\nmydata$parch &gt;= 1\n\n\n   [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n  [73]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [85]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [97] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [109]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [121] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [169] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [193] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [277]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [313]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [337] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [397]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [433]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [469] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [481] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [493] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [541]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [577] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [589]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [625]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n [745] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [829]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [877] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [913]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [925]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [949] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [961] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[1009] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#less-than-or-equal-to",
    "href": "dataManagement.html#less-than-or-equal-to",
    "title": "Data Management",
    "section": "9.6 Less Than or Equal To: <=",
    "text": "9.6 Less Than or Equal To: &lt;=\n\n\nCode\nmydata$parch &lt;= 1\n\n\n   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [49]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n  [73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n  [85]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [97]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [109] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [217]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [241] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [253]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [277] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [301]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [337]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [385]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [397] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [433] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [445] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [469]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [505]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [541] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [565]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [577]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [589] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [601]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [613]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [625]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [637]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [649]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [661]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [673]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [685]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [697]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [709]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [721]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [733] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [745]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [757]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [769]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [781]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [793]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [805]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [817]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [829] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [841]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [853]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [865]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [877]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [889]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [901]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [913]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [925]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [937]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [949]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [961]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [973] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [985]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [997]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[1009]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1021]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1033]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1045]  TRUE  TRUE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-in-a-value-of-another-vector-in",
    "href": "dataManagement.html#is-in-a-value-of-another-vector-in",
    "title": "Data Management",
    "section": "9.7 Is in a Value of Another Vector: %in%",
    "text": "9.7 Is in a Value of Another Vector: %in%\n\n\nCode\nanotherVector &lt;- c(0,1)\nmydata$parch %in% anotherVector\n\n\n   [1]  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [49]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n  [73]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n  [85]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n  [97]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [109] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [181]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [205]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [217]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [241] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [253]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [277] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [301]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [313]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [325]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [337]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [349]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [361]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [373]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [385]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [397] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [409]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [421]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [433] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [445] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [457]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [469]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [481]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [493]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [505]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [517]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [529]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [541] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [553]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [565]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [577]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [589] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [601]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [613]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [625]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [637]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [649]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [661]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [673]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [685]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [697]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [709]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [721]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [733] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n [745]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [757]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [769]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [781]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [793]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [805]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [817]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [829] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [841]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [853]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [865]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [877]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [889]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [901]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [913]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [925]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [937]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [949]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n [961]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [973] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [985]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [997]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[1009]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1021]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1033]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[1045]  TRUE  TRUE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-not-in-a-value-of-another-vector-ni",
    "href": "dataManagement.html#is-not-in-a-value-of-another-vector-ni",
    "title": "Data Management",
    "section": "9.8 Is Not in a Value of Another Vector: %ni%",
    "text": "9.8 Is Not in a Value of Another Vector: %ni%\nNote: this function is part of the petersenlab package and is not available in base R.\n\n\nCode\nmydata$parch %ni% anotherVector\n\n\n   [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n  [73] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [85] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [97] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [109]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [277]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [337] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [397]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [433]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [493] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [541]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [577] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [589]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [745] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [829]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [877] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [913] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [925] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [949] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [961] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[1009] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-missing-is.na",
    "href": "dataManagement.html#is-missing-is.na",
    "title": "Data Management",
    "section": "9.9 Is Missing: is.na()",
    "text": "9.9 Is Missing: is.na()\n\n\nCode\nis.na(mydata$prediction)\n\n\n   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [589] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [733] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [745] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [829] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [925] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1009] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#is-not-missing-is.na",
    "href": "dataManagement.html#is-not-missing-is.na",
    "title": "Data Management",
    "section": "9.10 Is Not Missing: !is.na()",
    "text": "9.10 Is Not Missing: !is.na()\n\n\nCode\n!is.na(mydata$prediction)\n\n\n   [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [15] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [29] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [43] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [57] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [71] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [85] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n  [99] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [113] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [127] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [141] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [155] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [169] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [183] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [197] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [211] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [225] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [239] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [253] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [267] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [281] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [295] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [309] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [323] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [337] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [351] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [365] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [379] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [393] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [407] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [421] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [435] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [449] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [463] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [477] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [491] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [505] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [519] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [533] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [547] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [561] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [575] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [589] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [603] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [617] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [631] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [645] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [659] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [673] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [687] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [701] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [715] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [729] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [743] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [757] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [771] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [785] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [799] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [813] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [827] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [841] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [855] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [869] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [883] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [897] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [911] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [925] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [939] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [953] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [967] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [981] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [995] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1009] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1023] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[1037] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#and",
    "href": "dataManagement.html#and",
    "title": "Data Management",
    "section": "9.11 And: &",
    "text": "9.11 And: &\n\n\nCode\n!is.na(mydata$prediction) & mydata$parch &gt;= 1\n\n\n   [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n  [73]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [85]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [97] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [109]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [121] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [169] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [193] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [277]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [313]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [337] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [397]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [433]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [469] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [481] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [493] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [541]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [577] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [589]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [625]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n [745] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [829]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [877] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [913]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [925]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [949] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [961] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[1009] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#or",
    "href": "dataManagement.html#or",
    "title": "Data Management",
    "section": "9.12 Or: |",
    "text": "9.12 Or: |\n\n\nCode\nis.na(mydata$prediction) | mydata$parch &gt;= 1\n\n\n   [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n  [13] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\n  [49] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n  [73]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [85]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n  [97] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [109]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [121] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [133] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [169] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE\n [193] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [205] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [217] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [241]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [253] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [265] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n [277]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [301]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [313]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [325] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n [337] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE\n [349] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE\n [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [385]  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [397]  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE\n [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [433]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [445]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE\n [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [469] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [481] FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n [493] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [505] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [529] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [541]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [553] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [565] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [577] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n [589]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [601] FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [625]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [673]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [685] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [697] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [721] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [733]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE\n [745] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\n [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [793] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [817]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [829]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n [877] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n [913]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE\n [925]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [937] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n [949] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [961] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [973]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [985] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n [997] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n[1009] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n[1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[1045] FALSE FALSE",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#one-variable",
    "href": "dataManagement.html#one-variable",
    "title": "Data Management",
    "section": "11.1 One Variable",
    "text": "11.1 One Variable\nTo subset one variable, use the following syntax:\n\n\nCode\nmydata$age\n\n\n   [1] 29.00  0.92  2.00 30.00 25.00 48.00 63.00 39.00 53.00 71.00 47.00 18.00\n  [13] 24.00 26.00 80.00 24.00 50.00 32.00 36.00 37.00 47.00 26.00 42.00 29.00\n  [25] 25.00 25.00 19.00 35.00 28.00 45.00 40.00 30.00 58.00 42.00 45.00 22.00\n  [37] 41.00 48.00 44.00 59.00 60.00 41.00 45.00 42.00 53.00 36.00 58.00 33.00\n  [49] 28.00 17.00 11.00 14.00 36.00 36.00 49.00 36.00 76.00 46.00 47.00 27.00\n  [61] 33.00 36.00 30.00 45.00 27.00 26.00 22.00 47.00 39.00 37.00 64.00 55.00\n  [73] 70.00 36.00 64.00 39.00 38.00 51.00 27.00 33.00 31.00 27.00 31.00 17.00\n  [85] 53.00  4.00 54.00 50.00 27.00 48.00 48.00 49.00 39.00 23.00 38.00 54.00\n  [97] 36.00 36.00 30.00 24.00 28.00 23.00 19.00 64.00 60.00 30.00 50.00 43.00\n [109] 22.00 60.00 48.00 37.00 35.00 47.00 35.00 22.00 45.00 24.00 49.00 71.00\n [121] 53.00 19.00 38.00 58.00 23.00 45.00 46.00 25.00 25.00 48.00 49.00 45.00\n [133] 35.00 40.00 27.00 24.00 55.00 52.00 42.00 55.00 16.00 44.00 51.00 42.00\n [145] 35.00 35.00 38.00 35.00 38.00 50.00 49.00 46.00 50.00 32.50 58.00 41.00\n [157] 42.00 45.00 39.00 49.00 30.00 35.00 42.00 55.00 16.00 51.00 29.00 21.00\n [169] 30.00 58.00 15.00 30.00 16.00 19.00 18.00 24.00 46.00 54.00 36.00 28.00\n [181] 65.00 44.00 33.00 37.00 30.00 55.00 47.00 37.00 31.00 23.00 58.00 19.00\n [193] 64.00 39.00 22.00 65.00 28.50 45.50 23.00 29.00 22.00 18.00 17.00 30.00\n [205] 52.00 47.00 56.00 38.00 22.00 43.00 31.00 45.00 33.00 46.00 36.00 33.00\n [217] 55.00 54.00 33.00 13.00 18.00 21.00 61.00 48.00 24.00 35.00 30.00 34.00\n [229] 40.00 35.00 50.00 39.00 56.00 28.00 56.00 56.00 24.00 18.00 24.00 23.00\n [241]  6.00 45.00 40.00 57.00 32.00 62.00 54.00 43.00 52.00 62.00 67.00 63.00\n [253] 61.00 48.00 18.00 52.00 39.00 48.00 49.00 17.00 39.00 31.00 40.00 61.00\n [265] 47.00 35.00 64.00 60.00 60.00 54.00 21.00 55.00 31.00 57.00 45.00 50.00\n [277] 27.00 50.00 21.00 51.00 21.00 31.00 62.00 36.00 30.00 28.00 30.00 18.00\n [289] 25.00 34.00 36.00 57.00 18.00 23.00 36.00 28.00 51.00 32.00 19.00 28.00\n [301]  1.00  4.00 12.00 36.00 34.00 19.00 23.00 26.00 42.00 27.00 24.00 15.00\n [313] 60.00 40.00 20.00 25.00 36.00 25.00 42.00 42.00  0.83 26.00 22.00 35.00\n [325] 19.00 44.00 54.00 52.00 37.00 29.00 25.00 45.00 29.00 28.00 29.00 28.00\n [337] 24.00  8.00 31.00 31.00 22.00 30.00 21.00  8.00 18.00 48.00 28.00 32.00\n [349] 17.00 29.00 24.00 25.00 18.00 18.00 34.00 54.00  8.00 42.00 34.00 27.00\n [361] 30.00 23.00 21.00 18.00 40.00 29.00 18.00 36.00 38.00 35.00 38.00 34.00\n [373] 34.00 16.00 26.00 47.00 21.00 21.00 24.00 24.00 34.00 30.00 52.00 30.00\n [385]  0.67 24.00 44.00  6.00 28.00 62.00 30.00  7.00 43.00 45.00 24.00 24.00\n [397] 49.00 48.00 55.00 24.00 32.00 21.00 18.00 20.00 23.00 36.00 54.00 50.00\n [409] 44.00 29.00 21.00 42.00 63.00 60.00 33.00 17.00 42.00 24.00 47.00 24.00\n [421] 22.00 32.00 23.00 34.00 24.00 22.00 35.00 45.00 57.00 31.00 26.00 30.00\n [433]  1.00  3.00 25.00 22.00 17.00 34.00 36.00 24.00 61.00 50.00 42.00 57.00\n [445]  1.00 31.00 24.00 30.00 40.00 32.00 30.00 46.00 13.00 41.00 19.00 39.00\n [457] 48.00 70.00 27.00 54.00 39.00 16.00 62.00 32.50 14.00  2.00  3.00 36.50\n [469] 26.00 19.00 28.00 20.00 29.00 39.00 22.00 23.00 29.00 28.00 50.00 19.00\n [481] 41.00 21.00 19.00 43.00 32.00 34.00 30.00 27.00  2.00  8.00 33.00 36.00\n [493] 34.00 30.00 28.00 23.00  0.83  3.00 24.00 50.00 19.00 21.00 26.00 25.00\n [505] 27.00 25.00 18.00 20.00 30.00 59.00 30.00 35.00 40.00 25.00 41.00 25.00\n [517] 18.50 14.00 50.00 23.00 28.00 27.00 29.00 27.00 40.00 31.00 30.00 23.00\n [529] 31.00 12.00 40.00 32.50 27.00 29.00  2.00  4.00 29.00  0.92  5.00 36.00\n [541] 33.00 66.00 31.00 26.00 24.00 42.00 13.00 16.00 35.00 16.00 25.00 20.00\n [553] 18.00 30.00 26.00 40.00  0.83 18.00 26.00 26.00 20.00 24.00 25.00 35.00\n [565] 18.00 32.00 19.00  4.00  6.00  2.00 17.00 38.00  9.00 11.00 39.00 27.00\n [577] 26.00 39.00 20.00 26.00 25.00 18.00 24.00 35.00  5.00  9.00  3.00 13.00\n [589]  5.00 40.00 23.00 38.00 45.00 21.00 23.00 17.00 30.00 23.00 13.00 20.00\n [601] 32.00 33.00  0.75  0.75  5.00 24.00 18.00 40.00 26.00 20.00 18.00 45.00\n [613] 27.00 22.00 19.00 26.00 22.00 20.00 32.00 21.00 18.00 26.00  6.00  9.00\n [625] 40.00 32.00 21.00 22.00 20.00 29.00 22.00 22.00 35.00 18.50 21.00 19.00\n [637] 18.00 21.00 30.00 18.00 38.00 17.00 17.00 21.00 21.00 21.00 28.00 24.00\n [649] 16.00 37.00 28.00 24.00 21.00 32.00 29.00 26.00 18.00 20.00 18.00 24.00\n [661] 36.00 24.00 31.00 31.00 22.00 30.00 70.50 43.00 35.00 27.00 19.00 30.00\n [673]  9.00  3.00 36.00 59.00 19.00 17.00 44.00 17.00 22.50 45.00 22.00 19.00\n [685] 30.00 29.00  0.33 34.00 28.00 27.00 25.00 24.00 22.00 21.00 17.00 36.50\n [697] 36.00 30.00 16.00  1.00  0.17 26.00 33.00 25.00 22.00 36.00 19.00 17.00\n [709] 42.00 43.00 32.00 19.00 30.00 24.00 23.00 33.00 65.00 24.00 23.00 22.00\n [721] 18.00 16.00 45.00 39.00 17.00 15.00 47.00  5.00 40.50 40.50 18.00 26.00\n [733] 21.00  9.00 18.00 16.00 48.00 25.00 22.00 16.00  9.00 33.00 41.00 31.00\n [745] 38.00  9.00  1.00 11.00 10.00 16.00 14.00 40.00 43.00 51.00 32.00 20.00\n [757] 37.00 28.00 19.00 24.00 17.00 28.00 24.00 20.00 23.50 41.00 26.00 21.00\n [769] 45.00 25.00 11.00 27.00 18.00 26.00 23.00 22.00 28.00 28.00  2.00 22.00\n [781] 43.00 28.00 27.00 42.00 30.00 27.00 25.00 29.00 21.00 20.00 48.00 17.00\n [793] 34.00 26.00 22.00 33.00 31.00 29.00  4.00  1.00 49.00 33.00 19.00 27.00\n [805] 23.00 32.00 27.00 20.00 21.00 32.00 17.00 21.00 30.00 21.00 33.00 22.00\n [817]  4.00 39.00 18.50 34.50 44.00 22.00 26.00  4.00 29.00 26.00  1.00 18.00\n [829] 36.00 25.00 37.00 22.00 26.00 29.00 29.00 22.00 22.00 32.00 34.50 36.00\n [841] 39.00 24.00 25.00 45.00 36.00 30.00 20.00 28.00 30.00 26.00 20.50 27.00\n [853] 51.00 23.00 32.00 24.00 22.00 29.00 30.50 35.00 33.00 15.00 35.00 24.00\n [865] 19.00 55.50 21.00 24.00 21.00 28.00 25.00  6.00 27.00 34.00 24.00 18.00\n [877] 22.00 15.00  1.00 20.00 19.00 33.00 12.00 14.00 29.00 28.00 18.00 26.00\n [889] 21.00 41.00 39.00 21.00 28.50 22.00 61.00 23.00 22.00  9.00 28.00 42.00\n [901] 31.00 28.00 32.00 20.00 23.00 20.00 20.00 16.00 31.00  2.00  6.00  3.00\n [913]  8.00 29.00  1.00  7.00  2.00 16.00 14.00 41.00 21.00 19.00 32.00  0.75\n [925]  3.00 26.00 21.00 25.00 22.00 25.00 24.00 28.00 19.00 25.00 18.00 32.00\n [937] 17.00 24.00 38.00 21.00 10.00  4.00  7.00  2.00  8.00 39.00 22.00 35.00\n [949] 50.00 47.00  2.00 18.00 41.00 50.00 16.00 25.00 38.50 14.50 24.00 21.00\n [961] 39.00  1.00 24.00  4.00 25.00 20.00 24.50 29.00 22.00 40.00 21.00 18.00\n [973]  4.00 10.00  9.00  2.00 40.00 45.00 19.00 30.00 32.00 33.00 23.00 21.00\n [985] 60.50 19.00 22.00 31.00 27.00  2.00 29.00 16.00 44.00 25.00 74.00 14.00\n [997] 24.00 25.00 34.00  0.42 16.00 32.00 30.50 44.00 25.00  7.00  9.00 29.00\n[1009] 36.00 18.00 63.00 11.50 40.50 10.00 36.00 30.00 33.00 28.00 28.00 47.00\n[1021] 18.00 31.00 16.00 31.00 22.00 20.00 14.00 22.00 22.00 32.50 38.00 51.00\n[1033] 18.00 21.00 47.00 28.50 21.00 27.00 36.00 27.00 15.00 45.50 14.50 26.50\n[1045] 27.00 29.00\n\n\nor:\n\n\nCode\nmydata[,\"age\"]\n\n\n   [1] 29.00  0.92  2.00 30.00 25.00 48.00 63.00 39.00 53.00 71.00 47.00 18.00\n  [13] 24.00 26.00 80.00 24.00 50.00 32.00 36.00 37.00 47.00 26.00 42.00 29.00\n  [25] 25.00 25.00 19.00 35.00 28.00 45.00 40.00 30.00 58.00 42.00 45.00 22.00\n  [37] 41.00 48.00 44.00 59.00 60.00 41.00 45.00 42.00 53.00 36.00 58.00 33.00\n  [49] 28.00 17.00 11.00 14.00 36.00 36.00 49.00 36.00 76.00 46.00 47.00 27.00\n  [61] 33.00 36.00 30.00 45.00 27.00 26.00 22.00 47.00 39.00 37.00 64.00 55.00\n  [73] 70.00 36.00 64.00 39.00 38.00 51.00 27.00 33.00 31.00 27.00 31.00 17.00\n  [85] 53.00  4.00 54.00 50.00 27.00 48.00 48.00 49.00 39.00 23.00 38.00 54.00\n  [97] 36.00 36.00 30.00 24.00 28.00 23.00 19.00 64.00 60.00 30.00 50.00 43.00\n [109] 22.00 60.00 48.00 37.00 35.00 47.00 35.00 22.00 45.00 24.00 49.00 71.00\n [121] 53.00 19.00 38.00 58.00 23.00 45.00 46.00 25.00 25.00 48.00 49.00 45.00\n [133] 35.00 40.00 27.00 24.00 55.00 52.00 42.00 55.00 16.00 44.00 51.00 42.00\n [145] 35.00 35.00 38.00 35.00 38.00 50.00 49.00 46.00 50.00 32.50 58.00 41.00\n [157] 42.00 45.00 39.00 49.00 30.00 35.00 42.00 55.00 16.00 51.00 29.00 21.00\n [169] 30.00 58.00 15.00 30.00 16.00 19.00 18.00 24.00 46.00 54.00 36.00 28.00\n [181] 65.00 44.00 33.00 37.00 30.00 55.00 47.00 37.00 31.00 23.00 58.00 19.00\n [193] 64.00 39.00 22.00 65.00 28.50 45.50 23.00 29.00 22.00 18.00 17.00 30.00\n [205] 52.00 47.00 56.00 38.00 22.00 43.00 31.00 45.00 33.00 46.00 36.00 33.00\n [217] 55.00 54.00 33.00 13.00 18.00 21.00 61.00 48.00 24.00 35.00 30.00 34.00\n [229] 40.00 35.00 50.00 39.00 56.00 28.00 56.00 56.00 24.00 18.00 24.00 23.00\n [241]  6.00 45.00 40.00 57.00 32.00 62.00 54.00 43.00 52.00 62.00 67.00 63.00\n [253] 61.00 48.00 18.00 52.00 39.00 48.00 49.00 17.00 39.00 31.00 40.00 61.00\n [265] 47.00 35.00 64.00 60.00 60.00 54.00 21.00 55.00 31.00 57.00 45.00 50.00\n [277] 27.00 50.00 21.00 51.00 21.00 31.00 62.00 36.00 30.00 28.00 30.00 18.00\n [289] 25.00 34.00 36.00 57.00 18.00 23.00 36.00 28.00 51.00 32.00 19.00 28.00\n [301]  1.00  4.00 12.00 36.00 34.00 19.00 23.00 26.00 42.00 27.00 24.00 15.00\n [313] 60.00 40.00 20.00 25.00 36.00 25.00 42.00 42.00  0.83 26.00 22.00 35.00\n [325] 19.00 44.00 54.00 52.00 37.00 29.00 25.00 45.00 29.00 28.00 29.00 28.00\n [337] 24.00  8.00 31.00 31.00 22.00 30.00 21.00  8.00 18.00 48.00 28.00 32.00\n [349] 17.00 29.00 24.00 25.00 18.00 18.00 34.00 54.00  8.00 42.00 34.00 27.00\n [361] 30.00 23.00 21.00 18.00 40.00 29.00 18.00 36.00 38.00 35.00 38.00 34.00\n [373] 34.00 16.00 26.00 47.00 21.00 21.00 24.00 24.00 34.00 30.00 52.00 30.00\n [385]  0.67 24.00 44.00  6.00 28.00 62.00 30.00  7.00 43.00 45.00 24.00 24.00\n [397] 49.00 48.00 55.00 24.00 32.00 21.00 18.00 20.00 23.00 36.00 54.00 50.00\n [409] 44.00 29.00 21.00 42.00 63.00 60.00 33.00 17.00 42.00 24.00 47.00 24.00\n [421] 22.00 32.00 23.00 34.00 24.00 22.00 35.00 45.00 57.00 31.00 26.00 30.00\n [433]  1.00  3.00 25.00 22.00 17.00 34.00 36.00 24.00 61.00 50.00 42.00 57.00\n [445]  1.00 31.00 24.00 30.00 40.00 32.00 30.00 46.00 13.00 41.00 19.00 39.00\n [457] 48.00 70.00 27.00 54.00 39.00 16.00 62.00 32.50 14.00  2.00  3.00 36.50\n [469] 26.00 19.00 28.00 20.00 29.00 39.00 22.00 23.00 29.00 28.00 50.00 19.00\n [481] 41.00 21.00 19.00 43.00 32.00 34.00 30.00 27.00  2.00  8.00 33.00 36.00\n [493] 34.00 30.00 28.00 23.00  0.83  3.00 24.00 50.00 19.00 21.00 26.00 25.00\n [505] 27.00 25.00 18.00 20.00 30.00 59.00 30.00 35.00 40.00 25.00 41.00 25.00\n [517] 18.50 14.00 50.00 23.00 28.00 27.00 29.00 27.00 40.00 31.00 30.00 23.00\n [529] 31.00 12.00 40.00 32.50 27.00 29.00  2.00  4.00 29.00  0.92  5.00 36.00\n [541] 33.00 66.00 31.00 26.00 24.00 42.00 13.00 16.00 35.00 16.00 25.00 20.00\n [553] 18.00 30.00 26.00 40.00  0.83 18.00 26.00 26.00 20.00 24.00 25.00 35.00\n [565] 18.00 32.00 19.00  4.00  6.00  2.00 17.00 38.00  9.00 11.00 39.00 27.00\n [577] 26.00 39.00 20.00 26.00 25.00 18.00 24.00 35.00  5.00  9.00  3.00 13.00\n [589]  5.00 40.00 23.00 38.00 45.00 21.00 23.00 17.00 30.00 23.00 13.00 20.00\n [601] 32.00 33.00  0.75  0.75  5.00 24.00 18.00 40.00 26.00 20.00 18.00 45.00\n [613] 27.00 22.00 19.00 26.00 22.00 20.00 32.00 21.00 18.00 26.00  6.00  9.00\n [625] 40.00 32.00 21.00 22.00 20.00 29.00 22.00 22.00 35.00 18.50 21.00 19.00\n [637] 18.00 21.00 30.00 18.00 38.00 17.00 17.00 21.00 21.00 21.00 28.00 24.00\n [649] 16.00 37.00 28.00 24.00 21.00 32.00 29.00 26.00 18.00 20.00 18.00 24.00\n [661] 36.00 24.00 31.00 31.00 22.00 30.00 70.50 43.00 35.00 27.00 19.00 30.00\n [673]  9.00  3.00 36.00 59.00 19.00 17.00 44.00 17.00 22.50 45.00 22.00 19.00\n [685] 30.00 29.00  0.33 34.00 28.00 27.00 25.00 24.00 22.00 21.00 17.00 36.50\n [697] 36.00 30.00 16.00  1.00  0.17 26.00 33.00 25.00 22.00 36.00 19.00 17.00\n [709] 42.00 43.00 32.00 19.00 30.00 24.00 23.00 33.00 65.00 24.00 23.00 22.00\n [721] 18.00 16.00 45.00 39.00 17.00 15.00 47.00  5.00 40.50 40.50 18.00 26.00\n [733] 21.00  9.00 18.00 16.00 48.00 25.00 22.00 16.00  9.00 33.00 41.00 31.00\n [745] 38.00  9.00  1.00 11.00 10.00 16.00 14.00 40.00 43.00 51.00 32.00 20.00\n [757] 37.00 28.00 19.00 24.00 17.00 28.00 24.00 20.00 23.50 41.00 26.00 21.00\n [769] 45.00 25.00 11.00 27.00 18.00 26.00 23.00 22.00 28.00 28.00  2.00 22.00\n [781] 43.00 28.00 27.00 42.00 30.00 27.00 25.00 29.00 21.00 20.00 48.00 17.00\n [793] 34.00 26.00 22.00 33.00 31.00 29.00  4.00  1.00 49.00 33.00 19.00 27.00\n [805] 23.00 32.00 27.00 20.00 21.00 32.00 17.00 21.00 30.00 21.00 33.00 22.00\n [817]  4.00 39.00 18.50 34.50 44.00 22.00 26.00  4.00 29.00 26.00  1.00 18.00\n [829] 36.00 25.00 37.00 22.00 26.00 29.00 29.00 22.00 22.00 32.00 34.50 36.00\n [841] 39.00 24.00 25.00 45.00 36.00 30.00 20.00 28.00 30.00 26.00 20.50 27.00\n [853] 51.00 23.00 32.00 24.00 22.00 29.00 30.50 35.00 33.00 15.00 35.00 24.00\n [865] 19.00 55.50 21.00 24.00 21.00 28.00 25.00  6.00 27.00 34.00 24.00 18.00\n [877] 22.00 15.00  1.00 20.00 19.00 33.00 12.00 14.00 29.00 28.00 18.00 26.00\n [889] 21.00 41.00 39.00 21.00 28.50 22.00 61.00 23.00 22.00  9.00 28.00 42.00\n [901] 31.00 28.00 32.00 20.00 23.00 20.00 20.00 16.00 31.00  2.00  6.00  3.00\n [913]  8.00 29.00  1.00  7.00  2.00 16.00 14.00 41.00 21.00 19.00 32.00  0.75\n [925]  3.00 26.00 21.00 25.00 22.00 25.00 24.00 28.00 19.00 25.00 18.00 32.00\n [937] 17.00 24.00 38.00 21.00 10.00  4.00  7.00  2.00  8.00 39.00 22.00 35.00\n [949] 50.00 47.00  2.00 18.00 41.00 50.00 16.00 25.00 38.50 14.50 24.00 21.00\n [961] 39.00  1.00 24.00  4.00 25.00 20.00 24.50 29.00 22.00 40.00 21.00 18.00\n [973]  4.00 10.00  9.00  2.00 40.00 45.00 19.00 30.00 32.00 33.00 23.00 21.00\n [985] 60.50 19.00 22.00 31.00 27.00  2.00 29.00 16.00 44.00 25.00 74.00 14.00\n [997] 24.00 25.00 34.00  0.42 16.00 32.00 30.50 44.00 25.00  7.00  9.00 29.00\n[1009] 36.00 18.00 63.00 11.50 40.50 10.00 36.00 30.00 33.00 28.00 28.00 47.00\n[1021] 18.00 31.00 16.00 31.00 22.00 20.00 14.00 22.00 22.00 32.50 38.00 51.00\n[1033] 18.00 21.00 47.00 28.50 21.00 27.00 36.00 27.00 15.00 45.50 14.50 26.50\n[1045] 27.00 29.00",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#particular-rows-of-one-variable",
    "href": "dataManagement.html#particular-rows-of-one-variable",
    "title": "Data Management",
    "section": "11.2 Particular Rows of One Variable",
    "text": "11.2 Particular Rows of One Variable\nTo subset one variable, use the following syntax:\n\n\nCode\nmydata$age[which(mydata$survived == 1)]\n\n\n  [1] 29.00  0.92 48.00 63.00 53.00 18.00 24.00 26.00 80.00 50.00 32.00 37.00\n [13] 47.00 26.00 42.00 29.00 25.00 19.00 35.00 28.00 40.00 30.00 58.00 45.00\n [25] 22.00 44.00 59.00 60.00 41.00 42.00 53.00 36.00 58.00 11.00 14.00 36.00\n [37] 36.00 76.00 47.00 27.00 33.00 36.00 30.00 45.00 26.00 22.00 39.00 64.00\n [49] 55.00 36.00 64.00 38.00 51.00 27.00 33.00 27.00 31.00 17.00 53.00  4.00\n [61] 54.00 27.00 48.00 48.00 49.00 23.00 38.00 54.00 36.00 24.00 28.00 23.00\n [73] 60.00 30.00 50.00 43.00 22.00 60.00 48.00 35.00 35.00 22.00 45.00 49.00\n [85] 53.00 19.00 58.00 23.00 45.00 25.00 25.00 48.00 49.00 35.00 27.00 24.00\n [97] 52.00 16.00 44.00 51.00 35.00 35.00 38.00 35.00 38.00 49.00 42.00 45.00\n[109] 39.00 49.00 30.00 35.00 55.00 16.00 51.00 21.00 58.00 15.00 16.00 18.00\n[121] 24.00 36.00 33.00 37.00 30.00 31.00 23.00 19.00 39.00 22.00 22.00 17.00\n[133] 30.00 52.00 56.00 43.00 45.00 33.00 33.00 54.00 13.00 18.00 21.00 48.00\n[145] 24.00 35.00 30.00 34.00 40.00 35.00 39.00 56.00 28.00 18.00 24.00 23.00\n[157]  6.00 45.00 40.00 32.00 54.00 43.00 52.00 62.00 48.00 18.00 39.00 48.00\n[169] 17.00 39.00 31.00 35.00 60.00 55.00 31.00 45.00 50.00 21.00 21.00 31.00\n[181] 36.00 28.00 36.00 36.00 32.00 19.00  1.00  4.00 12.00 36.00 34.00 19.00\n[193] 24.00 15.00 40.00 20.00 36.00 42.00  0.83 26.00 22.00 35.00 25.00 45.00\n[205] 28.00 24.00  8.00 31.00 22.00  8.00 48.00 28.00 24.00 18.00 34.00  8.00\n[217] 34.00 27.00 30.00 29.00 34.00  0.67 24.00  6.00 62.00  7.00 45.00 24.00\n[229] 24.00 48.00 55.00 20.00 54.00 29.00 42.00 17.00 24.00 23.00 24.00 45.00\n[241]  1.00  3.00 22.00 17.00 34.00 42.00  1.00 24.00 13.00 41.00 19.00 14.00\n[253]  2.00  3.00 20.00 29.00 22.00 29.00 50.00 21.00 19.00 32.00 30.00  2.00\n[265]  8.00 33.00 30.00 28.00  0.83  3.00 24.00 50.00 21.00 25.00 18.00 20.00\n[277] 30.00 30.00 40.00 50.00 28.00 27.00 31.00 31.00 12.00 40.00 32.50 29.00\n[289]  2.00  4.00 29.00  0.92  5.00 33.00 31.00 26.00 35.00 16.00 25.00 20.00\n[301] 18.00  0.83 18.00 26.00 19.00 17.00 27.00  3.00  5.00 23.00 38.00 45.00\n[313] 13.00 33.00  0.75  0.75  5.00 24.00 18.00 20.00 32.00 22.00 21.00 16.00\n[325] 32.00 18.00 22.00  9.00  3.00 36.00 17.00 45.00 30.00 29.00 36.50 36.00\n[337] 30.00  1.00  0.17 33.00 19.00 19.00 30.00 23.00 24.00 22.00  5.00 16.00\n[349]  9.00 31.00 24.00 45.00 27.00 26.00 22.00  2.00 22.00 27.00 29.00 21.00\n[361] 26.00  4.00  1.00 27.00 32.00 32.00 21.00  4.00 39.00  4.00 29.00 26.00\n[373] 25.00 22.00 26.00 22.00 20.00 27.00 23.00 32.00 24.00 15.00 21.00  6.00\n[385] 27.00 24.00 15.00  1.00 20.00 19.00 12.00 14.00 18.00 26.00 39.00 22.00\n[397] 22.00  9.00 32.00 31.00 25.00 32.00 21.00  1.00 24.00  4.00 25.00 29.00\n[409] 18.00 23.00 31.00 16.00 44.00 14.00 25.00  0.42 16.00 25.00  7.00  9.00\n[421] 29.00 18.00 63.00 22.00 38.00 47.00 15.00\n\n\nor:\n\n\nCode\nmydata[which(mydata$survived == 1), \"age\"]\n\n\n  [1] 29.00  0.92 48.00 63.00 53.00 18.00 24.00 26.00 80.00 50.00 32.00 37.00\n [13] 47.00 26.00 42.00 29.00 25.00 19.00 35.00 28.00 40.00 30.00 58.00 45.00\n [25] 22.00 44.00 59.00 60.00 41.00 42.00 53.00 36.00 58.00 11.00 14.00 36.00\n [37] 36.00 76.00 47.00 27.00 33.00 36.00 30.00 45.00 26.00 22.00 39.00 64.00\n [49] 55.00 36.00 64.00 38.00 51.00 27.00 33.00 27.00 31.00 17.00 53.00  4.00\n [61] 54.00 27.00 48.00 48.00 49.00 23.00 38.00 54.00 36.00 24.00 28.00 23.00\n [73] 60.00 30.00 50.00 43.00 22.00 60.00 48.00 35.00 35.00 22.00 45.00 49.00\n [85] 53.00 19.00 58.00 23.00 45.00 25.00 25.00 48.00 49.00 35.00 27.00 24.00\n [97] 52.00 16.00 44.00 51.00 35.00 35.00 38.00 35.00 38.00 49.00 42.00 45.00\n[109] 39.00 49.00 30.00 35.00 55.00 16.00 51.00 21.00 58.00 15.00 16.00 18.00\n[121] 24.00 36.00 33.00 37.00 30.00 31.00 23.00 19.00 39.00 22.00 22.00 17.00\n[133] 30.00 52.00 56.00 43.00 45.00 33.00 33.00 54.00 13.00 18.00 21.00 48.00\n[145] 24.00 35.00 30.00 34.00 40.00 35.00 39.00 56.00 28.00 18.00 24.00 23.00\n[157]  6.00 45.00 40.00 32.00 54.00 43.00 52.00 62.00 48.00 18.00 39.00 48.00\n[169] 17.00 39.00 31.00 35.00 60.00 55.00 31.00 45.00 50.00 21.00 21.00 31.00\n[181] 36.00 28.00 36.00 36.00 32.00 19.00  1.00  4.00 12.00 36.00 34.00 19.00\n[193] 24.00 15.00 40.00 20.00 36.00 42.00  0.83 26.00 22.00 35.00 25.00 45.00\n[205] 28.00 24.00  8.00 31.00 22.00  8.00 48.00 28.00 24.00 18.00 34.00  8.00\n[217] 34.00 27.00 30.00 29.00 34.00  0.67 24.00  6.00 62.00  7.00 45.00 24.00\n[229] 24.00 48.00 55.00 20.00 54.00 29.00 42.00 17.00 24.00 23.00 24.00 45.00\n[241]  1.00  3.00 22.00 17.00 34.00 42.00  1.00 24.00 13.00 41.00 19.00 14.00\n[253]  2.00  3.00 20.00 29.00 22.00 29.00 50.00 21.00 19.00 32.00 30.00  2.00\n[265]  8.00 33.00 30.00 28.00  0.83  3.00 24.00 50.00 21.00 25.00 18.00 20.00\n[277] 30.00 30.00 40.00 50.00 28.00 27.00 31.00 31.00 12.00 40.00 32.50 29.00\n[289]  2.00  4.00 29.00  0.92  5.00 33.00 31.00 26.00 35.00 16.00 25.00 20.00\n[301] 18.00  0.83 18.00 26.00 19.00 17.00 27.00  3.00  5.00 23.00 38.00 45.00\n[313] 13.00 33.00  0.75  0.75  5.00 24.00 18.00 20.00 32.00 22.00 21.00 16.00\n[325] 32.00 18.00 22.00  9.00  3.00 36.00 17.00 45.00 30.00 29.00 36.50 36.00\n[337] 30.00  1.00  0.17 33.00 19.00 19.00 30.00 23.00 24.00 22.00  5.00 16.00\n[349]  9.00 31.00 24.00 45.00 27.00 26.00 22.00  2.00 22.00 27.00 29.00 21.00\n[361] 26.00  4.00  1.00 27.00 32.00 32.00 21.00  4.00 39.00  4.00 29.00 26.00\n[373] 25.00 22.00 26.00 22.00 20.00 27.00 23.00 32.00 24.00 15.00 21.00  6.00\n[385] 27.00 24.00 15.00  1.00 20.00 19.00 12.00 14.00 18.00 26.00 39.00 22.00\n[397] 22.00  9.00 32.00 31.00 25.00 32.00 21.00  1.00 24.00  4.00 25.00 29.00\n[409] 18.00 23.00 31.00 16.00 44.00 14.00 25.00  0.42 16.00 25.00  7.00  9.00\n[421] 29.00 18.00 63.00 22.00 38.00 47.00 15.00",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#particular-columns-variables",
    "href": "dataManagement.html#particular-columns-variables",
    "title": "Data Management",
    "section": "11.3 Particular Columns (Variables)",
    "text": "11.3 Particular Columns (Variables)\nTo subset particular columns/variables, use the following syntax:\n\n11.3.1 Base R\n\n\nCode\nsubsetVars &lt;- c(\"survived\",\"age\",\"prediction\")\n\nmydata[,c(1,2,3)]\n\n\n\n  \n\n\n\nCode\nmydata[,c(\"survived\",\"age\",\"prediction\")]\n\n\n\n  \n\n\n\nCode\nmydata[,subsetVars]\n\n\n\n  \n\n\n\nOr, to drop columns:\n\n\nCode\ndropVars &lt;- c(\"sibsp\",\"parch\")\n\nmydata[,-c(5,6)]\n\n\n\n  \n\n\n\nCode\nmydata[,names(mydata) %ni% c(\"sibsp\",\"parch\")]\n\n\n\n  \n\n\n\nCode\nmydata[,names(mydata) %ni% dropVars]\n\n\n\n  \n\n\n\n\n\n11.3.2 Tidyverse\n\n\nCode\nmydata %&gt;%\n  select(survived, age, prediction)\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  select(survived:prediction)\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  select(all_of(subsetVars))\n\n\n\n  \n\n\n\nOr, to drop columns:\n\n\nCode\nmydata %&gt;%\n  select(-sibsp, -parch)\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  select(-c(sibsp:parch))\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  select(-all_of(dropVars))",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#particular-rows",
    "href": "dataManagement.html#particular-rows",
    "title": "Data Management",
    "section": "11.4 Particular Rows",
    "text": "11.4 Particular Rows\nTo subset particular rows, use the following syntax:\n\n11.4.1 Base R\n\n\nCode\nsubsetRows &lt;- c(1,3,5)\n\nmydata[c(1,3,5),]\n\n\n\n  \n\n\n\nCode\nmydata[subsetRows,]\n\n\n\n  \n\n\n\nCode\nmydata[which(mydata$survived == 1),]\n\n\n\n  \n\n\n\n\n\n11.4.2 Tidyverse\n\n\nCode\nmydata %&gt;%\n  filter(survived == 1)\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  filter(survived == 1, parch &lt;= 1)\n\n\n\n  \n\n\n\nCode\nmydata %&gt;%\n  filter(survived == 1 | parch &lt;= 1)",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#particular-rows-and-columns",
    "href": "dataManagement.html#particular-rows-and-columns",
    "title": "Data Management",
    "section": "11.5 Particular Rows and Columns",
    "text": "11.5 Particular Rows and Columns\nTo subset particular rows and columns, use the following syntax:\n\n11.5.1 Base R\n\n\nCode\nmydata[c(1,3,5), c(1,2,3)]\n\n\n\n  \n\n\n\nCode\nmydata[subsetRows, subsetVars]\n\n\n\n  \n\n\n\nCode\nmydata[which(mydata$survived == 1), subsetVars]\n\n\n\n  \n\n\n\n\n\n11.5.2 Tidyverse\n\n\nCode\nmydata %&gt;%\n  filter(survived == 1) %&gt;%\n  select(all_of(subsetVars))",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#all-data",
    "href": "dataManagement.html#all-data",
    "title": "Data Management",
    "section": "12.1 All Data",
    "text": "12.1 All Data\nTo view data, use the following syntax:\n\n\nCode\nView(mydata)",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#first-6-rowselements",
    "href": "dataManagement.html#first-6-rowselements",
    "title": "Data Management",
    "section": "12.2 First 6 Rows/Elements",
    "text": "12.2 First 6 Rows/Elements\nTo view only the first six rows (if a data frame) or elements (if a vector), use the following syntax:\n\n\nCode\nhead(mydata)\n\n\n\n  \n\n\n\nCode\nhead(mydata$age)\n\n\n[1] 29.00  0.92  2.00 30.00 25.00 48.00",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#data-structure",
    "href": "dataManagement.html#data-structure",
    "title": "Data Management",
    "section": "14.1 Data Structure",
    "text": "14.1 Data Structure\n\n\nCode\nstr(mydata)\n\n\n'data.frame':   1046 obs. of  7 variables:\n $ survived  : int  1 1 0 0 0 1 1 0 1 0 ...\n $ pclass    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ sex       : chr  \"female\" \"male\" \"female\" \"male\" ...\n $ age       : num  29 0.92 2 30 25 48 63 39 53 71 ...\n $ sibsp     : int  0 1 1 1 1 0 1 0 2 0 ...\n $ parch     : int  0 2 2 2 2 0 0 0 0 0 ...\n $ prediction: num  0.945 0.784 0.979 0.516 0.946 ...",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#data-dimensions",
    "href": "dataManagement.html#data-dimensions",
    "title": "Data Management",
    "section": "14.2 Data Dimensions",
    "text": "14.2 Data Dimensions\nNumber of rows and columns:\n\n\nCode\ndim(mydata)\n\n\n[1] 1046    7",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#number-of-elements",
    "href": "dataManagement.html#number-of-elements",
    "title": "Data Management",
    "section": "14.3 Number of Elements",
    "text": "14.3 Number of Elements\n\n\nCode\nlength(mydata$age)\n\n\n[1] 1046",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#number-of-missing-elements",
    "href": "dataManagement.html#number-of-missing-elements",
    "title": "Data Management",
    "section": "14.4 Number of Missing Elements",
    "text": "14.4 Number of Missing Elements\n\n\nCode\nlength(mydata$age[which(is.na(mydata$age))])\n\n\n[1] 0",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#number-of-non-missing-elements",
    "href": "dataManagement.html#number-of-non-missing-elements",
    "title": "Data Management",
    "section": "14.5 Number of Non-Missing Elements",
    "text": "14.5 Number of Non-Missing Elements\n\n\nCode\nlength(mydata$age[which(!is.na(mydata$age))])\n\n\n[1] 1046\n\n\nCode\nlength(na.omit(mydata$age))\n\n\n[1] 1046",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#overview",
    "href": "dataManagement.html#overview",
    "title": "Data Management",
    "section": "20.1 Overview",
    "text": "20.1 Overview\nMerging (also called joining) merges two data objects using a shared set of variables called “keys.” The keys are the variable(s) that uniquely identify each row (i.e., they account for the levels of nesting). In some data objects, the key might be the participant’s ID (e.g., participantID). However, some data objects have multiple keys. For instance, in long form data objects, each participant may have multiple rows corresponding to multiple timepoints. In this case, the keys are participantID and timepoint. If a participant has multiple rows corresponding to timepoints and measures, the keys are participantID, timepoint, and measure. In general, each row should have a value on each of the keys; there should be no missingness in the keys.\nTo merge two objects, the keys must be present in both objects. The keys are used to merge the variables in object 1 (x) with the variables in object 2 (y). Different merge types select different rows to merge.\nNote: if the two objects include variables with the same name (apart from the keys), R will not know how you want each to appear in the merged object. So, it will add a suffix (e.g., .x, .y) to each common variable to indicate which object (i.e., object x or object y) the variable came from, where object x is the first object—i.e., the object to which object y (the second object) is merged. In general, apart from the keys, you should not include variables with the same name in two objects to be merged. To prevent this, either remove or rename the shared variable in one of the objects, or include the shared variable as a key. However, as described above, you should include it as a key only if it uniquely identifies each row in terms of levels of nesting.",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#data-before-merging",
    "href": "dataManagement.html#data-before-merging",
    "title": "Data Management",
    "section": "20.2 Data Before Merging",
    "text": "20.2 Data Before Merging\nHere are the data in the mydata object:\n\n\nCode\nmydata\n\n\n\n  \n\n\n\nCode\ndim(mydata)\n\n\n[1] 1046   14\n\n\nHere are the data in the mydata2 object:\n\n\nCode\nmydata2\n\n\n\n  \n\n\n\nCode\ndim(mydata2)\n\n\n[1] 10  2",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#mergeTypes",
    "href": "dataManagement.html#mergeTypes",
    "title": "Data Management",
    "section": "20.3 Types of Joins",
    "text": "20.3 Types of Joins\n\n20.3.1 Visual Overview of Join Types\nBelow is a visual that depicts various types of merges/joins. Object x is the circle labeled as A. Object y is the circle labeled as B. The area of overlap in the Venn diagram indicates the rows on the keys that are shared between the two objects (e.g., participantID values 1, 2, and 3). The non-overlapping area indicates the rows on the keys that are unique to each object (e.g., participantID values 4, 5, and 6 in Object x and values 7, 8, and 9 in Object y). The shaded yellow area indicates which rows (on the keys) are kept in the merged object from each of the two objects, when using each of the merge types. For instance, a left outer join keeps the shared rows and the rows that are unique to object x, but it drops the rows that are unique to object y.\n\n\n\nTypes of merges/joins\n\n\nImage source: Predictive Hacks (archived at: https://perma.cc/WV7U-BS68)\n\n\n20.3.2 Full Outer Join\nA full outer join includes all rows in \\(x\\) or \\(y\\). It returns columns from \\(x\\) and \\(y\\). Here is how to merge two data frames using a full outer join (i.e., “full join”):\n\n\nCode\nfullJoinData &lt;- merge(mydata, mydata2, by = \"ID\", all = TRUE)\n\nfullJoinData\n\n\n\n  \n\n\n\nCode\ndim(fullJoinData)\n\n\n[1] 1051   15\n\n\nOr, alternatively, using tidyverse:\n\n\nCode\nfull_join(mydata, mydata2, by = \"ID\")\n\n\n\n  \n\n\n\n\n\n20.3.3 Left Outer Join\nA left outer join includes all rows in \\(x\\). It returns columns from \\(x\\) and \\(y\\). Here is how to merge two data frames using a left outer join (“left join”):\n\n\nCode\nleftJoinData &lt;- merge(mydata, mydata2, by = \"ID\", all.x = TRUE)\n\nleftJoinData\n\n\n\n  \n\n\n\nCode\ndim(leftJoinData)\n\n\n[1] 1046   15\n\n\nOr, alternatively, using tidyverse:\n\n\nCode\nleft_join(mydata, mydata2, by = \"ID\")\n\n\n\n  \n\n\n\n\n\n20.3.4 Right Outer Join\nA right outer join includes all rows in \\(y\\). It returns columns from \\(x\\) and \\(y\\). Here is how to merge two data frames using a right outer join (“right join”):\n\n\nCode\nrightJoinData &lt;- merge(mydata, mydata2, by = \"ID\", all.y = TRUE)\n\nrightJoinData\n\n\n\n  \n\n\n\nCode\ndim(rightJoinData)\n\n\n[1] 10 15\n\n\nOr, alternatively, using tidyverse:\n\n\nCode\nright_join(mydata, mydata2, by = \"ID\")\n\n\n\n  \n\n\n\n\n\n20.3.5 Inner Join\nAn inner join includes all rows that are in both \\(x\\) and \\(y\\). An inner join will return one row of \\(x\\) for each matching row of \\(y\\), and can duplicate values of records on either side (left or right) if \\(x\\) and \\(y\\) have more than one matching record. It returns columns from \\(x\\) and \\(y\\). Here is how to merge two data frames using an inner join:\n\n\nCode\ninnerJoinData &lt;- merge(mydata, mydata2, by = \"ID\", all.x = FALSE, all.y = FALSE)\n\ninnerJoinData\n\n\n\n  \n\n\n\nCode\ndim(innerJoinData)\n\n\n[1]  5 15\n\n\nOr, alternatively, using tidyverse:\n\n\nCode\ninner_join(mydata, mydata2, by = \"ID\")\n\n\n\n  \n\n\n\n\n\n20.3.6 Semi Join\nA semi join is a filter. A left semi join returns all rows from \\(x\\) with a match in \\(y\\). That is, it filters out records from \\(x\\) that are not in \\(y\\). Unlike an inner join, a left semi join will never duplicate rows of \\(x\\), and it includes columns from only \\(x\\) (not from \\(y\\)). Here is how to merge two data frames using a left semi join:\n\n\nCode\nsemiJoinData &lt;- semi_join(mydata, mydata2, by = \"ID\")\n\nsemiJoinData\n\n\n\n  \n\n\n\nCode\ndim(semiJoinData)\n\n\n[1]  5 14\n\n\n\n\n20.3.7 Anti Join\nAn anti join is a filter. A left anti join returns all rows from \\(x\\) without a match in \\(y\\). That is, it filters out records from \\(x\\) that are in \\(y\\). It returns columns from only \\(x\\) (not from \\(y\\)). Here is how to merge two data frames using a left anti join:\n\n\nCode\nantiJoinData &lt;- anti_join(mydata, mydata2, by = \"ID\")\n\nantiJoinData\n\n\n\n  \n\n\n\nCode\ndim(antiJoinData)\n\n\n[1] 1041   14\n\n\n\n\n20.3.8 Cross Join\nA cross join combines each row in \\(x\\) with each row in \\(y\\).\n\n\nCode\ncrossJoinData &lt;- cross_join(\n  data.frame(rater = c(\"Mother\",\"Father\",\"Teacher\")),\n  data.frame(timepoint = 1:3))\n\ncrossJoinData\n\n\n\n  \n\n\n\nCode\ndim(crossJoinData)\n\n\n[1] 9 2",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "dataManagement.html#createFunction",
    "href": "dataManagement.html#createFunction",
    "title": "Data Management",
    "section": "24.1 Create a Function",
    "text": "24.1 Create a Function\nNow, let’s put together what we have learned to create a useful function. Functions are useful if you want to perform an operation multiple times. Any operation that you want to perform multiple times, you can create a function to accomplish. Use of a function can save you time without needed to retype out all of the code each time. For instance, let’s say you want to convert temperature between Fahrenheit and Celsius, you could create a function to do that. In this case, our function has two arguments: temperature (in degrees) and unit of the original temperature (F for Fahrenheit or C for Celsius, where the default unit is Fahrenheit).\n\n\nCode\nconvert_temperature &lt;- function(temperature, unit = \"F\"){\n  if(unit == \"F\"){ # if the input temperature(s) in Fahrenheit\n    newtemp &lt;- (temperature - 32) / (9/5)\n  } else if(unit == \"C\"){ # if the input temperature(s) in Celsius\n    newtemp &lt;- (temperature * (9/5)) + 32\n  }\n  \n  return(newtemp)\n}\n\n\nNow we can use the function to convert temperatures between Fahrenheit and Celsius. A temperature of 32°F is equal to 0°C. A temperature of 0°C is equal to 89.6°F.\n\n\nCode\nconvert_temperature(\n  temperature = 32,\n  unit = \"F\"\n)\n\n\n[1] 0\n\n\nCode\nconvert_temperature(\n  temperature = 32,\n  unit = \"C\"\n)\n\n\n[1] 89.6\n\n\nWe can also convert the temperature for a vector of values at once:\n\n\nCode\nconvert_temperature(\n  temperature = c(0, 10, 20, 30, 40, 50),\n  unit = \"F\"\n)\n\n\n[1] -17.777778 -12.222222  -6.666667  -1.111111   4.444444  10.000000\n\n\nCode\nconvert_temperature(\n  temperature = c(0, 10, 20, 30, 40, 50),\n  unit = \"C\"\n)\n\n\n[1]  32  50  68  86 104 122\n\n\nBecause the default unit is “F”, we do not need to specify the unit if our input temperatures are in Fahrenheit:\n\n\nCode\nconvert_temperature(\n  c(0, 10, 20, 30, 40, 50)\n)\n\n\n[1] -17.777778 -12.222222  -6.666667  -1.111111   4.444444  10.000000",
    "crumbs": [
      "About",
      "Data Management"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n\n\n\n\n\n\n\nCode\nlibrary(\"petersenlab\")\nlibrary(\"MASS\")\nlibrary(\"tidyverse\")\nlibrary(\"psych\")\nlibrary(\"rms\")\nlibrary(\"robustbase\")\nlibrary(\"brms\")\nlibrary(\"cvTools\")\nlibrary(\"car\")\nlibrary(\"mgcv\")\nlibrary(\"AER\")\nlibrary(\"foreign\")\nlibrary(\"olsrr\")\nlibrary(\"quantreg\")\nlibrary(\"mblm\")\nlibrary(\"effects\")\nlibrary(\"correlation\")\nlibrary(\"interactions\")\nlibrary(\"lavaan\")\nlibrary(\"regtools\")\nlibrary(\"mice\")\nlibrary(\"XICOR\")\nlibrary(\"cocor\")\nlibrary(\"effectsize\")\nlibrary(\"rockchalk\")\nlibrary(\"yhat\")",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#install-libraries",
    "href": "regression.html#install-libraries",
    "title": "Regression",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#load-libraries",
    "href": "regression.html#load-libraries",
    "title": "Regression",
    "section": "",
    "text": "Code\nlibrary(\"petersenlab\")\nlibrary(\"MASS\")\nlibrary(\"tidyverse\")\nlibrary(\"psych\")\nlibrary(\"rms\")\nlibrary(\"robustbase\")\nlibrary(\"brms\")\nlibrary(\"cvTools\")\nlibrary(\"car\")\nlibrary(\"mgcv\")\nlibrary(\"AER\")\nlibrary(\"foreign\")\nlibrary(\"olsrr\")\nlibrary(\"quantreg\")\nlibrary(\"mblm\")\nlibrary(\"effects\")\nlibrary(\"correlation\")\nlibrary(\"interactions\")\nlibrary(\"lavaan\")\nlibrary(\"regtools\")\nlibrary(\"mice\")\nlibrary(\"XICOR\")\nlibrary(\"cocor\")\nlibrary(\"effectsize\")\nlibrary(\"rockchalk\")\nlibrary(\"yhat\")",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#linear-regression-model",
    "href": "regression.html#linear-regression-model",
    "title": "Regression",
    "section": "5.1 Linear regression model",
    "text": "5.1 Linear regression model\n\n\nCode\nmultipleRegressionModel &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(multipleRegressionModel)\n\n\n\nCall:\nlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.979 on 2871 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.262, Adjusted R-squared:  0.2615 \nF-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(multipleRegressionModel)\n\n\n                            2.5 %    97.5 %\n(Intercept)             1.0809881 1.3156128\nbpi_antisocialT1Sum     0.4290884 0.5019688\nbpi_anxiousDepressedSum 0.1035825 0.2179258\n\n\nCode\nprint(effectsize::standardize_parameters(multipleRegressionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |        95% CI\n----------------------------------------------------\n(Intercept)             |   1.62e-16 | [-0.03, 0.03]\nbpi antisocialT1Sum     |       0.46 | [ 0.42, 0.49]\nbpi anxiousDepressedSum |       0.10 | [ 0.06, 0.14]\n\n\n\n5.1.1 Remove missing data\n\n\nCode\nmultipleRegressionModelNoMissing &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.omit)\n\nmultipleRegressionModelNoMissing &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata %&gt;% select(bpi_antisocialT2Sum, bpi_antisocialT1Sum, bpi_anxiousDepressedSum) %&gt;% na.omit)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#linear-regression-model-on-correlationcovariance-matrix-for-pairwise-deletion",
    "href": "regression.html#linear-regression-model-on-correlationcovariance-matrix-for-pairwise-deletion",
    "title": "Regression",
    "section": "5.2 Linear regression model on correlation/covariance matrix (for pairwise deletion)",
    "text": "5.2 Linear regression model on correlation/covariance matrix (for pairwise deletion)\n\n\nCode\nmultipleRegressionModelPairwise &lt;- psych::setCor(\n  y = \"bpi_antisocialT2Sum\",\n  x = c(\"bpi_antisocialT1Sum\",\"bpi_anxiousDepressedSum\"),\n  data = cov(mydata[,c(\"bpi_antisocialT2Sum\",\"bpi_antisocialT1Sum\",\"bpi_anxiousDepressedSum\")], use = \"pairwise.complete.obs\"),\n  n.obs = nrow(mydata))\n\n\n\n\n\n\n\n\n\nCode\nsummary(multipleRegressionModelPairwise)\n\n\n\nMultiple Regression from matrix input \npsych::setCor(y = \"bpi_antisocialT2Sum\", x = c(\"bpi_antisocialT1Sum\", \n    \"bpi_anxiousDepressedSum\"), data = cov(mydata[, c(\"bpi_antisocialT2Sum\", \n    \"bpi_antisocialT1Sum\", \"bpi_anxiousDepressedSum\")], use = \"pairwise.complete.obs\"), \n    n.obs = nrow(mydata))\n\nMultiple Regression from matrix input \n\nBeta weights \n                        bpi_antisocialT2Sum\nbpi_antisocialT1Sum                    0.46\nbpi_anxiousDepressedSum                0.09\n\nMultiple R \nbpi_antisocialT2Sum \n               0.51 \n\nMultiple R2 \nbpi_antisocialT2Sum \n               0.26 \n\nCohen's set correlation R2 \n[1] 0.26\n\nSquared Canonical Correlations\nNULL\n\n\nCode\nmultipleRegressionModelPairwise[c(\"coefficients\",\"se\",\"Probability\",\"R2\",\"shrunkenR2\")]\n\n\n$coefficients\n                        bpi_antisocialT2Sum\nbpi_antisocialT1Sum               0.4560961\nbpi_anxiousDepressedSum           0.0899642\n\n$se\n                        bpi_antisocialT2Sum\nbpi_antisocialT1Sum             0.009253718\nbpi_anxiousDepressedSum         0.009253718\n\n$Probability\n                        bpi_antisocialT2Sum\nbpi_antisocialT1Sum            0.000000e+00\nbpi_anxiousDepressedSum        2.959352e-22\n\n$R2\nbpi_antisocialT2Sum \n           0.256918 \n\n$shrunkenR2\nbpi_antisocialT2Sum \n           0.256789",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#linear-regression-model-with-robust-covariance-matrix-rms",
    "href": "regression.html#linear-regression-model-with-robust-covariance-matrix-rms",
    "title": "Regression",
    "section": "5.3 Linear regression model with robust covariance matrix (rms)",
    "text": "5.3 Linear regression model with robust covariance matrix (rms)\n\n\nCode\nrmsMultipleRegressionModel &lt;- robcov(ols(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE))\n\nrmsMultipleRegressionModel\n\n\nFrequencies of Missing Values Due to Each Variable\n    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                   7501                    7613                    7616 \n\nLinear Regression Model\n\nols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, x = TRUE, y = TRUE)\n\n                Model Likelihood    Discrimination    \n                      Ratio Test           Indexes    \nObs    2874    LR chi2    873.09    R2       0.262    \nsigma1.9786    d.f.            2    R2 adj   0.261    \nd.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    \n\nResiduals\n\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\n                        Coef   S.E.   t     Pr(&gt;|t|)\nIntercept               1.1983 0.0622 19.26 &lt;0.0001 \nbpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 \nbpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 \n\n\nCode\nconfint(rmsMultipleRegressionModel)\n\n\n                             2.5 %    97.5 %\nIntercept               1.07631713 1.3202837\nbpi_antisocialT1Sum     0.42056957 0.5104877\nbpi_anxiousDepressedSum 0.09606644 0.2254418\n\n\nCode\nprint(effectsize::standardize_parameters(rmsMultipleRegressionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |        95% CI\n----------------------------------------------------\nIntercept               |   1.62e-16 | [-0.03, 0.03]\nbpi antisocialT1Sum     |       0.46 | [ 0.42, 0.49]\nbpi anxiousDepressedSum |       0.10 | [ 0.06, 0.14]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#robust-linear-regression-mm-type-iteratively-reweighted-least-squares-regression",
    "href": "regression.html#robust-linear-regression-mm-type-iteratively-reweighted-least-squares-regression",
    "title": "Regression",
    "section": "5.4 Robust linear regression (MM-type iteratively reweighted least squares regression)",
    "text": "5.4 Robust linear regression (MM-type iteratively reweighted least squares regression)\n\n\nCode\nrobustLinearRegression &lt;- lmrob(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(robustLinearRegression)\n\n\n\nCall:\nlmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, na.action = na.exclude)\n \\--&gt; method = \"MM\"\nResiduals:\n     Min       1Q   Median       3Q      Max \n-8.43518 -1.06680 -0.06707  1.14090 13.05599 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.94401    0.05406  17.464  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.49135    0.02237  21.966  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.15766    0.03102   5.083 3.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 1.628 \n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.326, Adjusted R-squared:  0.3255 \nConvergence in 14 IRWLS iterations\n\nRobustness weights: \n 12 observations c(52,347,354,517,709,766,768,979,1618,2402,2403,2404)\n     are outliers with |weight| = 0 ( &lt; 3.5e-05); \n 283 weights are ~= 1. The remaining 2579 ones are summarized as\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0001311 0.8599000 0.9470000 0.8814000 0.9816000 0.9990000 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        3.479e-05         2.365e-11         5.000e-01         5.000e-01 \n     nResample         max.it         groups        n.group       best.r.s \n           500             50              5            400              2 \n      k.fast.s          k.max    maxit.scale      trace.lev            mts \n             1            200            200              0           1000 \n    compute.rd fast.s.large.n \n             0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nCode\nconfint(robustLinearRegression)\n\n\n                             2.5 %    97.5 %\n(Intercept)             0.83801565 1.0499981\nbpi_antisocialT1Sum     0.44749135 0.5352118\nbpi_anxiousDepressedSum 0.09683728 0.2184779\n\n\nCode\nprint(effectsize::standardize_parameters(robustLinearRegression), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |         95% CI\n-----------------------------------------------------\n(Intercept)             |      -0.08 | [-0.11, -0.05]\nbpi antisocialT1Sum     |       0.48 | [ 0.44,  0.52]\nbpi anxiousDepressedSum |       0.10 | [ 0.06,  0.14]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#least-trimmed-squares-regression-for-removing-outliers",
    "href": "regression.html#least-trimmed-squares-regression-for-removing-outliers",
    "title": "Regression",
    "section": "5.5 Least trimmed squares regression (for removing outliers)",
    "text": "5.5 Least trimmed squares regression (for removing outliers)\n\n\nCode\nltsRegression &lt;- robustbase::ltsReg(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(ltsRegression)\n\n\n\nCall:\nltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n\nResiduals (from reweighted LS):\n    Min      1Q  Median      3Q     Max \n-3.9305 -0.9091  0.0000  0.9997  3.9036 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \nIntercept                0.74932    0.04795  15.626  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.54338    0.01524  35.647  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.13826    0.02343   5.901 4.06e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.534 on 2710 degrees of freedom\nMultiple R-Squared: 0.4121, Adjusted R-squared: 0.4116 \nF-statistic: 949.7 on 2 and 2710 DF,  p-value: &lt; 2.2e-16 \n\n\nCode\nconfint(robustLinearRegression)\n\n\n                             2.5 %    97.5 %\n(Intercept)             0.83801565 1.0499981\nbpi_antisocialT1Sum     0.44749135 0.5352118\nbpi_anxiousDepressedSum 0.09683728 0.2184779",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#bayesian-linear-regression",
    "href": "regression.html#bayesian-linear-regression",
    "title": "Regression",
    "section": "5.6 Bayesian linear regression",
    "text": "5.6 Bayesian linear regression\n\n\nCode\nbayesianRegularizedRegression &lt;- brms::brm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  chains = 4,\n  iter = 2000,\n  seed = 52242)\n\n\n\n\nCode\nsummary(bayesianRegularizedRegression)\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \n   Data: mydata (Number of observations: 2874) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   1.20      0.06     1.08     1.32 1.00     4472\nbpi_antisocialT1Sum         0.47      0.02     0.43     0.50 1.00     3350\nbpi_anxiousDepressedSum     0.16      0.03     0.10     0.22 1.00     3184\n                        Tail_ESS\nIntercept                   3070\nbpi_antisocialT1Sum         3292\nbpi_anxiousDepressedSum     2958\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.98      0.03     1.93     2.03 1.00     4104     3454\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nprint(effectsize::standardize_parameters(bayesianRegularizedRegression, method = \"basic\"), digits = 2)\n\n\n# Standardization method: basic\n\nComponent   |               Parameter | Std. Median |       95% CI\n------------------------------------------------------------------\nconditional |             (Intercept) |        0.00 | [0.00, 0.00]\nconditional |     bpi_antisocialT1Sum |        0.46 | [0.42, 0.49]\nconditional | bpi_anxiousDepressedSum |        0.10 | [0.07, 0.14]\nsigma       |                   sigma |        1.98 | [1.93, 2.03]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#generalized-regression-model",
    "href": "regression.html#generalized-regression-model",
    "title": "Regression",
    "section": "6.1 Generalized regression model",
    "text": "6.1 Generalized regression model\nIn this example, we predict a count variable that has a poisson distribution. We could change the distribution.\n\n\nCode\ngeneralizedRegressionModel &lt;- glm(\n  countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = \"poisson\",\n  na.action = na.exclude)\n\nsummary(generalizedRegressionModel)\n\n\n\nCall:\nglm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    family = \"poisson\", data = mydata, na.action = na.exclude)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.459999   0.019650  23.410  &lt; 2e-16 ***\nbpi_antisocialT1Sum     0.141577   0.004891  28.948  &lt; 2e-16 ***\nbpi_anxiousDepressedSum 0.047567   0.008036   5.919 3.23e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5849.4  on 2873  degrees of freedom\nResidual deviance: 4562.6  on 2871  degrees of freedom\n  (8656 observations deleted due to missingness)\nAIC: 11482\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nconfint(generalizedRegressionModel)\n\n\n                             2.5 %     97.5 %\n(Intercept)             0.42136094 0.49838751\nbpi_antisocialT1Sum     0.13196544 0.15113708\nbpi_anxiousDepressedSum 0.03177425 0.06327445\n\n\nCode\nprint(effectsize::standardize_parameters(generalizedRegressionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |       95% CI\n---------------------------------------------------\n(Intercept)             |       0.91 | [0.89, 0.94]\nbpi antisocialT1Sum     |       0.32 | [0.30, 0.34]\nbpi anxiousDepressedSum |       0.07 | [0.05, 0.09]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#generalized-regression-model-rms",
    "href": "regression.html#generalized-regression-model-rms",
    "title": "Regression",
    "section": "6.2 Generalized regression model (rms)",
    "text": "6.2 Generalized regression model (rms)\nIn this example, we predict a count variable that has a poisson distribution. We could change the distribution.\n\n\nCode\nrmsGeneralizedRegressionModel &lt;- rms::Glm(\n  countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE,\n  family = \"poisson\")\n\nrmsGeneralizedRegressionModel\n\n\nFrequencies of Missing Values Due to Each Variable\n          countVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                   7501                    7613                    7616 \n\nGeneral Linear Model\n\nrms::Glm(formula = countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    family = \"poisson\", data = mydata, x = TRUE, y = TRUE)\n\n                       Model Likelihood    \n                             Ratio Test    \n          Obs2874    LR chi2    1286.72    \nResidual d.f.2871    d.f.             2    \n          g 0.387    Pr(&gt; chi2) &lt;0.0001    \n\n                        Coef   S.E.   Wald Z Pr(&gt;|Z|)\nIntercept               0.4600 0.0196 23.41  &lt;0.0001 \nbpi_antisocialT1Sum     0.1416 0.0049 28.95  &lt;0.0001 \nbpi_anxiousDepressedSum 0.0476 0.0080  5.92  &lt;0.0001 \n\n\nCode\nconfint(rmsGeneralizedRegressionModel)\n\n\nError in `summary.rms()`:\n! adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum\n\n\nCode\nprint(effectsize::standardize_parameters(rmsGeneralizedRegressionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |       95% CI\n---------------------------------------------------\nIntercept               |       0.91 | [0.89, 0.94]\nbpi antisocialT1Sum     |       0.32 | [0.30, 0.34]\nbpi anxiousDepressedSum |       0.07 | [0.05, 0.09]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#bayesian-generalized-linear-model",
    "href": "regression.html#bayesian-generalized-linear-model",
    "title": "Regression",
    "section": "6.3 Bayesian generalized linear model",
    "text": "6.3 Bayesian generalized linear model\nIn this example, we predict a count variable that has a poisson distribution. We could change the distribution. For example, we could use Gamma regression, family = Gamma, when the response variable is continuous and positive, and the coefficient of variation–rather than the variance–is constant.\n\n\nCode\nbayesianGeneralizedLinearRegression &lt;- brm(\n  countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = poisson,\n  chains = 4,\n  seed = 52242,\n  iter = 2000)\n\n\n\n\nCode\nsummary(bayesianGeneralizedLinearRegression)\n\n\n Family: poisson \n  Links: mu = log \nFormula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \n   Data: mydata (Number of observations: 2874) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   0.46      0.02     0.42     0.50 1.00     3095\nbpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2776\nbpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2620\n                        Tail_ESS\nIntercept                   2987\nbpi_antisocialT1Sum         2762\nbpi_anxiousDepressedSum     2689\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nprint(effectsize::standardize_parameters(bayesianGeneralizedLinearRegression, method = \"basic\"), digits = 2)\n\n\n# Standardization method: basic\n\nParameter               | Std. Median |       95% CI\n----------------------------------------------------\n(Intercept)             |        0.00 | [0.00, 0.00]\nbpi_antisocialT1Sum     |        0.32 | [0.30, 0.34]\nbpi_anxiousDepressedSum |        0.07 | [0.05, 0.09]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#robust-generalized-regression",
    "href": "regression.html#robust-generalized-regression",
    "title": "Regression",
    "section": "6.4 Robust generalized regression",
    "text": "6.4 Robust generalized regression\n\n\nCode\nrobustGeneralizedRegression &lt;- robustbase::glmrob(\n  countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = \"poisson\",\n  na.action = na.exclude)\n\nsummary(robustGeneralizedRegression)\n\n\n\nCall:  robustbase::glmrob(formula = countVariable ~ bpi_antisocialT1Sum +      bpi_anxiousDepressedSum, family = \"poisson\", data = mydata,      na.action = na.exclude) \n\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             0.365934   0.020794  17.598  &lt; 2e-16 ***\nbpi_antisocialT1Sum     0.156275   0.005046  30.969  &lt; 2e-16 ***\nbpi_anxiousDepressedSum 0.050526   0.008332   6.064 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRobustness weights w.r * w.x: \n 2273 weights are ~= 1. The remaining 601 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1286  0.5550  0.7574  0.7183  0.8874  0.9982 \n\nNumber of observations: 2874 \nFitted by method 'Mqle'  (in 5 iterations)\n\n(Dispersion parameter for poisson family taken to be 1)\n\nNo deviance values available \nAlgorithmic parameters: \n   acc    tcc \n0.0001 1.3450 \nmaxit \n   50 \ntest.acc \n  \"coef\" \n\n\nCode\nconfint(robustGeneralizedRegression)\n\n\nError in `glm.control()`:\n! unused arguments (acc = 1e-04, test.acc = \"coef\", tcc = 1.345)\n\n\nCode\nprint(effectsize::standardize_parameters(robustGeneralizedRegression), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |       95% CI\n---------------------------------------------------\n(Intercept)             |       0.87 | [0.84, 0.89]\nbpi antisocialT1Sum     |       0.35 | [0.33, 0.38]\nbpi anxiousDepressedSum |       0.07 | [0.05, 0.10]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#ordinal-regression-model",
    "href": "regression.html#ordinal-regression-model",
    "title": "Regression",
    "section": "6.5 Ordinal regression model",
    "text": "6.5 Ordinal regression model\n\n\nCode\nordinalRegressionModel1 &lt;- MASS::polr(\n  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata)\n\nordinalRegressionModel2 &lt;- rms::lrm(\n  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE)\n\nordinalRegressionModel3 &lt;- rms::orm(\n  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE)\n\nsummary(ordinalRegressionModel1)\n\n\nCall:\nMASS::polr(formula = orderedVariable ~ bpi_antisocialT1Sum + \n    bpi_anxiousDepressedSum, data = mydata)\n\nCoefficients:\n                         Value Std. Error t value\nbpi_antisocialT1Sum     0.4404    0.01862  23.647\nbpi_anxiousDepressedSum 0.1427    0.02615   5.457\n\nIntercepts:\n      Value   Std. Error t value\n0|1   -0.6001  0.0630    -9.5185\n1|2    0.6766  0.0585    11.5656\n2|3    1.5801  0.0634    24.9191\n3|4    2.4110  0.0720    33.4816\n4|5    3.1617  0.0825    38.3006\n5|6    3.8560  0.0949    40.6120\n6|7    4.5367  0.1106    41.0316\n7|8    5.1667  0.1291    40.0147\n8|9    5.8129  0.1545    37.6263\n9|10   6.5364  0.1962    33.3193\n10|11  7.1835  0.2513    28.5820\n11|12  7.8469  0.3331    23.5566\n12|14  8.7818  0.5113    17.1741\n\nResidual Deviance: 11088.49 \nAIC: 11118.49 \n(8656 observations deleted due to missingness)\n\n\nCode\nconfint(ordinalRegressionModel1)\n\n\n                             2.5 %    97.5 %\nbpi_antisocialT1Sum     0.40403039 0.4770422\nbpi_anxiousDepressedSum 0.09149919 0.1940080\n\n\nCode\nordinalRegressionModel2\n\n\nFrequencies of Missing Values Due to Each Variable\n        orderedVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                   7501                    7613                    7616 \n\nLogistic Regression Model\n\nrms::lrm(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, x = TRUE, y = TRUE)\n\n\nFrequencies of Responses\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n455 597 527 444 313 205 133  80  52  33  16   9   6   4 \n\n                       Model Likelihood       Discrimination    Rank Discrim.    \n                             Ratio Test              Indexes          Indexes    \nObs          2874    LR chi2     881.26       R2       0.268    C       0.705    \nmax |deriv| 6e-06    d.f.             2      R2(2,2874)0.264    Dxy     0.410    \n                     Pr(&gt; chi2) &lt;0.0001    R2(2,2803.4)0.269    gamma   0.426    \n                                              Brier    0.195    tau-a   0.350    \n\n                        Coef   S.E.   Wald Z Pr(&gt;|Z|)\nbpi_antisocialT1Sum     0.4404 0.0186 23.65  &lt;0.0001 \nbpi_anxiousDepressedSum 0.1427 0.0261  5.46  &lt;0.0001 \n\n\nCode\nprint(effectsize::standardize_parameters(ordinalRegressionModel2), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |         95% CI\n-----------------------------------------------------\ny&gt;=1                    |       2.01 | [ 1.90,  2.12]\ny&gt;=2                    |       0.73 | [ 0.65,  0.81]\ny&gt;=3                    |      -0.17 | [-0.25, -0.09]\ny&gt;=4                    |      -1.00 | [-1.09, -0.91]\ny&gt;=5                    |      -1.75 | [-1.86, -1.65]\ny&gt;=6                    |      -2.45 | [-2.58, -2.32]\ny&gt;=7                    |      -3.13 | [-3.29, -2.97]\ny&gt;=8                    |      -3.76 | [-3.96, -3.56]\ny&gt;=9                    |      -4.40 | [-4.66, -4.14]\ny&gt;=10                   |      -5.13 | [-5.48, -4.78]\ny&gt;=11                   |      -5.78 | [-6.24, -5.31]\ny&gt;=12                   |      -6.44 | [-7.07, -5.81]\ny&gt;=14                   |      -7.37 | [-8.36, -6.39]\nbpi antisocialT1Sum     |       0.99 | [ 0.91,  1.08]\nbpi anxiousDepressedSum |       0.21 | [ 0.13,  0.28]\n\n- Response is unstandardized.\n\n\nCode\nordinalRegressionModel3\n\n\nFrequencies of Missing Values Due to Each Variable\n        orderedVariable     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                   7501                    7613                    7616 \n\nLogistic (Proportional Odds) Ordinal Regression Model\n\nrms::orm(formula = orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, x = TRUE, y = TRUE)\n\n\nFrequencies of Responses\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n455 597 527 444 313 205 133  80  52  33  16   9   6   4 \n\n                        Model Likelihood               Discrimination    Rank Discrim.    \n                              Ratio Test                      Indexes          Indexes    \nObs           2874    LR chi2     881.26    R2                  0.268    rho     0.506    \nESS         2803.4    d.f.             2    R2(2,2874)          0.264    Dxy     0.410    \nDistinct Y      14    Pr(&gt; chi2) &lt;0.0001    R2(2,2803.4)        0.269                     \nMedian Y         3    Score chi2  916.79    |Pr(Y&gt;=median)-0.5| 0.188                     \nmax |deriv|  2e-12    Pr(&gt; chi2) &lt;0.0001                                                  \n\n                        Coef   S.E.   Wald Z Pr(&gt;|Z|)\nbpi_antisocialT1Sum     0.4404 0.0186 23.65  &lt;0.0001 \nbpi_anxiousDepressedSum 0.1427 0.0261  5.46  &lt;0.0001 \n\n\nCode\nconfint(ordinalRegressionModel3)\n\n\n                              2.5 %     97.5 %\ny&gt;=1                             NA         NA\ny&gt;=2                    -0.79129972 -0.5619711\ny&gt;=3                             NA         NA\ny&gt;=4                             NA         NA\ny&gt;=5                             NA         NA\ny&gt;=6                             NA         NA\ny&gt;=7                             NA         NA\ny&gt;=8                             NA         NA\ny&gt;=9                             NA         NA\ny&gt;=10                            NA         NA\ny&gt;=11                            NA         NA\ny&gt;=12                            NA         NA\ny&gt;=14                            NA         NA\nbpi_antisocialT1Sum      0.40390078  0.4769035\nbpi_anxiousDepressedSum  0.09143921  0.1939303\n\n\nCode\nprint(effectsize::standardize_parameters(ordinalRegressionModel3), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |       95% CI\n---------------------------------------------------\ny&gt;=2                    |       0.73 | [0.65, 0.81]\nbpi antisocialT1Sum     |       0.99 | [0.91, 1.08]\nbpi anxiousDepressedSum |       0.21 | [0.13, 0.28]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#bayesian-ordinal-regression-model",
    "href": "regression.html#bayesian-ordinal-regression-model",
    "title": "Regression",
    "section": "6.6 Bayesian ordinal regression model",
    "text": "6.6 Bayesian ordinal regression model\n\n\nCode\nbayesianOrdinalRegression &lt;- brm(\n  orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = cumulative(),\n  chains = 4,\n  seed = 52242,\n  iter = 2000)\n\n\n\n\nCode\nsummary(bayesianOrdinalRegression)\n\n\n Family: cumulative \n  Links: mu = logit \nFormula: orderedVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \n   Data: mydata (Number of observations: 2874) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept[1]               -0.60      0.06    -0.73    -0.48 1.00     4659\nIntercept[2]                0.67      0.06     0.55     0.79 1.00     6134\nIntercept[3]                1.57      0.06     1.45     1.70 1.00     5916\nIntercept[4]                2.40      0.07     2.26     2.54 1.00     5305\nIntercept[5]                3.15      0.08     2.99     3.31 1.00     4846\nIntercept[6]                3.84      0.09     3.66     4.03 1.00     4741\nIntercept[7]                4.52      0.11     4.31     4.74 1.00     4796\nIntercept[8]                5.15      0.13     4.90     5.40 1.00     4908\nIntercept[9]                5.79      0.15     5.50     6.11 1.00     5172\nIntercept[10]               6.51      0.20     6.14     6.90 1.00     5196\nIntercept[11]               7.17      0.25     6.69     7.69 1.00     5867\nIntercept[12]               7.87      0.34     7.25     8.56 1.00     5503\nIntercept[13]               8.91      0.53     7.99    10.06 1.00     5868\nbpi_antisocialT1Sum         0.44      0.02     0.40     0.48 1.00     4321\nbpi_anxiousDepressedSum     0.14      0.03     0.09     0.19 1.00     4514\n                        Tail_ESS\nIntercept[1]                3538\nIntercept[2]                3487\nIntercept[3]                3551\nIntercept[4]                3532\nIntercept[5]                3866\nIntercept[6]                3599\nIntercept[7]                3222\nIntercept[8]                3125\nIntercept[9]                2991\nIntercept[10]               3098\nIntercept[11]               3043\nIntercept[12]               2906\nIntercept[13]               2873\nbpi_antisocialT1Sum         2945\nbpi_anxiousDepressedSum     3156\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nprint(effectsize::standardize_parameters(bayesianOrdinalRegression, method = \"basic\"), digits = 2)\n\n\n# Standardization method: basic\n\nParameter               | Std. Median |         95% CI\n------------------------------------------------------\nIntercept[1]            |        0.00 | [ 0.00,  0.00]\nIntercept[2]            |        1.51 | [ 1.25,  1.78]\nIntercept[3]            |        2.26 | [ 2.08,  2.45]\nIntercept[4]            |        2.40 | [ 2.26,  2.54]\nIntercept[5]            |        3.15 | [ 2.99,  3.31]\nIntercept[6]            |        3.84 | [ 3.66,  4.03]\nIntercept[7]            |        4.52 | [ 4.31,  4.74]\nIntercept[8]            |        5.14 | [ 4.90,  5.40]\nIntercept[9]            |        5.78 | [ 5.50,  6.11]\nIntercept[10]           |        6.50 | [ 6.14,  6.90]\nIntercept[11]           |        7.15 | [ 6.69,  7.69]\nIntercept[12]           |        7.84 | [ 7.25,  8.56]\nIntercept[13]           |        8.87 | [ 7.99, 10.06]\nbpi_antisocialT1Sum     |        0.44 | [ 0.40,  0.48]\nbpi_anxiousDepressedSum |        0.14 | [ 0.09,  0.19]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#bayesian-count-regression-model",
    "href": "regression.html#bayesian-count-regression-model",
    "title": "Regression",
    "section": "6.7 Bayesian count regression model",
    "text": "6.7 Bayesian count regression model\n\n\nCode\nbayesianCountRegression &lt;- brm(\n  countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = \"poisson\",\n  chains = 4,\n  seed = 52242,\n  iter = 2000)\n\n\n\n\nCode\nsummary(bayesianCountRegression)\n\n\n Family: poisson \n  Links: mu = log \nFormula: countVariable ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \n   Data: mydata (Number of observations: 2874) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   0.46      0.02     0.42     0.50 1.00     3095\nbpi_antisocialT1Sum         0.14      0.00     0.13     0.15 1.00     2776\nbpi_anxiousDepressedSum     0.05      0.01     0.03     0.06 1.00     2620\n                        Tail_ESS\nIntercept                   2987\nbpi_antisocialT1Sum         2762\nbpi_anxiousDepressedSum     2689\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nprint(effectsize::standardize_parameters(bayesianCountRegression, method = \"basic\"), digits = 2)\n\n\n# Standardization method: basic\n\nParameter               | Std. Median |       95% CI\n----------------------------------------------------\n(Intercept)             |        0.00 | [0.00, 0.00]\nbpi_antisocialT1Sum     |        0.32 | [0.30, 0.34]\nbpi_anxiousDepressedSum |        0.07 | [0.05, 0.09]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#logistic-regression-model-rms",
    "href": "regression.html#logistic-regression-model-rms",
    "title": "Regression",
    "section": "6.8 Logistic regression model (rms)",
    "text": "6.8 Logistic regression model (rms)\n\n\nCode\nlogisticRegressionModel &lt;- rms::robcov(rms::lrm(\n  female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE))\n\nlogisticRegressionModel\n\n\nFrequencies of Missing Values Due to Each Variable\n                 female     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                      2                    7613                    7616 \n\nLogistic Regression Model\n\nrms::lrm(formula = female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, x = TRUE, y = TRUE)\n\n                       Model Likelihood       Discrimination    Rank Discrim.    \n                             Ratio Test              Indexes          Indexes    \nObs          3914    LR chi2      66.63       R2       0.023    C       0.571    \n 0           1965    d.f.             2      R2(2,3914)0.016    Dxy     0.142    \n 1           1949    Pr(&gt; chi2) &lt;0.0001    R2(2,2935.5)0.022    gamma   0.147    \nmax |deriv| 1e-07                             Brier    0.246    tau-a   0.071    \n\n                        Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept                0.3002 0.0529  5.67  &lt;0.0001 \nbpi_antisocialT1Sum     -0.1244 0.0166 -7.48  &lt;0.0001 \nbpi_anxiousDepressedSum  0.0382 0.0253  1.51  0.1314  \n\n\nCode\nconfint(logisticRegressionModel)\n\n\nError in `summary.rms()`:\n! adjustment values not defined here or with datadist for bpi_antisocialT1Sum bpi_anxiousDepressedSum\n\n\nCode\nprint(effectsize::standardize_parameters(logisticRegressionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |         95% CI\n-----------------------------------------------------\nIntercept               |  -9.88e-03 | [-0.07,  0.05]\nbpi antisocialT1Sum     |      -0.29 | [-0.36, -0.21]\nbpi anxiousDepressedSum |       0.06 | [-0.02,  0.13]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#bayesian-logistic-regression-model",
    "href": "regression.html#bayesian-logistic-regression-model",
    "title": "Regression",
    "section": "6.9 Bayesian logistic regression model",
    "text": "6.9 Bayesian logistic regression model\n\n\nCode\nbayesianLogisticRegression &lt;- brm(\n  female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = bernoulli,\n  chains = 4,\n  seed = 52242,\n  iter = 2000)\n\n\n\n\nCode\nsummary(bayesianLogisticRegression)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: female ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \n   Data: mydata (Number of observations: 3914) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   0.30      0.05     0.20     0.41 1.00     4125\nbpi_antisocialT1Sum        -0.13      0.02    -0.16    -0.09 1.00     2943\nbpi_anxiousDepressedSum     0.04      0.03    -0.01     0.09 1.00     3084\n                        Tail_ESS\nIntercept                   3269\nbpi_antisocialT1Sum         2857\nbpi_anxiousDepressedSum     2730\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nprint(effectsize::standardize_parameters(bayesianLogisticRegression, method = \"basic\"), digits = 2)\n\n\n# Standardization method: basic\n\nParameter               | Std. Median |         95% CI\n------------------------------------------------------\n(Intercept)             |        0.00 | [ 0.00,  0.00]\nbpi_antisocialT1Sum     |       -0.29 | [-0.36, -0.22]\nbpi_anxiousDepressedSum |        0.06 | [-0.02,  0.13]\n\n- Response is unstandardized.",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#mean-center-predictors",
    "href": "regression.html#mean-center-predictors",
    "title": "Regression",
    "section": "8.1 Mean Center Predictors",
    "text": "8.1 Mean Center Predictors\nMake sure to mean-center or orthogonalize predictors before computing the interaction term.\n\n\nCode\nstates$Illiteracy_centered &lt;- scale(states$Illiteracy, scale = FALSE)\nstates$Murder_centered &lt;- scale(states$Murder, scale = FALSE)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#model",
    "href": "regression.html#model",
    "title": "Regression",
    "section": "8.2 Model",
    "text": "8.2 Model\n\n\nCode\ninteractionModel &lt;- lm(\n  Income ~ Illiteracy_centered + Murder_centered + Illiteracy_centered:Murder_centered + HS.Grad,\n  data = states)\n\nsummary(interactionModel)\n\n\n\nCall:\nlm(formula = Income ~ Illiteracy_centered + Murder_centered + \n    Illiteracy_centered:Murder_centered + HS.Grad, data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-916.27 -244.42   28.42  228.14 1221.16 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          2421.46     594.36   4.074 0.000185 ***\nIlliteracy_centered                    37.12     192.56   0.193 0.848005    \nMurder_centered                        17.07      25.52   0.669 0.507085    \nHS.Grad                                40.76      10.92   3.733 0.000530 ***\nIlliteracy_centered:Murder_centered   -97.04      35.86  -2.706 0.009584 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 459.5 on 45 degrees of freedom\nMultiple R-squared:  0.4864,    Adjusted R-squared:  0.4407 \nF-statistic: 10.65 on 4 and 45 DF,  p-value: 3.689e-06\n\n\nCode\nprint(effectsize::standardize_parameters(interactionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter                             | Std. Coef. |         95% CI\n-------------------------------------------------------------------\n(Intercept)                           |       0.24 | [-0.04,  0.53]\nIlliteracy centered                   |       0.04 | [-0.35,  0.42]\nMurder centered                       |       0.10 | [-0.21,  0.41]\nHS Grad                               |       0.54 | [ 0.25,  0.82]\nIlliteracy centered × Murder centered |      -0.36 | [-0.62, -0.09]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#plots",
    "href": "regression.html#plots",
    "title": "Regression",
    "section": "8.3 Plots",
    "text": "8.3 Plots\n\n\nCode\ninteractions::interact_plot(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered)\n\n\n\n\n\n\n\n\n\nCode\ninteractions::interact_plot(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  plot.points = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ninteractions::interact_plot(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  interval = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ninteractions::johnson_neyman(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  alpha = .05)\n\n\nJOHNSON-NEYMAN INTERVAL\n\nWhen Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of\nIlliteracy_centered is p &lt; .05.\n\nNote: The range of observed values of Murder_centered is [-5.98, 7.72]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#simple-slopes-analysis",
    "href": "regression.html#simple-slopes-analysis",
    "title": "Regression",
    "section": "8.4 Simple Slopes Analysis",
    "text": "8.4 Simple Slopes Analysis\n\n\nCode\ninteractions::sim_slopes(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  johnson_neyman = FALSE)\n\n\nSIMPLE SLOPES ANALYSIS\n\nSlope of Illiteracy_centered when Murder_centered = -3.69154e+00 (- 1 SD): \n\n    Est.     S.E.   t val.      p\n-------- -------- -------- ------\n  395.34   274.84     1.44   0.16\n\nSlope of Illiteracy_centered when Murder_centered = -1.24345e-16 (Mean): \n\n   Est.     S.E.   t val.      p\n------- -------- -------- ------\n  37.12   192.56     0.19   0.85\n\nSlope of Illiteracy_centered when Murder_centered =  3.69154e+00 (+ 1 SD): \n\n     Est.     S.E.   t val.      p\n--------- -------- -------- ------\n  -321.10   183.49    -1.75   0.09\n\n\nCode\ninteractions::sim_slopes(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  modx.values = c(0, 5, 10),\n  johnson_neyman = FALSE)\n\n\nSIMPLE SLOPES ANALYSIS\n\nSlope of Illiteracy_centered when Murder_centered =  0.00: \n\n   Est.     S.E.   t val.      p\n------- -------- -------- ------\n  37.12   192.56     0.19   0.85\n\nSlope of Illiteracy_centered when Murder_centered =  5.00: \n\n     Est.     S.E.   t val.      p\n--------- -------- -------- ------\n  -448.07   202.17    -2.22   0.03\n\nSlope of Illiteracy_centered when Murder_centered = 10.00: \n\n     Est.     S.E.   t val.      p\n--------- -------- -------- ------\n  -933.27   330.10    -2.83   0.01",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#johnson-neyman-intervals",
    "href": "regression.html#johnson-neyman-intervals",
    "title": "Regression",
    "section": "8.5 Johnson-Neyman intervals",
    "text": "8.5 Johnson-Neyman intervals\nIndicates all the values of the moderator for which the slope of the predictor is statistically significant.\n\n\nCode\ninteractions::sim_slopes(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  johnson_neyman = TRUE)\n\n\nJOHNSON-NEYMAN INTERVAL\n\nWhen Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of\nIlliteracy_centered is p &lt; .05.\n\nNote: The range of observed values of Murder_centered is [-5.98, 7.72]\n\nSIMPLE SLOPES ANALYSIS\n\nSlope of Illiteracy_centered when Murder_centered = -3.69154e+00 (- 1 SD): \n\n    Est.     S.E.   t val.      p\n-------- -------- -------- ------\n  395.34   274.84     1.44   0.16\n\nSlope of Illiteracy_centered when Murder_centered = -1.24345e-16 (Mean): \n\n   Est.     S.E.   t val.      p\n------- -------- -------- ------\n  37.12   192.56     0.19   0.85\n\nSlope of Illiteracy_centered when Murder_centered =  3.69154e+00 (+ 1 SD): \n\n     Est.     S.E.   t val.      p\n--------- -------- -------- ------\n  -321.10   183.49    -1.75   0.09\n\n\nCode\ninteractions::probe_interaction(\n  interactionModel,\n  pred = Illiteracy_centered,\n  modx = Murder_centered,\n  cond.int = TRUE,\n  interval = TRUE,\n  jnplot = TRUE)\n\n\nJOHNSON-NEYMAN INTERVAL\n\nWhen Murder_centered is OUTSIDE the interval [-8.13, 4.37], the slope of\nIlliteracy_centered is p &lt; .05.\n\nNote: The range of observed values of Murder_centered is [-5.98, 7.72]\n\n\n\n\n\n\n\n\n\nSIMPLE SLOPES ANALYSIS\n\nWhen Murder_centered = -3.69154e+00 (- 1 SD): \n\n                                        Est.     S.E.   t val.      p\n---------------------------------- --------- -------- -------- ------\nSlope of Illiteracy_centered          395.34   274.84     1.44   0.16\nConditional intercept                4523.23   134.99    33.51   0.00\n\nWhen Murder_centered = -1.24345e-16 (Mean): \n\n                                        Est.     S.E.   t val.      p\n---------------------------------- --------- -------- -------- ------\nSlope of Illiteracy_centered           37.12   192.56     0.19   0.85\nConditional intercept                4586.22    85.52    53.63   0.00\n\nWhen Murder_centered =  3.69154e+00 (+ 1 SD): \n\n                                        Est.     S.E.   t val.      p\n---------------------------------- --------- -------- -------- ------\nSlope of Illiteracy_centered         -321.10   183.49    -1.75   0.09\nConditional intercept                4649.22   118.97    39.08   0.00",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#listwiseDeletion",
    "href": "regression.html#listwiseDeletion",
    "title": "Regression",
    "section": "10.1 Listwise deletion",
    "text": "10.1 Listwise deletion\nListwise deletion deletes every row in the data file that has a missing value for one of the model variables.\n\n\nCode\nlistwiseDeletionModel &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(listwiseDeletionModel)\n\n\n\nCall:\nlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.979 on 2871 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.262, Adjusted R-squared:  0.2615 \nF-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nconfint(listwiseDeletionModel)\n\n\n                            2.5 %    97.5 %\n(Intercept)             1.0809881 1.3156128\nbpi_antisocialT1Sum     0.4290884 0.5019688\nbpi_anxiousDepressedSum 0.1035825 0.2179258\n\n\nCode\nprint(effectsize::standardize_parameters(listwiseDeletionModel), digits = 2)\n\n\n# Standardization method: refit\n\nParameter               | Std. Coef. |        95% CI\n----------------------------------------------------\n(Intercept)             |   1.62e-16 | [-0.03, 0.03]\nbpi antisocialT1Sum     |       0.46 | [ 0.42, 0.49]\nbpi anxiousDepressedSum |       0.10 | [ 0.06, 0.14]",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#pairwiseDeletion",
    "href": "regression.html#pairwiseDeletion",
    "title": "Regression",
    "section": "10.2 Pairwise deletion",
    "text": "10.2 Pairwise deletion\nAlso see here:\n\nhttps://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari/173002#173002 (archived at https://perma.cc/GH5T-RXD9)\nhttps://stats.stackexchange.com/questions/299792/r-lm-covariance-matrix-manual-calculation-failure (archived at https://perma.cc/F7EL-AUFZ)\nhttps://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re (archived at https://perma.cc/KU3X-FB2C)\nhttps://stats.stackexchange.com/questions/105006/how-to-perform-a-bivariate-regression-using-pairwise-deletion-of-missing-values (archived at https://perma.cc/QWQ5-2TLW)\nhttps://dl.dropboxusercontent.com/s/4sf0et3p47ykctx/MissingPairwise.sps (archived at https://perma.cc/UC4K-2L9T)\n\nAdapted from here: https://stefvanbuuren.name/fimd/sec-simplesolutions.html#sec:pairwise (archived at https://perma.cc/EGU6-3M3Q)\n\n\nCode\nmodelData &lt;- mydata[,c(\"bpi_antisocialT2Sum\",\"bpi_antisocialT1Sum\",\"bpi_anxiousDepressedSum\")]\nvarMeans &lt;- colMeans(modelData, na.rm = TRUE)\nvarCovariances &lt;- cov(modelData, use = \"pairwise\")\n\npairwiseRegression_syntax &lt;- '\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum\n  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum\n  bpi_antisocialT2Sum ~ 1\n'\n\npairwiseRegression_fit &lt;- lavaan::lavaan(\n  pairwiseRegression_syntax,\n  sample.mean = varMeans,\n  sample.cov = varCovariances,\n  sample.nobs = sum(complete.cases(modelData))\n)\n\nsummary(\n  pairwiseRegression_fit,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         4\n\n  Number of observations                          2874\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  bpi_antisocialT2Sum ~                                                      \n    bpi_antsclT1Sm         0.441    0.018   24.611    0.000    0.441    0.456\n    bp_nxsDprssdSm         0.137    0.028    4.854    0.000    0.137    0.090\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .bpi_antsclT2Sm    1.175    0.058   20.125    0.000    1.175    0.523\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .bpi_antsclT2Sm    3.755    0.099   37.908    0.000    3.755    0.743\n\nR-Square:\n                   Estimate\n    bpi_antsclT2Sm    0.257",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#fiml",
    "href": "regression.html#fiml",
    "title": "Regression",
    "section": "10.3 Full-information maximum likelihood (FIML)",
    "text": "10.3 Full-information maximum likelihood (FIML)\n\n\nCode\nfimlRegression_syntax &lt;- '\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum\n  bpi_antisocialT2Sum ~~ bpi_antisocialT2Sum\n  bpi_antisocialT2Sum ~ 1\n'\n\nfimlRegression_fit &lt;- lavaan::lavaan(\n  fimlRegression_syntax,\n  data = mydata,\n  missing = \"ML\",\n)\n\nsummary(\n  fimlRegression_fit,\n  standardized = TRUE,\n  rsquare = TRUE)\n\n\nlavaan 0.6-21 ended normally after 11 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         4\n\n                                                  Used       Total\n  Number of observations                          3914       11530\n  Number of missing patterns                         2            \n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nRegressions:\n                        Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  bpi_antisocialT2Sum ~                                                      \n    bpi_antsclT1Sm         0.466    0.019   25.062    0.000    0.466    0.466\n    bp_nxsDprssdSm         0.161    0.029    5.516    0.000    0.161    0.102\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .bpi_antsclT2Sm    1.198    0.060   20.039    0.000    1.198    0.516\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .bpi_antsclT2Sm    3.911    0.103   37.908    0.000    3.911    0.725\n\nR-Square:\n                   Estimate\n    bpi_antsclT2Sm    0.275",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#imputation",
    "href": "regression.html#imputation",
    "title": "Regression",
    "section": "10.4 Multiple imputation",
    "text": "10.4 Multiple imputation\n\n\nCode\nmodelData_imputed &lt;- mice::mice(\n  modelData,\n  m = 5,\n  method = \"pmm\") # predictive mean matching; can choose among many methods\n\n\n\n iter imp variable\n  1   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  1   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  1   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  1   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  1   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  2   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  2   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  2   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  2   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  2   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  3   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  3   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  3   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  3   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  3   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  4   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  4   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  4   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  4   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  4   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  5   1  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  5   2  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  5   3  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  5   4  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n  5   5  bpi_antisocialT2Sum  bpi_antisocialT1Sum  bpi_anxiousDepressedSum\n\n\nCode\nimputedData_fit &lt;- with(\n  modelData_imputed,\n  lm(bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum))\n\nimputedData_pooledEstimates &lt;- pool(imputedData_fit)\nsummary(imputedData_pooledEstimates)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#linear-regression-1",
    "href": "regression.html#linear-regression-1",
    "title": "Regression",
    "section": "16.1 Linear Regression",
    "text": "16.1 Linear Regression\n\n\nCode\nmultipleRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)\nsummary(multipleRegressionModelBootstrapped)\n\n\n\n  \n\n\n\nCode\nconfint(multipleRegressionModelBootstrapped, level = .95, type = \"bca\")\n\n\nBootstrap percent confidence intervals\n\n                            2.5 %    97.5 %\n(Intercept)             1.0727049 1.3213438\nbpi_antisocialT1Sum     0.4212631 0.5108728\nbpi_anxiousDepressedSum 0.0974418 0.2245887\n\n\nCode\nhist(multipleRegressionModelBootstrapped)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#generalized-regression",
    "href": "regression.html#generalized-regression",
    "title": "Regression",
    "section": "16.2 Generalized Regression",
    "text": "16.2 Generalized Regression\n\n\nCode\ngeneralizedRegressionModelBootstrapped &lt;- Boot(multipleRegressionModelNoMissing, R = 1000)\nsummary(generalizedRegressionModelBootstrapped)\n\n\n\n  \n\n\n\nCode\nconfint(generalizedRegressionModelBootstrapped, level = .95, type = \"bca\")\n\n\nBootstrap percent confidence intervals\n\n                             2.5 %    97.5 %\n(Intercept)             1.06720638 1.3271064\nbpi_antisocialT1Sum     0.42003141 0.5129344\nbpi_anxiousDepressedSum 0.09482516 0.2232879\n\n\nCode\nhist(generalizedRegressionModelBootstrapped)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#k-fold-cross-validation",
    "href": "regression.html#k-fold-cross-validation",
    "title": "Regression",
    "section": "17.1 K-fold cross validation",
    "text": "17.1 K-fold cross validation\n\n\nCode\nkFolds &lt;- 10\nreplications &lt;- 20\n\nfolds &lt;- cvFolds(nrow(mydata), K = kFolds, R = replications)\n\nfitLm &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = \"na.exclude\")\n\nfitLmrob &lt;- lmrob(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = \"na.exclude\")\n\nfitLts &lt;- ltsReg(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = \"na.exclude\")\n\ncvFitLm &lt;- cvLm(\n  fitLm,\n  K = kFolds,\n  R = replications)\n\ncvFitLmrob &lt;- cvLmrob(\n  fitLmrob,\n  K = kFolds,\n  R = replications)\n\ncvFitLts &lt;- cvLts(\n  fitLts,\n  K = kFolds,\n  R = replications)\n\ncvFits &lt;- cvSelect(\n  OLS = cvFitLm,\n  MM = cvFitLmrob,\n  LTS = cvFitLts)\ncvFits\n\n\n\n10-fold CV results:\n  Fit       CV\n1 OLS 1.980368\n2  MM 1.026487\n3 LTS 1.006971\n\nBest model:\n   CV \n\"LTS\" \n\n\nCode\nbwplot(\n  cvFits,\n  xlab = \"Root Mean Square Error\",\n  xlim = c(0, max(cvFits$cv$CV) + 0.2))",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#effect-plots",
    "href": "regression.html#effect-plots",
    "title": "Regression",
    "section": "18.1 Effect Plots",
    "text": "18.1 Effect Plots\n\n\nCode\nallEffects(multipleRegressionModel)\n\n\n model: bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum\n\n bpi_antisocialT1Sum effect\nbpi_antisocialT1Sum\n       0      3.2      6.5      9.8       13 \n1.394591 2.884283 4.420527 5.956772 7.446463 \n\n bpi_anxiousDepressedSum effect\nbpi_anxiousDepressedSum\n       0        2        4        6        8 \n2.502636 2.824145 3.145653 3.467161 3.788669 \n\n\nCode\nplot(allEffects(multipleRegressionModel))",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#confidence-ellipses",
    "href": "regression.html#confidence-ellipses",
    "title": "Regression",
    "section": "18.2 Confidence Ellipses",
    "text": "18.2 Confidence Ellipses\n\n\nCode\nconfidenceEllipse(\n  multipleRegressionModel,\n  levels = c(0.5, .95))",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#data-ellipse",
    "href": "regression.html#data-ellipse",
    "title": "Regression",
    "section": "18.3 Data Ellipse",
    "text": "18.3 Data Ellipse\n\n\nCode\nmydata_nomissing &lt;- na.omit(mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")])\ndataEllipse(\n  mydata_nomissing$bpi_antisocialT1Sum,\n  mydata_nomissing$bpi_antisocialT2Sum,\n  levels = c(0.5, .95))",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#assumptions",
    "href": "regression.html#assumptions",
    "title": "Regression",
    "section": "19.1 Assumptions",
    "text": "19.1 Assumptions\n\n19.1.1 1. Linear relation between predictors and outcome\n\n19.1.1.1 Ways to Test\n\n19.1.1.1.1 Before Model Fitting\n\nscatterplot matrix\ndistance correlation\n\n\n19.1.1.1.1.1 Scatterplot Matrix\n\n\nCode\nscatterplotMatrix(\n  ~ bpi_antisocialT2Sum + bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata)\n\n\n\n\n\n\n\n\n\n\n\n19.1.1.1.1.2 Distance correlation\nThe distance correlation is an index of the degree of the linear and non-linear association between two variables.\n\n\nCode\ncorrelation(\n  mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")],\n  method = \"distance\",\n  p_adjust = \"none\")\n\n\n\n  \n\n\n\n\n\n\n19.1.1.1.2 After Model Fitting\nCheck for nonlinearities (non-horizontal line) in plots of:\n\nResiduals versus fitted values (Residual Plots)—best\nResiduals versus predictors (Residual Plots)\nOutcome versus fitted values (Marginal Model Plots)\nOutcome versus predictors, ignoring other predictors (Marginal Model Plots)\nOutcome versus predictors, controlling for other predictors (Added-Variable Plots)\n\n\n\n\n19.1.1.2 Ways to Handle\n\nTransform outcome/predictor variables (Box-Cox transformations)\nSemi-parametric regression models: Generalized additive models (GAM)\nNon-parametric regression models: Nearest-Neighbor Kernel Regression\n\n\n19.1.1.2.1 Semi-parametric or non-parametric regression models\nhttp://www.lisa.stat.vt.edu/?q=node/7517 (archived at https://web.archive.org/web/20180113065042/http://www.lisa.stat.vt.edu/?q=node/7517)\nNote: using semi-parametric or non-parametric models increases fit in context of nonlinearity at the expense of added complexity. Make sure to avoid fitting an overly complex model (e.g., use k-fold cross validation). Often, the simpler (generalized) linear model is preferable to semi-paremetric or non-parametric approaches\n\n19.1.1.2.1.1 Semi-parametric: Generalized Additive Models\nhttp://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models (archived at https://web.archive.org/web/20170213041653/http://documents.software.dell.com/Statistics/Textbook/Generalized-Additive-Models)\n\n\nCode\ngeneralizedAdditiveModel &lt;- gam(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  family = gaussian(),\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(generalizedAdditiveModel)\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nbpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum\n\nParametric coefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.261   Deviance explained = 26.2%\nGCV = 3.9191  Scale est. = 3.915     n = 2874\n\n\nCode\nconfint(generalizedAdditiveModel)\n\n\nError in `glm.control()`:\n! unused arguments (nthreads = 1, ncv.threads = 1, irls.reg = 0, mgcv.tol = 1e-07, mgcv.half = 15, rank.tol = 1.49011611938477e-08, nlm = list(7, 1e-06, 2, 1e-04, 200, FALSE), optim = list(1e+07), newton = list(1e-06, 5, 2, 30, FALSE), idLinksBases = TRUE, scalePenalty = TRUE, efs.lspmax = 15, efs.tol = 0.1, keepData = FALSE, scale.est = \"fletcher\", edge.correct = FALSE)\n\n\n\n\n19.1.1.2.1.2 Non-parametric: Nearest-Neighbor Kernel Regression\n\n\n\n\n\n19.1.2 2. Exogeneity\nExogeneity means that the association between predictors and outcome is fully causal and unrelated to other variables.\n\n19.1.2.1 Ways to Test\n\nDurbin-Wu-Hausman test of endogeneity\n\n\n19.1.2.1.1 Durbin-Wu-Hausman test of endogeneity\nThe instrumental variables (2SLS) estimator is implemented in the R package AER as command:\n\n\nCode\nivreg(y ~ x1 + x2 + w1 + w2 | z1 + z2 + z3 + w1 + w2)\n\n\nwhere x1 and x2 are endogenous regressors, w1 and w2 exogeneous regressors, and z1 to z3 are excluded instruments.\nDurbin-Wu-Hausman test:\n\n\nCode\nhsng2 &lt;- read.dta(\"https://www.stata-press.com/data/r11/hsng2.dta\") #archived at https://perma.cc/7P2Q-ARKR\n\n\n\n\nCode\nfiv &lt;- ivreg(\n  rent ~ hsngval + pcturban | pcturban + faminc + reg2 + reg3 + reg4,\n  data = hsng2) #Housing values are likely endogeneous and therefore instrumented by median family income (faminc) and 3 regional dummies (reg2, reg4, reg4)\n\nsummary(\n  fiv,\n  diagnostics = TRUE)\n\n\n\nCall:\nivreg(formula = rent ~ hsngval + pcturban | pcturban + faminc + \n    reg2 + reg3 + reg4, data = hsng2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-84.1948 -11.6023  -0.5239   8.6583  73.6130 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.207e+02  1.571e+01   7.685 7.55e-10 ***\nhsngval     2.240e-03  3.388e-04   6.612 3.17e-08 ***\npcturban    8.152e-02  3.082e-01   0.265    0.793    \n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   4  44     13.30  3.5e-07 ***\nWu-Hausman         1  46     15.91 0.000236 ***\nSargan             3  NA     11.29 0.010268 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.86 on 47 degrees of freedom\nMultiple R-Squared: 0.5989, Adjusted R-squared: 0.5818 \nWald test: 42.66 on 2 and 47 DF,  p-value: 2.731e-11 \n\n\nThe Eicker-Huber-White covariance estimator which is robust to heteroscedastic error terms is reported after estimation with vcov = sandwich in coeftest()\nFirst stage results are reported by explicitly estimating them. For example:\n\n\nCode\nfirst &lt;- lm(\n  hsngval ~ pcturban + faminc + reg2 + reg3 + reg4,\n  data = hsng2)\n\nsummary(first)\n\n\n\nCall:\nlm(formula = hsngval ~ pcturban + faminc + reg2 + reg3 + reg4, \n    data = hsng2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10504  -5223  -1162   2939  46756 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.867e+04  1.200e+04  -1.557 0.126736    \npcturban     1.822e+02  1.150e+02   1.584 0.120289    \nfaminc       2.731e+00  6.819e-01   4.006 0.000235 ***\nreg2        -5.095e+03  4.122e+03  -1.236 0.223007    \nreg3        -1.778e+03  4.073e+03  -0.437 0.664552    \nreg4         1.341e+04  4.048e+03   3.314 0.001849 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9253 on 44 degrees of freedom\nMultiple R-squared:  0.6908,    Adjusted R-squared:  0.6557 \nF-statistic: 19.66 on 5 and 44 DF,  p-value: 3.032e-10\n\n\nIn case of a single endogenous variable (K = 1), the F-statistic to assess weak instruments is reported after estimating the first stage with, for example:\n\n\nCode\nwaldtest(\n  first,\n  . ~ . - faminc - reg2 - reg3 - reg4)\n\n\n\n  \n\n\n\nor in case of heteroscedatistic errors:\n\n\nCode\nwaldtest(\n  first,\n  . ~ . - faminc - reg2 - reg3- reg4,\n  vcov = sandwich)\n\n\n\n  \n\n\n\n\n\n\n19.1.2.2 Ways to Handle\n\nConduct an experiment/RCT with random assignment\nInstrumental variables\n\n\n\n\n19.1.3 3. Homoscedasticity of residuals\nHomoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome/predictors (i.e., the residuals show constant variance as a function of outcome/predictors).\n\n19.1.3.1 Ways to Test\n\nPlot residuals vs. outcome and predictor variables (Residual Plots)\nPlot residuals vs. fitted values (Residual Plots)\nTime Series data: Plot residuals vs. time\nSpread-level plot\nBreusch-Pagan test: bptest() function from lmtest package\nGoldfeld-Quandt Test\n\n\n19.1.3.1.1 Residuals vs. outcome and predictor variables\nPlot residuals, or perhaps their absolute values, versus the outcome and predictor variables (Residual Plots). Examine whether residual variance is constant at all levels of other variables or whether it increases/decreases as a function of another variable (or shows some others structure, e.g., small variance at low and high levels of a predictor and high variance in the middle). Note that this is different than whether the residuals show non-linearities—i.e., a non-horizontal line, which would indicate a nonlinear association between variables (see Assumption #1, above). Rather, here we are examining whether there is change in the variance as a function of another variable (e.g., a fan-shaped Residual Plot)\n\n\n19.1.3.1.2 Spread-level plot\nExamining whether level (e.g., mean) depends on spread (e.g., variance)—plot of log of the absolute Studentized residuals against the log of the fitted values\n\n\nCode\nspreadLevelPlot(multipleRegressionModel)\n\n\n\n\n\n\n\n\n\n\nSuggested power transformation:  0.5304728 \n\n\n\n\n19.1.3.1.3 Breusch-Pagan test\nhttps://www.rdocumentation.org/packages/lmtest/versions/0.9-40/topics/bptest (archived at https://perma.cc/K4WC-7TVW)\n\n\nCode\nbptest(multipleRegressionModel)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  multipleRegressionModel\nBP = 94.638, df = 2, p-value &lt; 2.2e-16\n\n\n\n\n19.1.3.1.4 Goldfeld-Quandt Test\n\n\nCode\ngqtest(multipleRegressionModel)\n\n\n\n    Goldfeld-Quandt test\n\ndata:  multipleRegressionModel\nGQ = 1.0755, df1 = 1434, df2 = 1434, p-value = 0.08417\nalternative hypothesis: variance increases from segment 1 to 2\n\n\n\n\n19.1.3.1.5 Test of dependence of spread on level\n\n\nCode\nncvTest(multipleRegressionModel)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 232.1196, Df = 1, p = &lt; 2.22e-16\n\n\n\n\n19.1.3.1.6 Test of dependence of spread on predictors\n\n\nCode\nncvTest(\n  multipleRegressionModel,\n  ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum)\n\n\nNon-constant Variance Score Test \nVariance formula: ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum \nChisquare = 233.2878, Df = 2, p = &lt; 2.22e-16\n\n\n\n\n\n19.1.3.2 Ways to Handle\n\nIf residual variance increases as a function of the fitted values, consider a poisson regression model (especially for count data), for which the variance does increase with the mean\nIf residual variance differs as a function of model predictors, consider adding interactions/product terms (the effect of one variable depends on the level of another variable)\nTry transforming the outcome variable to be normally distributed (log-transform if the errors seem consistent in percentage rather than absolute terms)\nIf the error variance is proportional to a variable \\(z\\), then fit the model using Weighted Least Squares (WLS), with the weights given be \\(1/z\\)\nWeighted least squares (WLS) using the “weights” argument of the lm() function; to get weights, see: https://stats.stackexchange.com/a/100410/20338 (archived at https://perma.cc/C6BY-G9MS)\nHuber-White standard errors (a.k.a. “Sandwich” estimates) from a heteroscedasticity-corrected covariance matrix\n\ncoeftest() function from the sandwich package along with hccm sandwich estimates from the car package\nrobcov() function from the rms package\n\nTime series data: ARCH (auto-regressive conditional heteroscedasticity) models\nTime series data: seasonal patterns can be addressed by applying log transformation to outcome variable\n\n\n19.1.3.2.1 Huber-White standard errors\nStandard errors (SEs) on the diagonal increase\n\n\nCode\nvcov(multipleRegressionModel)\n\n\n                          (Intercept) bpi_antisocialT1Sum\n(Intercept)              0.0035795220       -0.0006533375\nbpi_antisocialT1Sum     -0.0006533375        0.0003453814\nbpi_anxiousDepressedSum -0.0003167540       -0.0002574524\n                        bpi_anxiousDepressedSum\n(Intercept)                       -0.0003167540\nbpi_antisocialT1Sum               -0.0002574524\nbpi_anxiousDepressedSum            0.0008501565\n\n\nCode\nhccm(multipleRegressionModel)\n\n\nError in `V %*% t(X) %*% apply(X, 2, \"*\", (e^2) / factor)`:\n! non-conformable arguments\n\n\nCode\nsummary(multipleRegressionModel)\n\n\n\nCall:\nlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              1.19830    0.05983  20.029  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.979 on 2871 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.262, Adjusted R-squared:  0.2615 \nF-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncoeftest(multipleRegressionModel, vcov = sandwich)\n\n\n\nt test of coefficients:\n\n                        Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)             1.198300   0.062211 19.2618 &lt; 2.2e-16 ***\nbpi_antisocialT1Sum     0.465529   0.022929 20.3030 &lt; 2.2e-16 ***\nbpi_anxiousDepressedSum 0.160754   0.032991  4.8727  1.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(multipleRegressionModel, vcov = hccm)\n\n\nError in `V %*% t(X) %*% apply(X, 2, \"*\", (e^2) / factor)`:\n! non-conformable arguments\n\n\nCode\nrobcov(ols(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  x = TRUE,\n  y = TRUE))\n\n\nFrequencies of Missing Values Due to Each Variable\n    bpi_antisocialT2Sum     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n                   7501                    7613                    7616 \n\nLinear Regression Model\n\nols(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, x = TRUE, y = TRUE)\n\n                Model Likelihood    Discrimination    \n                      Ratio Test           Indexes    \nObs    2874    LR chi2    873.09    R2       0.262    \nsigma1.9786    d.f.            2    R2 adj   0.261    \nd.f.   2871    Pr(&gt; chi2) 0.0000    g        1.278    \n\nResiduals\n\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\n                        Coef   S.E.   t     Pr(&gt;|t|)\nIntercept               1.1983 0.0622 19.26 &lt;0.0001 \nbpi_antisocialT1Sum     0.4655 0.0229 20.30 &lt;0.0001 \nbpi_anxiousDepressedSum 0.1608 0.0330  4.87 &lt;0.0001 \n\n\nUse “cluster” variable to account for nested data within-subject:\n\n\nCode\nrobcov(ols(\n  t_ext ~ m_ext + age,\n  data = mydata,\n  x = TRUE,\n  y = TRUE),\n  cluster = mydata$tcid) #account for nested data within subject\n\n\n\n\n\n\n19.1.4 4. Errors are independent\nIndependent errors means that the errors are uncorrelated with each other.\n\n19.1.4.1 Ways to Test\n\nIntraclass correlation coefficient (ICC) from mixed model\nPlot residuals vs. predictors (Residual Plots)\nTime Series data: Residual time series plot (residuals vs. row number)\nTime Series data: Table or plot of residual autocorrelations\nTime Series data: Durbin-Watson statistic for test of significant residual autocorrelation at lag 1\n\n\n\n19.1.4.2 Ways to Handle\n\nGeneralized least squares (GLS) models are capable of handling correlated errors: https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/gls.html (archived at https://perma.cc/RHZ6-5GT8)\nRegression with cluster variable\n\nrobcov() from rms package\n\nHierarchical linear modeling\n\nLinear mixed effects models\nGeneralized linear mixed effects models\nNonlinear mixed effects models\n\n\n\n\n\n19.1.5 5. No multicollinearity\nMulticollinearity occurs when the predictors are correlated with each other.\n\n19.1.5.1 Ways to Test\n\nVariance Inflation Factor (VIF)\nGeneralized Variance Inflation Factor (GVIF)—when models have related regressors (multiple polynomial terms or contrasts from same predictor)\nCorrelation\nTolerance\nCondition Index\n\n\n19.1.5.1.1 Variance Inflation Factor (VIF)\n\\[\n\\text{VIF} = 1/\\text{Tolerance}\n\\]\nIf the variance inflation factor of a predictor variable were 5.27 (\\(\\sqrt{5.27} = 2.3\\)), this means that the standard error for the coefficient of that predictor variable is 2.3 times as large (i.e., confidence interval is 2.3 times wider) as it would be if that predictor variable were uncorrelated with the other predictor variables.\nVIF = 1: Not correlated\n1 &lt; VIF &lt; 5: Moderately correlated\nVIF &gt; 5 to 10: Highly correlated (multicollinearity present)\n\n\nCode\nvif(multipleRegressionModel)\n\n\n    bpi_antisocialT1Sum bpi_anxiousDepressedSum \n               1.291545                1.291545 \n\n\n\n\n19.1.5.1.2 Generalized Variance Inflation Factor (GVIF)\nUseful when models have related regressors (multiple polynomial terms or contrasts from same predictor)\n\n\n19.1.5.1.3 Correlation\ncorrelation among all independent variables the correlation coefficients should be smaller than .08\n\n\nCode\ncor(\n  mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_anxiousDepressedSum\")],\n  use = \"pairwise.complete.obs\")\n\n\n                        bpi_antisocialT1Sum bpi_anxiousDepressedSum\nbpi_antisocialT1Sum                1.000000                0.497279\nbpi_anxiousDepressedSum            0.497279                1.000000\n\n\n\n\n19.1.5.1.4 Tolerance\nThe tolerance is an index of the influence of one independent variable on all other independent variables.\n\\[\n\\text{tolerance} = 1/\\text{VIF}\n\\]\nT &lt; 0.2: there might be multicollinearity in the data\nT &lt; 0.01: there certainly is multicollinarity in the data\n\n\n19.1.5.1.5 Condition Index\nThe condition index is calculated using a factor analysis on the independent variables. Values of 10-30 indicate a mediocre multicollinearity in the regression variables. Values &gt; 30 indicate strong multicollinearity.\nFor how to interpret, see here: https://stats.stackexchange.com/a/87610/20338 (archived at https://perma.cc/Y4J8-MY7Q)\n\n\nCode\nols_eigen_cindex(multipleRegressionModel)\n\n\n\n  \n\n\n\n\n\n\n19.1.5.2 Ways to Handle\n\nRemove highly correlated (i.e., redundant) predictors\nAverage the correlated predictors\nPrincipal Component Analysis for data reduction\nStandardize predictors\nCenter the data (deduct the mean)\nSingular-value decomposition of the model matrix or the mean-centered model matrix\nConduct a factor analysis and rotate the factors to ensure independence of the factors\n\n\n\n\n19.1.6 6. Errors are normally distributed\n\n19.1.6.1 Ways to Test\n\nProbability Plots\n\nNormal Quantile (QQ) Plots (based on non-cumulative distribution of residuals)\nNormal Probability (PP) Plots (based on cumulative distribution of residuals)\n\nDensity Plot of Residuals\nStatistical Tests\n\nKolmogorov-Smirnov test\nShapiro-Wilk test\nJarque-Bera test\nAnderson-Darling test (best test)\n\nExamine influence of outliers\n\n\n\n19.1.6.2 Ways to Handle\n\nApply a transformation to the predictor or outcome variable\nExclude outliers\nRobust regression\n\nBest when no outliers: MM-type regression estimator\n\nlmrob()/glmrob() function of robustbase package\nIteratively reweighted least squares (IRLS): rlm(, method = \"MM\") function of MASS package: http://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)\n\nMost resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers\n\nltsReg() function of robustbase package (best)\n\nBest when single predictor: Theil-Sen estimator\n\nmblm(, repeated = FALSE) function of mblm package\n\nRobust correlation\n\nSpearman’s rho: cor(, method = \"spearman\")\nPercentage bend correlation\nMinimum vollume ellipsoid\nMinimum covariance determinant:\nWinsorized correlation\nBiweight midcorrelation\nXi (\\(\\xi\\)) correlation\n\nNot great options:\n\nQuantile (L1) regression: rq() function of quantreg package\n\n\n\n\n19.1.6.2.1 Transformations of Outcome Variable\n\n19.1.6.2.1.1 Box-Cox Transformation\nUseful if the outcome is strictly positive (or add a constant to outcome to make it strictly positive)\nlambda = -1: inverse transformation\nlambda = -0.5: 1/sqrt(Y)\nlambda = 0: log transformation\nlambda = 0.5: square root\nlambda = 0.333: cube root\nlambda = 1: no transformation\nlambda = 2: squared\n\n\nRaw distribution\n\n\nCode\nplot(density(na.omit(mydata$bpi_antisocialT2Sum)))\n\n\n\n\n\n\n\n\n\n\n\nAdd constant to outcome to make it strictly positive\n\n\nCode\nstrictlyPositiveDV &lt;- lm(\n  bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\n\n\n\nIdentify best power transformation (lambda)\nConsider rounding the power to a common value (square root = .5; cube root = .333; squared = 2)\n\n\nCode\nboxCox(strictlyPositiveDV)\n\n\n\n\n\n\n\n\n\nCode\npowerTransform(strictlyPositiveDV) \n\n\nEstimated transformation parameter \n      Y1 \n0.234032 \n\n\nCode\ntransformedDV &lt;- powerTransform(strictlyPositiveDV)\n\nsummary(transformedDV)\n\n\nbcPower Transformation to Normality \n   Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\nY1     0.234        0.23       0.1825       0.2856\n\nLikelihood ratio test that transformation parameter is equal to 0\n (log transformation)\n                           LRT df       pval\nLR test, lambda = (0) 78.04052  1 &lt; 2.22e-16\n\nLikelihood ratio test that no transformation is needed\n                           LRT df       pval\nLR test, lambda = (1) 853.8675  1 &lt; 2.22e-16\n\n\n\n\nTransform the DV\n\n\nCode\nmydata$bpi_antisocialT2SumTransformed &lt;- bcPower(mydata$bpi_antisocialT2Sum + 1, coef(transformedDV))\n\nplot(density(na.omit(mydata$bpi_antisocialT2SumTransformed)))\n\n\n\n\n\n\n\n\n\n\n\nCompare residuals from model with and without transformation\n\nModel without transformation\n\n\nCode\nsummary(modelWithoutTransformation &lt;- lm(\n  bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude))\n\n\n\nCall:\nlm(formula = bpi_antisocialT2Sum + 1 ~ bpi_antisocialT1Sum + \n    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3755 -1.2337 -0.2212  0.9911 12.8017 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              2.19830    0.05983  36.743  &lt; 2e-16 ***\nbpi_antisocialT1Sum      0.46553    0.01858  25.049  &lt; 2e-16 ***\nbpi_anxiousDepressedSum  0.16075    0.02916   5.513 3.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.979 on 2871 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.262, Adjusted R-squared:  0.2615 \nF-statistic: 509.6 on 2 and 2871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(density(na.omit(studres(modelWithoutTransformation))))\n\n\n\n\n\n\n\n\n\n\n\nModel with transformation\n\n\nCode\nsummary(modelWithTransformation &lt;- lm(\n  bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude))\n\n\n\nCall:\nlm(formula = bpi_antisocialT2SumTransformed ~ bpi_antisocialT1Sum + \n    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3429 -0.4868  0.0096  0.4922  2.9799 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.800373   0.022024  36.342  &lt; 2e-16 ***\nbpi_antisocialT1Sum     0.165476   0.006841  24.189  &lt; 2e-16 ***\nbpi_anxiousDepressedSum 0.055910   0.010733   5.209 2.03e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7283 on 2871 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.2477,    Adjusted R-squared:  0.2472 \nF-statistic: 472.7 on 2 and 2871 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nplot(density(na.omit(studres(modelWithTransformation))))\n\n\n\n\n\n\n\n\n\n\n\n\nConstructed Variable Test & Plot\nA significant p-value indicates a strong need to transform variable:\n\n\nCode\nmultipleRegressionModel_constructedVariable &lt;- update(\n  multipleRegressionModel,\n  . ~ . + boxCoxVariable(bpi_antisocialT1Sum + 1))\nsummary(multipleRegressionModel_constructedVariable)$coef[\"boxCoxVariable(bpi_antisocialT1Sum + 1)\", , drop = FALSE]\n\n\n                                           Estimate Std. Error   t value\nboxCoxVariable(bpi_antisocialT1Sum + 1) -0.06064732 0.04683415 -1.294938\n                                         Pr(&gt;|t|)\nboxCoxVariable(bpi_antisocialT1Sum + 1) 0.1954457\n\n\nPlot allows us to see whether the need for transformation is spread through data or whether it is just dependent on a small fraction of observations:\n\n\nCode\navPlots(multipleRegressionModel_constructedVariable, \"boxCoxVariable(bpi_antisocialT1Sum + 1)\")\n\n\n\n\n\n\n\n\n\n\n\nInverse Response Plot\nThe black line is the best-fitting power transformation:\n\n\nCode\ninverseResponsePlot(strictlyPositiveDV, lambda = c(-1,-0.5,0,1/3,.5,1,2))\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n19.1.6.2.1.2 Yeo-Johnson Transformations\nUseful if the outcome is not strictly positive.\n\n\nCode\nyjPower(DV, lambda)\n\n\n\n\n\n19.1.6.2.2 Transformations of Predictor Variable\n\n19.1.6.2.2.1 Component-Plus-Residual Plots (Partial Residual Plots)\nLinear model:\n\n\nCode\ncrPlots(multipleRegressionModelNoMissing, order = 1)\n\n\n\n\n\n\n\n\n\nQuadratic model:\n\n\nCode\ncrPlots(multipleRegressionModelNoMissing, order = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmultipleRegressionModel_quadratic &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(multipleRegressionModel_quadratic)\n\n\n\nCall:\nlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + I(bpi_antisocialT1Sum^2) + \n    bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6238 -1.2610 -0.2484  0.9923 12.9008 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               1.099237   0.076509  14.367  &lt; 2e-16 ***\nbpi_antisocialT1Sum       0.547680   0.043723  12.526  &lt; 2e-16 ***\nI(bpi_antisocialT1Sum^2) -0.010221   0.004925  -2.075    0.038 *  \nbpi_anxiousDepressedSum   0.161734   0.029144   5.549 3.13e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.977 on 2870 degrees of freedom\n  (8656 observations deleted due to missingness)\nMultiple R-squared:  0.2631,    Adjusted R-squared:  0.2623 \nF-statistic: 341.5 on 3 and 2870 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nanova(\n  multipleRegressionModel_quadratic,\n  multipleRegressionModel)\n\n\n\n  \n\n\n\nCode\ncrPlots(multipleRegressionModel_quadratic, order = 1)\n\n\n\n\n\n\n\n\n\n\n\n19.1.6.2.2.2 CERES Plot (Combining conditional Expectations and RESiduals)\nUseful when nonlinear associations among the predictors are very strong (component-plus-residual plots may appear nonlinear even though the true partial regression is linear, a phenonomen called leakage)\n\n\nCode\nceresPlots(multipleRegressionModel)\n\n\n\n\n\n\n\n\n\nCode\nceresPlots(multipleRegressionModel_quadratic)\n\n\nError in `plot.window()`:\n! need finite 'ylim' values\n\n\n\n\n\n\n\n\n\n\n\n19.1.6.2.2.3 Box-Tidwell Method for Choosing Predictor Transformations\npredictors must be strictly positive (or add a constant to make it strictly positive)\n\n\nCode\nboxTidwell(\n  bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1),\n  other.x = NULL, #list variables not to be transformed in other.x\n  data = mydata,\n  na.action = na.exclude)\n\n\n                               MLE of lambda Score Statistic (t) Pr(&gt;|t|)\nI(bpi_antisocialT1Sum + 1)           0.90820             -1.0591   0.2897\nI(bpi_anxiousDepressedSum + 1)       0.36689             -1.0650   0.2870\n\niterations =  4 \n\nScore test for null hypothesis that all lambdas = 1:\nF = 1.4056, df = 2 and 2869, Pr(&gt;F) = 0.2454\n\n\n\n\n19.1.6.2.2.4 Constructed-Variables Plot\n\n\nCode\nmultipleRegressionModel_cv &lt;- lm(\n  bpi_antisocialT2Sum ~ I(bpi_antisocialT1Sum + 1) + I(bpi_anxiousDepressedSum + 1) + I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) + I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)),\n  data = mydata,\n  na.action = na.exclude)\n\nsummary(multipleRegressionModel_cv)$coef[\"I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))\", , drop = FALSE]\n\n\n                                                    Estimate Std. Error\nI(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -0.1691003 0.06887107\n                                                    t value   Pr(&gt;|t|)\nI(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum)) -2.455317 0.01418393\n\n\nCode\nsummary(multipleRegressionModel_cv)$coef[\"I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))\", , drop = FALSE]\n\n\n                                                            Estimate Std. Error\nI(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.00828867  0.1384918\n                                                             t value  Pr(&gt;|t|)\nI(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum)) 0.05984953 0.9522831\n\n\nCode\navPlots(multipleRegressionModel_cv, \"I(bpi_antisocialT1Sum * log(bpi_antisocialT1Sum))\")\n\n\n\n\n\n\n\n\n\nCode\navPlots(multipleRegressionModel_cv, \"I(bpi_anxiousDepressedSum * log(bpi_anxiousDepressedSum))\")\n\n\n\n\n\n\n\n\n\n\n\n\n19.1.6.2.3 Robust models\nResources\n\nFor comparison of methods, see Book: “Modern Methods for Robust Regression”\nhttp://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)\nhttps://stats.stackexchange.com/a/46234/20338 (archived at https://perma.cc/WC57-9GA7)\nhttps://cran.r-project.org/web/views/Robust.html (archived at https://perma.cc/THN5-KNY3)\n\n\n19.1.6.2.3.1 Robust correlation\n\nSpearman’s rho\nSpearman’s rho is a non-parametric correlation.\n\n\nCode\ncor.test(\n  mydata$bpi_antisocialT1Sum,\n  mydata$bpi_antisocialT2Sum) #Pearson r, correlation that is sensitive to outliers\n\n\n\n    Pearson's product-moment correlation\n\ndata:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum\nt = 31.274, df = 2873, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4761758 0.5307381\nsample estimates:\n      cor \n0.5039595 \n\n\nCode\ncor.test(\n  mydata$bpi_antisocialT1Sum,\n  mydata$bpi_antisocialT2Sum, method = \"spearman\") #Spearman's rho, a rank correlation that is less sensitive to outliers\n\n\n\n    Spearman's rank correlation rho\n\ndata:  mydata$bpi_antisocialT1Sum and mydata$bpi_antisocialT2Sum\nS = 1997347186, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.4956973 \n\n\n\n\nMinimum vollume ellipsoid\n\n\nCode\ncov.mve(\n  na.omit(mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")]),\n  cor = TRUE)\n\n\n$center\nbpi_antisocialT1Sum bpi_antisocialT2Sum \n           2.272655            2.248461 \n\n$cov\n                    bpi_antisocialT1Sum bpi_antisocialT2Sum\nbpi_antisocialT1Sum            2.587323            1.404446\nbpi_antisocialT2Sum            1.404446            3.007363\n\n$msg\n[1] \"143 singular samples of size 3 out of 1500\"\n\n$crit\n[1] 3.943936\n\n$best\n   [1]    1    3    8    9   10   12   13   14   15   16   17   18   19   21\n  [15]   22   23   24   32   39   40   42   43   44   46   48   50   51   55\n  [29]   56   57   60   61   65   67   70   71   73   76   79   80   81   82\n  [43]   86   87   88   93   94   95   97   98   99  102  104  105  106  107\n  [57]  108  116  117  119  120  121  122  123  124  129  130  131  132  133\n  [71]  134  138  139  140  142  143  145  146  148  149  150  153  154  155\n  [85]  160  161  164  166  168  170  176  177  178  179  180  181  182  183\n  [99]  184  187  188  190  191  192  194  195  196  197  198  200  201  205\n [113]  206  207  209  213  214  227  228  229  231  232  233  234  235  236\n [127]  237  238  239  241  242  243  244  245  246  247  248  249  251  253\n [141]  254  255  256  261  263  265  268  272  275  281  282  284  288  289\n [155]  290  293  294  295  296  297  298  299  300  301  304  307  308  310\n [169]  313  315  329  330  331  332  333  334  335  339  341  342  343  345\n [183]  350  357  359  364  367  370  371  374  376  379  380  381  382  386\n [197]  387  388  389  390  391  397  399  403  404  407  408  409  410  411\n [211]  413  416  421  422  423  425  426  430  431  432  435  438  440  441\n [225]  442  443  444  445  448  449  450  451  453  454  455  456  459  461\n [239]  462  463  464  466  468  469  471  473  474  477  478  480  483  484\n [253]  486  493  495  497  498  499  501  502  503  504  505  506  507  509\n [267]  510  513  518  519  520  521  522  523  524  525  527  528  529  531\n [281]  532  533  534  535  536  537  538  539  541  543  545  546  550  553\n [295]  554  557  564  566  568  572  578  579  582  583  585  599  600  605\n [309]  608  609  610  615  616  617  618  619  620  621  622  623  625  626\n [323]  627  631  633  634  635  636  637  639  641  642  643  646  647  648\n [337]  649  650  660  661  662  667  670  673  674  675  676  679  680  681\n [351]  682  683  685  687  691  693  696  701  705  706  707  710  713  714\n [365]  715  717  718  719  720  721  722  723  728  730  732  733  734  737\n [379]  741  742  743  744  745  746  747  751  752  758  762  763  764  765\n [393]  769  772  774  776  779  780  782  783  785  786  789  791  798  799\n [407]  801  803  805  806  808  809  810  811  812  813  814  817  818  819\n [421]  822  824  826  827  828  829  832  834  836  837  838  839  840  843\n [435]  847  848  849  854  857  858  859  860  862  865  866  867  868  869\n [449]  871  872  873  874  877  880  881  888  889  892  893  895  898  899\n [463]  900  902  903  904  905  906  907  909  912  913  914  918  919  920\n [477]  921  923  925  926  927  928  931  932  933  934  935  936  937  939\n [491]  940  943  944  946  949  950  951  954  955  956  957  960  962  964\n [505]  965  967  970  972  973  974  975  976  977  980  983  985  987  988\n [519]  990  998 1001 1002 1003 1004 1005 1006 1007 1008 1013 1014 1015 1016\n [533] 1017 1021 1022 1023 1024 1025 1026 1027 1029 1030 1032 1034 1036 1037\n [547] 1040 1041 1043 1044 1047 1050 1051 1054 1055 1057 1058 1061 1062 1064\n [561] 1065 1066 1067 1072 1075 1076 1079 1083 1086 1087 1088 1091 1094 1095\n [575] 1097 1101 1105 1109 1111 1112 1113 1114 1115 1119 1121 1123 1124 1125\n [589] 1128 1129 1136 1138 1139 1144 1145 1146 1147 1148 1149 1150 1152 1154\n [603] 1158 1160 1162 1163 1164 1165 1171 1172 1176 1177 1178 1179 1180 1181\n [617] 1182 1184 1185 1186 1187 1188 1189 1197 1198 1200 1201 1203 1205 1206\n [631] 1207 1208 1214 1215 1216 1217 1218 1219 1220 1221 1223 1225 1229 1230\n [645] 1231 1232 1233 1234 1235 1242 1243 1244 1245 1246 1247 1248 1249 1250\n [659] 1251 1254 1255 1259 1261 1262 1263 1265 1266 1267 1269 1271 1272 1274\n [673] 1275 1276 1280 1282 1283 1284 1285 1287 1288 1290 1291 1292 1293 1294\n [687] 1296 1299 1301 1303 1304 1306 1309 1310 1311 1312 1314 1315 1316 1318\n [701] 1319 1320 1322 1323 1325 1326 1333 1334 1336 1337 1338 1339 1341 1343\n [715] 1344 1346 1347 1348 1349 1350 1352 1353 1354 1355 1357 1358 1359 1360\n [729] 1361 1362 1363 1365 1366 1369 1370 1371 1372 1373 1374 1375 1377 1381\n [743] 1382 1383 1387 1388 1389 1390 1391 1395 1396 1397 1398 1399 1400 1401\n [757] 1405 1408 1412 1413 1414 1417 1419 1421 1422 1423 1426 1428 1430 1432\n [771] 1433 1434 1438 1439 1440 1444 1445 1447 1448 1449 1456 1458 1459 1464\n [785] 1467 1469 1471 1479 1483 1484 1485 1486 1489 1495 1499 1500 1502 1506\n [799] 1508 1513 1514 1515 1516 1518 1519 1521 1522 1524 1525 1527 1531 1532\n [813] 1533 1534 1535 1536 1537 1539 1540 1541 1542 1543 1544 1545 1547 1549\n [827] 1551 1554 1560 1561 1563 1565 1569 1571 1572 1575 1576 1577 1583 1585\n [841] 1586 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1600 1602\n [855] 1604 1605 1611 1612 1613 1614 1617 1621 1623 1625 1626 1627 1634 1640\n [869] 1641 1644 1656 1657 1661 1664 1672 1673 1674 1675 1677 1686 1687 1689\n [883] 1690 1691 1692 1694 1699 1701 1705 1708 1709 1710 1711 1712 1713 1714\n [897] 1716 1719 1721 1722 1724 1731 1732 1738 1742 1750 1753 1754 1757 1759\n [911] 1760 1761 1763 1764 1765 1766 1770 1772 1774 1775 1778 1780 1781 1791\n [925] 1795 1797 1798 1801 1802 1803 1804 1806 1807 1809 1812 1813 1815 1817\n [939] 1818 1821 1824 1827 1834 1835 1836 1837 1840 1846 1847 1848 1849 1850\n [953] 1851 1852 1855 1859 1860 1863 1864 1866 1867 1868 1870 1881 1884 1888\n [967] 1890 1892 1902 1904 1915 1921 1924 1925 1926 1927 1932 1933 1935 1936\n [981] 1938 1940 1943 1947 1948 1950 1951 1954 1955 1956 1957 1960 1961 1962\n [995] 1963 1966 1967 1969 1970 1971 1972 1973 1974 1979 1981 1982 1984 1986\n[1009] 1990 1992 1993 1994 1995 1999 2002 2005 2006 2011 2012 2013 2017 2019\n[1023] 2021 2022 2027 2029 2032 2034 2036 2038 2042 2043 2045 2049 2051 2053\n[1037] 2054 2055 2059 2061 2062 2063 2068 2069 2070 2071 2073 2074 2076 2077\n[1051] 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2094 2096\n[1065] 2097 2099 2100 2101 2102 2104 2105 2106 2108 2110 2112 2113 2114 2116\n[1079] 2117 2118 2119 2123 2124 2125 2127 2129 2130 2131 2137 2138 2139 2142\n[1093] 2143 2145 2151 2157 2158 2160 2161 2166 2167 2168 2169 2174 2175 2176\n[1107] 2177 2178 2179 2180 2181 2183 2185 2188 2189 2190 2191 2200 2201 2202\n[1121] 2203 2205 2206 2207 2208 2210 2212 2215 2219 2221 2222 2223 2225 2226\n[1135] 2227 2228 2231 2232 2234 2236 2237 2238 2239 2241 2242 2245 2248 2249\n[1149] 2255 2257 2258 2259 2260 2261 2262 2264 2265 2269 2271 2272 2273 2275\n[1163] 2280 2283 2285 2290 2291 2294 2297 2298 2299 2301 2302 2306 2307 2310\n[1177] 2311 2314 2316 2319 2320 2322 2323 2324 2325 2326 2329 2330 2331 2332\n[1191] 2334 2336 2337 2338 2340 2341 2342 2343 2344 2345 2348 2349 2350 2351\n[1205] 2352 2354 2356 2364 2367 2368 2369 2371 2373 2377 2378 2379 2384 2385\n[1219] 2388 2398 2400 2402 2410 2411 2414 2415 2416 2418 2420 2424 2425 2428\n[1233] 2429 2437 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453\n[1247] 2462 2463 2464 2465 2466 2473 2475 2476 2477 2481 2483 2485 2489 2490\n[1261] 2491 2495 2496 2497 2498 2499 2500 2503 2504 2506 2507 2508 2510 2511\n[1275] 2512 2513 2522 2523 2525 2526 2527 2528 2529 2530 2531 2532 2533 2535\n[1289] 2536 2537 2539 2541 2543 2546 2547 2554 2561 2562 2563 2564 2565 2570\n[1303] 2571 2572 2573 2575 2578 2582 2583 2584 2585 2588 2591 2595 2597 2598\n[1317] 2599 2605 2606 2607 2613 2614 2615 2617 2618 2619 2621 2627 2629 2636\n[1331] 2638 2639 2640 2641 2642 2643 2644 2645 2647 2653 2655 2656 2657 2658\n[1345] 2659 2662 2663 2664 2667 2668 2670 2672 2673 2674 2676 2677 2678 2679\n[1359] 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698 2699 2700\n[1373] 2701 2703 2704 2707 2709 2711 2713 2714 2718 2721 2722 2723 2724 2727\n[1387] 2729 2731 2733 2736 2738 2739 2740 2741 2742 2746 2748 2753 2757 2759\n[1401] 2760 2763 2764 2765 2766 2776 2777 2778 2781 2783 2784 2786 2787 2788\n[1415] 2789 2790 2792 2794 2795 2799 2800 2803 2804 2806 2807 2808 2809 2810\n[1429] 2813 2814 2817 2818 2819 2821 2822 2825 2831 2832 2834 2838 2839 2844\n[1443] 2846 2847 2848 2849 2850 2851 2852 2855 2856 2857 2862 2863 2868 2869\n[1457] 2870 2871 2872 2873\n\n$cor\n                    bpi_antisocialT1Sum bpi_antisocialT2Sum\nbpi_antisocialT1Sum           1.0000000           0.5034855\nbpi_antisocialT2Sum           0.5034855           1.0000000\n\n$n.obs\n[1] 2875\n\n\n\n\nMinimum covariance determinant\n\n\nCode\ncov.mcd(\n  na.omit(mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")]),\n  cor = TRUE)\n\n\n$center\nbpi_antisocialT1Sum bpi_antisocialT2Sum \n           2.181181            2.094954 \n\n$cov\n                    bpi_antisocialT1Sum bpi_antisocialT2Sum\nbpi_antisocialT1Sum            2.379036            1.096437\nbpi_antisocialT2Sum            1.096437            2.482409\n\n$msg\n[1] \"160 singular samples of size 3 out of 1500\"\n\n$crit\n[1] -0.3839279\n\n$best\n   [1]    1    3    8    9   10   12   13   14   15   16   17   18   19   21\n  [15]   22   23   24   39   40   42   43   44   46   48   50   55   56   57\n  [29]   60   61   63   65   67   69   70   76   79   80   81   82   86   87\n  [43]   88   93   95   97   98   99  102  104  105  106  107  108  116  117\n  [57]  119  120  121  122  123  124  129  130  131  132  133  134  138  139\n  [71]  140  141  142  143  145  148  149  150  153  154  155  160  161  166\n  [85]  168  169  170  176  177  178  179  180  181  182  183  184  187  188\n  [99]  190  191  192  194  195  196  197  198  200  201  202  205  206  207\n [113]  209  212  213  214  227  228  231  232  233  234  235  237  238  239\n [127]  240  241  242  243  244  245  246  247  249  251  253  254  255  256\n [141]  261  262  265  268  269  272  275  281  282  284  289  290  291  293\n [155]  294  295  296  297  298  299  300  301  304  307  308  310  313  315\n [169]  329  330  331  332  333  334  335  339  341  342  343  345  346  350\n [183]  355  356  357  359  364  367  370  371  376  379  380  381  382  386\n [197]  387  388  389  390  391  397  399  401  403  404  407  408  409  410\n [211]  411  413  416  421  422  423  425  426  430  431  432  435  437  438\n [225]  440  441  442  443  445  448  449  450  451  453  455  456  458  459\n [239]  461  462  463  464  466  467  468  469  471  473  474  476  477  478\n [253]  480  482  483  484  486  493  494  495  496  497  498  499  500  501\n [267]  502  503  504  505  506  507  509  510  513  518  519  520  521  522\n [281]  523  524  525  527  528  529  531  532  533  534  535  536  537  538\n [295]  539  541  543  545  546  550  553  554  557  558  562  564  566  568\n [309]  572  575  578  579  582  583  585  599  600  601  605  608  609  610\n [323]  615  616  617  618  619  620  621  622  623  625  626  627  628  631\n [337]  633  634  635  636  637  639  641  642  643  646  647  648  649  650\n [351]  657  660  662  667  670  672  673  674  675  676  679  680  681  682\n [365]  683  685  686  687  690  691  693  696  701  704  705  706  707  710\n [379]  713  714  715  717  718  719  720  721  722  723  728  730  732  733\n [393]  734  736  737  740  741  742  743  744  745  746  747  751  752  758\n [407]  762  763  764  765  769  772  774  776  779  780  782  783  786  789\n [421]  791  798  799  801  802  803  805  806  808  809  810  811  812  813\n [435]  814  817  818  819  822  824  826  827  828  832  834  836  837  838\n [449]  839  840  842  843  847  848  849  852  855  857  858  859  860  861\n [463]  862  865  866  867  868  869  871  872  873  874  877  880  881  882\n [477]  888  889  892  893  895  898  899  900  901  902  903  905  906  907\n [491]  909  912  913  914  918  920  921  923  925  926  927  928  931  932\n [505]  933  934  935  936  937  939  940  941  943  944  945  946  949  950\n [519]  951  954  955  956  957  959  960  962  964  965  967  970  972  973\n [533]  974  975  976  977  980  983  985  987  988  990  992  995  998 1000\n [547] 1001 1002 1003 1004 1005 1006 1007 1008 1013 1014 1015 1016 1017 1019\n [561] 1021 1022 1023 1024 1025 1026 1027 1029 1030 1032 1034 1036 1037 1041\n [575] 1043 1044 1046 1047 1049 1050 1051 1053 1054 1055 1057 1058 1059 1061\n [589] 1064 1066 1067 1070 1072 1075 1076 1079 1083 1086 1087 1088 1091 1094\n [603] 1095 1097 1101 1105 1109 1111 1112 1113 1114 1115 1119 1121 1123 1124\n [617] 1125 1128 1129 1132 1135 1136 1138 1139 1144 1145 1146 1147 1148 1149\n [631] 1150 1152 1154 1158 1160 1162 1163 1164 1165 1171 1176 1177 1178 1179\n [645] 1180 1181 1182 1184 1185 1186 1187 1188 1189 1190 1191 1197 1198 1200\n [659] 1201 1203 1205 1206 1207 1208 1209 1214 1215 1216 1217 1218 1219 1220\n [673] 1221 1223 1224 1229 1230 1231 1232 1233 1234 1235 1242 1243 1244 1245\n [687] 1246 1247 1248 1249 1250 1251 1254 1255 1259 1261 1262 1263 1265 1266\n [701] 1267 1269 1271 1272 1273 1274 1275 1276 1280 1281 1282 1283 1284 1285\n [715] 1286 1287 1288 1289 1290 1291 1292 1293 1294 1296 1299 1301 1302 1303\n [729] 1304 1306 1309 1310 1311 1312 1314 1315 1316 1318 1319 1320 1322 1323\n [743] 1325 1326 1333 1334 1336 1337 1338 1339 1341 1343 1344 1347 1348 1349\n [757] 1350 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1365\n [771] 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1380 1381\n [785] 1382 1383 1387 1388 1389 1390 1391 1395 1396 1397 1398 1399 1400 1401\n [799] 1402 1405 1408 1412 1413 1414 1417 1419 1421 1422 1423 1424 1426 1428\n [813] 1430 1432 1433 1434 1438 1439 1440 1444 1445 1447 1448 1449 1456 1458\n [827] 1459 1464 1467 1468 1469 1470 1471 1483 1485 1486 1489 1495 1499 1500\n [841] 1502 1504 1505 1506 1508 1513 1514 1515 1516 1517 1518 1519 1521 1522\n [855] 1524 1525 1527 1531 1532 1533 1534 1535 1536 1537 1539 1540 1541 1542\n [869] 1543 1544 1545 1546 1547 1548 1549 1551 1553 1554 1560 1561 1563 1565\n [883] 1569 1571 1572 1575 1576 1577 1583 1584 1585 1586 1588 1589 1590 1591\n [897] 1592 1593 1594 1595 1596 1597 1598 1600 1602 1604 1605 1606 1611 1612\n [911] 1613 1614 1617 1621 1623 1624 1625 1626 1627 1634 1640 1641 1644 1652\n [925] 1656 1657 1661 1664 1665 1672 1674 1675 1686 1687 1689 1690 1691 1692\n [939] 1694 1699 1701 1705 1706 1708 1709 1710 1711 1712 1714 1716 1718 1719\n [953] 1721 1722 1723 1724 1731 1732 1738 1739 1742 1750 1753 1754 1757 1759\n [967] 1760 1761 1763 1764 1765 1766 1772 1774 1775 1776 1777 1778 1781 1791\n [981] 1795 1797 1798 1801 1802 1803 1804 1806 1807 1809 1810 1812 1813 1815\n [995] 1817 1818 1821 1824 1827 1834 1835 1836 1837 1840 1842 1846 1847 1848\n[1009] 1849 1850 1851 1852 1855 1858 1859 1860 1861 1863 1864 1866 1867 1868\n[1023] 1870 1881 1884 1885 1888 1890 1892 1902 1904 1910 1915 1921 1922 1923\n[1037] 1924 1925 1926 1927 1932 1933 1935 1936 1938 1940 1941 1943 1947 1948\n[1051] 1950 1951 1954 1955 1956 1957 1960 1961 1962 1963 1966 1967 1969 1970\n[1065] 1971 1972 1973 1974 1979 1981 1984 1986 1988 1990 1992 1993 1994 1995\n[1079] 1999 2002 2005 2006 2009 2011 2012 2017 2019 2021 2022 2027 2029 2032\n[1093] 2034 2036 2038 2042 2043 2045 2049 2051 2053 2054 2055 2059 2061 2062\n[1107] 2063 2068 2069 2070 2071 2073 2074 2077 2078 2079 2080 2081 2082 2083\n[1121] 2084 2085 2086 2087 2088 2089 2094 2096 2097 2098 2099 2100 2101 2102\n[1135] 2104 2105 2108 2110 2112 2113 2114 2116 2117 2118 2119 2123 2124 2125\n[1149] 2126 2127 2129 2130 2131 2137 2138 2139 2143 2145 2151 2157 2158 2160\n[1163] 2161 2163 2166 2167 2168 2169 2171 2174 2175 2176 2177 2178 2179 2180\n[1177] 2181 2183 2185 2188 2189 2190 2191 2200 2201 2202 2203 2205 2206 2207\n[1191] 2208 2210 2212 2215 2219 2221 2222 2223 2225 2226 2227 2228 2231 2232\n[1205] 2234 2236 2237 2238 2239 2241 2242 2245 2248 2249 2253 2255 2257 2258\n[1219] 2259 2260 2261 2262 2264 2269 2271 2272 2273 2274 2275 2280 2283 2285\n[1233] 2290 2291 2297 2298 2299 2301 2302 2305 2306 2307 2311 2314 2316 2317\n[1247] 2319 2320 2322 2323 2324 2325 2326 2328 2329 2330 2331 2332 2334 2336\n[1261] 2337 2338 2340 2341 2342 2343 2344 2345 2348 2349 2350 2351 2352 2354\n[1275] 2356 2359 2362 2364 2367 2368 2369 2371 2373 2377 2378 2379 2380 2384\n[1289] 2385 2386 2388 2398 2400 2402 2410 2411 2414 2415 2416 2418 2420 2424\n[1303] 2425 2428 2429 2437 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451\n[1317] 2452 2453 2454 2462 2463 2464 2465 2466 2467 2469 2472 2473 2475 2477\n[1331] 2481 2483 2485 2489 2490 2491 2492 2495 2496 2497 2498 2499 2500 2503\n[1345] 2504 2506 2507 2508 2509 2510 2511 2512 2513 2520 2522 2523 2525 2526\n[1359] 2527 2528 2529 2530 2531 2532 2533 2535 2536 2539 2541 2543 2545 2546\n[1373] 2547 2554 2561 2562 2563 2564 2565 2566 2570 2571 2572 2573 2575 2577\n[1387] 2578 2581 2582 2583 2584 2585 2588 2590 2591 2597 2598 2599 2603 2604\n[1401] 2605 2606 2607 2608 2613 2614 2615 2617 2618 2619 2620 2621 2627 2634\n[1415] 2635 2636 2638 2639 2640 2641 2642 2643 2644 2645 2647 2653 2655 2656\n[1429] 2657 2658 2659 2663 2664 2667 2668 2669 2670 2672 2673 2674 2676 2677\n[1443] 2678 2679 2680 2682 2685 2686 2687 2688 2689 2692 2693 2694 2695 2698\n[1457] 2700 2701 2702 2703 2704 2707 2709 2711 2713 2714 2717 2718 2721 2722\n[1471] 2723 2724 2727 2728 2729 2731 2733 2736 2738 2739 2740 2741 2742 2746\n[1485] 2748 2753 2757 2759 2760 2763 2764 2765 2766 2771 2773 2776 2777 2778\n[1499] 2781 2783 2784 2786 2788 2789 2790 2792 2797 2799 2800 2803 2804 2806\n[1513] 2807 2808 2809 2810 2813 2814 2817 2819 2821 2825 2826 2831 2832 2834\n[1527] 2838 2839 2844 2846 2847 2848 2849 2850 2851 2852 2855 2856 2857 2862\n[1541] 2863 2868 2869 2870 2871 2872 2873\n\n$cor\n                    bpi_antisocialT1Sum bpi_antisocialT2Sum\nbpi_antisocialT1Sum           1.0000000           0.4511764\nbpi_antisocialT2Sum           0.4511764           1.0000000\n\n$n.obs\n[1] 2875\n\n\n\n\nWinsorized correlation\n\n\nCode\ncorrelation(\n  mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")],\n  winsorize = 0.2,\n  p_adjust = \"none\")\n\n\n\n  \n\n\n\n\n\nPercentage bend correlation\n\n\nCode\ncorrelation(\n  mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")],\n  method = \"percentage\",\n  p_adjust = \"none\")\n\n\n\n  \n\n\n\n\n\nBiweight midcorrelation\n\n\nCode\ncorrelation(\n  mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")],\n  method = \"biweight\",\n  p_adjust = \"none\")\n\n\n\n  \n\n\n\n\n\nXi (\\(\\xi\\))\nXi (\\(\\xi\\)) is an index of the degree of dependence between two variables\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009-2022. https://doi.org/10.1080/01621459.2020.1758115\n\n\nCode\nset.seed(52242) # for reproducibility\n\nXICOR::xicor(\n  mydata$bpi_antisocialT1Sum,\n  mydata$bpi_antisocialT2Sum,\n  method = \"permutation\",\n  pvalue = TRUE\n)\n\n\n$xi\n[1] 0.1751217\n\n$sd\n[1] 0.012695\n\n$pval\n[1] 0\n\n\n\n\n\n19.1.6.2.3.2 Robust regression with a single predictor\n\nTheil-Sen estimator\nThe Theil-Sen single median estimator is robust to outliers; have to remove missing values first\n\n\nCode\nmydata_subset &lt;- na.omit(mydata[,c(\"bpi_antisocialT1Sum\",\"bpi_antisocialT2Sum\")])[1:400,]\nmblm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum,\n  dataframe = mydata_subset,\n  repeated = FALSE)\n\n\n\nCall:\nmblm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum, dataframe = mydata_subset, \n    repeated = FALSE)\n\nCoefficients:\n        (Intercept)  bpi_antisocialT1Sum  \n                1.0                  0.6  \n\n\n\n\n\n19.1.6.2.3.3 Robust multiple regression\nBest when no outliers: MM-type regression estimator\n\nRobust linear regression\nMM-type regression estimator (best):\n\n\nCode\nlmrob(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\n\n\nCall:\nlmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,     data = mydata, na.action = na.exclude)\n \\--&gt; method = \"MM\"\nCoefficients:\n            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  \n                 0.9440                   0.4914                   0.1577  \n\n\nIteratively reweighted least squares (IRLS): http://www.ats.ucla.edu/stat/r/dae/rreg.htm (archived at https://web.archive.org/web/20161119025907/http://www.ats.ucla.edu/stat/r/dae/rreg.htm)\n\n\nCode\nrlm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\n\nCall:\nrlm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    data = mydata, na.action = na.exclude)\nConverged in 6 iterations\n\nCoefficients:\n            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n              0.9838033               0.4873118               0.1620020 \n\nDegrees of freedom: 2874 total; 2871 residual\n  (8656 observations deleted due to missingness)\nScale estimate: 1.62 \n\n\n\n\nRobust generalized regression\n\n\nCode\nglmrob(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  family = \"poisson\",\n  na.action = \"na.exclude\")\n\n\n\nCall:  glmrob(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +      bpi_anxiousDepressedSum, family = \"poisson\", data = mydata,      na.action = \"na.exclude\") \n\nCoefficients:\n            (Intercept)      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  \n                0.37281                  0.15589                  0.05026  \n\nNumber of observations: 2874 \nFitted by method  'Mqle' \n\n\nMost resistant to outliers: Least trimmed squares (LTS)—but not good as a standalone estimator, better for identifying outliers\n\n\nCode\nltsReg(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude)\n\n\n\nCall:\nltsReg.formula(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum +     bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n\nCoefficients:\n              Intercept      bpi_antisocialT1Sum  bpi_anxiousDepressedSum  \n                 0.7813                   0.5145                   0.1318  \n\nScale estimate 1.75 \n\n\n\nNot great options:\nQuantile (L1) regression: rq() function of quantreg package\nhttps://data.library.virginia.edu/getting-started-with-quantile-regression/ (archived at https://perma.cc/FSV4-5DCR)\n\n\nCode\nquantiles &lt;- 1:9/10\n\nquantileRegressionModel &lt;- rq(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  tau = quantiles,\n  na.action = na.exclude)\n\nquantileRegressionModel\n\n\nCall:\nrq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\nCoefficients:\n                           tau= 0.1  tau= 0.2   tau= 0.3 tau= 0.4 tau= 0.5\n(Intercept)             -0.19607843 0.0000000 0.09090909      0.5     0.88\nbpi_antisocialT1Sum      0.19607843 0.3333333 0.45454545      0.5     0.52\nbpi_anxiousDepressedSum  0.09803922 0.1111111 0.13636364      0.1     0.12\n                         tau= 0.6 tau= 0.7 tau= 0.8  tau= 0.9\n(Intercept)             1.0000000   1.3750   2.1250 3.1000000\nbpi_antisocialT1Sum     0.5913978   0.6250   0.6250 0.6428571\nbpi_anxiousDepressedSum 0.2043011   0.1875   0.1875 0.2285714\n\nDegrees of freedom: 11530 total; 11527 residual\n\n\nCode\nsummary(quantileRegressionModel)\n\n\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.1\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)             -0.19608  0.05341   -3.67101  0.00025\nbpi_antisocialT1Sum      0.19608  0.03492    5.61452  0.00000\nbpi_anxiousDepressedSum  0.09804  0.03536    2.77276  0.00559\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.2\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              0.00000  0.04196    0.00000  1.00000\nbpi_antisocialT1Sum      0.33333  0.02103   15.85299  0.00000\nbpi_anxiousDepressedSum  0.11111  0.02669    4.16376  0.00003\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.3\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              0.09091  0.05830    1.55945  0.11900\nbpi_antisocialT1Sum      0.45455  0.02584   17.59380  0.00000\nbpi_anxiousDepressedSum  0.13636  0.03601    3.78723  0.00016\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.4\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              0.50000  0.06410    7.80035  0.00000\nbpi_antisocialT1Sum      0.50000  0.02374   21.06425  0.00000\nbpi_anxiousDepressedSum  0.10000  0.03372    2.96584  0.00304\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.5\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              0.88000  0.06584   13.36547  0.00000\nbpi_antisocialT1Sum      0.52000  0.02491   20.87561  0.00000\nbpi_anxiousDepressedSum  0.12000  0.03771    3.18245  0.00148\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.6\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              1.00000  0.04246   23.55356  0.00000\nbpi_antisocialT1Sum      0.59140  0.02146   27.55553  0.00000\nbpi_anxiousDepressedSum  0.20430  0.03156    6.47422  0.00000\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.7\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              1.37500  0.07716   17.82011  0.00000\nbpi_antisocialT1Sum      0.62500  0.02397   26.07659  0.00000\nbpi_anxiousDepressedSum  0.18750  0.03760    4.98623  0.00000\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.8\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              2.12500  0.09498   22.37224  0.00000\nbpi_antisocialT1Sum      0.62500  0.03524   17.73509  0.00000\nbpi_anxiousDepressedSum  0.18750  0.05040    3.72020  0.00020\n\nCall: rq(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum, \n    tau = quantiles, data = mydata, na.action = na.exclude)\n\ntau: [1] 0.9\n\nCoefficients:\n                        Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)              3.10000  0.11730   26.42749  0.00000\nbpi_antisocialT1Sum      0.64286  0.04797   13.40136  0.00000\nbpi_anxiousDepressedSum  0.22857  0.06439    3.54960  0.00039\n\n\nPlot to examine the association between the predictor and the outcome at different levels (i.e., quantiles) of the predictor:\n\n\nCode\nggplot(\n  mydata,\n  aes(bpi_antisocialT1Sum, bpi_antisocialT2Sum)) + \n  geom_point() + \n  geom_quantile(\n    quantiles = quantiles,\n    aes(color = factor(after_stat(quantile))))\n\n\n\n\n\n\n\n\n\nBelow is a plot that examines the difference in quantile coefficients (and associated confidence intervals) at different levels (i.e., quantiles) of the predictor. The x-axis is the quantile of the predictor. The y-axis is the slope coefficient. Each black dot is the slope coefficient for the given quantile of the predictor. The red lines are the least squares estimate of the slope coefficient and its confidence interval.\n\n\nCode\nplot(summary(quantileRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(\n  summary(quantileRegressionModel),\n  parm = \"bpi_antisocialT1Sum\")",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "regression.html#examining-model-assumptions",
    "href": "regression.html#examining-model-assumptions",
    "title": "Regression",
    "section": "19.2 Examining Model Assumptions",
    "text": "19.2 Examining Model Assumptions\n\n19.2.1 Distribution of Residuals\n\n19.2.1.1 QQ plots\nhttps://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html (archived at https://perma.cc/9UC3-3WRX)\n\n\nCode\nqqPlot(multipleRegressionModel, main = \"QQ Plot\", id = TRUE)\n\n\n\n\n\n\n\n\n\n[1] 8955 8956\n\n\nCode\nqqnorm(resid(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid(rmsMultipleRegressionModel))\nqqnorm(resid(robustLinearRegression))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid(ltsRegression))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid(generalizedRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid(rmsGeneralizedRegressionModel))\n\n\nError in `residuals.Glm()`:\n! argument \"type\" is missing, with no default\n\n\nCode\nqqnorm(resid(robustGeneralizedRegression))\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid(multilevelRegressionModel))\n\n\nError in `h()`:\n! error in evaluating the argument 'object' in selecting a method for function 'resid': object 'multilevelRegressionModel' not found\n\n\n\n\n19.2.1.2 PP plots\n\n\nCode\nppPlot(multipleRegressionModel)\n\n\n\n\n\n\n\n\n\nCode\nppPlot(rmsMultipleRegressionModel)\nppPlot(robustLinearRegression)\n\n\n\n\n\n\n\n\n\nCode\nppPlot(ltsRegression)\n\n\n\n\n\n\n\n\n\nCode\nppPlot(generalizedRegressionModel)\n\n\n\n\n\n\n\n\n\nCode\nppPlot(rmsGeneralizedRegressionModel)\n\n\nError in `residuals.Glm()`:\n! argument \"type\" is missing, with no default\n\n\nCode\nppPlot(robustGeneralizedRegression)\n\n\n\n\n\n\n\n\n\n\n\n19.2.1.3 Density Plot of Residuals\n\n\nCode\nstudentizedResiduals &lt;- na.omit(rstudent(multipleRegressionModel))\nplot(\n  density(studentizedResiduals),\n  col=\"red\")\nxfit &lt;- seq(\n  min(studentizedResiduals, na.rm = TRUE),\n  max(studentizedResiduals, na.rm = TRUE),\n  length = 40)\nlines(\n  xfit,\n  dnorm(xfit),\n  col=\"gray\") #compare to normal distribution\n\n\n\n\n\n\n\n\n\n\n\n\n19.2.2 Residual Plots\nResidual plots are plots of the residuals versus observed/fitted values.\nIncludes plots of a) model residuals versus observed values on predictors and b) model residuals versus model fitted values.\nNote: have to remove na.action = na.exclude\nTests include:\n\nlack-of-fit test for every numeric predictor, t-test for the regressor, added to the model, indicating no lack-of-fit for this type\nTukey’s test for nonadditivity: adding the squares of the fitted values to the model and refitting (evaluates adequacy of model fit)\n\n\n\nCode\nresidualPlots(\n  multipleRegressionModelNoMissing,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\n                        Test stat Pr(&gt;|Test stat|)  \nbpi_antisocialT1Sum       -2.0755          0.03803 *\nbpi_anxiousDepressedSum   -1.2769          0.20175  \nTukey test                -2.3351          0.01954 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n19.2.3 Marginal Model Plots\nMarginal model plots are plots of the outcome versus predictors/fitted values.\nIncludes plots of a) observed outcome values versus values on predictors (ignoring the other predictors) and b) observed outcome values versus model fitted values.\n\n\nCode\nmarginalModelPlots(\n  multipleRegressionModel,\n  sd = TRUE,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n19.2.4 Added-Variable Plots\nAdded-variable plots are plots of the partial association between the outcome and each predictor, controlling for all other predictors.\nUseful for identifying jointly influential observations and studying the impact of observations on the regression coefficients.\ny-axis: residuals from model with all predictors excluding the predictor of interest\nx-axis: residuals from model regressing predictor of interest on all other predictors\n\n\nCode\navPlots(\n  multipleRegressionModel,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\n\n19.2.4.1 Refit model removing jointly influential observations\n\n\nCode\nmultipleRegressionModel_removeJointlyInfluentialObs &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(6318,6023,4022,4040))\n\navPlots(\n  multipleRegressionModel_removeJointlyInfluentialObs,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ncompareCoefs(\n  multipleRegressionModel,\n  multipleRegressionModel_removeJointlyInfluentialObs)\n\n\nCalls:\n1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(6318, 6023, 4022, 4040),\n   na.action = na.exclude)\n\n                        Model 1 Model 2\n(Intercept)              1.1983  1.1949\nSE                       0.0598  0.0599\n                                       \nbpi_antisocialT1Sum      0.4655  0.4649\nSE                       0.0186  0.0188\n                                       \nbpi_anxiousDepressedSum  0.1608  0.1665\nSE                       0.0292  0.0295\n                                       \n\n\n\n\n\n19.2.5 Outlier test\nLocates the largest Studentized residuals in absolute value and computes the Bonferroni-corrected p-values based on a t-test for linear models.\nTest of outlyingness, i.e., how likely one would have a residual of a given magnitude in a normal distribution with the same sample size\nNote: it does not test how extreme an observation is relative to its distribution (i.e., leverage)\n\n\nCode\noutlierTest(multipleRegressionModel)\n\n\n     rstudent unadjusted p-value Bonferroni p\n8955 6.519574         8.2999e-11   2.3854e-07\n8956 6.519574         8.2999e-11   2.3854e-07\n8957 6.182195         7.2185e-10   2.0746e-06\n2560 4.914941         9.3800e-07   2.6958e-03\n1385 4.573975         4.9884e-06   1.4337e-02\n\n\n\n\n19.2.6 Observations with high leverage\nIdentifies observations with high leverage (i.e., high hat values)\nhat values are an index of leverage (observations that are far from the center of the regressor space and have greater influence on OLS regression coefficients)\n\n\nCode\nhist(hatvalues(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(hatvalues(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\ninfluenceIndexPlot(\n  multipleRegressionModel,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\nCode\ninfluencePlot(\n  multipleRegressionModel,\n  id = TRUE) # circle size is proportional to Cook's Distance\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\nCode\nleveragePlots(\n  multipleRegressionModel,\n  id = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n19.2.7 Observations with high influence (on OLS regression coefficients)\nhttps://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html (archived at https://perma.cc/T2TS-PZZ4)\n\n\nCode\nhead(influence.measures(multipleRegressionModel)$infmat)\n\n\n        dfb.1_     dfb.b_T1     dfb.b_DS       dffit     cov.r       cook.d\n1  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA\n2  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA\n3 -0.006546394  0.008357248 -0.006308529 -0.01004886 1.0024362 0.0000336708\n4  0.042862749 -0.025185789 -0.007782877  0.04286275 0.9998621 0.0006121902\n5  0.000000000  0.000000000  0.000000000          NA 1.0000000           NA\n6 -0.018327367  0.010769006  0.003327823 -0.01832737 1.0015775 0.0001119888\n           hat\n1 0.0000000000\n2 0.0000000000\n3 0.0014593072\n4 0.0009143185\n5 0.0000000000\n6 0.0009143185\n\n\n\n\n19.2.8 DFBETA\n\n\nCode\nhead(dfbeta(multipleRegressionModel))\n\n\n    (Intercept) bpi_antisocialT1Sum bpi_anxiousDepressedSum\n1  0.0000000000        0.0000000000            0.000000e+00\n2  0.0000000000        0.0000000000            0.000000e+00\n3 -0.0003917284        0.0001553400           -1.839704e-04\n4  0.0025639901       -0.0004679817           -2.268890e-04\n5  0.0000000000        0.0000000000            0.000000e+00\n6 -0.0010966309        0.0002001580            9.704151e-05\n\n\nCode\nhist(dfbeta(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\ndfbetasPlots(\n  multipleRegressionModel,\n  id = TRUE)\n\n\nError in `dfbetasPlots.lm()`:\n! argument 2 matches multiple formal arguments\n\n\n\n\n19.2.9 DFFITS\n\n\nCode\nhead(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])\n\n\n     8472      4468      1917      8955      8956      1986 \n0.2664075 0.2428065 0.2237760 0.1972271 0.1972271 0.1923034 \n\n\nCode\nhist(dffits(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(dffits(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(dffits(multipleRegressionModel)[order(dffits(multipleRegressionModel), decreasing = TRUE)])\n\n\n\n\n\n\n\n\n\n\n\n19.2.10 Cook’s Distance\nObservations that are both outlying (have a high residual from the regression line) and have high leverage (are far from the center of the regressor space) have high influence on the OLS regression coefficients. An observation will have less influence if it lies on the regression line (not an outlier, i.e., has a low residual) or if it has low leverage (i.e., has a value near the center of a predictor’s distribution).\n\n\nCode\nhead(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])\n\n\n      2765       1371       2767       8472       6170       6023 \n0.05488391 0.03624388 0.03624388 0.02358305 0.02323282 0.02315496 \n\n\nCode\nhist(cooks.distance(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(cooks.distance(multipleRegressionModel))\n\n\n\n\n\n\n\n\n\nCode\nplot(cooks.distance(multipleRegressionModel)[order(cooks.distance(multipleRegressionModel), decreasing = TRUE)])\n\n\n\n\n\n\n\n\n\n\n19.2.10.1 Refit model removing values with high cook’s distance\n\n\nCode\nmultipleRegressionModel_2 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371))\n\nmultipleRegressionModel_3 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371,2767))\n\nmultipleRegressionModel_4 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371,2767,8472))\n\nmultipleRegressionModel_5 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371,2767,8472,6170))\n\nmultipleRegressionModel_6 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371,2767,8472,6170,6023))\n\nmultipleRegressionModel_7 &lt;- lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = na.exclude,\n  subset = -c(2765,1371,2767,8472,6170,6023,2766))\n\n\n\n19.2.10.1.1 Examine how much regression coefficients change when excluding influential observations\n\n\nCode\ncompareCoefs(\n  multipleRegressionModel,\n  multipleRegressionModel_2,\n  multipleRegressionModel_3,\n  multipleRegressionModel_4,\n  multipleRegressionModel_5,\n  multipleRegressionModel_6,\n  multipleRegressionModel_7)\n\n\nCalls:\n1: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, na.action = na.exclude)\n2: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371), na.action =\n   na.exclude)\n3: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767), \n  na.action = na.exclude)\n4: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472),\n   na.action = na.exclude)\n5: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, \n  6170), na.action = na.exclude)\n6: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, \n  6170, 6023), na.action = na.exclude)\n7: lm(formula = bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + \n  bpi_anxiousDepressedSum, data = mydata, subset = -c(2765, 1371, 2767, 8472, \n  6170, 6023, 2766), na.action = na.exclude)\n\n                        Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7\n(Intercept)              1.1983  1.1704  1.1577  1.1676  1.1620  1.1537  1.1439\nSE                       0.0598  0.0597  0.0596  0.0596  0.0596  0.0595  0.0595\n                                                                               \nbpi_antisocialT1Sum      0.4655  0.4739  0.4779  0.4748  0.4744  0.4793  0.4836\nSE                       0.0186  0.0185  0.0185  0.0185  0.0185  0.0185  0.0185\n                                                                               \nbpi_anxiousDepressedSum  0.1608  0.1691  0.1726  0.1699  0.1770  0.1742  0.1743\nSE                       0.0292  0.0290  0.0290  0.0289  0.0290  0.0290  0.0289\n                                                                               \n\n\n\n\n\n19.2.10.2 Examine how much regression coefficients change when using least trimmed squares (LTS) that is resistant to outliers\n\n\nCode\ncoef(lm(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action=\"na.exclude\"))\n\n\n            (Intercept)     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n              1.1983004               0.4655286               0.1607541 \n\n\nCode\ncoef(ltsReg(\n  bpi_antisocialT2Sum ~ bpi_antisocialT1Sum + bpi_anxiousDepressedSum,\n  data = mydata,\n  na.action = \"na.exclude\"))\n\n\n              Intercept     bpi_antisocialT1Sum bpi_anxiousDepressedSum \n              0.7439567               0.5432644               0.1524929 \n\n\n\n\n\n19.2.11 Resources\nBook: An R Companion to Applied Regression\nhttps://people.duke.edu/~rnau/testing.htm (archived at https://perma.cc/9CXH-GBSN)\nhttps://www.statmethods.net/stats/rdiagnostics.html (archived at https://perma.cc/7JX8-K6BY)\nhttps://socserv.socsci.mcmaster.ca/jfox/Courses/Brazil-2009/index.html (archived at https://perma.cc/4A2P-9S49)\nhttps://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions (archived at https://perma.cc/Q4NS-X8TJ)",
    "crumbs": [
      "About",
      "Regression"
    ]
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R",
    "section": "",
    "text": "“R will always be arcane to those who do not make a serious effort to learn it. It is not meant to be intuitive and easy for casual users to just plunge into. It is far too complex and powerful for that. But the rewards are great for serious data analysts who put in the effort.”\n— Berton Gunter R-help August 2007 (archived at https://perma.cc/KY9N-2FTT)\n— Evelyn Hall and Simon ‘Yoda’ Blomberg, R-help April 2005 (archived at https://perma.cc/KY9N-2FTT)",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#base-r",
    "href": "R.html#base-r",
    "title": "R",
    "section": "1.1 Base R",
    "text": "1.1 Base R\nHere are a slew of resources for learning base R (in addition to the documents in the lab’s Primer Articles folder):\n\nR for Data Science book: https://r4ds.hadley.nz\n\nTutorials from the book are available via the r4ds.tutorials package: https://ppbds.github.io/r4ds.tutorials/\n\nUse this Intro to R: https://www.statmethods.net (archived at https://perma.cc/SPQ2-DJKM)\n\nI used this resource considerably when I was learning R\n\nCodeacademy has free online courses:\n\nBasics of using R: https://www.codecademy.com/learn/learn-r (archived at https://perma.cc/JHS2-EYUU)\n\nLearn how to become a better coder: https://www.r-bloggers.com/10-top-tips-for-becoming-a-better-coder/ (archived at https://perma.cc/H3W7-QDAM)\nHow to become fluent in R: https://www.sharpsightlabs.com/blog/are-you-fluent-r/ (archived at https://perma.cc/RWL5-GTWV)\nVideo training courses in R skills: https://www.pluralsight.com/search?q=R (archived at https://perma.cc/U4ZZ-UE4X)\nBrowse the Cookbook for R to find solutions to common tasks and problems: http://www.cookbook-r.com (archived at https://perma.cc/5ERJ-VRFR)\nBrowse the R Graph Gallery to find examples of various graphs: https://r-graph-gallery.com\nFree Coursera courses on R: https://pairach.com/2012/12/22/learn-to-use-r-for-free-with-coursera/ (archived at https://perma.cc/XJ5U-5E4W)\nMOOCs and courses to learn R: https://www.r-bloggers.com/moocs-and-courses-to-learn-r/ (archived at https://perma.cc/MQ25-E3ME)\nWatch these videos from Coursera: https://blog.revolutionanalytics.com/2012/12/coursera-videos.html (archived at https://perma.cc/6FU4-PAQW)\nRStudio Webinars: https://www.rstudio.com/resources/webinars/ (archived at https://perma.cc/6RNR-98JB)\nUCLA Stats Website: https://stats.idre.ucla.edu/r/ (archived at https://perma.cc/M3N6-96RY)\nTake this Introduction to R course: https://www.datacamp.com/courses/free-introduction-to-r (archived at https://perma.cc/6ZM9-37L9)\nTeaching R in a Kinder, Gentler, More Effective Manner: https://github.com/matloff/TidyverseSkeptic (archived at https://perma.cc/QU9F-FBS8)\nLearn R interactively with swirl: https://swirlstats.com (archived at https://perma.cc/TT9V-U663)\nUse the learnr package: https://blog.rstudio.com/2017/07/11/introducing-learnr/ (archived at https://perma.cc/XGJ8-EXSN)\nYou will sometimes find relevant articles on R-bloggers: https://www.r-bloggers.com (archived at https://perma.cc/EL3X-ZXBB)",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#statistics-using-r",
    "href": "R.html#statistics-using-r",
    "title": "R",
    "section": "1.2 Statistics (using R)",
    "text": "1.2 Statistics (using R)\nThe following are resources for learning statistics using R.\n\nR for Data Science: https://r4ds.hadley.nz\nUCLA Stats Website: https://stats.oarc.ucla.edu/other/dae/ (archived at https://perma.cc/FVG5-5X82)\nCodeacademy has free online courses:\n\nDescriptive analysis and basic hypothesis testing: https://www.codecademy.com/learn/learn-statistics-with-r\n\nFree textbook on Learning Statistics with R: https://learningstatisticswithr.com (archived at https://perma.cc/SY87-5GCX)\nAn excellent introductory textbook on Discovering Statistics using R: https://www.amazon.com/Discovering-Statistics-Using-Andy-Field/dp/1446200469 (archived at https://perma.cc/9LGU-7ZME)\nMy textbook that teaches R/statistics through the lens of fantasy football: https://isaactpetersen.github.io/Fantasy-Football-Analytics-Textbook",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#tidyverse",
    "href": "R.html#tidyverse",
    "title": "R",
    "section": "1.3 tidyverse",
    "text": "1.3 tidyverse\nThe following are resources for learning tidyverse, which is a collection of R packages for data management:\n\nhttps://www.tidyverse.org/learn/ (archived at https://perma.cc/5ZUM-XGES)\nhttps://www.linkedin.com/learning/learning-the-r-tidyverse/welcome?u=42459020 (archived at https://perma.cc/TD56-FX8R)",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#questions",
    "href": "R.html#questions",
    "title": "R",
    "section": "1.4 Getting Help/Questions",
    "text": "1.4 Getting Help/Questions\nIf you have R questions, you can ask them in a number of places:\n\nForums:\n\nPosit: https://forum.posit.co\nStackOverflow: https://stackoverflow.com/questions/tagged/r\nReddit: https://www.reddit.com/r/rstats/\n\nThe R mailing list: https://stat.ethz.ch/mailman/listinfo/r-help\n\nThe following article provides additional resources and good guidance: https://www.r-bloggers.com/where-to-get-help-with-your-r-question/ (archived at https://perma.cc/4RRE-KL33).\nWhen posting a question on forums or mailing lists, keep a few things in mind:\n\nRead the posting guidelines before posting!\nBe respectful of other people and their time. R is free software. People are offering their free time to help. They are under no obligation to help you. If you are disrespectful or act like they owe you anything, you will rub people the wrong way and will be less likely to get help.\nProvide a minimal, reproducible example. Providing a minimal, reproducible example can be crucial for getting a helpful response. By going to the trouble of creating a minimal, reproducible example and identifying the minimum conditions necessary to reproduce the issue, you will often figure out how to resolve it. Here are guidelines on providing a minimal, reproducible example: https://stackoverflow.com/help/minimal-reproducible-example (archived at https://perma.cc/6NUB-UTYF). Here are a good example and guidelines for providing a minimal, reproducible example in R: https://stackoverflow.com/a/5963610 (archived at https://perma.cc/PC9L-DQZG). Provide a reprex whenever possible: https://reprex.tidyverse.org.",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#create-rstudio-project",
    "href": "R.html#create-rstudio-project",
    "title": "R",
    "section": "7.1 Create Rstudio Project",
    "text": "7.1 Create Rstudio Project\nFor each data analysis project (i.e., each GitLab/GitHub repo), create an RStudio Project. This helps keep your project files organized.",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#use-r-notebooks-for-computational-notebooks",
    "href": "R.html#use-r-notebooks-for-computational-notebooks",
    "title": "R",
    "section": "7.2 Use R Notebooks for “Computational Notebooks”",
    "text": "7.2 Use R Notebooks for “Computational Notebooks”\nUsing R Notebooks for “Computational Notebooks” is helpful for reproducible code that can be shared with others. To create computational notebooks see the Markdown section on computational notebooks in the Data Analysis guides.",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#separate-sections-in-code",
    "href": "R.html#separate-sections-in-code",
    "title": "R",
    "section": "7.3 Separate sections in code",
    "text": "7.3 Separate sections in code\n\nIn R scripts, use sections.\n\nTo insert a section in RStudio, use CTRL-Shift-R or “Code” - “Insert Section”\n\nIn R Notebooks/Markdown, use Headers and code chunks.\n\nHeaders: 1, 2, or 3 pound signs\nCode Chunks: Ctrl+Alt+I; or click “Insert” button then “R”",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#naming-variables",
    "href": "R.html#naming-variables",
    "title": "R",
    "section": "7.4 Naming variables",
    "text": "7.4 Naming variables\n\nUse meaningful variable names; we want to know what a variable represents without having to consult an external codebook for every variable\nVariable names should include the prefix for the measure followed by an underscore\n\ne.g., cbcl_ for the Child Behavior Checklist variables\n\nUse lower camel case for variable naming\n\ne.g., prefix_thisIsTheVariableName\n\nDo not include spaces in variable names",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#comment-code-frequently-and-clearly",
    "href": "R.html#comment-code-frequently-and-clearly",
    "title": "R",
    "section": "7.5 Comment code frequently and clearly!",
    "text": "7.5 Comment code frequently and clearly!\nIt is important to comment code frequently and clearly. You want you (and others) to easily be able to understand your code if you come back to it several years later!",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#dontSaveWorkspace",
    "href": "R.html#dontSaveWorkspace",
    "title": "R",
    "section": "7.6 Don’t save your workspace image",
    "text": "7.6 Don’t save your workspace image\nFor reproducibility purposes, it is important not to save your workspace image (archived at https://perma.cc/9SCZ-L4DE). It is best practices to begin R each session with a clean workspace. If there is a .Rdata file in the same folder as the Rstudio Project, Rstudio will automatically load the objects into the workspace at the beginning of the session. This is problematic because those objects can interact/interfere with the code and can lead to problems with replicability for others who are running the code without those objects in the workspace. When you exit RStudio, RStudio asks if you want to “Save workspace image to [filepath]/.Rdata?” Make sure to select “Don’t Save”! However, do make sure to save your R scripts before exiting Rstudio.",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#troubleshooting",
    "href": "R.html#troubleshooting",
    "title": "R",
    "section": "13.1 Troubleshooting",
    "text": "13.1 Troubleshooting\n\n13.1.1 Pandoc error\nThis error may appear if you are attempting to render a markdown file\npandoc version 1.12.3 or higher is required and was not found.\nThe solution to this problem can be found at this link (archived at https://perma.cc/YX57-BPRS)",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#slacking-specific-users",
    "href": "R.html#slacking-specific-users",
    "title": "R",
    "section": "17.1 Slacking Specific Users",
    "text": "17.1 Slacking Specific Users\nIt is also possible to slack specific users with instructions found at this link (archived at https://perma.cc/59U5-V4GQ).",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#working-with-renv-for-package-management",
    "href": "R.html#working-with-renv-for-package-management",
    "title": "R",
    "section": "20.1 Working with renv for Package Management",
    "text": "20.1 Working with renv for Package Management\nrenv is used for reproducibility, by helping with package management (tracking package versions, etc.):\nhttps://rstudio.github.io/renv/articles/renv.html\nBefore updating a package locally, make sure that it is available in the Posit Package Manager (so it can be available to GitHub Actions):\nhttps://packagemanager.posit.co\n\n20.1.1 Updating the Package\n\nTo update the package, run the following in R:\n::: {.cell}\n\nCode\n# 1. Update packages in package environment\nrenv::upgrade()\nrenv::update()\nrenv::snapshot()\n\n# 2. Add/edit code\n\n# 3. Update documentation\nroxygen2::roxygenise()\n\n# 4. Update package version (or can do this manually)\nusethis::use_version()\n\n:::\nThen, build the package: Ctrl-Shift-B\nThen, check the package: Ctrl-Shift-E\nThen, run R CMD check\nThen, install the package:\n::: {.cell}\n\nCode\nrenv::install(\"C:/R/Packages/petersenlab\") #PC\nrenv::install(\"/Library/Frameworks/R.framework/Packages/petersenlab\") #Mac\n\n:::\n\n\n\n20.1.2 Installing Packages\nTo install new packages in the package environment, run the following in R:\n\n\nCode\nrenv::install(\"NAME_OF_PACKAGE\")\n\n\nor:\n\n\nCode\ninstall.packages(\"NAME_OF_PACKAGE\")",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#rCmdCheck",
    "href": "R.html#rCmdCheck",
    "title": "R",
    "section": "20.2 R CMD check",
    "text": "20.2 R CMD check\n\nBuild the source package (.tar.gz file)\n\nClick on the “Build” tab in the top-right pane of RStudio, and then click “Build Source Package”, or run the following code in the R console:\n\n::: {.cell}\n\nCode\ndevtools::build(pkg = \"D:/Documents/GitHub/petersenlab\")\n\n:::\nOpen terminal in RStudio\n\nAfter the package source (.tar.gz file) is built, open a terminal window directly in RStudio by clicking on the “Terminal” tab at the bottom of RStudio\n\nRun R CMD check --as-cran\n\nIn the terminal window, navigate to the directory where your package source (.tar.gz file) is located (commonly up one directory):\n\ncd ..\nThen, run R CMD check --as-cran followed by the name of your package tarball (making sure to update the package version in the filename). For example:\nR CMD check --as-cran petersenlab_1.0.0.tar.gz\n\n\n20.2.1 Troubleshooting\n\n20.2.1.1 If errors compiling the PDF manual\n\nIn the terminal:\ncd ./petersenlab\nR CMD Rd2pdf . --output=man/figures/manual.pdf --force --no-preview --no-clean\nThen, delete the created folder whose name begins with “.Rd2pdf…”\nIn the R Console, build the source package in R (or using the instructions described above):\n::: {.cell}\n\nCode\ndevtools::build(pkg = \"D:/Documents/GitHub/petersenlab\")\n\n:::\nIn the terminal (making sure to update the package version in the filename):\ncd ..\nR CMD check --as-cran petersenlab_1.0.0.tar.gz\n\n\n\n20.2.1.2 no visible binding for global variable; Undefined global functions or variables\nFor example:\nno visible binding for global variable\n    'moderatorVal_centered'\n  Undefined global functions or variables:\n    moderatorVal_centered predictorVal_centered\nSolution: set each variable to NULL in the package function before it is mentioned. For example:\npredictorVal_centered &lt;- moderatorVal_centered &lt;- NULL",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#r-cmd-check-via-github-actions",
    "href": "R.html#r-cmd-check-via-github-actions",
    "title": "R",
    "section": "20.3 R CMD check via GitHub Actions",
    "text": "20.3 R CMD check via GitHub Actions\n\n\nCode\nusethis::use_github_action(\"check-standard\")",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#useful-keyboard-shortcuts-for-package-authoring",
    "href": "R.html#useful-keyboard-shortcuts-for-package-authoring",
    "title": "R",
    "section": "20.4 Useful keyboard shortcuts for package authoring:",
    "text": "20.4 Useful keyboard shortcuts for package authoring:\nInstall Package: ‘Ctrl + Shift + B’\nCheck Package: ‘Ctrl + Shift + E’\nTest Package: ‘Ctrl + Shift + T’\n\n\nCode\nrenv::install(\"C:/R/Packages/petersenlab\")\nrenv::snapshot()\nrenv::install()",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#pkgdown",
    "href": "R.html#pkgdown",
    "title": "R",
    "section": "20.5 pkgdown",
    "text": "20.5 pkgdown\nRun once to configure your package to use pkgdown:\n\n\nCode\nusethis::use_pkgdown()\n\n\nThen use pkgdown to build your website:\n\n\nCode\npkgdown::build_site()",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#steps-to-add-functions",
    "href": "R.html#steps-to-add-functions",
    "title": "R",
    "section": "20.6 Steps to Add Functions",
    "text": "20.6 Steps to Add Functions\n\nAdd .R file with the function\nAdd the function to the _pkgdown.yml file\nUpdate version number\nrenv::upgrade()\nrenv::update()\nrenv::snapshot()\nroxygen2::roxygenise()\nInstall Package: ‘Ctrl + Shift + B’\nCheck Package: ‘Ctrl + Shift + E’\nR CMD check\nCommit and push changes\nUpdate release version in GitHub",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#add-sub-packages",
    "href": "R.html#add-sub-packages",
    "title": "R",
    "section": "20.7 Add sub-packages",
    "text": "20.7 Add sub-packages\n\n\nCode\ndevtools::build(pkg = \"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage1\")\ndevtools::build(pkg = \"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage2\")\n\ninstall.packages(\"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage1_0.1.0.tar.gz\", repos = NULL, source = TRUE)\ninstall.packages(\"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage2_0.1.0.tar.gz\", repos = NULL, source = TRUE)\n\nremotes::install_local(\"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage2_0.1.0.tar.gz\")\nremotes::install_local(\"D:/Documents/GitHub/petersenlab/inst/extdata/testpackage2_0.1.0.tar.gz\")",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#submit-package-to-cran",
    "href": "R.html#submit-package-to-cran",
    "title": "R",
    "section": "20.8 Submit Package to CRAN",
    "text": "20.8 Submit Package to CRAN\nhttps://cran.r-project.org/submit.html",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#resources",
    "href": "R.html#resources",
    "title": "R",
    "section": "20.9 Resources",
    "text": "20.9 Resources\nOfficial documentation for CRAN:\n\nhttps://cran.r-project.org/doc/manuals/R-exts.html\n\nUnofficial documentation:\n\nhttps://rpubs.com/YaRrr/rpackageintro\nhttps://r-pkgs.org\nhttps://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf\nhttps://portal.stats.ox.ac.uk/userdata/ruth/APTS2012/Rcourse10.pdf\nhttps://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf\nhttps://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/\n\n\n20.9.1 For Package Development Tasks\n\nhttps://usethis.r-lib.org/index.html\n\n\n\n20.9.2 For Documentation\n\nhttps://github.com/r-lib/roxygen2#roxygen2\nhttps://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/\n\n\n\n20.9.3 Creating Vignettes\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook/package-vignette.html\n\n\n\n20.9.4 For Licensing\n\nhttps://usethis.r-lib.org/reference/licenses.html",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#general-troubleshooting-tips",
    "href": "R.html#general-troubleshooting-tips",
    "title": "R",
    "section": "21.1 General Troubleshooting Tips",
    "text": "21.1 General Troubleshooting Tips\n\nRun code line-by-line to identify the source of the error\n\nIf you are using RStudio, a red line will often appear on the lefthand side of the screen to indicate the particular line(s) of code that result in an error\nIt may be useful to clear your working environment in R before trying to diagnose an error to be sure that you are not experiencing any interference from existing objects in the environment\nRunning code line-by-line is particularly useful because it allows you to inspect the data at each step of its processing\n\nData issues are commonly the culprit of code errors (see below)\n\n\nCheck for typos\n\nAre all brackets/parentheses/etc. closed?\nDoes the object/variable that you are referring to exist? (If the object does not exisit, you will likely get an error message indicating [object name] not found)\n\nHas it been defined somewhere prior in the code?\nDoes it depend on something else to be created?\nIs its name spelled correctly?\nHighlighting the object or variable in question and using CTRL-F to search for it is a good way to check these things\n\nDid you spell the function you want to use correctly?\nDo you have commas separating items within a list/function/etc.?\n\nDoes your working environment have what it needs?\n\nAre all necessary packages/libraries installed? Do any require an update?\nAre all expected objects (dataframes, lists, environment variables, etc) present? Are they correct?\n\nInvestigate the structure of your data\n\nErrors such as non-numeric argument to binary operator indicate that the function you are trying to use is expecting to recieve numeric data as an input but one or more of the inputs are not numeric\n\nThe function class(data$variable) (replace “data” and “variable” with whatever you are trying to invesitgate) is useful for determining how R is interpreting your data. Often, data that look like numbers can end up being stored as a character (text) variable. This can be due to import/export processes or due to an issue with data entry.\n\nFor example, when 00 is entered instead of 0 in a numeric field R is likely to interpret the variable as a character instead of a number when importing the data\n\n\nCheck to make sure that all expected rows/columns are present and that they look the way you expect them to\n\nIf your dataset has been created by joining/merging multiple dataframes, any duplicate columns not accounted for when joining may have a suffix appended to distinguish them (i.e., variable.x or variable.y)\nTo resolve this, either call the appended variable in subsequent manipulations (variable.x instead of variable) or deal with the duplicated columns before/during the joining process (e.g., include duplicate columns in the by argument of joining function or remove the redundant column from one of the datasets to be joined)\n\nIf you have been creating/computing variables in your dataset, ensure that they are being computed as expected\n\nCheck for NA (missing) or NaN (not a number) values. Such values may not be an issue or error in all cases, but if you are not expecting that as a result of your computations, this might suggest an issue with the code and/or data structure\nCheck whether the computed values are reasonable",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#a-single-error-preventing-a-wiki-site-from-rendering",
    "href": "R.html#a-single-error-preventing-a-wiki-site-from-rendering",
    "title": "R",
    "section": "21.2 A single error preventing a Wiki site from rendering",
    "text": "21.2 A single error preventing a Wiki site from rendering\nCode can be wrapped in try() statements so that if an error occurs, the script will simply “skip” to the next line of code. These try() statements are particularly useful when rendering a site; if they are not used, a single error will halt the entire process. try() statements allow the code to continue running in spite of a line or two erroring out.\nIf you have already incorporated try() statements and are still experiencing fatal errors in code, check to make sure that the smallest possible “unit” of code is wrapped in try() statements, rather than large sections of code. See here for more information.",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "R.html#warning-packagename-package-in-filepath-library-will-not-be-updated",
    "href": "R.html#warning-packagename-package-in-filepath-library-will-not-be-updated",
    "title": "R",
    "section": "21.3 Warning: PACKAGENAME package in FILEPATH library will not be updated",
    "text": "21.3 Warning: PACKAGENAME package in FILEPATH library will not be updated\nThis warning might indicate that it cannot update the package because it is part of the base R installation. To fix this:\n\nfind and delete the package that is part of the base R installation in the library directory of the R installation (e.g., C:\\R\\R-4.3.1\\library\\PACKAGENAME\\)\nmake sure the package is also not in any other package installation locations (e.g., C:\\R\\Packages\\PACKAGENAME\\); if it is, delete it from there as well\nthen, after deleting the package from these locations, restart R and run install.packages(\"PACKAGENAME\") to reinstall the package\nclose R\nif the package was part of the base R installation, move the installed package folder C:\\R\\Packages\\PACKAGENAME\\ to the library directory of the R installation (e.g., C:\\R\\R-4.3.1\\library\\PACKAGENAME\\)",
    "crumbs": [
      "About",
      "R"
    ]
  },
  {
    "objectID": "osf.html",
    "href": "osf.html",
    "title": "Open Science Framework",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n#remotes::install_github(\"paulhendricks/anonymizer\")\n\n\n\n\n\n\n\nCode\nlibrary(\"anonymizer\")\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\nCode\nset.seed(52242)\n\nsampleSize &lt;- 100\n\nID &lt;- 1:sampleSize\nX &lt;- rnorm(sampleSize)\nY &lt;- rnorm(sampleSize)\n\nmydata &lt;- data.frame(\n  ID = ID,\n  X = X,\n  Y = Y)",
    "crumbs": [
      "About",
      "Open Science Framework"
    ]
  },
  {
    "objectID": "osf.html#install-libraries",
    "href": "osf.html#install-libraries",
    "title": "Open Science Framework",
    "section": "",
    "text": "Code\n#install.packages(\"remotes\")\n#remotes::install_github(\"DevPsyLab/petersenlab\")\n#remotes::install_github(\"paulhendricks/anonymizer\")",
    "crumbs": [
      "About",
      "Open Science Framework"
    ]
  },
  {
    "objectID": "osf.html#load-libraries",
    "href": "osf.html#load-libraries",
    "title": "Open Science Framework",
    "section": "",
    "text": "Code\nlibrary(\"anonymizer\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "About",
      "Open Science Framework"
    ]
  },
  {
    "objectID": "osf.html#simulate-data",
    "href": "osf.html#simulate-data",
    "title": "Open Science Framework",
    "section": "",
    "text": "Code\nset.seed(52242)\n\nsampleSize &lt;- 100\n\nID &lt;- 1:sampleSize\nX &lt;- rnorm(sampleSize)\nY &lt;- rnorm(sampleSize)\n\nmydata &lt;- data.frame(\n  ID = ID,\n  X = X,\n  Y = Y)",
    "crumbs": [
      "About",
      "Open Science Framework"
    ]
  },
  {
    "objectID": "jamovi.html",
    "href": "jamovi.html",
    "title": "jamovi",
    "section": "",
    "text": "1 Overview of jamovi\njamovi is similar to SPSS in that it has a graphical user interface that allows point-and-click for analysis. Unlike SPSS, however, jamovi is free. jamovi can also provide the R code that used to perform the analysis.\n\n\n2 Best Practices\n\nCreate a syntax file for performing all commands, including importing data, computing variables, recoding variables, running analyses, etc. The benefit of using syntax files (and code-based files, more generally) is that they allow you to reproduce your findings again with the same data file. This is important so you do not have to remember all of the steps you followed to generate the analysis. This also allows you to quickly re-run the analysis if the data file is updated due to the collection of more data. To generate the syntax for a particular step, change to “syntax mode” to see the R syntax that is used to perform the analysis step. If you are unsure what jamovi command is needed, run the command via “point and click” in the interface; however, instead of clicking “OK”, click “Paste”.\n\nHave one syntax file for importing data, computing composites, recoding variables, and saving data (e.g., import.R).\nHave a separate syntax file for generating frequencies, descriptive statistics, and correlation matrices (e.g., descriptives.R).\nHave a separate syntax file for running analyses (e.g., analyses.R).\nComment your syntax frequently\n\nto comment use the following convention:\n\n#type comment\n\n\nSave your syntax frequently\n\n\n\n\n3 Install modules for jamovi\nTo install modules for jamovi, see here: https://dev.jamovi.org/tuts0101-getting-started.html (archived at https://perma.cc/E3NY-8MGH)\n\n\n4 Structural Equation Modeling in jamovi\nSEMLj module for jamovi:\n\nhttps://github.com/semlj/semlj\nhttps://semlj.github.io\n\n\n\n\n\n\n\nReuseCC BY 4.0",
    "crumbs": [
      "About",
      "jamovi"
    ]
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "",
    "text": "Analysis scripts for High Performance Computing (HPC) at the University of Iowa. Currently uses Argon.\n\n\nWorkflow: https://workflow.uiowa.edu/entry/new/3282/11927336\nCluster Systems Documentation: https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513411/Cluster+Systems+Documentation (archived at https://perma.cc/EKB8-ZKR7)\nArgon Cluster Documentation: https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513466/Argon+Cluster (archived at https://perma.cc/6HST-VV6Y)\n\n\n\nSignificant amounts of data storage are provided on Argon, but data are not backed up in any way unless special arrangements are made. It is the responsibility of the user to back up important information.\n\n\n\nUser agrees to refrain from storing Restricted data on HPC resources. Data is classified as Restricted when the unauthorized disclosure, alteration or destruction of that data could cause a significant level of risk to the University or its affiliates. Examples of Restricted data include data protected by state or federal privacy regulations and data pertaining to identified human subjects that has not been deidentified.\n\n\n\nAt present there are no fees for the use of the Argon cluster for low-priority usage. For large users, or those who want access to dedicated resources, the option of purchasing or renting supplemental system hardware may be available. If you are interested in dedicated hardware, contact research-computing@uiowa.edu.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#links-to-documentation",
    "href": "hpc.html#links-to-documentation",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "",
    "text": "Workflow: https://workflow.uiowa.edu/entry/new/3282/11927336\nCluster Systems Documentation: https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513411/Cluster+Systems+Documentation (archived at https://perma.cc/EKB8-ZKR7)\nArgon Cluster Documentation: https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513466/Argon+Cluster (archived at https://perma.cc/6HST-VV6Y)",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#data-storage",
    "href": "hpc.html#data-storage",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "",
    "text": "Significant amounts of data storage are provided on Argon, but data are not backed up in any way unless special arrangements are made. It is the responsibility of the user to back up important information.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#sensitive-data",
    "href": "hpc.html#sensitive-data",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "",
    "text": "User agrees to refrain from storing Restricted data on HPC resources. Data is classified as Restricted when the unauthorized disclosure, alteration or destruction of that data could cause a significant level of risk to the University or its affiliates. Examples of Restricted data include data protected by state or federal privacy regulations and data pertaining to identified human subjects that has not been deidentified.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#fees",
    "href": "hpc.html#fees",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "",
    "text": "At present there are no fees for the use of the Argon cluster for low-priority usage. For large users, or those who want access to dedicated resources, the option of purchasing or renting supplemental system hardware may be available. If you are interested in dedicated hardware, contact research-computing@uiowa.edu.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#windows",
    "href": "hpc.html#windows",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "3.1 Windows",
    "text": "3.1 Windows\n\nRight click on This PC and select Map Network Drive\nSelect Y and type \\\\data.hpc.uiowa.edu\\argon_home\\Documents",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#ssh",
    "href": "hpc.html#ssh",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "4.1 SSH",
    "text": "4.1 SSH\nargon.hpc.uiowa.edu",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#windows-1",
    "href": "hpc.html#windows-1",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "4.2 Windows",
    "text": "4.2 Windows\nWindows Explorer:\n\\\\data.hpc.uiowa.edu\\argon_home (username: itpetersen@uiowa.edu or hawkid@uiowa.edu)\nUsing SecureCRT for an SSH connection: 1. Download SecureCRT from UIowa Informational Technology Services: https://its.uiowa.edu/securecrt 2. In Hostname type argon.hpc.uiowa.edu and in username type HawkID 3. Click connect",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#mac",
    "href": "hpc.html#mac",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "4.3 Mac",
    "text": "4.3 Mac\nMac OS Terminal:\nssh itpetersen@argon.hpc.uiowa.edu (on campus)\nssh -p 40 itpetersen@argon.hpc.uiowa.edu (off campus without VPN)",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#initial-run",
    "href": "hpc.html#initial-run",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.1 Initial run",
    "text": "7.1 Initial run\n\n7.1.1 Installing Linux Packages to Install R Packages\nmodule load stack/2022.2\nmodule load r/4.2.2_gcc-9.5.0\nmodule load geos/3.9.1_gcc-9.5.0\nmodule load gdal/2.4.4_gcc-9.5.0\nmodule load proj/5.2.0_gcc-9.5.0\nmodule load gsl/2.7.1_gcc-9.5.0\nmodule load nlopt/2.7.0_gcc-9.5.0\nmodule load jags/4.3.0_gcc-9.5.0-dev\nmodule load zlib/1.2.13_gcc-9.5.0-dev\nmodule load cmake/3.25.0_gcc-9.5.0\nmodule load glpk/4.65_gcc-9.5.0-dev\nmodule load libxml2/2.10.3_gcc-9.5.0\nmodule load r-nloptr\nmodule load r-zlibbioc/1.44.0_gcc-9.5.0\nmodule load r-data-table/1.14.4_gcc-9.5.0\nmodule load r-stringi/1.7.8_gcc-9.5.0\nmodule load r-selectr/0.4-2_gcc-9.5.0\nmodule load r-generics/0.1.3_gcc-9.5.0\nmodule load r-fansi/1.0.3_gcc-9.5.0\nmodule load r-utf8/1.2.2_gcc-9.5.0\nmodule load r-pkgconfig/2.0.3_gcc-9.5.0\nmodule load r-gtable/0.3.1_gcc-9.5.0\nmodule load r-scales/1.2.1_gcc-9.5.0\nmodule load r-tzdb/0.3.0_gcc-9.5.0\nmodule load r-timechange/0.1.1_gcc-9.5.0\nmodule load r-dbi/1.1.3_gcc-9.5.0\n\n\n7.1.2 Load the Latest Stack\nmodule load stack\n\n\n7.1.3 Stack 2022.2\nmodule load stack/2022.2\nmodule load r/4.2.2_gcc-9.5.0\ncd path",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#load-software-stacks",
    "href": "hpc.html#load-software-stacks",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.2 Load Software Stacks",
    "text": "7.2 Load Software Stacks\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513440/Argon+Software+List (archived at https://perma.cc/WJ4Q-GDUS)\nmodule load stack/2022.2",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#job-script",
    "href": "hpc.html#job-script",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.3 Job Script",
    "text": "7.3 Job Script\n#!/bin/sh\n\n# Set working directory\ncd /Users/itpetersen/Documents/Projects/Bayesian_IRT/\n\n# Specify qsub options\n#$ -pe smp 4\n#$ -M isaac-t-petersen@uiowa.edu\n#$ -m eas\n#$ -l mf=512G\n#$ -l h_vmem=512G\n#$ -cwd\n#$ -q UI-HM\n#$ -e /Users/itpetersen/Documents/Projects/Bayesian_IRT/Output/\n#$ -o /Users/itpetersen/Documents/Projects/Bayesian_IRT/Output/\n\n# Load the environment modules\nmodule load stack/2022.2\nmodule load r/4.2.2_gcc-9.5.0\n\n# Run the R script\nRscript ./Analyses/factorScores.R\n\n7.3.1 qsub options\n-pe smp 4: specify a parellel environment and number of cores to be used (smp = shared memory parallel environment) - The OMP_NUM_THREADS variable is set to ‘1’ by default. If your code can take advantage of the threading then specify OMP_NUM_THREADS to be equal to the number of job cores per node requested.\n-M isaac-t-petersen@uiowa.edu: Set the email address to receive email about jobs. This must be your University of Iowa email address.\n-m eas: Specify when to send an email message (; ; ; ; )\n\nb = beginning of job\ne = end of job\na = when job is aborted\ns = when job is suspended\nn = no mail is sent\n\n-l mf=512G: request a particular quantity of memory you expect to use (to be available for your computation to start; the request is only applicable at scheduling time. It is not a limit.)\n-l h_vmem=512G: request a particular quantity of virtual memory you expect to use (to be available for your computation to start; the request is only applicable at scheduling time. It is not a limit.)\n-cwd: Determines whether the job will be executed from the current working directory. If not specified, the job will be run from your home directory.\n-q UI-HM: specify queue\n-e /Users/itpetersen/Documents/Projects/Bayesian_IRT/Output/: Name of a file or directory for standard error.\n-o /Users/itpetersen/Documents/Projects/Bayesian_IRT/Output/: Name of a file or directory for standard output.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#submit-job",
    "href": "hpc.html#submit-job",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.4 Submit Job",
    "text": "7.4 Submit Job\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513450/Basic+Job+Submission (archived at https://perma.cc/2SS2-LEJR)\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513452/Advanced+Job+Submission (archived at https://perma.cc/8H6G-2M2F)\ncd path/to/dataSet123\n[cd /Users/itpetersen/Documents/Projects/Test/Jobs]\nqsub myscript.job\nJob dependency (run Job B when Job A is finished):\nqsub -hold_jid JOB_ID test_B.job\n\n7.4.1 Bayesian IRT\ncd /Users/itpetersen/Documents/Projects/Bayesian_IRT/Jobs\nqsub bayesianIRT.job\nqsub -hold_jid JOB_ID factorScores.job\n\n\n7.4.2 Multiple Imputation\ncd /Users/itpetersen/Documents/Projects/Multiple_Imputation/Jobs\nqsub srs_selfRegulation.R",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#queues",
    "href": "hpc.html#queues",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.5 Queues",
    "text": "7.5 Queues\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513468/Queues+and+Policies (archived at https://perma.cc/UUR7-XLBZ)\nUI\nUI-HM\nUI-GPU-HM\nUI-DEVELOP\nall.q",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#queue-status",
    "href": "hpc.html#queue-status",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.6 Queue Status",
    "text": "7.6 Queue Status\nqstat -g c -q QUEUE_NAME\nqstat -g c -q UI",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#job-status",
    "href": "hpc.html#job-status",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.7 Job Status",
    "text": "7.7 Job Status\nqstat -u itpetersen\nqstat -j JOB_ID",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#job-time-and-memory",
    "href": "hpc.html#job-time-and-memory",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.8 Job Time and Memory",
    "text": "7.8 Job Time and Memory\n\nTerminated jobs: qacct -j JOB_ID\nJobs in progress: qstat -j JOB_ID | grep usage",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#cancel-job",
    "href": "hpc.html#cancel-job",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.9 Cancel Job",
    "text": "7.9 Cancel Job\nqdel JOB_ID",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#run-r-script",
    "href": "hpc.html#run-r-script",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "7.10 Run R Script",
    "text": "7.10 Run R Script\nSee here",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#find-which-versions-of-r-are-installed",
    "href": "hpc.html#find-which-versions-of-r-are-installed",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.1 Find which versions of R are installed",
    "text": "8.1 Find which versions of R are installed\nmodule spider R\nBut, there may be more recent version of R installed in the “Additional Software Stacks” (https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513440/Argon+Software+List; archived at https://perma.cc/WJ4Q-GDUS)",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#compile-r",
    "href": "hpc.html#compile-r",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.2 Compile R",
    "text": "8.2 Compile R\nIf you want to compile a more recent version of R than is available in the software stacks, see here (archived at https://perma.cc/C6EX-EZL4).",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#install-r-packages",
    "href": "hpc.html#install-r-packages",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.3 Install R packages",
    "text": "8.3 Install R packages\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76514803/Adding+R+programs+to+a+personal+library (archived at https://perma.cc/3SRR-2JE7)\n\nfirst load the R environment module\n\nmodule load r/4.2.2_gcc-9.5.0\n\nlaunch R\n\nR\n\nthen install the package, (the code below uses a generic package_name):\n\ninstall.packages(\"package_name\", repos = \"http://cran.r-project.org\", dependencies = TRUE, type = \"source\", Ncpus = 40)\nIf using packrat, it is preferable to install packages by source, if possible, but you can remove type = source if you want to install binaries\nAssuming that you do not have a personal library directory you will see\n\nWarning in install.packages(\"package_name\", repos = \"http://cran.r-project.org\") :\nlib = \"/opt/R/3.0.2/lib64/R/library\"' is not writable\n\nWould you like to use a personal library instead?  (y/n)\nSelect y\nSelect y again when prompted to create the directory; your package should download and install into your personal library directory\n\nExample:\n\n\nCode\ninstall.packages(c(\n    \"renv\",\"psych\",\"tidyverse\",\"data.table\",\"nlme\",\"lme4\",\"mirt\",\"TeachingDemos\",\"Amelia\",\"mice\",\"miceadds\",\"abind\",\"future\",\"lavaan\",\"blavaan\",\"Rcpp\",\"igraph\",\"shinystan\",\"StanHeaders\",\"brms\",\"rstan\",\"rjags\"),\n    repos = \"http://cran.r-project.org\",\n    dependencies = TRUE,\n    Ncpus = 40)\n\n\n\n8.3.1 Install R package locally from source\ninstall.packages(path_to_file, repos = NULL, type = \"source\", Ncpus = 40)\nExample:\n\n\nCode\ninstall.packages(c(\n    \"renv\",\"psych\",\"tidyverse\",\"data.table\",\"nlme\",\"lme4\",\"mirt\",\"TeachingDemos\",\"Amelia\",\"mice\",\"miceadds\",\"abind\",\"future\",\"lavaan\",\"blavaan\",\"Rcpp\",\"igraph\",\"shinystan\",\"StanHeaders\",\"brms\",\"rstan\",\"rjags\"),\n    repos = \"http://cran.r-project.org\",\n    type = \"source\",\n    dependencies = TRUE,\n    Ncpus = 40)\n\n\n\n\n8.3.2 Managing packages for a project using renv\n\nCreate a directory where you want you want to store the project.\ncd to the above directory\nStart R\nLoad the renv package: library(\"renv\")\nAt the R prompt, initialize the renv project on the local repository of R packages with:\n\nrenv::init(project = \"/Users/itpetersen/Documents/Projects/SelfRegulation_IRT/\")\nor renv::init() if you are in the intended working directory\n\nYou must restart your R session in the given project directory after running init in order for the changes to take effect!\nFrom this point on you are working in a renv project. Installed packages will go into a library within this project. After initializing the renv project on the local repository of R packages, packages from the local repository can be installed with renv::install():\n\nrenv::install(\"package_name\")\n\nTo restart a renv project, simply start R from the directory created in step (1). The project will initialize automatically.\n\nTo update version of renv: renv::upgrade()\nTo install packages: renv::install(\"package_name\")\nTo update packages: renv::update()\nTo save the current state of your library: renv::snapshot()\nTo restore the state of your library from the lock file: renv::restore()\nTo disable renv on a project: renv::deactivate()\n\n8.3.2.1 To control which packages to install with renv using a DESCRIPTION file\nIf you want to control which packages are installed in a renv project, you can use a DESCRIPTION file to specify the packages that should be installed:\n\nCreate a DESCRIPTION file in the project directory with the following format:\nType: project\nDescription: My project.\nDepends:\n    packageName1,\n    packageName2,\n    packageName3\nRun renv::settings$snapshot.type(\"explicit\") to suppress dependency discover and to enable “explicit” mode: https://rstudio.github.io/renv/reference/dependencies.html#explicit-dependencies\nRun renv::init(bare=TRUE) to initialize the project without attempting to discover and install R package dependencies\n\n\n\n\n8.3.3 To install a new package for a project after renv has been initialized\n\nFirst, load the R environment module (see Install R packages section above)\ncd to the directory of the R project\nlaunch R\n\nR\n\nthen install the package, (the code below uses a generic package_name):\n\ninstall.packages(\"package_name\", repos = \"http://cran.r-project.org\", type = \"source\", dependencies = TRUE, Ncpus = 40)\ninstall.packages(c(\"renv\",\"psych\",\"tidyverse\",\"data.table\",\"nlme\",\"lme4\",\"mirt\",\"TeachingDemos\",\"Amelia\",\"mice\",\"miceadds\",\"abind\",\"future\",\"lavaan\",\"blavaan\",\"Rcpp\",\"igraph\",\"shinystan\",\"StanHeaders\",\"brms\",\"rstan\",\"rjags\",\"renv\"), repos = \"http://cran.r-project.org\", type = \"source\", dependencies = TRUE, Ncpus = 40)\n\nrenv::snapshot()",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#update-packages",
    "href": "hpc.html#update-packages",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.4 Update Packages",
    "text": "8.4 Update Packages\n\n\nCode\nupdate.packages(ask = FALSE)",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#runRscript",
    "href": "hpc.html#runRscript",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.5 Run R Script",
    "text": "8.5 Run R Script\nmodule load r/4.0.5_gcc-9.3.0\ncd path/to/dataSet123\n[cd /Users/itpetersen/Documents/Projects/Test/Analyses]\nRscript path/to/program.R\n[Rscript test.R]",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#quit-r",
    "href": "hpc.html#quit-r",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "8.6 Quit R",
    "text": "8.6 Quit R\nq()",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#having-trouble-installing-packages",
    "href": "hpc.html#having-trouble-installing-packages",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "10.1 Having trouble installing packages?",
    "text": "10.1 Having trouble installing packages?\nSometimes Argon will fail to install packages in an R workspace. We advise the following steps:\n\nSometimes R provides an error stating something like “fatal error: modulename: No such file or directory”. In this instance, you may wish to exist R with q() and load all the modules that begin with the listed module name, as listed above in “Installing Linux Packages to Install R Packages”. Then reopen R and try to install the package again.\nYou can also try installing the package from binary using the RStudio Package Manager (RSPM):\nremotes::install_github(\"cran4linux/rspm\")\nrspm::enable()\ninstall.packages(\"PACKAGE_NAME\")\nIf that does not work, you can try downloading the .tar file directly from the CRAN repository. Then copy the file into your Argon folder and type ‘install.packages(“/Users/path/to/directory/package_name”, repos = NULL, type = “source”)’. Note that you may need to download an older version of the package (e.g., from the CRAN Archive), such that it is compatible with the version of R you are running on Argon (which it typically not the most recent R version):\ninstall.packages(\"http://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.0.1.tar.gz\", repos = NULL, type = \"source\")\nCheck if you are installing the necessary modules within your job script:\nmodule load stack/2022.2\nmodule load r/4.2.2_gcc-9.5.0\nAdd a line to your R script to make sure it is accessing the same repository where you installed the packages manually:\n.libPaths(c(\"/old_Users/HAWKID/Rlibs\", \n        \"/cvmfs/argon.hpc.uiowa.edu/2022.2/apps/linux-centos7-broadwell/gcc-9.5.0/r-4.2.2-lv7yirk/rlib/R/library\"))\nIf the prior steps fail, email research-computing@uiowa.edu with the error, asking for help to figure out how to install the packages.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#error-when-installing-packages-from-source",
    "href": "hpc.html#error-when-installing-packages-from-source",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "10.2 Error When Installing Packages from Source",
    "text": "10.2 Error When Installing Packages from Source\nYou may have to set environment variables in each module file to help the compiler find headers and libraries. Note that if you run into any C++ code, you will need to set the CPLUS_INCLUDE_PATH variable.\n\n10.2.1 nloptr\n\nRun the following commands\n\nmodule load stack/2022.1\nmodule load r/4.1.3_gcc-9.4.0\nmodule load nlopt\n\nSet LIBRARY_PATH so linker can find library while launching R session (single line below):\n\nLIBRARY_PATH=$ROOT_NLOPT/lib64:$LIBRARY_PATH R\n\nIn R console, install nloptr (two lines below)\n\n\n\nCode\ninstall.packages(verbose = 1, \"nloptr\")\n\n\n\n\n10.2.2 tkrplot\n\nRun the following commands\n\nmodule load stack/2022.1\nmodule load r/4.1.3_gcc-9.4.0\n\nSet LIBRARY_PATH so linker can find library while launching R session (single line below):\n\nC_INCLUDE_PATH=$ROOT_XPROTO/include LIBRARY_PATH=$ROOT_LIBXEXT/lib:$ROOT_LIBXSCRNSAVER/lib R\n\nIn R console, install tkrplot (two lines below)\n\n\n\nCode\ninstall.packages(verbose = 1, \"tkrplot\")",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#error-reading-from-connection",
    "href": "hpc.html#error-reading-from-connection",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "10.3 Error Reading from Connection",
    "text": "10.3 Error Reading from Connection\nError in unserialize(node$con) : error reading from connection\nCalls: parlmice ... FUN -&gt; recvData -&gt; recvData.SOCK0node -&gt; unserialize\nThere likely wasn’t sufficient memory for a given core. Try increasing the max memory available and decreasing the number of cores and/or slots, so there is more memory available per core:\nhttps://stackoverflow.com/questions/46186375/r-parallel-error-in-unserializenodecon-in-hpc (archived at https://perma.cc/MF6V-NAVS)\nhttps://stackoverflow.com/questions/17015598/error-calling-serialize-r-function (archived at https://perma.cc/3Q75-DA2D)\nhttps://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#Memory_load (archived at https://perma.cc/2JRF-8Y5F)",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#old-stacks",
    "href": "hpc.html#old-stacks",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "11.1 Old Stacks",
    "text": "11.1 Old Stacks\n\n11.1.1 Stack 2022.1\nmodule load stack/2022.1\nmodule load r/4.1.3_gcc-9.4.0\ncd path\n\n\n11.1.2 Stack 2021.1\nmodule load stack/2021.1\nmodule load r/4.0.5_gcc-9.3.0\ncd path\n\n\n11.1.3 Stack 2020.2\nmodule load stack/2020.2\nmodule load r/4.0.2_gcc-8.4.0\ncd path\n\n\n11.1.4 Stack 2020.1\nmodule load stack/2020.1\nmodule load r/3.6.2_gcc-9.2.0\ncd path",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  },
  {
    "objectID": "hpc.html#managing-packages-for-a-project-using-packrat",
    "href": "hpc.html#managing-packages-for-a-project-using-packrat",
    "title": "High-Performance Computing (Argon Supercomputer)",
    "section": "11.2 Managing packages for a project using packrat",
    "text": "11.2 Managing packages for a project using packrat\nPlease note: we now use renv rather than packrat for package management\nhttps://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76514803/Adding+R+programs+to+a+personal+library (archived at https://perma.cc/3SRR-2JE7)\n\nCreate a directory where you want you want to store the project.\ncd to the above directory\nStart R\nLoad the packrat package: library(\"packrat\")\nAt the R prompt, initialize the packrat project on the local repository of R packages with: packrat::init(project = \"/Users/itpetersen/Documents/Projects/INSERT_PROJECT_NAME/\", options = list(local.repos = \"/Users/itpetersen/R/x86_64-pc-linux-gnu-library/4.0\"))\nYou must restart your R session in the given project directory after running init in order for the changes to take effect!\nFrom this point on you are working in a packrat project. Installed packages will go into a library within this project. After initializing the packrat project on the local repository of R packages, packages from the local repository can be installed with packrat::install_local():\n\npackrat::install_local(\"package_name\")\n\nTo restart a packrat project simply start R from the directory created in step (1). The project will initialize automatically.\n\nTo save the current state of your library: packrat::snapshot(); if that command fails due to an error when fetching sources, try packrat::snapshot(snapshot.sources = FALSE)\nTo disable packrat on a project: disable(restart = FALSE)\n\n11.2.1 To install a new package for a project after packrat has been initialized\n\nFirst, load the R environment module (see Install R packages section above)\ncd to the directory of the R project\nlaunch R\n\nR\n\nthen install the package, (the code below uses a generic package_name):\n\ninstall.packages(\"package_name\", repos = \"http://cran.r-project.org\", type = \"source\", dependencies = TRUE, Ncpus = 40)\ninstall.packages(c(\"packrat\",\"psych\",\"tidyverse\",\"data.table\",\"nlme\",\"lme4\",\"mirt\",\"TeachingDemos\",\"Amelia\",\"mice\",\"miceadds\",\"abind\",\"future\",\"lavaan\",\"blavaan\",\"Rcpp\",\"igraph\",\"shinystan\",\"StanHeaders\",\"brms\",\"rstan\",\"rjags\",\"renv\"), repos = \"http://cran.r-project.org\", type = \"source\", dependencies = TRUE, Ncpus = 40)\n\npackrat::snapshot(); if that command fails due to an error when fetching sources, try packrat::snapshot(snapshot.sources = FALSE)\n\n\n\n11.2.2 Build packrat environment on a compute node (only if necessary if package load fails due to issues building packages)\n\nqlogin – for more info, see here: https://uiowa.atlassian.net/wiki/spaces/hpcdocs/pages/76513454/Qlogin+for+Interactive+Sessions (archived at https://perma.cc/Y3A9-WQ3W)\nmodule load stack/2020.2-base_arch – this will ensure that the modules point to the lowest common multiarchitecture on Argon and will run on all nodes\nmodule load r\ncd to the directory of the R project\nR\nCreate packrat project (see above)\nInstall packages\nTake a package snapshot using packrat\nexit – to exit the qlogin session\n\nNote: Your packrat environment will then be linked to the proper glpk library and will run on any Argon node. You do not need to use the 2020.2-base_arch module at run time, only build time.",
    "crumbs": [
      "About",
      "High-Performance Computing"
    ]
  }
]